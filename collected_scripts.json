{
  "system_overview": "このシステムは...（必要に応じて書き換え）",
  "settings": {
    "data_directory": {
      "value": "data",
      "description": "データディレクトリのパス。"
    },
    "log_level": {
      "value": "INFO",
      "description": "ログレベルの設定（例: INFO、DEBUG、ERROR）。"
    },
    "output_directory": {
      "value": "outputs",
      "description": "出力ディレクトリのパス。"
    },
    "database_path": {
      "value": "data/data.db",
      "description": "データベースのパス。"
    },
    "problem_templates_directory": {
      "value": "templates",
      "description": "問題テンプレートディレクトリのパス。"
    },
    "output_tex_file": {
      "value": "practice_problems.tex",
      "description": "生成されるLaTeXファイルの名前。"
    },
    "latex_path": {
      "value": "C:/Users/KEN/Desktop/i_dont_undstand_jpn/texlive/2024/bin/windows/xelatex.exe",
      "description": "LaTeXコンパイラのパス。ニホンゴダメ絶対"
    },
    "cjk_main_font": {
      "value": "Yu Gothic",
      "description": "使用するCJKフォント。"
    },
    "problem_types": {
      "value": {
        "probability_definition": 0.05,
        "conditional_probability": 0.05,
        "distribution_functions": 0.05,
        "joint_distribution": 0.05,
        "statistical_inference": 0.1,
        "regression_analysis": 0.1,
        "linear_combination": 0.1,
        "distribution_properties": 0.1,
        "high_moment": 0.1,
        "variance_analysis": 0.1,
        "nonparametric_test": 0.1,
        "beta_binomial_conjugate": 0.05,
        "gamma_poisson_conjugate": 0.05,
        "dirichlet_multinomial_conjugate": 0.05,
        "binomial_poisson_approx": 0.05,
        "poisson_normal_approx": 0.05
      },
      "description": "問題タイプとその割合。"
    },
    "pdf_generation": {
      "value": {
        "problem_count": 9
      },
      "description": "PDF生成の設定。"
    },
    "gui_settings": {
      "value": {
        "window_title": "統計検定1級 インタラクティブ問題解答システム",
        "font_size": 14
      },
      "description": "GUIの設定。"
    },
    "enable_visualization": {
      "value": true,
      "description": "可視化機能の有効化。"
    }
  },
  "scripts": [
    {
      "path": ".vscode\\launch.json",
      "overview": "JSON解析エラー: Expecting property name enclosed in double quotes: line 12 column 6 (char 226)\n",
      "content": "{\n \"version\": \"0.2.0\",\n \"configurations\": [\n\n   {\n     \"name\": \"Python: 現在のファイル\",\n     \"type\": \"python\",\n     \"request\": \"launch\",\n     \"program\": \"${file}\",\n     \"console\": \"integratedTerminal\",\n     \"justMyCode\": true,\n     //\"args\": [\"--generate-pdf\"]\n   }\n ]\n}"
    },
    {
      "path": "your_project\\config.json",
      "overview": "JSONファイル (辞書)。キー: settings\n",
      "content": "{\n  \"settings\": {\n    \"data_directory\": {\n      \"value\": \"data\",\n      \"description\": \"データディレクトリのパス。\"\n    },\n    \"log_level\": {\n      \"value\": \"INFO\",\n      \"description\": \"ログレベルの設定（例: INFO、DEBUG、ERROR）。\"\n    },\n    \"output_directory\": {\n      \"value\": \"outputs\",\n      \"description\": \"出力ディレクトリのパス。\"\n    },\n    \"database_path\": {\n      \"value\": \"data/data.db\",\n      \"description\": \"データベースのパス。\"\n    },\n    \"problem_templates_directory\": {\n      \"value\": \"templates\",\n      \"description\": \"問題テンプレートディレクトリのパス。\"\n    },\n    \"output_tex_file\": {\n      \"value\": \"practice_problems.tex\",\n      \"description\": \"生成されるLaTeXファイルの名前。\"\n    },\n    \"latex_path\": {\n      \"value\": \"C:/Users/KEN/Desktop/i_dont_undstand_jpn/texlive/2024/bin/windows/xelatex.exe\",\n      \"description\": \"LaTeXコンパイラのパス。ニホンゴダメ絶対\"\n    },\n    \"cjk_main_font\": {\n      \"value\": \"Yu Gothic\",\n      \"description\": \"使用するCJKフォント。\"\n    },\n    \"problem_types\": {\n      \"value\": {\n        \"probability_definition\": 0.05,\n        \"conditional_probability\": 0.05,\n        \"distribution_functions\": 0.05,\n        \"joint_distribution\": 0.05,\n        \"statistical_inference\": 0.1,\n        \"regression_analysis\": 0.1,\n        \"linear_combination\": 0.1,\n        \"distribution_properties\": 0.1,\n        \"high_moment\": 0.1,\n        \"variance_analysis\": 0.1,\n        \"nonparametric_test\": 0.1,\n        \"beta_binomial_conjugate\": 0.05,\n        \"gamma_poisson_conjugate\": 0.05,\n        \"dirichlet_multinomial_conjugate\": 0.05,\n        \"binomial_poisson_approx\": 0.05,\n        \"poisson_normal_approx\": 0.05\n      },\n      \"description\": \"問題タイプとその割合。\"\n    },\n    \"pdf_generation\": {\n      \"value\": {\n        \"problem_count\": 9\n      },\n      \"description\": \"PDF生成の設定。\"\n    },\n    \"gui_settings\": {\n      \"value\": {\n        \"window_title\": \"統計検定1級 インタラクティブ問題解答システム\",\n        \"font_size\": 14\n      },\n      \"description\": \"GUIの設定。\"\n    },\n    \"enable_visualization\": {\n      \"value\": true,\n      \"description\": \"可視化機能の有効化。\"\n    }\n  }\n}"
    },
    {
      "path": "your_project\\config.py",
      "overview": "Pythonコード。\nクラス: Config。\n関数: __init__, load_config, get。\n",
      "content": "import json\nimport os\nimport logging\nfrom typing import Any\n\nclass Config:\n    def __init__(self, config_file='config.json'):\n        script_dir = os.path.dirname(os.path.abspath(__file__))\n        self.config_file = os.path.join(script_dir, config_file)\n        self.settings = self.load_config()\n\n    def load_config(self):\n        if not os.path.exists(self.config_file):\n            logging.error(f\"Config file not found: {self.config_file}\")\n            return {}\n        try:\n            with open(self.config_file, 'r', encoding='utf-8') as f:\n                content = f.read().strip()\n                if not content:\n                    logging.error(\"Config file is empty.\")\n                    return {}\n                return json.loads(content)\n        except Exception as e:\n            logging.error(f\"Error loading config: {e}\")\n            return {}\n\n    def get(self, *keys: Any, default: Any = None) -> Any:\n        # keys から文字列のみを取り出す\n        real_keys = [k for k in keys if isinstance(k, str)]\n        data = self.settings.get(\"settings\", {})\n        for k in real_keys:\n            if isinstance(data, dict) and k in data:\n                data = data[k]\n                if isinstance(data, dict) and \"value\" in data:\n                    data = data[\"value\"]\n            else:\n                return default\n        return data\n\nconfig = Config()\n"
    },
    {
      "path": "your_project\\database.py",
      "overview": "Pythonコード。\nクラス: DatabaseManager。\n関数: __init__, setup_database, save_problem。\n",
      "content": "import sqlite3\nimport os\nfrom config import config\n\nclass DatabaseManager:\n\n    def __init__(self):\n        script_dir = os.path.dirname(os.path.abspath(__file__))\n        db_path = config.get('database_path', default='data/data.db')\n        if db_path is None:\n            db_path = 'data/data.db'\n        self.db_name = os.path.join(script_dir, db_path)\n        os.makedirs(os.path.dirname(self.db_name), exist_ok=True)\n\n    def setup_database(self):\n        conn = sqlite3.connect(self.db_name)\n        c = conn.cursor()\n        c.execute('CREATE TABLE IF NOT EXISTS problems (\\n                        id TEXT PRIMARY KEY,\\n                        date_created TEXT,\\n                        problem_text TEXT,\\n                        solution_text TEXT,\\n                        problem_type TEXT)')\n        conn.commit()\n        conn.close()\n\n    def save_problem(self, problem_id, date_created, problem_text, solution_text, problem_type):\n        conn = sqlite3.connect(self.db_name)\n        c = conn.cursor()\n        c.execute('INSERT INTO problems VALUES (?, ?, ?, ?, ?)', (problem_id, date_created, problem_text, solution_text, problem_type))\n        conn.commit()\n        conn.close()"
    },
    {
      "path": "your_project\\graph.py",
      "overview": "Pythonコード。\nクラス: ProbabilityDistributionVisualizer, ProbabilityDistributionVisualizer。\n関数: __init__, _safe_savefig, plot_probability, __init__, plot_probability, plot_t_test, plot_regression, plot_time_series, plot_econometrics, plot_multivariate_normal, plot_distribution_properties, plot_high_moment, plot_variance_analysis, plot_nonparametric_test, plot_linear_combination, confidence_ellipse, mvn_pdf, ecdf。\n",
      "content": "import matplotlib\nimport matplotlib.pyplot as plt\nimport math\nimport numpy as np\nimport logging\nimport os\nfrom math import factorial, exp\nfrom scipy.stats import norm, t as t_dist, f as f_dist, poisson, expon\nfrom statsmodels.graphics.tsaplots import plot_acf\nimport traceback\n\nclass ProbabilityDistributionVisualizer:\n\n    def __init__(self):\n        pass\n\n    def _safe_savefig(self, output_path):\n        \"\"\"\n        画像を確実にPNGで出力し、ファイルサイズなどをログするヘルパー関数。\n        \"\"\"\n        try:\n            (_, ext) = os.path.splitext(output_path)\n            plt.savefig(output_path, format='png')\n            plt.close()\n            if os.path.exists(output_path):\n                fsize = os.path.getsize(output_path)\n                logging.info(f'Saved figure: {output_path} (size: {fsize} bytes)')\n            else:\n                logging.warning(f'File not found after saving: {output_path}')\n        except Exception as e:\n            logging.error(f'Failed to save figure to {output_path}, error={e}')\n            plt.close()\n\n    def plot_probability(self, params: dict, output_path: str):\n        ptype = params.get('problem_type')\n        try:\n            if ptype == 'binomial':\n                n = params.get('n', 10)\n                p = params.get('p', 0.5)\n                k = range(n + 1)\n                pmf = [math.comb(n, i) * p ** i * (1 - p) ** (n - i) for i in k]\n                plt.figure(figsize=(6, 4))\n                plt.bar(k, pmf)\n                plt.xlabel('k')\n                plt.ylabel('P(X=k)')\n                plt.title(f'Binomial Distribution (n={n}, p={p})')\n                plt.tight_layout()\n                self._safe_savefig(output_path)\n                return output_path\n        except Exception as e:\n            logging.error(f'分布プロット中にエラー: {e}')\n            logging.error(traceback.format_exc())\n            return None\n\nclass ProbabilityDistributionVisualizer:\n\n    def __init__(self):\n        pass\n\n    def plot_probability(self, params, output_path):\n        ptype = params.get('problem_type')\n        try:\n            if ptype == 'binomial':\n                p = params['p']\n                n = params['n']\n                k = params['k']\n                x = range(n + 1)\n                from math import comb\n                pmf = [comb(n, i) * p ** i * (1 - p) ** (n - i) for i in x]\n                cdf = np.cumsum(pmf)\n                plt.figure(figsize=(10, 5))\n                plt.subplot(1, 2, 1)\n                plt.bar(x, pmf, color='skyblue')\n                plt.title(f'Binomial PMF n={n},p={p}')\n                plt.xlabel('k')\n                plt.ylabel('P(X=k)')\n                plt.axvline(k, color='red', linestyle='--', label='Target k')\n                plt.legend()\n                plt.subplot(1, 2, 2)\n                plt.step(x, cdf, where='post', color='green')\n                plt.title('CDF')\n                plt.xlabel('k')\n                plt.ylabel('P(X<=k)')\n                plt.axvline(k, color='red', linestyle='--')\n                plt.tight_layout()\n                plt.savefig(output_path)\n                plt.close()\n                return True\n            elif ptype == 'poisson':\n                lam = params['lambda']\n                k = params['k']\n                x = range(k + 10 + 1)\n                pmf = [lam ** i * exp(-lam) / factorial(i) for i in x]\n                cdf = np.cumsum(pmf)\n                plt.figure(figsize=(10, 5))\n                plt.subplot(1, 2, 1)\n                plt.bar(x, pmf, color='orange')\n                plt.title(f'Poisson(lambda={lam}) PMF')\n                plt.xlabel('k')\n                plt.ylabel('P(X=k)')\n                plt.axvline(k, color='red', linestyle='--', label='Target k')\n                plt.legend()\n                plt.subplot(1, 2, 2)\n                plt.step(x, cdf, where='post', color='green')\n                plt.title('CDF')\n                plt.xlabel('k')\n                plt.ylabel('P(X<=k)')\n                plt.axvline(k, color='red', linestyle='--')\n                plt.tight_layout()\n                plt.savefig(output_path)\n                plt.close()\n                return True\n            elif ptype == 'conditional_probability':\n                P_A = params['P_A']\n                P_BA = params['P_B_given_A']\n                P_AB = params['P_A_and_B']\n                plt.figure()\n                vals = [P_A, P_BA, P_AB]\n                labels = ['P(A)', 'P(B|A)', 'P(A∩B)']\n                plt.bar(labels, vals, color=['blue', 'green', 'red'])\n                plt.title('Conditional Probability Visualization')\n                plt.ylabel('Probability')\n                plt.tight_layout()\n                plt.savefig(output_path)\n                plt.close()\n                return True\n            else:\n                return False\n        except Exception as e:\n            logging.error(f'Error in plot_probability: {e}')\n            return False\n\n    def plot_t_test(self, params, output_path):\n        try:\n            alpha = params['alpha']\n            df = params['df']\n            t_stat = params['t_stat']\n            critical_value = params['critical_value']\n            x = np.linspace(t_dist.ppf(0.001, df), t_dist.ppf(0.999, df), 1000)\n            y = t_dist.pdf(x, df)\n            plt.figure(figsize=(10, 5))\n            plt.plot(x, y, 'b-', label=f't-dist(df={df})')\n            plt.axvline(x=critical_value, color='r', linestyle='--', label='critical +')\n            plt.axvline(x=-critical_value, color='r', linestyle='--', label='critical -')\n            plt.axvline(x=t_stat, color='g', label='t-stat')\n            p_area_x = x[x > critical_value]\n            plt.fill_between(p_area_x, t_dist.pdf(p_area_x, df), color='red', alpha=0.3)\n            p_area_x2 = x[x < -critical_value]\n            plt.fill_between(p_area_x2, t_dist.pdf(p_area_x2, df), color='red', alpha=0.3)\n            plt.title('t-test visualization')\n            plt.xlabel('t')\n            plt.ylabel('pdf')\n            plt.legend()\n            plt.tight_layout()\n            plt.savefig(output_path)\n            plt.close()\n            return True\n        except Exception as e:\n            logging.error(f't検定グラフ生成エラー: {e}')\n            return False\n\n    def plot_regression(self, x_values, y_values, beta_0_hat, beta_1_hat, output_path):\n        try:\n            import statsmodels.api as sm\n            X = sm.add_constant(x_values)\n            model = sm.OLS(y_values, X).fit()\n            residuals = model.resid\n            (fig, axes) = plt.subplots(2, 2, figsize=(10, 10))\n            ax = axes[0, 0]\n            ax.scatter(x_values, y_values, color='blue', label='data')\n            x_line = np.linspace(min(x_values), max(x_values), 100)\n            y_line = beta_0_hat + beta_1_hat * x_line\n            ax.plot(x_line, y_line, color='red', label='reg line')\n            ax.set_title('Data & Regression Line')\n            ax.set_xlabel('X')\n            ax.set_ylabel('Y')\n            ax.legend()\n            ax = axes[0, 1]\n            ax.hist(residuals, bins=20, color='green', alpha=0.7)\n            ax.set_title('Residual Histogram')\n            ax.set_xlabel('Residual')\n            ax.set_ylabel('Frequency')\n            sm.qqplot(residuals, line='45', ax=axes[1, 0], color='purple')\n            axes[1, 0].set_title('Q-Q plot of Residuals')\n            fitted = model.fittedvalues\n            ax = axes[1, 1]\n            ax.scatter(fitted, residuals, color='orange')\n            ax.axhline(y=0, color='red', linestyle='--')\n            ax.set_title('Residuals vs Fitted')\n            ax.set_xlabel('Fitted')\n            ax.set_ylabel('Residuals')\n            fig.tight_layout()\n            fig.savefig(output_path)\n            plt.close(fig)\n            return True\n        except Exception as e:\n            logging.error(f'回帰分析グラフ生成中にエラー: {e}')\n            return False\n\n    def plot_time_series(self, params, output_path):\n        try:\n            ts = params['time_series']\n            (fig, axes) = plt.subplots(2, 1, figsize=(10, 8))\n            axes[0].plot(ts, color='blue')\n            axes[0].set_title('Time Series Data')\n            axes[0].set_xlabel('Time')\n            axes[0].set_ylabel('Value')\n            plot_acf(ts, ax=axes[1])\n            axes[1].set_title('Autocorrelation Function')\n            fig.tight_layout()\n            fig.savefig(output_path)\n            plt.close(fig)\n            return True\n        except Exception as e:\n            logging.error(f'時系列分析グラフエラー: {e}')\n            return False\n\n    def plot_econometrics(self, params, output_path):\n        try:\n            import statsmodels.api as sm\n            X = np.column_stack((params['x1_values'], params['x2_values']))\n            Y = np.array(params['y_values'])\n            Xc = sm.add_constant(X)\n            model = sm.OLS(Y, Xc).fit()\n            residuals = model.resid\n            fitted = model.fittedvalues\n            (fig, axes) = plt.subplots(2, 2, figsize=(10, 10))\n            ax = axes[0, 0]\n            ax.scatter(params['x1_values'], Y, color='blue', alpha=0.7, label='X1-Y')\n            ax.set_title('X1 vs Y')\n            ax.set_xlabel('X1')\n            ax.set_ylabel('Y')\n            ax = axes[0, 1]\n            ax.scatter(params['x2_values'], Y, color='green', alpha=0.7, label='X2-Y')\n            ax.set_title('X2 vs Y')\n            ax.set_xlabel('X2')\n            ax.set_ylabel('Y')\n            ax = axes[1, 0]\n            ax.hist(residuals, bins=20, color='gray', alpha=0.7)\n            ax.set_title('Residuals Histogram')\n            ax.set_xlabel('Residual')\n            ax = axes[1, 1]\n            ax.scatter(fitted, residuals, color='red', alpha=0.7)\n            ax.axhline(y=0, color='black', linestyle='--')\n            ax.set_title('Residuals vs Fitted')\n            ax.set_xlabel('Fitted')\n            ax.set_ylabel('Residuals')\n            fig.tight_layout()\n            fig.savefig(output_path)\n            plt.close(fig)\n            return True\n        except Exception as e:\n            logging.error(f'計量経済学グラフエラー: {e}')\n            return False\n\n    def plot_multivariate_normal(self, params, output_path):\n        try:\n            mu = params['mu']\n            sigma = np.array(params['sigma'])\n            fig = plt.figure(figsize=(10, 10))\n            from matplotlib.patches import Ellipse\n            import matplotlib.transforms as transforms\n\n            def confidence_ellipse(mu, cov, ax, n_std=1.96, facecolor='none', **kwargs):\n                pearson = cov[0, 1] / np.sqrt(cov[0, 0] * cov[1, 1])\n                ell_radius_x = np.sqrt(1 + pearson)\n                ell_radius_y = np.sqrt(1 - pearson)\n                ellipse = Ellipse((0, 0), width=ell_radius_x * 2, height=ell_radius_y * 2, facecolor=facecolor, **kwargs)\n                scale_x = np.sqrt(cov[0, 0]) * n_std\n                scale_y = np.sqrt(cov[1, 1]) * n_std\n                transf = transforms.Affine2D().rotate_deg(45).scale(scale_x, scale_y).translate(mu[0], mu[1])\n                ellipse.set_transform(transf + ax.transData)\n                return ax.add_patch(ellipse)\n            ax = fig.add_subplot(2, 2, 1)\n            ax.set_title('Confidence Ellipse')\n            confidence_ellipse(mu, sigma, ax, edgecolor='red')\n            ax.scatter(mu[0], mu[1], c='blue', marker='x', label='mean')\n            ax.legend()\n            ax.set_xlabel('X1')\n            ax.set_ylabel('X2')\n            ax2 = fig.add_subplot(2, 2, 2)\n            x = np.linspace(mu[0] - 4 * np.sqrt(sigma[0, 0]), mu[0] + 4 * np.sqrt(sigma[0, 0]), 100)\n            y = np.linspace(mu[1] - 4 * np.sqrt(sigma[1, 1]), mu[1] + 4 * np.sqrt(sigma[1, 1]), 100)\n            (X, Y) = np.meshgrid(x, y)\n            pos = np.dstack((X, Y))\n\n            def mvn_pdf(x, mu, cov):\n                n = 2\n                det = np.linalg.det(cov)\n                inv = np.linalg.inv(cov)\n                diff = x - mu\n                return 1.0 / (2 * np.pi * np.sqrt(det)) * np.exp(-0.5 * (diff @ inv @ diff.T))\n            Z = np.empty(X.shape)\n            for i in range(X.shape[0]):\n                for j in range(X.shape[1]):\n                    Z[i, j] = mvn_pdf(np.array([X[i, j], Y[i, j]]), np.array(mu), sigma)\n            ax2.contour(X, Y, Z, levels=5, cmap='Blues')\n            ax2.set_title('Contour')\n            ax3 = fig.add_subplot(2, 2, 3)\n            X_marg = norm(loc=mu[0], scale=np.sqrt(sigma[0, 0]))\n            x_line = np.linspace(mu[0] - 4 * np.sqrt(sigma[0, 0]), mu[0] + 4 * np.sqrt(sigma[0, 0]), 100)\n            y_line = X_marg.pdf(x_line)\n            ax3.plot(x_line, y_line, 'r-')\n            ax3.set_title('Marginal X1 distribution')\n            ax3.set_xlabel('X1')\n            ax3.set_ylabel('pdf')\n            ax4 = fig.add_subplot(2, 2, 4)\n            Y_marg = norm(loc=mu[1], scale=np.sqrt(sigma[1, 1]))\n            y_line = Y_marg.pdf(x_line)\n            ax4.plot(x_line, y_line, 'g-')\n            ax4.set_title('Marginal X2 distribution')\n            ax4.set_xlabel('X2')\n            ax4.set_ylabel('pdf')\n            fig.tight_layout()\n            fig.savefig(output_path)\n            plt.close(fig)\n            return True\n        except Exception as e:\n            logging.error(f'多変量正規分布グラフエラー: {e}')\n            return False\n\n    def plot_distribution_properties(self, params, output_path):\n        try:\n            dist = params['distribution']\n            plt.figure(figsize=(10, 5))\n            if dist == '正規分布':\n                mu = 0\n                sigma = 1\n                x = np.linspace(mu - 4 * sigma, mu + 4 * sigma, 200)\n                y = norm.pdf(x, mu, sigma)\n                plt.plot(x, y, 'b-')\n                plt.axvline(mu, color='r', linestyle='--', label='mean')\n                plt.axvline(mu + sigma, color='g', linestyle=':', label='mean+sigma')\n                plt.axvline(mu - sigma, color='g', linestyle=':')\n                plt.title('Normal Distribution (mu=0,sigma=1)')\n                plt.legend()\n            elif dist == 'ポアソン分布':\n                lam = 3\n                x = np.arange(0, 15)\n                y = poisson.pmf(x, lam)\n                plt.bar(x, y, color='skyblue')\n                plt.axvline(lam, color='r', linestyle='--', label='mean=lambda=3')\n                plt.title('Poisson(lambda=3)')\n                plt.legend()\n            elif dist == '指数分布':\n                lam = 1\n                x = np.linspace(0, 5, 200)\n                y = expon.pdf(x, scale=1 / lam)\n                plt.plot(x, y, 'b-')\n                plt.axvline(1 / lam, color='r', linestyle='--', label='mean=1/lambda')\n                plt.title('Exponential(lambda=1)')\n                plt.legend()\n            plt.tight_layout()\n            plt.savefig(output_path)\n            plt.close()\n            return True\n        except Exception as e:\n            logging.error(f'分布性質グラフエラー: {e}')\n            return False\n\n    def plot_high_moment(self, params, output_path):\n        try:\n            mu = params['mu']\n            sigma = params['sigma']\n            n = params['n']\n            moment = params['moment']\n            x = np.linspace(mu - 4 * sigma, mu + 4 * sigma, 200)\n            y = norm.pdf(x, mu, sigma)\n            plt.figure(figsize=(10, 5))\n            plt.plot(x, y, 'b-', label='Normal pdf')\n            plt.axvline(mu, color='r', linestyle='--', label='mean')\n            plt.axvline(mu + sigma, color='g', linestyle=':', label='mu±sigma')\n            plt.axvline(mu - sigma, color='g', linestyle=':')\n            plt.title(f'Normal distribution (mu={mu}, sigma={sigma}). n={n}-th moment={moment}')\n            plt.legend()\n            plt.tight_layout()\n            plt.savefig(output_path)\n            plt.close()\n            return True\n        except Exception as e:\n            logging.error(f'高次モーメントグラフエラー: {e}')\n            return False\n\n    def plot_variance_analysis(self, params, output_path):\n        try:\n            group_count = params['group_count']\n            sample_sizes = params['sample_sizes']\n            means = params['means']\n            variances = params['variances']\n            data = []\n            for i in range(group_count):\n                np.random.seed(i)\n                samples = np.random.normal(means[i], np.sqrt(variances[i]), sample_sizes[i])\n                data.append(samples)\n            plt.figure(figsize=(8, 6))\n            plt.boxplot(data, labels=[f'Group{i + 1}' for i in range(group_count)])\n            plt.title('ANOVA: Boxplots of groups')\n            plt.ylabel('Value')\n            plt.tight_layout()\n            plt.savefig(output_path)\n            plt.close()\n            return True\n        except Exception as e:\n            logging.error(f'分散分析グラフエラー: {e}')\n            return False\n\n    def plot_nonparametric_test(self, params, output_path):\n        try:\n            s1 = params['sample1']\n            s2 = params['sample2']\n\n            def ecdf(data):\n                d_sorted = np.sort(data)\n                y = np.arange(1, len(d_sorted) + 1) / len(d_sorted)\n                return (d_sorted, y)\n            (x1, y1) = ecdf(s1)\n            (x2, y2) = ecdf(s2)\n            plt.figure(figsize=(8, 6))\n            plt.step(x1, y1, where='post', label='Sample1 ECDF', color='blue')\n            plt.step(x2, y2, where='post', label='Sample2 ECDF', color='red')\n            plt.title('Nonparametric Test Visualization')\n            plt.xlabel('Value')\n            plt.ylabel('ECDF')\n            plt.legend()\n            plt.tight_layout()\n            plt.savefig(output_path)\n            plt.close()\n            return True\n        except Exception as e:\n            logging.error(f'ノンパラ検定グラフエラー: {e}')\n            return False\n\n    def plot_linear_combination(self, params, output_path):\n        try:\n            a = params['a']\n            b = params['b']\n            mu1 = params['mu1']\n            mu2 = params['mu2']\n            sig1 = params['sigma1_squared']\n            sig2 = params['sigma2_squared']\n            np.random.seed(123)\n            x = np.random.normal(mu1, np.sqrt(sig1), 1000)\n            y = np.random.normal(mu2, np.sqrt(sig2), 1000)\n            Z = a * x + b * y\n            plt.figure(figsize=(8, 6))\n            plt.hist(Z, bins=30, density=True, alpha=0.7, color='purple', label='Simulated Z')\n            E_Z = a * mu1 + b * mu2\n            Var_Z = a ** 2 * sig1 + b ** 2 * sig2\n            X_line = np.linspace(E_Z - 4 * np.sqrt(Var_Z), E_Z + 4 * np.sqrt(Var_Z), 200)\n            Y_line = norm.pdf(X_line, E_Z, np.sqrt(Var_Z))\n            plt.plot(X_line, Y_line, 'r-', label='Theoretical PDF')\n            plt.title('Linear Combination Distribution')\n            plt.xlabel('Z')\n            plt.ylabel('Density')\n            plt.legend()\n            plt.tight_layout()\n            plt.savefig(output_path)\n            plt.close()\n            return True\n        except Exception as e:\n            logging.error(f'線形結合グラフエラー: {e}')\n            return False"
    },
    {
      "path": "your_project\\gui.py",
      "overview": "Pythonコード。\nクラス: InteractiveSolverGUI。\n関数: __init__, new_problem, check_answer, export_pdf, run。\n",
      "content": "import tkinter as tk\nimport tkinter.font as tkfont\nfrom tkinter import messagebox\nfrom PIL import Image, ImageTk\nfrom config import config\nfrom problem_generator import ProblemGenerator\nimport os\n\nclass InteractiveSolverGUI:\n\n    def __init__(self):\n        self.generator = ProblemGenerator()\n        self.root = tk.Tk()\n        title = config.get('gui_settings', 'window_title', default='問題解答システム')\n        self.root.title(title)\n        font_size = config.get('gui_settings', 'font_size', default=14)\n        tkfont.nametofont('TkDefaultFont').configure(size=font_size)\n        frame_problem = tk.Frame(self.root)\n        frame_problem.pack(fill=tk.BOTH, expand=True)\n        self.problem_text_widget = tk.Text(frame_problem, wrap=tk.WORD, height=10)\n        self.problem_text_widget.configure(state=tk.DISABLED)\n        scroll_problem = tk.Scrollbar(frame_problem, command=self.problem_text_widget.yview)\n        self.problem_text_widget['yscrollcommand'] = scroll_problem.set\n        self.problem_text_widget.pack(side=tk.LEFT, fill=tk.BOTH, expand=True)\n        scroll_problem.pack(side=tk.RIGHT, fill=tk.Y)\n        self.current_image = None\n        frame_answer = tk.Frame(self.root)\n        frame_answer.pack(fill=tk.X, padx=5, pady=5)\n        tk.Label(frame_answer, text='あなたの解答:').pack(side=tk.LEFT)\n        self.answer_entry = tk.Entry(frame_answer, width=30)\n        self.answer_entry.pack(side=tk.LEFT, padx=5)\n        check_btn = tk.Button(frame_answer, text='解答を表示', command=self.check_answer)\n        check_btn.pack(side=tk.LEFT, padx=5)\n        frame_solution = tk.Frame(self.root)\n        frame_solution.pack(fill=tk.BOTH, expand=True)\n        tk.Label(frame_solution, text='解答:').pack(anchor='w')\n        self.solution_text_widget = tk.Text(frame_solution, wrap=tk.WORD, height=10)\n        self.solution_text_widget.configure(state=tk.DISABLED)\n        scroll_solution = tk.Scrollbar(frame_solution, command=self.solution_text_widget.yview)\n        self.solution_text_widget['yscrollcommand'] = scroll_solution.set\n        self.solution_text_widget.pack(side=tk.LEFT, fill=tk.BOTH, expand=True)\n        scroll_solution.pack(side=tk.RIGHT, fill=tk.Y)\n        frame_controls = tk.Frame(self.root)\n        frame_controls.pack(fill=tk.X, pady=5)\n        next_btn = tk.Button(frame_controls, text='次の問題', command=self.new_problem)\n        next_btn.pack(side=tk.LEFT, padx=10)\n        pdf_btn = tk.Button(frame_controls, text='PDF出力', command=self.export_pdf)\n        pdf_btn.pack(side=tk.LEFT, padx=10)\n        self.solved_problems = []\n        self.new_problem()\n\n    def new_problem(self):\n        \"\"\"新しい問題を生成して表示する\"\"\"\n        # 修正：generate_problem に selected_topic を明示的に渡す（ここでは None）\n        result = self.generator.generate_problem(selected_topic=None)\n        self.solution_text_widget.configure(state=tk.NORMAL)\n        self.solution_text_widget.delete('1.0', tk.END)\n        self.solution_text_widget.configure(state=tk.DISABLED)\n        self.answer_entry.delete(0, tk.END)\n        self.problem_text_widget.configure(state=tk.NORMAL)\n        self.problem_text_widget.delete('1.0', tk.END)\n        self.problem_text_widget.configure(state=tk.DISABLED)\n        self.current_image = None\n        if not result:\n            self.problem_text_widget.configure(state=tk.NORMAL)\n            self.problem_text_widget.insert(tk.END, '問題を生成できませんでした。')\n            self.problem_text_widget.configure(state=tk.DISABLED)\n            return\n        (_, _, _, problem_text, solution_text, graph_filename) = result\n        self.problem_text_widget.configure(state=tk.NORMAL)\n        self.problem_text_widget.insert(tk.END, problem_text + '\\n')\n        if graph_filename:\n            image_path = os.path.join(self.generator.output_dir, graph_filename)\n            try:\n                img = Image.open(image_path)\n                photo = ImageTk.PhotoImage(img)\n                self.current_image = photo\n                self.problem_text_widget.image_create(tk.END, image=self.current_image)\n                self.problem_text_widget.insert(tk.END, '\\n')\n            except Exception as e:\n                print(f'画像の読み込みでエラー: {e}')\n        self.problem_text_widget.configure(state=tk.DISABLED)\n        self.current_solution_text = solution_text\n        self.current_problem_text = problem_text\n        self.current_graph = graph_filename\n        self.current_solved = False\n\n    def check_answer(self):\n        \"\"\"解答を確認して解説を表示する\"\"\"\n        if hasattr(self, 'current_solution_text'):\n            self.solution_text_widget.configure(state=tk.NORMAL)\n            self.solution_text_widget.delete('1.0', tk.END)\n            self.solution_text_widget.insert(tk.END, self.current_solution_text)\n            self.solution_text_widget.configure(state=tk.DISABLED)\n            if not self.current_solved:\n                self.solved_problems.append({'problem_text': self.current_problem_text, 'solution_text': self.current_solution_text, 'graph_filename': self.current_graph})\n                self.current_solved = True\n\n    def export_pdf(self):\n        \"\"\"解いた問題セットをPDFファイルに出力する\"\"\"\n        if not self.solved_problems:\n            messagebox.showwarning('警告', '解いた問題がありません。')\n            return\n        header = ['\\\\\\\\documentclass{article}', '\\\\\\\\usepackage{amsmath}', '\\\\\\\\usepackage{amssymb}', '\\\\\\\\usepackage{graphicx}', '\\\\\\\\usepackage{float}', '\\\\\\\\usepackage{geometry}', '\\\\\\\\usepackage{xeCJK}', '\\\\\\\\usepackage{fontspec}', '\\\\\\\\setmainfont{Times New Roman}', f\"\\\\\\\\setCJKmainfont{{{config.get('cjk_main_font', default='Yu Gothic') or 'Yu Gothic'}}}\", '\\\\\\\\geometry{a4paper, margin=1in}', '\\\\\\\\begin{document}']\n        latex_lines = header.copy()\n        for (idx, prob) in enumerate(self.solved_problems, start=1):\n            latex_lines.append(f'\\\\\\\\section*{{問題 {idx}}}')\n            latex_lines.append(prob['problem_text'])\n            if prob.get('graph_filename'):\n                latex_lines.append('\\\\\\\\begin{figure}[H]')\n                latex_lines.append('\\\\\\\\centering')\n                latex_lines.append(f\"\\\\\\\\includegraphics[width=0.8\\\\\\\\textwidth]{{graphs/{prob['graph_filename']}}}\")\n                latex_lines.append('\\\\\\\\end{figure}')\n            latex_lines.append('\\\\\\\\subsection*{解答}')\n            latex_lines.append(prob['solution_text'])\n            latex_lines.append('\\\\\\\\newpage')\n        latex_lines.append('\\\\\\\\clearpage')\n        latex_lines.append(f\"\\\\\\\\input{{../{config.get('problem_templates_directory', default='templates') or 'templates'}/distribution_relations.tex}}\")\n        latex_lines.append('\\\\\\\\end{document}')\n        script_dir = os.path.dirname(os.path.abspath(__file__))\n        out_dir = config.get('output_directory', default='outputs') or 'outputs'\n        output_dir = os.path.join(script_dir, out_dir)\n        os.makedirs(output_dir, exist_ok=True)\n        tex_path = os.path.join(output_dir, config.get('output_tex_file', default='practice_problems.tex') or 'practice_problems.tex')\n        try:\n            with open(tex_path, 'w', encoding='utf-8') as f:\n                f.write('\\n'.join(latex_lines))\n        except Exception as e:\n            messagebox.showerror('エラー', f'PDF出力用ファイルの作成に失敗しました: {e}')\n            return\n        latex_path = config.get('latex_path', default='xelatex') or ''\n        if not latex_path or not os.path.exists(latex_path):\n            messagebox.showerror('エラー', f'LaTeXコンパイラが見つかりません: {latex_path}')\n            return\n        try:\n            proc = os.spawnl(os.P_WAIT, latex_path, latex_path, '-interaction=nonstopmode', os.path.basename(tex_path))\n        except Exception as e:\n            messagebox.showerror('エラー', f'PDFの生成中にエラー: {e}')\n            return\n        pdf_path = tex_path.replace('.tex', '.pdf')\n        if os.path.exists(pdf_path):\n            messagebox.showinfo('PDF出力', f'PDFを出力しました: {pdf_path}')\n        else:\n            messagebox.showerror('エラー', 'PDFファイルが生成されませんでした。')\n\n    def run(self):\n        \"\"\"GUIのメインループを開始する\"\"\"\n        self.root.mainloop()\n"
    },
    {
      "path": "your_project\\main.py",
      "overview": "Pythonコード。\n関数: main, choose_gui, choose_pdf。\n",
      "content": "import sys\nimport tkinter as tk\nimport tkinter.font as tkfont\nimport logging\nfrom main_app import MainApp\ntry:\n    from interactive_solver_ui import InteractiveSolverUI\nexcept ImportError:\n    from gui import InteractiveSolverGUI as InteractiveSolverUI\nfrom config import config\nlog_level = config.get('log_level', default='INFO')\nlogging.basicConfig(level=getattr(logging, str(log_level).upper(), logging.INFO))\n\ndef main():\n    \"\"\"モード選択GUIを表示し、ユーザーの選択に応じてGUIまたはPDFモードを起動します。\"\"\"\n    pdf_gen_settings = config.get('pdf_generation', default={})\n    problem_count = pdf_gen_settings.get('problem_count', 9)\n    root = tk.Tk()\n    root.title('モード選択')\n    default_font = tkfont.nametofont('TkDefaultFont')\n    default_font.configure(size=12)\n    instruction = tk.Label(root, text='起動モードを選択してください:', font=('', 12))\n    instruction.pack(padx=20, pady=10)\n    selected_mode = {'mode': None}\n\n    def choose_gui():\n        selected_mode['mode'] = 'GUI'\n        root.quit()\n\n    def choose_pdf():\n        selected_mode['mode'] = 'PDF'\n        root.quit()\n    btn_gui = tk.Button(root, text='GUIモード', font=('', 12), width=15, command=choose_gui)\n    btn_pdf = tk.Button(root, text='PDF生成モード', font=('', 12), width=15, command=choose_pdf)\n    btn_gui.pack(pady=5)\n    btn_pdf.pack(pady=5)\n    root.mainloop()\n    root.destroy()\n    mode = selected_mode['mode']\n    if mode == 'GUI':\n        app_ui = InteractiveSolverUI()\n        app_ui.run()\n    elif mode == 'PDF':\n        app = MainApp()\n        app.generate_and_compile(problem_count)\n        print('PDFの生成が完了しました。')\n    else:\n        print('モードが選択されなかったため、終了します。')\nif __name__ == '__main__':\n    main()"
    },
    {
      "path": "your_project\\main_app.py",
      "overview": "Pythonコード。\nクラス: MainApp。\n関数: __init__, generate_latex_header, generate_and_compile。\n",
      "content": "import os\nimport subprocess\nimport logging\nimport traceback\nfrom config import config\nfrom problem_generator import ProblemGenerator\n\nclass MainApp:\n\n    def __init__(self):\n        self.enable_visualization = config.get('enable_visualization', default=True)\n        if self.enable_visualization is None:\n            self.enable_visualization = True\n        self.problem_templates_dir = config.get('problem_templates_directory', default='templates')\n        if self.problem_templates_dir is None:\n            self.problem_templates_dir = 'templates'\n        self.output_directory = config.get('output_directory', default='outputs')\n        if self.output_directory is None:\n            self.output_directory = 'outputs'\n        self.latex_path = config.get('latex_path', default='xelatex')\n        if not self.latex_path:\n            self.latex_path = 'xelatex'\n        self.output_tex_file = config.get('output_tex_file', default='practice_problems.tex')\n        if not self.output_tex_file:\n            self.output_tex_file = 'practice_problems.tex'\n        self.generator = ProblemGenerator()\n\n    def generate_latex_header(self):\n        cjk_font = config.get('cjk_main_font', default='Yu Gothic')\n        if cjk_font is None:\n            cjk_font = 'Yu Gothic'\n        header = [\n            '\\\\documentclass{article}',\n            '\\\\usepackage{amsmath}',\n            '\\\\usepackage{amssymb}',\n            '\\\\usepackage{graphicx}',\n            '\\\\usepackage{float}',\n            '\\\\usepackage{geometry}',\n            '\\\\usepackage{xeCJK}',\n            '\\\\usepackage{fontspec}',\n            '\\\\setmainfont{Times New Roman}',\n            f'\\\\setCJKmainfont{{{cjk_font}}}',\n            '\\\\geometry{a4paper, margin=1in}',\n            '\\\\begin{document}'\n        ]\n        return header\n\n    def generate_and_compile(self, problem_count: int):\n        graph_files = []  # 生成されたグラフファイルのパスを記録するリスト\n        try:\n            problems = [self.generator.generate_problem() for _ in range(problem_count)]\n            latex_lines = self.generate_latex_header()\n            for i, p in enumerate(problems, start=1):\n                if p is None:\n                    continue\n                # ここでは p はタプル形式で返っていると仮定\n                # (problem_id, date_created, problem_type, problem_text, solution_text, graph_filename)\n                _, _, _, problem_text, solution_text, graph_filename = p\n                latex_lines.append(f'\\\\section*{{問題 {i}}}')\n                latex_lines.append(problem_text)\n                if self.enable_visualization and graph_filename:\n                    latex_lines.append('\\\\begin{figure}[H]')\n                    latex_lines.append('\\\\centering')\n                    latex_lines.append(f\"\\\\includegraphics[width=0.8\\\\textwidth]{{graphs/{graph_filename}}}\")\n                    latex_lines.append('\\\\end{figure}')\n                    graph_files.append(os.path.join(self.generator.output_dir, graph_filename))\n                latex_lines.append('\\\\newpage')\n                latex_lines.append(f'\\\\section*{{解答 {i}}}')\n                latex_lines.append(solution_text)\n                latex_lines.append('\\\\newpage')\n            latex_lines.append('\\\\end{document}')\n            output_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), self.output_directory)\n            os.makedirs(output_dir, exist_ok=True)\n            tex_file_path = os.path.join(output_dir, self.output_tex_file)\n            with open(tex_file_path, 'w', encoding='utf-8') as f:\n                f.write('\\n'.join(latex_lines))\n            if not self.latex_path or not os.path.exists(self.latex_path):\n                print(f'LaTeXコンパイラが見つかりません: {self.latex_path}')\n                return\n            process = subprocess.run([self.latex_path, '-interaction=nonstopmode', os.path.basename(tex_file_path)],\n                                       cwd=os.path.dirname(tex_file_path),\n                                       stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n            log_file_path = os.path.join(os.path.dirname(tex_file_path), 'latex_compile.log')\n            with open(log_file_path, 'w', encoding='utf-8') as f:\n                f.write(process.stdout or '')\n                if process.stderr:\n                    f.write('\\nERROR:\\n' + process.stderr)\n            if process.returncode != 0:\n                print(\"LaTeXコンパイルでエラー\")\n            else:\n                pdf_file = tex_file_path.replace('.tex', '.pdf')\n                if os.path.exists(pdf_file):\n                    print(f\"PDF生成成功: {pdf_file}\")\n                else:\n                    print(\"PDFファイル未発見\")\n        except Exception as e:\n            logging.error(f'PDF生成中にエラー: {e}')\n            logging.error(traceback.format_exc())\n        finally:\n            # PDF生成後にグラフファイルを削除\n            for file_path in graph_files:\n                try:\n                    if os.path.exists(file_path):\n                        os.remove(file_path)\n                        logging.info(f\"削除しました: {file_path}\")\n                except Exception as e:\n                    logging.error(f\"グラフファイル {file_path} の削除に失敗: {e}\")\n\nif __name__ == '__main__':\n    app = MainApp()\n    # ここでは例として5問生成する\n    app.generate_and_compile(5)\n"
    },
    {
      "path": "your_project\\problem_generator.py",
      "overview": "Pythonコード。\nクラス: ProblemGenerator。\n関数: __init__, load_topics, get_problem_types_by_topic, generate_problem。\n",
      "content": "import random\nimport os\nimport logging\nimport uuid\nimport traceback\nfrom datetime import datetime\nfrom config import config\nfrom database import DatabaseManager\nfrom problem_types.problem_factory import ProblemFactory\nimport json\n\nclass ProblemGenerator:\n\n    def __init__(self):\n        # 修正：データベースのテーブル作成を必ず実行する\n        self.database = DatabaseManager()\n        self.database.setup_database()  # ← テーブルが存在しない場合の対策\n        self.factory = ProblemFactory()\n        self.topics = self.load_topics()\n        # 出力先グラフディレクトリの設定\n        script_dir = os.path.dirname(os.path.abspath(__file__))\n        out_dir = config.get('output_directory', default='outputs') or 'outputs'\n        self.output_dir = os.path.join(script_dir, out_dir, 'graphs')\n        os.makedirs(self.output_dir, exist_ok=True)\n\n    def load_topics(self):\n        script_dir = os.path.dirname(os.path.abspath(__file__))\n        topics_file = os.path.join(script_dir, 'topics.json')\n        if not os.path.exists(topics_file):\n            raise FileNotFoundError(f\"'{topics_file}' not found.\")\n        with open(topics_file, 'r', encoding='utf-8') as f:\n            data = json.load(f)\n        return data.get('topics', {})\n\n    def get_problem_types_by_topic(self, topic: str):\n        return self.topics.get(topic, [])\n\n    def generate_problem(self, selected_topic: str = None):\n        try:\n            if selected_topic:\n                allowed_types = self.get_problem_types_by_topic(selected_topic)\n                weighted_types = {ptype: wt for (ptype, wt) in config.get('problem_types', {}).items() if ptype in allowed_types}\n                if not weighted_types and allowed_types:\n                    weighted_types = {ptype: 1.0 for ptype in allowed_types}\n            else:\n                weighted_types = config.get('problem_types', {})\n            if not weighted_types:\n                weighted_types = {ptype: 1.0 for ptype in self.factory.problem_classes.keys()}\n            types = list(weighted_types.keys())\n            weights = list(weighted_types.values())\n            problem_type = random.choices(types, weights=weights, k=1)[0]\n            problem = self.factory.create_problem(problem_type)\n            if problem is None:\n                return None\n            problem.generate_parameters()\n            problem_text = problem.generate_problem_text()\n            solution_text = problem.generate_solution_text()\n            problem_id = str(uuid.uuid4())\n            date_created = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n            graph_filename = None\n            if config.get('enable_visualization', default=True):\n                graph_filename = f\"graph_{uuid.uuid4().hex}.png\"\n                graph_filepath = os.path.join(self.output_dir, graph_filename)\n                if not problem.generate_graph(graph_filepath):\n                    graph_filename = None\n            self.database.save_problem(problem_id, date_created, problem_text, solution_text, problem_type)\n            return problem_id, date_created, problem_type, problem_text, solution_text, graph_filename\n        except Exception as e:\n            logging.error(f'問題生成中にエラー: {e}')\n            logging.error(traceback.format_exc())\n            return None\n"
    },
    {
      "path": "your_project\\sympy_solver.py",
      "overview": "Pythonコード。\nクラス: SympySolver。\n関数: __init__, check_equivalence。\n",
      "content": "class SympySolver:\n\n    def __init__(self):\n        pass\n\n    def check_equivalence(self, user_input, correct_answer):\n        return (False, '')"
    },
    {
      "path": "your_project\\topics.json",
      "overview": "JSONファイル (辞書)。キー: topics\n",
      "content": "{\n\n  \"topics\": {\n\n    \"確率論\": [\n\n      \"probability_definition\",\n\n      \"conditional_probability\",\n\n      \"distribution_functions\",\n\n      \"joint_distribution\",\n\n      \"probability\"\n\n    ],\n\n    \"統計的推定\": [\n\n      \"statistical_inference\",\n\n      \"t_test\"\n\n    ],\n\n    \"統計的検定\": [\n\n      \"statistical_inference\",\n\n      \"t_test\"\n\n    ],\n\n    \"回帰分析\": [\n\n      \"regression_analysis\"\n\n    ],\n\n    \"分散分析\": [\n\n      \"variance_analysis\"\n\n    ],\n\n    \"ノンパラメトリック検定\": [\n\n      \"nonparametric_test\"\n\n    ]\n\n  }\n\n}\n\n"
    },
    {
      "path": "your_project\\.vscode\\launch.json",
      "overview": "JSON解析エラー: Expecting property name enclosed in double quotes: line 11 column 13 (char 286)\n",
      "content": "{\n    \"version\": \"0.2.0\",\n    \"configurations\": [\n        {\n            \"name\": \"Python: 現在のファイル\",\n            \"type\": \"debugpy\",\n            \"request\": \"launch\",\n            \"program\": \"${file}\",\n            \"console\": \"integratedTerminal\",\n            \"justMyCode\": true,\n            //\"args\": [\"--generate-pdf\"]\n        }\n    ]\n}"
    },
    {
      "path": "your_project\\problem_types\\conjugate_problems.py",
      "overview": "Pythonコード。\nクラス: BetaBinomialConjugateProblem, GammaPoissonConjugateProblem, DirichletMultinomialConjugateProblem, BinomialPoissonApproxProblem, PoissonNormalApproxProblem。\n関数: __init__, generate_parameters, generate_explanation, generate_graph, __init__, generate_parameters, generate_explanation, generate_graph, __init__, generate_parameters, generate_explanation, generate_graph, __init__, generate_parameters, generate_explanation, generate_graph, __init__, generate_parameters, generate_explanation, generate_graph, B, B。\n",
      "content": "import math\nimport random\nfrom problem_types.problem import Problem\nfrom math import comb, factorial, exp, gamma\nimport numpy as np\n\nclass BetaBinomialConjugateProblem(Problem):\n\n    def __init__(self):\n        # 専用テンプレート（例：beta_binomial_conjugate_problem.tex）を利用する場合\n        super().__init__('beta_binomial_conjugate_problem.tex')\n\n    def generate_parameters(self):\n        self.params['alpha'] = random.randint(1, 5)\n        self.params['beta'] = random.randint(1, 5)\n        self.params['n'] = random.randint(5, 20)\n        self.params['k'] = random.randint(0, self.params['n'])\n        def B(x, y):\n            return (gamma(x) * gamma(y)) / gamma(x + y)\n        p_x = comb(self.params['n'], self.params['k']) * B(self.params['k'] + self.params['alpha'], self.params['n'] - self.params['k'] + self.params['beta']) / B(self.params['alpha'], self.params['beta'])\n        self.params['probability'] = round(p_x, 4)\n\n    def generate_explanation(self):\n        return \"Beta + Binomial -> Beta-Binomial の関係を利用\"\n\n    def generate_graph(self, output_path):\n        return False\n\nclass GammaPoissonConjugateProblem(Problem):\n\n    def __init__(self):\n        super().__init__('gamma_poisson_conjugate_problem.tex')\n\n    def generate_parameters(self):\n        self.params['alpha'] = random.randint(1, 5)\n        self.params['beta'] = round(random.uniform(0.5, 2.0), 2)\n        self.params['k'] = random.randint(0, 20)\n        p = self.params['beta'] / (self.params['beta'] + 1)\n        q = 1 - p\n        negbin_p = comb(self.params['k'] + self.params['alpha'] - 1, self.params['k']) * (q ** self.params['k']) * (p ** self.params['alpha'])\n        self.params['probability'] = round(negbin_p, 4)\n\n    def generate_explanation(self):\n        return \"Gamma + Poisson -> Negative Binomial の関係を利用\"\n\n    def generate_graph(self, output_path):\n        return False\n\nclass DirichletMultinomialConjugateProblem(Problem):\n\n    def __init__(self):\n        super().__init__('dirichlet_multinomial_conjugate_problem.tex')\n\n    def generate_parameters(self):\n        categories = random.randint(3, 5)\n        self.params['alpha_vec'] = [random.randint(1, 5) for _ in range(categories)]\n        self.params['n'] = random.randint(20, 100)\n        counts = [random.randint(0, self.params['n']) for _ in range(categories)]\n        total_counts = sum(counts)\n        if total_counts == 0:\n            counts[0] = self.params['n']\n        else:\n            factor = self.params['n'] / total_counts\n            counts = [int(c * factor) for c in counts]\n            diff = self.params['n'] - sum(counts)\n            if diff != 0:\n                counts[0] += diff\n        self.params['counts'] = counts\n        def B(alpha):\n            import numpy as np\n            return (np.prod([gamma(a) for a in alpha])) / gamma(sum(alpha))\n        alpha_x = [self.params['alpha_vec'][i] + counts[i] for i in range(categories)]\n        num = B(alpha_x)\n        den = B(self.params['alpha_vec'])\n        multinomial_coef = math.factorial(self.params['n'])\n        for c in counts:\n            multinomial_coef /= math.factorial(c)\n        p_x = multinomial_coef * (num / den)\n        self.params['probability'] = round(p_x, 4)\n\n    def generate_explanation(self):\n        return \"Dirichlet + Multinomial -> Dirichlet-Multinomial の関係を利用\"\n\n    def generate_graph(self, output_path):\n        return False\n\nclass BinomialPoissonApproxProblem(Problem):\n\n    def __init__(self):\n        super().__init__('binomial_poisson_approx_problem.tex')\n\n    def generate_parameters(self):\n        lam = random.uniform(2, 5)\n        n = random.randint(50, 200)\n        p = lam / n\n        k = random.randint(0, int(lam * 3))\n        binom_p = comb(n, k) * (p ** k) * ((1 - p) ** (n - k))\n        poisson_p = (lam ** k) * exp(-lam) / factorial(k)\n        self.params['n'] = n\n        self.params['p'] = round(p, 6)\n        self.params['k'] = k\n        self.params['lambda'] = round(lam, 3)\n        self.params['binom_p'] = round(binom_p, 6)\n        self.params['poisson_p'] = round(poisson_p, 6)\n\n    def generate_explanation(self):\n        return \"Binomial -> Poisson近似条件に基づく計算\"\n\n    def generate_graph(self, output_path):\n        return False\n\nclass PoissonNormalApproxProblem(Problem):\n\n    def __init__(self):\n        super().__init__('poisson_normal_approx_problem.tex')\n\n    def generate_parameters(self):\n        lam = random.randint(30, 100)\n        low = max(0, int(lam - 3 * math.sqrt(lam)))\n        high = int(lam + 3 * math.sqrt(lam))\n        k = random.randint(low, high)\n        poisson_p = (lam ** k) * exp(-lam) / factorial(k)\n        self.params['lambda'] = lam\n        self.params['k'] = k\n        self.params['poisson_p'] = round(poisson_p, 6)\n        self.params['mean'] = lam\n        self.params['variance'] = lam\n\n    def generate_explanation(self):\n        return \"Poisson -> Normal近似 (λ大) の条件下での近似\"\n\n    def generate_graph(self, output_path):\n        return False\n"
    },
    {
      "path": "your_project\\problem_types\\problem.py",
      "overview": "Pythonコード。\nクラス: Problem, ProbabilityProblem, StatisticalInferenceProblem, RegressionAnalysisProblem, TimeSeriesAnalysisProblem, EconometricsProblem, LinearCombinationProblem, HighMomentProblem, MultivariateNormalProblem, ProbabilityDefinitionProblem, ConditionalProbabilityProblem, DistributionPropertiesProblem, JointDistributionProblem, VarianceAnalysisProblem, NonParametricTestProblem。\n関数: __init__, generate_parameters, generate_problem_text, generate_solution_text, generate_explanation, generate_graph, __init__, generate_parameters, _variant_binomial, _variant_poisson, _variant_conditional_probability, generate_explanation, generate_graph, __init__, generate_parameters, __init__, generate_parameters, __init__, generate_parameters, __init__, generate_parameters, __init__, generate_parameters, generate_explanation, __init__, generate_parameters, __init__, generate_parameters, generate_explanation, __init__, generate_parameters, generate_explanation, __init__, generate_parameters, generate_explanation, __init__, generate_parameters, generate_explanation, generate_graph, __init__, generate_parameters, generate_explanation, __init__, generate_parameters, generate_explanation, __init__, generate_parameters, generate_explanation。\n",
      "content": "from abc import ABC, abstractmethod\nimport os\nimport random\nimport logging\nimport traceback\nfrom math import comb, exp, factorial\nimport numpy as np\nfrom config import config\nfrom jinja2 import Environment, FileSystemLoader\nfrom graph import ProbabilityDistributionVisualizer\nfrom scipy import stats\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\n\nclass Problem(ABC):\n\n    def __init__(self, template_name: str):\n        self.params = {}\n        self.template_name = template_name\n        # テンプレートディレクトリは設定から取得\n        templates_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), '..', config.get('problem_templates_directory', default='templates'))\n        self.env = Environment(loader=FileSystemLoader(templates_dir), autoescape=False)\n        self.visualizer = ProbabilityDistributionVisualizer()\n\n    @abstractmethod\n    def generate_parameters(self):\n        pass\n\n    def generate_problem_text(self) -> str:\n        try:\n            template = self.env.get_template(self.template_name)\n            return template.render(**self.params, show_solution=False)\n        except Exception as e:\n            logging.error(f\"テンプレートの読み込みエラー: {e}\")\n            return \"テンプレートが見つかりません\"\n\n    def generate_solution_text(self) -> str:\n        try:\n            template = self.env.get_template(self.template_name)\n            self.params['explanation'] = self.generate_explanation()\n            return template.render(**self.params, show_solution=True)\n        except Exception as e:\n            logging.error(f\"テンプレートの読み込みエラー: {e}\")\n            return \"テンプレートが見つかりません\"\n\n    def generate_explanation(self) -> str:\n        return \"\"\n\n    # すべての問題でグラフ生成を必ず実施するデフォルト実装\n    def generate_graph(self, output_path: str):\n        try:\n            plt.figure(figsize=(8, 6))\n            # もし self.params に \"data\" があればヒストグラムを描画\n            if \"data\" in self.params and isinstance(self.params[\"data\"], list) and len(self.params[\"data\"]) > 0:\n                plt.hist(self.params[\"data\"], bins=10, color=\"skyblue\", edgecolor=\"black\", alpha=0.8)\n                plt.title(f\"Histogram for {self.__class__.__name__}\")\n                plt.xlabel(\"Value\")\n                plt.ylabel(\"Frequency\")\n            else:\n                # それ以外の場合、サイン波をプロットする例\n                x = np.linspace(0, 2 * np.pi, 100)\n                y = np.sin(x)\n                plt.plot(x, y, label=\"sin(x)\", color=\"blue\")\n                plt.title(f\"Sine Wave for {self.__class__.__name__}\")\n                plt.xlabel(\"x\")\n                plt.ylabel(\"sin(x)\")\n                plt.legend()\n            plt.grid(True)\n            plt.tight_layout()\n            plt.savefig(output_path, format=\"png\")\n            plt.close()\n            return True\n        except Exception as e:\n            logging.error(f\"グラフ生成エラー: {e}\")\n            return False\n\n# 以下は各問題クラスの例です。必要に応じて各クラスで generate_graph をオーバーライドしてください。\n\nclass ProbabilityProblem(Problem):\n    def __init__(self):\n        super().__init__('probability_problem.tex')\n    def generate_parameters(self):\n        variants = [self._variant_binomial, self._variant_poisson, self._variant_conditional_probability]\n        v = random.choice(variants)\n        v()\n    def _variant_binomial(self):\n        self.params['problem_type'] = 'binomial'\n        self.params['n'] = random.randint(5, 20)\n        self.params['p'] = round(random.uniform(0.1, 0.9), 2)\n        self.params['k'] = random.randint(0, self.params['n'])\n        prob = comb(self.params['n'], self.params['k']) * (self.params['p'] ** self.params['k']) * ((1 - self.params['p']) ** (self.params['n'] - self.params['k']))\n        self.params['probability'] = round(prob, 6)\n    def _variant_poisson(self):\n        self.params['problem_type'] = 'poisson'\n        self.params['lambda'] = round(random.uniform(0.5, 5.0), 2)\n        self.params['k'] = random.randint(0, 10)\n        lam = self.params['lambda']\n        k = self.params['k']\n        prob = (lam ** k) * exp(-lam) / factorial(k)\n        self.params['probability'] = round(prob, 6)\n    def _variant_conditional_probability(self):\n        self.params['problem_type'] = 'conditional_probability'\n        self.params['P_A'] = round(random.uniform(0.1, 0.9), 2)\n        self.params['P_B_given_A'] = round(random.uniform(0.1, 0.9), 2)\n        self.params['P_A_and_B'] = round(self.params['P_A'] * self.params['P_B_given_A'], 6)\n    def generate_explanation(self):\n        t = self.params.get('problem_type', '')\n        if t == 'binomial':\n            return \"二項分布の公式を使用\"\n        elif t == 'poisson':\n            return \"ポアソン分布の公式を使用\"\n        elif t == 'conditional_probability':\n            return \"条件付き確率 P(A∩B)=P(A)*P(B|A) を利用\"\n    def generate_graph(self, output_path: str):\n        return self.visualizer.plot_probability(self.params, output_path)\n\nclass StatisticalInferenceProblem(Problem):\n    def __init__(self):\n        super().__init__('statistical_inference_problem.tex')\n    def generate_parameters(self):\n        self.params['sample_mean'] = round(random.uniform(50, 100), 2)\n        self.params['sample_std'] = round(random.uniform(5, 15), 2)\n        self.params['n'] = random.randint(30, 100)\n        self.params['population_mean'] = round(random.uniform(50, 100), 2)\n        self.params['alpha'] = round(random.uniform(0.01, 0.1), 2)\n        df = self.params['n'] - 1\n        t_stat = (self.params['sample_mean'] - self.params['population_mean']) / (self.params['sample_std'] / self.params['n'] ** 0.5)\n        t_stat = round(t_stat, 4)\n        cv = round(abs(stats.t.ppf(1 - self.params['alpha'] / 2, df)), 4)\n        reject = '棄却' if abs(t_stat) > cv else '棄却しない'\n        self.params['t_stat'] = t_stat\n        self.params['critical_value'] = cv\n        self.params['reject_null'] = reject\n        self.params['df'] = df\n\nclass RegressionAnalysisProblem(Problem):\n    def __init__(self):\n        super().__init__('regression_analysis_problem.tex')\n    def generate_parameters(self):\n        self.params['n'] = random.randint(20, 50)\n        self.params['beta0'] = round(random.uniform(0, 5), 2)\n        self.params['beta1'] = round(random.uniform(0, 5), 2)\n        self.params['noise_variance'] = round(random.uniform(1, 5), 2)\n\nclass TimeSeriesAnalysisProblem(Problem):\n    def __init__(self):\n        super().__init__('time_series_analysis_problem.tex')\n    def generate_parameters(self):\n        self.params['phi'] = round(random.uniform(0.1, 0.9), 2)\n        self.params['sigma'] = round(random.uniform(1, 5), 2)\n\nclass EconometricsProblem(Problem):\n    def __init__(self):\n        super().__init__('econometrics_problem.tex')\n    def generate_parameters(self):\n        self.params['elasticity'] = round(random.uniform(0.1, 1.0), 3)\n        self.params['std_error'] = round(random.uniform(0.01, 0.1), 4)\n\nclass LinearCombinationProblem(Problem):\n    def __init__(self):\n        super().__init__('linear_combination_problem.tex')\n    def generate_parameters(self):\n        self.params['a'] = round(random.uniform(1, 5), 2)\n        self.params['b'] = round(random.uniform(1, 5), 2)\n        self.params['mu1'] = round(random.uniform(10, 50), 2)\n        self.params['mu2'] = round(random.uniform(10, 50), 2)\n        self.params['sigma1'] = round(random.uniform(1, 5), 2)\n        self.params['sigma2'] = round(random.uniform(1, 5), 2)\n    def generate_explanation(self) -> str:\n        return '線形結合の期待値・分散計算'\n\nclass HighMomentProblem(Problem):\n    def __init__(self):\n        super().__init__('high_moment_problem.tex')\n    def generate_parameters(self):\n        self.params['data'] = [round(random.uniform(-3, 3), 2) for _ in range(random.randint(5, 10))]\n\nclass MultivariateNormalProblem(Problem):\n    def __init__(self):\n        super().__init__('multivariate_normal_problem.tex')\n    def generate_parameters(self):\n        self.params['mu'] = [round(random.uniform(0, 5), 2) for _ in range(2)]\n        self.params['cov_matrix'] = [[round(random.uniform(0, 1), 2) for _ in range(2)] for _ in range(2)]\n    def generate_explanation(self) -> str:\n        return '多変量正規分布の性質を利用'\n\nclass ProbabilityDefinitionProblem(Problem):\n    def __init__(self):\n        super().__init__('probability_definition_problem.tex')\n    def generate_parameters(self):\n        self.params['P_A'] = round(random.uniform(0.1, 0.9), 2)\n        self.params['P_B'] = round(random.uniform(0.1, 0.9), 2)\n    def generate_explanation(self) -> str:\n        return '独立性利用 P(A∩B)=P(A)*P(B)'\n\nclass ConditionalProbabilityProblem(Problem):\n    def __init__(self):\n        super().__init__('conditional_probability_problem.tex')\n    def generate_parameters(self):\n        self.params['P_A'] = round(random.uniform(0.1, 0.9), 2)\n        self.params['P_B_given_A'] = round(random.uniform(0.1, 0.9), 2)\n    def generate_explanation(self) -> str:\n        return 'P(A∩B)=P(A)*P(B|A)'\n\nclass DistributionPropertiesProblem(Problem):\n    def __init__(self):\n        super().__init__('distribution_properties_problem.tex')\n    def generate_parameters(self):\n        self.params['distribution'] = '正規分布'\n        self.params['properties'] = {\n            'mean': 0,\n            'variance': 1\n        }\n    def generate_explanation(self):\n        return \"正規分布の平均と分散を利用して計算\"\n    def generate_graph(self, output_path: str):\n        return self.visualizer.plot_distribution_properties(self.params, output_path)\n\nclass JointDistributionProblem(Problem):\n    def __init__(self):\n        super().__init__('distribution_relations.tex')\n    def generate_parameters(self):\n        self.params['x'] = round(random.uniform(0, 5), 2)\n        self.params['y'] = round(random.uniform(0, 5), 2)\n    def generate_explanation(self) -> str:\n        return '同時→周辺→条件付き確率'\n\nclass VarianceAnalysisProblem(Problem):\n    def __init__(self):\n        super().__init__('variance_analysis_problem.tex')\n    def generate_parameters(self):\n        self.params['group_count'] = random.randint(2, 4)\n        self.params['sample_sizes'] = [random.randint(5, 20) for _ in range(self.params['group_count'])]\n        self.params['means'] = [round(random.uniform(10, 50), 2) for _ in range(self.params['group_count'])]\n        self.params['variances'] = [round(random.uniform(1, 5), 2) for _ in range(self.params['group_count'])]\n        total_n = sum(self.params['sample_sizes'])\n        group_count = self.params['group_count']\n        grand_mean = sum((self.params['means'][i] * self.params['sample_sizes'][i] for i in range(group_count))) / total_n\n        ssb = sum((self.params['sample_sizes'][i] * (self.params['means'][i] - grand_mean) ** 2 for i in range(group_count)))\n        ssw = sum(((self.params['sample_sizes'][i] - 1) * self.params['variances'][i] for i in range(group_count)))\n        df_between = group_count - 1\n        df_within = total_n - group_count\n        msb = ssb / df_between if df_between > 0 else 0\n        msw = ssw / df_within if df_within > 0 else 0\n        F = msb / msw if msw != 0 else 0\n        alpha = 0.05\n        F_critical = stats.f.ppf(1 - alpha, df_between, df_within) if df_between > 0 and df_within > 0 else float('inf')\n        F = round(F, 4)\n        F_critical = round(F_critical, 4)\n        reject = '棄却する' if F > F_critical else '棄却しない'\n        self.params['F_value'] = F\n        self.params['F_critical'] = F_critical\n        self.params['reject_null'] = reject\n    def generate_explanation(self) -> str:\n        return '一元配置分散分析による検定'\n\nclass NonParametricTestProblem(Problem):\n    def __init__(self):\n        super().__init__('nonparametric_test_problem.tex')\n    def generate_parameters(self):\n        self.params['sample1'] = [round(random.uniform(0, 100), 2) for _ in range(random.randint(5, 15))]\n        self.params['sample2'] = [round(random.uniform(0, 100), 2) for _ in range(random.randint(5, 15))]\n    def generate_explanation(self) -> str:\n        return 'ノンパラ検定で中央値差を検定'\n"
    },
    {
      "path": "your_project\\problem_types\\problem_factory.py",
      "overview": "Pythonコード。\nクラス: ProblemFactory。\n関数: __init__, create_problem。\n",
      "content": "import logging\nimport traceback\nfrom problem_types.problem import (\n    ProbabilityProblem,\n    StatisticalInferenceProblem,\n    RegressionAnalysisProblem,\n    TimeSeriesAnalysisProblem,\n    EconometricsProblem,\n    LinearCombinationProblem,\n    HighMomentProblem,\n    MultivariateNormalProblem,\n    ProbabilityDefinitionProblem,\n    ConditionalProbabilityProblem,\n    #DistributionFunctionsProblem,\n    JointDistributionProblem,\n    VarianceAnalysisProblem,\n    NonParametricTestProblem\n)\nfrom problem_types.distribution_properties_problem import DistributionPropertiesProblem\nfrom problem_types.conjugate_problems import (\n    BetaBinomialConjugateProblem,\n    GammaPoissonConjugateProblem,\n    DirichletMultinomialConjugateProblem,\n    BinomialPoissonApproxProblem,\n    PoissonNormalApproxProblem\n)\n\nclass ProblemFactory:\n    def __init__(self):\n        self.problem_classes = {\n            'probability': ProbabilityProblem,\n            'statistical_inference': StatisticalInferenceProblem,\n            'regression_analysis': RegressionAnalysisProblem,\n            'time_series_analysis': TimeSeriesAnalysisProblem,\n            'econometrics': EconometricsProblem,\n            'linear_combination': LinearCombinationProblem,\n            'distribution_properties': DistributionPropertiesProblem,  # ここを新規追加\n            'high_moment': HighMomentProblem,\n            'multivariate_normal': MultivariateNormalProblem,\n            'probability_definition': ProbabilityDefinitionProblem,\n            'conditional_probability': ConditionalProbabilityProblem,\n            #'#distribution_functions': DistributionFunctionsProblem,\n            'joint_distribution': JointDistributionProblem,\n            't_test': StatisticalInferenceProblem,\n            'variance_analysis': VarianceAnalysisProblem,\n            'nonparametric_test': NonParametricTestProblem,\n            'beta_binomial_conjugate': BetaBinomialConjugateProblem,\n            'gamma_poisson_conjugate': GammaPoissonConjugateProblem,\n            'dirichlet_multinomial_conjugate': DirichletMultinomialConjugateProblem,\n            'binomial_poisson_approx': BinomialPoissonApproxProblem,\n            'poisson_normal_approx': PoissonNormalApproxProblem\n        }\n\n    def create_problem(self, problem_type: str):\n        try:\n            problem_class = self.problem_classes.get(problem_type)\n            if problem_class is None:\n                raise KeyError(f\"Problem type '{problem_type}' is not defined.\")\n            return problem_class()\n        except Exception as e:\n            logging.error(f'問題インスタンス生成エラー: {e}')\n            logging.error(traceback.format_exc())\n            return None\n"
    },
    {
      "path": "your_project\\templates\\distribution_properties_problem.tex",
      "overview": "TeXファイル。行数: 7\n",
      "content": "% distribution_properties_problem.tex\n{% if not show_solution %}\n{{ distribution }} の平均と分散を求めよ。\n{% else %}\n平均: ${{ properties.mean }}$, 分散: ${{ properties.variance }}$\n{{ explanation }}\n{% endif %}\n"
    },
    {
      "path": "your_project\\templates\\distribution_relations.tex",
      "overview": "TeXファイル。行数: 14\n",
      "content": "% distribution_relations.tex\n\n\\section*{Distribution Relations}\n\n- Beta+Binomial -> Beta-Binomial\n\n- Gamma+Poisson -> Negative Binomial\n\n- Dirichlet+Multinomial -> Dirichlet-Multinomial\n\n- Binomial->Poisson approximation\n\n- Poisson->Normal approximation\n\n"
    },
    {
      "path": "your_project\\templates\\econometrics_problem.tex",
      "overview": "TeXファイル。行数: 12\n",
      "content": "% econometrics_problem.tex\n\n{% if not show_solution %}\n\n計量経済学モデルに関する問題\n\n{% else %}\n\n解答と推定量\n\n{{ explanation }}\n\n{% endif %}"
    },
    {
      "path": "your_project\\templates\\high_moment_problem.tex",
      "overview": "TeXファイル。行数: 12\n",
      "content": "% high_moment_problem.tex\n\n{% if not show_solution %}\n\n正規分布の{{ n }}次モーメントは？(例: $E[X^{n}]$ )\n\n{% else %}\n\n$E[X^{n}]={{ moment }}$\n\n{{ explanation }}\n\n{% endif %}"
    },
    {
      "path": "your_project\\templates\\linear_combination_problem.tex",
      "overview": "TeXファイル。行数: 12\n",
      "content": "% linear_combination_problem.tex\n\n{% if not show_solution %}\n\nZ=aX+bY のE[Z],Var[Z]\n\n{% else %}\n\nE[Z],Var[Z]\n\n{{ explanation }}\n\n{% endif %}"
    },
    {
      "path": "your_project\\templates\\multivariate_normal_problem.tex",
      "overview": "TeXファイル。行数: 12\n",
      "content": "% multivariate_normal_problem.tex\n\n{% if not show_solution %}\n\n多変量正規に関する問題\n\n{% else %}\n\n解答\n\n{{ explanation }}\n\n{% endif %}"
    },
    {
      "path": "your_project\\templates\\nonparametric_test_problem.tex",
      "overview": "TeXファイル。行数: 12\n",
      "content": "% nonparametric_test_problem.tex\n\n{% if not show_solution %}\n\nノンパラ検定問題\n\n{% else %}\n\n検定結果\n\n{{ explanation }}\n\n{% endif %}"
    },
    {
      "path": "your_project\\templates\\poisson_normal_approx_problem.tex",
      "overview": "TeXファイル。行数: 12\n",
      "content": "% poisson_normal_approx_problem.tex\n\n{% if not show_solution %}\n\nPoisson→Normal近似\n\n{% else %}\n\nPoisson P=${{ poisson_p }}$, mean=${{mean}}$ var=${{variance}}$\n\n{{ explanation }}\n\n{% endif %}"
    },
    {
      "path": "your_project\\templates\\probability_definition_problem.tex",
      "overview": "TeXファイル。行数: 12\n",
      "content": "% probability_definition_problem.tex\n\n{% if not show_solution %}\n\nP(A∩B)求めよ\n\n{% else %}\n\n結果\n\n{{ explanation }}\n\n{% endif %}"
    },
    {
      "path": "your_project\\templates\\probability_problem.tex",
      "overview": "TeXファイル。行数: 14\n",
      "content": "% probability_problem.tex\n\n{% if not show_solution %}\n\n確率計算問題（例）\n\n問題タイプ: {{ problem_type }}\n\n{% else %}\n\n解答と説明: {{ explanation }}\n\n計算結果: P = {{ probability }}\n\n{% endif %}"
    },
    {
      "path": "your_project\\templates\\regression_analysis_problem.tex",
      "overview": "TeXファイル。行数: 12\n",
      "content": "% regression_analysis_problem.tex\n\n{% if not show_solution %}\n\n回帰分析問題\n\n{% else %}\n\n回帰係数結果\n\n{{ explanation }}\n\n{% endif %}"
    },
    {
      "path": "your_project\\templates\\statistical_inference_problem.tex",
      "overview": "TeXファイル。行数: 12\n",
      "content": "% statistical_inference_problem.tex\n\n{% if not show_solution %}\n\n統計的推定/検定問題\n\n{% else %}\n\n検定結果\n\n{{ explanation }}\n\n{% endif %}"
    },
    {
      "path": "your_project\\templates\\time_series_analysis_problem.tex",
      "overview": "TeXファイル。行数: 12\n",
      "content": "% time_series_analysis_problem.tex\n\n{% if not show_solution %}\n\n時系列分析問題\n\n{% else %}\n\n解答と説明\n\n{{ explanation }}\n\n{% endif %}"
    },
    {
      "path": "your_project\\templates\\variance_analysis_problem.tex",
      "overview": "TeXファイル。行数: 12\n",
      "content": "% variance_analysis_problem.tex\n\n{% if not show_solution %}\n\n分散分析問題\n\n{% else %}\n\nANOVA結果\n\n{{ explanation }}\n\n{% endif %}"
    },
    {
      "path": "collected_scripts (2).json",
      "overview": "JSONファイル (辞書)。キー: system_overview, settings, scripts\n",
      "content": "{\n  \"system_overview\": \"確率統計の網羅的理解\",\n  \"settings\": {\n    \"data_directory\": {\n      \"value\": \"data\",\n      \"description\": \"データディレクトリのパス。\"\n    },\n    \"log_level\": {\n      \"value\": \"INFO\",\n      \"description\": \"ログレベルの設定（例: INFO、DEBUG、ERROR）。\"\n    },\n    \"output_directory\": {\n      \"value\": \"outputs\",\n      \"description\": \"出力ディレクトリのパス。\"\n    },\n    \"database_path\": {\n      \"value\": \"data/data.db\",\n      \"description\": \"データベースのパス。\"\n    },\n    \"problem_templates_directory\": {\n      \"value\": \"templates\",\n      \"description\": \"問題テンプレートディレクトリのパス。\"\n    },\n    \"output_tex_file\": {\n      \"value\": \"practice_problems.tex\",\n      \"description\": \"生成されるLaTeXファイルの名前。\"\n    },\n    \"latex_path\": {\n      \"value\": \"C:/Users/KEN/Desktop/TAROML/texlive/2024/bin/windows/xelatex.exe\",\n      \"description\": \"LaTeXコンパイラのパス。\"\n    },\n    \"cjk_main_font\": {\n      \"value\": \"Yu Gothic\",\n      \"description\": \"使用するCJKフォント。\"\n    },\n    \"problem_types\": {\n      \"value\": {\n        \"probability_definition\": 0.05,\n        \"conditional_probability\": 0.05,\n        \"distribution_functions\": 0.05,\n        \"joint_distribution\": 0.05,\n        \"statistical_inference\": 0.1,\n        \"regression_analysis\": 0.1,\n        \"linear_combination\": 0.1,\n        \"distribution_properties\": 0.1,\n        \"high_moment\": 0.1,\n        \"variance_analysis\": 0.1,\n        \"nonparametric_test\": 0.1,\n        \"beta_binomial_conjugate\": 0.05,\n        \"gamma_poisson_conjugate\": 0.05,\n        \"dirichlet_multinomial_conjugate\": 0.05,\n        \"binomial_poisson_approx\": 0.05,\n        \"poisson_normal_approx\": 0.05\n      },\n      \"description\": \"問題タイプとその割合。\"\n    },\n    \"pdf_generation\": {\n      \"value\": {\n        \"problem_count\": 9\n      },\n      \"description\": \"PDF生成の設定。\"\n    },\n    \"gui_settings\": {\n      \"value\": {\n        \"window_title\": \"統計検定1級 インタラクティブ問題解答システム\",\n        \"font_size\": 14\n      },\n      \"description\": \"GUIの設定。\"\n    },\n    \"enable_visualization\": {\n      \"value\": true,\n      \"description\": \"可視化機能の有効化。\"\n    }\n  },\n  \"scripts\": [\n    {\n      \"path\": \".vscode\\\\launch.json\",\n      \"overview\": \"このファイルはテキストファイルまたはその他の形式です。\\n行数: 17\\n\",\n      \"content\": \"{\\n    // IntelliSense を使用して利用可能な属性を学べます。\\n    // 既存の属性の説明をホバーして表示します。\\n    // 詳細情報は次を確認してください: https://go.microsoft.com/fwlink/?linkid=830387\\n    \\\"version\\\": \\\"0.2.0\\\",\\n    \\\"configurations\\\": [\\n        {\\n            \\\"name\\\": \\\"Python: 現在のファイル\\\",\\n            \\\"type\\\": \\\"python\\\",\\n            \\\"request\\\": \\\"launch\\\",\\n            \\\"program\\\": \\\"${file}\\\",\\n            \\\"console\\\": \\\"integratedTerminal\\\",\\n            \\\"justMyCode\\\": true,\\n            \\\"args\\\": [\\\"--generate-pdf\\\"]\\n\\n        }\\n    ]\\n}\"\n    },\n    {\n      \"path\": \"統計検定1級\\\\config.json\",\n      \"overview\": \"このファイルはテキストファイルまたはその他の形式です。\\n行数: 37\\n\",\n      \"content\": \"\"\n    },\n    {\n      \"path\": \"統計検定1級\\\\config.py\",\n      \"overview\": \"このスクリプトはPythonコードです。\\n定義されているクラス: Config。\\n定義されている関数: __init__, load_config, get。\\n\",\n      \"content\": \"# config.py\\nimport json, os\\nclass Config:\\n    def __init__(self, config_file='config.json'):\\n        script_dir=os.path.dirname(os.path.abspath(__file__))\\n        self.config_file=os.path.join(script_dir,config_file)\\n        self.settings=self.load_config()\\n    def load_config(self):\\n        if not os.path.exists(self.config_file):\\n            # ファイルが無ければ空dict\\n            return {}\\n        try:\\n            with open(self.config_file,'r',encoding='utf-8')as f:\\n                content=f.read().strip()\\n                if not content:\\n                    # 空ファイルなら{}扱い\\n                    return {}\\n                return json.loads(content)\\n        except Exception:\\n            # JSONパース失敗時も{}\\n            return {}\\n    def get(self,*keys,default=None):\\n        data=self.settings\\n        for k in keys:\\n            if isinstance(data, dict) and k in data:\\n                data=data[k]\\n            else:\\n                return default\\n        return data\\n\\nconfig=Config()\"\n    },\n    {\n      \"path\": \"統計検定1級\\\\database.py\",\n      \"overview\": \"このスクリプトはPythonコードです。\\n定義されているクラス: DatabaseManager。\\n定義されている関数: __init__, setup_database, save_problem。\\n\",\n      \"content\": \"# database.py\\nimport sqlite3,os\\nfrom config import config\\nclass DatabaseManager:\\n    def __init__(self):\\n        script_dir=os.path.dirname(os.path.abspath(__file__))\\n        db_path=config.get('settings','database_path',default='data/data.db')\\n        if db_path is None:\\n            db_path='data/data.db'\\n        self.db_name=os.path.join(script_dir,db_path)\\n        os.makedirs(os.path.dirname(self.db_name), exist_ok=True)\\n    def setup_database(self):\\n        conn=sqlite3.connect(self.db_name)\\n        c=conn.cursor()\\n        c.execute('CREATE TABLE IF NOT EXISTS problems (id TEXT PRIMARY KEY,date_created TEXT,problem_text TEXT,solution_text TEXT,problem_type TEXT)')\\n        conn.commit()\\n        conn.close()\\n    def save_problem(self,problem_id,date_created,problem_text,solution_text,problem_type):\\n        conn=sqlite3.connect(self.db_name)\\n        c=conn.cursor()\\n        c.execute('INSERT INTO problems VALUES (?,?,?,?,?)',(problem_id,date_created,problem_text,solution_text,problem_type))\\n        conn.commit()\\n        conn.close()\"\n    },\n    {\n      \"path\": \"統計検定1級\\\\graph.py\",\n      \"overview\": \"このスクリプトはPythonコードです。\\n定義されているクラス: ProbabilityDistributionVisualizer。\\n定義されている関数: __init__, plot_probability, plot_t_test, plot_regression, plot_time_series, plot_econometrics, plot_multivariate_normal, plot_distribution_properties, plot_high_moment, plot_variance_analysis, plot_nonparametric_test, plot_linear_combination, confidence_ellipse, mvn_pdf, ecdf。\\n\",\n      \"content\": \"# graph.py\\nimport matplotlib\\nmatplotlib.use('Agg')\\nimport matplotlib.pyplot as plt\\nimport math\\nimport numpy as np\\nimport logging\\nimport os\\nfrom math import factorial, exp\\nfrom scipy.stats import norm, t as t_dist, f as f_dist, poisson, expon\\nfrom statsmodels.graphics.tsaplots import plot_acf\\nimport traceback\\n\\nclass ProbabilityDistributionVisualizer:\\n    def __init__(self):\\n        pass\\n\\n    def plot_probability(self, params, output_path):\\n        # PMF + CDF、kを強調\\n        ptype = params.get('problem_type')\\n        try:\\n            if ptype == 'binomial':\\n                p=params['p']\\n                n=params['n']\\n                k=params['k']\\n                x=range(n+1)\\n                from math import comb\\n                pmf=[comb(n,i)*(p**i)*((1-p)**(n-i)) for i in x]\\n                cdf=np.cumsum(pmf)\\n\\n                plt.figure(figsize=(10,5))\\n                plt.subplot(1,2,1)\\n                plt.bar(x,pmf,color='skyblue')\\n                plt.title(f'Binomial PMF n={n},p={p}')\\n                plt.xlabel('k')\\n                plt.ylabel('P(X=k)')\\n                plt.axvline(k,color='red',linestyle='--',label='Target k')\\n                plt.legend()\\n\\n                plt.subplot(1,2,2)\\n                plt.step(x,cdf,where='post',color='green')\\n                plt.title('CDF')\\n                plt.xlabel('k')\\n                plt.ylabel('P(X<=k)')\\n                plt.axvline(k,color='red',linestyle='--')\\n                plt.tight_layout()\\n                plt.savefig(output_path)\\n                plt.close()\\n                return True\\n            elif ptype == 'poisson':\\n                lam=params['lambda']\\n                k=params['k']\\n                x=range(k+10+1)\\n                pmf=[(lam**i)*exp(-lam)/factorial(i) for i in x]\\n                cdf=np.cumsum(pmf)\\n\\n                plt.figure(figsize=(10,5))\\n                plt.subplot(1,2,1)\\n                plt.bar(x,pmf,color='orange')\\n                plt.title(f'Poisson(lambda={lam}) PMF')\\n                plt.xlabel('k')\\n                plt.ylabel('P(X=k)')\\n                plt.axvline(k,color='red',linestyle='--',label='Target k')\\n                plt.legend()\\n\\n                plt.subplot(1,2,2)\\n                plt.step(x,cdf,where='post',color='green')\\n                plt.title('CDF')\\n                plt.xlabel('k')\\n                plt.ylabel('P(X<=k)')\\n                plt.axvline(k,color='red',linestyle='--')\\n                plt.tight_layout()\\n                plt.savefig(output_path)\\n                plt.close()\\n                return True\\n            elif ptype == 'conditional_probability':\\n                # 条件付き確率の場合は特別なグラフ(分割表)を可視化\\n                P_A=params['P_A']\\n                P_BA=params['P_B_given_A']\\n                P_AB=params['P_A_and_B']\\n                # 簡単な棒グラフでP(A),P(B|A),P(A∩B)の関係を表示\\n                plt.figure()\\n                vals=[P_A,P_BA,P_AB]\\n                labels=['P(A)','P(B|A)','P(A∩B)']\\n                plt.bar(labels,vals,color=['blue','green','red'])\\n                plt.title('Conditional Probability Visualization')\\n                plt.ylabel('Probability')\\n                plt.tight_layout()\\n                plt.savefig(output_path)\\n                plt.close()\\n                return True\\n            else:\\n                # 未知タイプには何もしない\\n                return False\\n        except Exception as e:\\n            logging.error(f\\\"Error in plot_probability: {e}\\\")\\n            return False\\n\\n    def plot_t_test(self, params, output_path):\\n        # t分布+棄却域+標本平均から計算されたt値など\\n        # 追加で、標準正規近似や、p値部分の色塗りなど\\n        try:\\n            alpha = params['alpha']\\n            df = params['df']\\n            t_stat = params['t_stat']\\n            critical_value = params['critical_value']\\n\\n            x = np.linspace(t_dist.ppf(0.001, df), t_dist.ppf(0.999, df), 1000)\\n            y = t_dist.pdf(x, df)\\n\\n            plt.figure(figsize=(10,5))\\n            plt.plot(x, y, 'b-', label=f't-dist(df={df})')\\n            # 臨界値に線\\n            plt.axvline(x=critical_value,color='r',linestyle='--',label='critical +')\\n            plt.axvline(x=-critical_value,color='r',linestyle='--',label='critical -')\\n            # t統計量\\n            plt.axvline(x=t_stat,color='g',label='t-stat')\\n\\n            # p値領域を色付け\\n            # 両側検定として|t|>crit\\n            p_area_x = x[x>critical_value]\\n            plt.fill_between(p_area_x,t_dist.pdf(p_area_x,df),color='red',alpha=0.3)\\n            p_area_x2 = x[x<-critical_value]\\n            plt.fill_between(p_area_x2,t_dist.pdf(p_area_x2,df),color='red',alpha=0.3)\\n\\n            plt.title('t-test visualization')\\n            plt.xlabel('t')\\n            plt.ylabel('pdf')\\n            plt.legend()\\n            plt.tight_layout()\\n\\n            # 保存\\n            plt.savefig(output_path)\\n            plt.close()\\n            return True\\n        except Exception as e:\\n            logging.error(f\\\"t検定グラフ生成エラー: {e}\\\")\\n            return False\\n\\n    def plot_regression(self, x_values, y_values, beta_0_hat, beta_1_hat, output_path):\\n        # 回帰直線+データ点 + 残差ヒスト + Q-Qプロットなど複数図\\n        try:\\n            import statsmodels.api as sm\\n            X = sm.add_constant(x_values)\\n            model = sm.OLS(y_values, X).fit()\\n            residuals = model.resid\\n\\n            fig,axes = plt.subplots(2,2,figsize=(10,10))\\n\\n            # Scatter + regression line\\n            ax=axes[0,0]\\n            ax.scatter(x_values,y_values,color='blue',label='data')\\n            x_line=np.linspace(min(x_values),max(x_values),100)\\n            y_line=beta_0_hat+beta_1_hat*x_line\\n            ax.plot(x_line,y_line,color='red',label='reg line')\\n            ax.set_title('Data & Regression Line')\\n            ax.set_xlabel('X')\\n            ax.set_ylabel('Y')\\n            ax.legend()\\n\\n            # Residual histogram\\n            ax=axes[0,1]\\n            ax.hist(residuals,bins=20,color='green',alpha=0.7)\\n            ax.set_title('Residual Histogram')\\n            ax.set_xlabel('Residual')\\n            ax.set_ylabel('Frequency')\\n\\n            # Q-Q plot of residuals\\n            sm.qqplot(residuals, line='45', ax=axes[1,0],color='purple')\\n            axes[1,0].set_title('Q-Q plot of Residuals')\\n\\n            # Residuals vs fitted\\n            fitted=model.fittedvalues\\n            ax=axes[1,1]\\n            ax.scatter(fitted,residuals,color='orange')\\n            ax.axhline(y=0,color='red',linestyle='--')\\n            ax.set_title('Residuals vs Fitted')\\n            ax.set_xlabel('Fitted')\\n            ax.set_ylabel('Residuals')\\n\\n            fig.tight_layout()\\n            fig.savefig(output_path)\\n            plt.close(fig)\\n            return True\\n        except Exception as e:\\n            logging.error(f\\\"回帰分析グラフ生成中にエラー: {e}\\\")\\n            return False\\n\\n    def plot_time_series(self, params, output_path):\\n        # 時系列データ + ACFプロット\\n        try:\\n            ts = params['time_series']\\n\\n            fig,axes=plt.subplots(2,1,figsize=(10,8))\\n            axes[0].plot(ts, color='blue')\\n            axes[0].set_title('Time Series Data')\\n            axes[0].set_xlabel('Time')\\n            axes[0].set_ylabel('Value')\\n\\n            plot_acf(ts,ax=axes[1])\\n            axes[1].set_title('Autocorrelation Function')\\n\\n            fig.tight_layout()\\n            fig.savefig(output_path)\\n            plt.close(fig)\\n            return True\\n        except Exception as e:\\n            logging.error(f\\\"時系列分析グラフエラー: {e}\\\")\\n            return False\\n\\n    def plot_econometrics(self, params, output_path):\\n        # X1,YやX2,Y 散布図 + 残差分析\\n        try:\\n            import statsmodels.api as sm\\n            X = np.column_stack((params['x1_values'], params['x2_values']))\\n            Y = np.array(params['y_values'])\\n            Xc = sm.add_constant(X)\\n            model = sm.OLS(Y,Xc).fit()\\n            residuals = model.resid\\n            fitted = model.fittedvalues\\n\\n            fig,axes=plt.subplots(2,2,figsize=(10,10))\\n\\n            # X1 vs Y\\n            ax=axes[0,0]\\n            ax.scatter(params['x1_values'],Y,color='blue',alpha=0.7,label='X1-Y')\\n            ax.set_title('X1 vs Y')\\n            ax.set_xlabel('X1')\\n            ax.set_ylabel('Y')\\n\\n            # X2 vs Y\\n            ax=axes[0,1]\\n            ax.scatter(params['x2_values'],Y,color='green',alpha=0.7,label='X2-Y')\\n            ax.set_title('X2 vs Y')\\n            ax.set_xlabel('X2')\\n            ax.set_ylabel('Y')\\n\\n            # Residuals histogram\\n            ax=axes[1,0]\\n            ax.hist(residuals,bins=20,color='gray',alpha=0.7)\\n            ax.set_title('Residuals Histogram')\\n            ax.set_xlabel('Residual')\\n\\n            # Residuals vs Fitted\\n            ax=axes[1,1]\\n            ax.scatter(fitted,residuals,color='red',alpha=0.7)\\n            ax.axhline(y=0,color='black',linestyle='--')\\n            ax.set_title('Residuals vs Fitted')\\n            ax.set_xlabel('Fitted')\\n            ax.set_ylabel('Residuals')\\n\\n            fig.tight_layout()\\n            fig.savefig(output_path)\\n            plt.close(fig)\\n            return True\\n        except Exception as e:\\n            logging.error(f\\\"計量経済学グラフエラー: {e}\\\")\\n            return False\\n\\n    def plot_multivariate_normal(self, params, output_path):\\n        # 信頼楕円 + 等高線 + 周辺分布\\n        try:\\n            mu = params['mu']\\n            sigma = np.array(params['sigma'])\\n\\n            fig = plt.figure(figsize=(10,10))\\n            from matplotlib.patches import Ellipse\\n            import matplotlib.transforms as transforms\\n\\n            def confidence_ellipse(mu,cov,ax,n_std=1.96,facecolor='none',**kwargs):\\n                pearson = cov[0,1]/np.sqrt(cov[0,0]*cov[1,1])\\n                ell_radius_x = np.sqrt(1+pearson)\\n                ell_radius_y = np.sqrt(1-pearson)\\n                ellipse=Ellipse((0,0),width=ell_radius_x*2,height=ell_radius_y*2,facecolor=facecolor,**kwargs)\\n                scale_x = np.sqrt(cov[0,0])*n_std\\n                scale_y = np.sqrt(cov[1,1])*n_std\\n                transf = transforms.Affine2D().rotate_deg(45).scale(scale_x,scale_y).translate(mu[0],mu[1])\\n                ellipse.set_transform(transf+ax.transData)\\n                return ax.add_patch(ellipse)\\n\\n            ax=fig.add_subplot(2,2,1)\\n            ax.set_title('Confidence Ellipse')\\n            confidence_ellipse(mu,sigma,ax,edgecolor='red')\\n            ax.scatter(mu[0],mu[1],c='blue',marker='x',label='mean')\\n            ax.legend()\\n            ax.set_xlabel('X1')\\n            ax.set_ylabel('X2')\\n\\n            # 等高線\\n            ax2=fig.add_subplot(2,2,2)\\n            x = np.linspace(mu[0]-4*np.sqrt(sigma[0,0]),mu[0]+4*np.sqrt(sigma[0,0]),100)\\n            y = np.linspace(mu[1]-4*np.sqrt(sigma[1,1]),mu[1]+4*np.sqrt(sigma[1,1]),100)\\n            X,Y = np.meshgrid(x,y)\\n            pos = np.dstack((X,Y))\\n            def mvn_pdf(x, mu, cov):\\n                n=2\\n                det=np.linalg.det(cov)\\n                inv=np.linalg.inv(cov)\\n                diff=(x - mu)\\n                return (1./(2*np.pi*np.sqrt(det)))*np.exp(-0.5*(diff@inv@diff.T))\\n            Z=np.empty(X.shape)\\n            for i in range(X.shape[0]):\\n                for j in range(X.shape[1]):\\n                    Z[i,j]=mvn_pdf(np.array([X[i,j],Y[i,j]]),np.array(mu),sigma)\\n            ax2.contour(X,Y,Z,levels=5,cmap='Blues')\\n            ax2.set_title('Contour')\\n\\n            # 周辺分布\\n            ax3=fig.add_subplot(2,2,3)\\n            # marginal X1\\n            X_marg = norm(loc=mu[0],scale=np.sqrt(sigma[0,0]))\\n            x_line = np.linspace(mu[0]-4*np.sqrt(sigma[0,0]),mu[0]+4*np.sqrt(sigma[0,0]),100)\\n            y_line = X_marg.pdf(x_line)\\n            ax3.plot(x_line,y_line,'r-')\\n            ax3.set_title('Marginal X1 distribution')\\n            ax3.set_xlabel('X1')\\n            ax3.set_ylabel('pdf')\\n\\n            ax4=fig.add_subplot(2,2,4)\\n            Y_marg=norm(loc=mu[1],scale=np.sqrt(sigma[1,1]))\\n            y_line=Y_marg.pdf(x_line)\\n            ax4.plot(x_line,y_line,'g-')\\n            ax4.set_title('Marginal X2 distribution')\\n            ax4.set_xlabel('X2')\\n            ax4.set_ylabel('pdf')\\n\\n            fig.tight_layout()\\n            fig.savefig(output_path)\\n            plt.close(fig)\\n            return True\\n        except Exception as e:\\n            logging.error(f\\\"多変量正規分布グラフエラー: {e}\\\")\\n            return False\\n\\n    def plot_distribution_properties(self, params, output_path):\\n        # 選択された分布に対し、PDF+平均分散表示ライン\\n        try:\\n            dist = params['distribution']\\n            plt.figure(figsize=(10,5))\\n            if dist=='正規分布':\\n                mu=0; sigma=1\\n                x=np.linspace(mu-4*sigma,mu+4*sigma,200)\\n                y=norm.pdf(x,mu,sigma)\\n                plt.plot(x,y,'b-')\\n                plt.axvline(mu,color='r',linestyle='--',label='mean')\\n                plt.axvline(mu+sigma,color='g',linestyle=':',label='mean+sigma')\\n                plt.axvline(mu-sigma,color='g',linestyle=':')\\n                plt.title('Normal Distribution (mu=0,sigma=1)')\\n                plt.legend()\\n            elif dist=='ポアソン分布':\\n                lam=3\\n                x=np.arange(0,15)\\n                y=poisson.pmf(x,lam)\\n                plt.bar(x,y,color='skyblue')\\n                plt.axvline(lam,color='r',linestyle='--',label='mean=lambda=3')\\n                plt.title('Poisson(lambda=3)')\\n                plt.legend()\\n            elif dist=='指数分布':\\n                lam=1\\n                x=np.linspace(0,5,200)\\n                y=expon.pdf(x,scale=1/lam)\\n                plt.plot(x,y,'b-')\\n                plt.axvline(1/lam,color='r',linestyle='--',label='mean=1/lambda')\\n                plt.title('Exponential(lambda=1)')\\n                plt.legend()\\n            plt.tight_layout()\\n            plt.savefig(output_path)\\n            plt.close()\\n            return True\\n        except Exception as e:\\n            logging.error(f\\\"分布性質グラフエラー: {e}\\\")\\n            return False\\n\\n    def plot_high_moment(self, params, output_path):\\n        # 正規分布pdf上に平均、±σ、n次モーメント近傍など\\n        try:\\n            mu=params['mu']\\n            sigma=params['sigma']\\n            n=params['n']\\n            moment=params['moment']\\n            x=np.linspace(mu-4*sigma,mu+4*sigma,200)\\n            y=norm.pdf(x,mu,sigma)\\n\\n            plt.figure(figsize=(10,5))\\n            plt.plot(x,y,'b-',label='Normal pdf')\\n            plt.axvline(mu,color='r',linestyle='--',label='mean')\\n            plt.axvline(mu+sigma,color='g',linestyle=':',label='mu±sigma')\\n            plt.axvline(mu-sigma,color='g',linestyle=':')\\n            plt.title(f'Normal distribution (mu={mu}, sigma={sigma}). n={n}-th moment={moment}')\\n            plt.legend()\\n            plt.tight_layout()\\n            plt.savefig(output_path)\\n            plt.close()\\n            return True\\n        except Exception as e:\\n            logging.error(f\\\"高次モーメントグラフエラー: {e}\\\")\\n            return False\\n\\n    def plot_variance_analysis(self, params, output_path):\\n        # 箱ひげ図など\\n        try:\\n            group_count=params['group_count']\\n            sample_sizes=params['sample_sizes']\\n            means=params['means']\\n            variances=params['variances']\\n\\n            # データを仮に正規生成して可視化\\n            # 単に箱ひげ図で分布の違いを視覚化\\n            data=[]\\n            for i in range(group_count):\\n                # 各グループ：mean, varから乱数生成\\n                np.random.seed(i)\\n                samples=np.random.normal(means[i],np.sqrt(variances[i]),sample_sizes[i])\\n                data.append(samples)\\n\\n            plt.figure(figsize=(8,6))\\n            plt.boxplot(data,labels=[f'Group{i+1}' for i in range(group_count)])\\n            plt.title('ANOVA: Boxplots of groups')\\n            plt.ylabel('Value')\\n            plt.tight_layout()\\n            plt.savefig(output_path)\\n            plt.close()\\n            return True\\n        except Exception as e:\\n            logging.error(f\\\"分散分析グラフエラー: {e}\\\")\\n            return False\\n\\n    def plot_nonparametric_test(self, params, output_path):\\n        # ノンパラ検定：2サンプルECDFなど\\n        try:\\n            s1=params['sample1']\\n            s2=params['sample2']\\n\\n            def ecdf(data):\\n                d_sorted = np.sort(data)\\n                y = np.arange(1,len(d_sorted)+1)/len(d_sorted)\\n                return d_sorted,y\\n\\n            x1,y1=ecdf(s1)\\n            x2,y2=ecdf(s2)\\n\\n            plt.figure(figsize=(8,6))\\n            plt.step(x1,y1,where='post',label='Sample1 ECDF',color='blue')\\n            plt.step(x2,y2,where='post',label='Sample2 ECDF',color='red')\\n            plt.title('Nonparametric Test Visualization')\\n            plt.xlabel('Value')\\n            plt.ylabel('ECDF')\\n            plt.legend()\\n            plt.tight_layout()\\n            plt.savefig(output_path)\\n            plt.close()\\n            return True\\n        except Exception as e:\\n            logging.error(f\\\"ノンパラ検定グラフエラー: {e}\\\")\\n            return False\\n\\n    def plot_linear_combination(self, params, output_path):\\n        # 合成Z=aX+bYをシミュレーション\\n        try:\\n            a=params['a']\\n            b=params['b']\\n            mu1=params['mu1']\\n            mu2=params['mu2']\\n            sig1=params['sigma1_squared']\\n            sig2=params['sigma2_squared']\\n            # シミュレーションでZ生成\\n            np.random.seed(123)\\n            x = np.random.normal(mu1, np.sqrt(sig1),1000)\\n            y = np.random.normal(mu2, np.sqrt(sig2),1000)\\n            Z=a*x+b*y\\n            plt.figure(figsize=(8,6))\\n            plt.hist(Z,bins=30,density=True,alpha=0.7,color='purple',label='Simulated Z')\\n            # 理論分布：N(a*mu1+b*mu2, a²sigma1+b²sigma2)\\n            E_Z=a*mu1+b*mu2\\n            Var_Z=a**2*sig1+b**2*sig2\\n            X_line=np.linspace(E_Z-4*np.sqrt(Var_Z),E_Z+4*np.sqrt(Var_Z),200)\\n            Y_line=norm.pdf(X_line,E_Z,np.sqrt(Var_Z))\\n            plt.plot(X_line,Y_line,'r-',label='Theoretical PDF')\\n            plt.title('Linear Combination Distribution')\\n            plt.xlabel('Z')\\n            plt.ylabel('Density')\\n            plt.legend()\\n            plt.tight_layout()\\n            plt.savefig(output_path)\\n            plt.close()\\n            return True\\n        except Exception as e:\\n            logging.error(f\\\"線形結合グラフエラー: {e}\\\")\\n            return False\\n\"\n    },\n    {\n      \"path\": \"統計検定1級\\\\gui.py\",\n      \"overview\": \"このスクリプトはPythonコードです。\\n定義されているクラス: InteractiveSolverGUI。\\n定義されている関数: __init__, run。\\n\",\n      \"content\": \"# gui.py\\nimport tkinter as tk\\nclass InteractiveSolverGUI:\\n    def __init__(self):\\n        self.root=tk.Tk()\\n        self.root.title('統計検定1級 インタラクティブ問題解答システム')\\n    def run(self):\\n        self.root.mainloop()\"\n    },\n    {\n      \"path\": \"統計検定1級\\\\main.py\",\n      \"overview\": \"このスクリプトはPythonコードです。\\n定義されている関数: main。\\n\",\n      \"content\": \"# main.py\\nimport sys\\nfrom main_app import MainApp\\nfrom gui import InteractiveSolverGUI\\nfrom config import config\\n\\ndef main():\\n    pdf_gen=config.get('settings','pdf_generation',default={})\\n    problem_count=pdf_gen.get('problem_count',9)\\n\\n    if len(sys.argv)>1 and sys.argv[1]=='--generate-pdf':\\n        app=MainApp()\\n        app.generate_and_compile(problem_count)\\n    else:\\n        gui=InteractiveSolverGUI()\\n        gui.run()\\n\\nif __name__=='__main__':\\n    main()\\n\"\n    },\n    {\n      \"path\": \"統計検定1級\\\\main_app.py\",\n      \"overview\": \"このスクリプトはPythonコードです。\\n定義されているクラス: MainApp。\\n定義されている関数: __init__, generate_latex_header, generate_and_compile, compile_latex。\\n\",\n      \"content\": \"# main_app.py\\nimport os,subprocess\\nfrom config import config\\nfrom problem_generator import ProblemGenerator\\nimport logging\\nimport traceback\\n\\nclass MainApp:\\n    def __init__(self):\\n        # 必要設定を__init__でまとめて取得\\n        self.output_tex_file = config.get('settings','output_tex_file',default='practice_problems.tex')\\n        if self.output_tex_file is None:\\n            self.output_tex_file='practice_problems.tex'\\n\\n        self.enable_visualization = config.get('settings','enable_visualization',default=True)\\n        self.problem_templates_dir = config.get('settings','problem_templates_directory',default='templates')\\n        self.output_directory = config.get('settings','output_directory',default='outputs')\\n        self.latex_path = config.get('settings','latex_path',default='xelatex')\\n\\n        # ProblemGeneratorの生成\\n        self.generator = ProblemGenerator()\\n\\n    def generate_latex_header(self):\\n        cjk_font=config.get('settings','cjk_main_font',default='Yu Gothic')\\n        if cjk_font is None:\\n            cjk_font='Yu Gothic'\\n        header=[\\n            '\\\\\\\\documentclass{article}',\\n            '\\\\\\\\usepackage{amsmath}',\\n            '\\\\\\\\usepackage{amssymb}',\\n            '\\\\\\\\usepackage{graphicx}',\\n            '\\\\\\\\usepackage{float}',\\n            '\\\\\\\\usepackage{geometry}',\\n            '\\\\\\\\usepackage{xeCJK}',\\n            '\\\\\\\\usepackage{fontspec}',\\n            '\\\\\\\\setmainfont{Times New Roman}',\\n            f'\\\\\\\\setCJKmainfont{{{cjk_font}}}',\\n            '\\\\\\\\geometry{a4paper, margin=1in}',\\n            '\\\\\\\\begin{document}'\\n        ]\\n        return header\\n\\n    def generate_and_compile(self,problem_count):\\n        script_dir=os.path.dirname(os.path.abspath(__file__))\\n        latex_content=self.generate_latex_header()\\n\\n        out_dir = self.output_directory\\n        if out_dir is None:\\n            out_dir='outputs'\\n        output_dir=os.path.join(script_dir,out_dir)\\n        os.makedirs(output_dir,exist_ok=True)\\n        tex_file_path=os.path.join(output_dir,self.output_tex_file)\\n\\n        for idx in range(1,problem_count+1):\\n            try:\\n                result=self.generator.generate_problem()\\n                if result:\\n                    problem_id,date_created,problem_type,problem_text,solution_text,graph_filename=result\\n\\n                    latex_content.append(f'\\\\\\\\section*{{問題 {idx}}}')\\n                    latex_content.append(problem_text)\\n\\n                    if self.enable_visualization and graph_filename:\\n                        latex_content.append('\\\\\\\\begin{figure}[H]')\\n                        latex_content.append('\\\\\\\\centering')\\n                        graph_relative_path=os.path.join('graphs',graph_filename).replace('\\\\\\\\','/')\\n                        latex_content.append(f'\\\\\\\\includegraphics[width=0.8\\\\\\\\textwidth]{{{graph_relative_path}}}')\\n                        latex_content.append('\\\\\\\\end{figure}')\\n\\n                    latex_content.append('\\\\\\\\subsection*{解答}')\\n                    latex_content.append(solution_text)\\n                    latex_content.append('\\\\\\\\newpage')\\n                else:\\n                    logging.warning(f\\\"問題 {idx} の生成に失敗しました。\\\")\\n                    print(f\\\"問題 {idx} の生成に失敗しました。\\\")\\n            except Exception as e:\\n                logging.error(f\\\"問題 {idx} の生成中にエラー: {e}\\\")\\n                logging.error(traceback.format_exc())\\n                print(f\\\"問題 {idx} 生成エラー。ログを確認\\\")\\n\\n        templates_dir = self.problem_templates_dir\\n        if templates_dir is None:\\n            templates_dir='templates'\\n        latex_content.append('\\\\\\\\clearpage')\\n        latex_content.append('\\\\\\\\input{../'+templates_dir+'/distribution_relations.tex}')\\n\\n        latex_content.append('\\\\\\\\end{document}')\\n\\n        with open(tex_file_path,'w',encoding='utf-8')as tex_file:\\n            tex_file.write('\\\\n'.join(latex_content))\\n\\n        self.compile_latex(tex_file_path)\\n\\n    def compile_latex(self,tex_file_path):\\n        tex_file_name=os.path.basename(tex_file_path)\\n        latex_path=self.latex_path\\n        if not latex_path or not os.path.exists(latex_path):\\n            print(f\\\"LaTeXコンパイラが見つかりません: {latex_path}\\\")\\n            return\\n        process=subprocess.run([latex_path,'-interaction=nonstopmode',tex_file_name],cwd=os.path.dirname(tex_file_path),stdout=subprocess.PIPE,stderr=subprocess.PIPE,text=True)\\n        log_file_path=os.path.join(os.path.dirname(tex_file_path),'latex_compile.log')\\n        with open(log_file_path,'w',encoding='utf-8')as f:\\n            f.write(process.stdout or '')\\n            f.write(process.stderr or '')\\n        if process.returncode!=0:\\n            print(\\\"LaTeXコンパイルでエラー\\\")\\n        else:\\n            pdf_file=tex_file_path.replace('.tex','.pdf')\\n            if os.path.exists(pdf_file):\\n                print(f\\\"PDF生成成功: {pdf_file}\\\")\\n            else:\\n                print(\\\"PDFファイル未発見\\\")\\n\"\n    },\n    {\n      \"path\": \"統計検定1級\\\\problem_generator.py\",\n      \"overview\": \"このスクリプトはPythonコードです。\\n定義されているクラス: ProblemGenerator。\\n定義されている関数: __init__, load_topics, get_problem_types_by_topic, generate_problem。\\n\",\n      \"content\": \"# problem_generator.py\\nimport random,os,logging,uuid\\nfrom datetime import datetime\\nfrom config import config\\nfrom database import DatabaseManager\\nfrom problem_types.problem_factory import ProblemFactory\\nimport json\\nimport traceback\\n\\nclass ProblemGenerator:\\n    def __init__(self):\\n        self.db_path = config.get('settings','database_path',default='data/data.db')\\n        self.output_directory = config.get('settings','output_directory',default='outputs')\\n        self.enable_visualization = config.get('settings','enable_visualization',default=True)\\n        self.problem_types_weights = config.get('settings','problem_types',default={})\\n\\n        self.factory=ProblemFactory()\\n        self.db_manager=DatabaseManager()\\n        self.db_manager.setup_database()\\n\\n        script_dir=os.path.dirname(os.path.abspath(__file__))\\n        out_dir=self.output_directory\\n        if out_dir is None:\\n            out_dir='outputs'\\n        self.output_dir=os.path.join(script_dir,out_dir,'graphs')\\n        os.makedirs(self.output_dir,exist_ok=True)\\n        self.topics_data=self.load_topics()\\n\\n    def load_topics(self):\\n        script_dir=os.path.dirname(os.path.abspath(__file__))\\n        topics_file=os.path.join(script_dir,'topics.json')\\n        if not os.path.exists(topics_file):\\n            raise FileNotFoundError(f\\\"'{topics_file}'がない\\\")\\n        with open(topics_file,'r',encoding='utf-8')as f:\\n            return json.load(f)\\n\\n    def get_problem_types_by_topic(self,topic):\\n        return self.topics_data['topics'].get(topic,[])\\n\\n    def generate_problem(self,selected_topic=None):\\n        try:\\n            ptypes=self.problem_types_weights\\n            if selected_topic:\\n                problem_types=self.get_problem_types_by_topic(selected_topic)\\n                if not problem_types:\\n                    return None\\n                problem_type=random.choice(problem_types)\\n            else:\\n                pts=list(ptypes.keys())\\n                pwt=list(ptypes.values())\\n                if not pts:\\n                    pts=['probability']\\n                    pwt=[1.0]\\n                problem_type=random.choices(pts,weights=pwt,k=1)[0]\\n\\n            problem=self.factory.create_problem(problem_type)\\n            problem.generate_parameters()\\n            problem_text=problem.generate_problem_text()\\n            solution_text=problem.generate_solution_text()\\n\\n            enable_vis=self.enable_visualization\\n            if enable_vis is None:\\n                enable_vis=True\\n\\n            graph_filename=None\\n            if enable_vis:\\n                graph_filename=f\\\"graph_{uuid.uuid4().hex}.png\\\"\\n                graph_filepath=os.path.join(self.output_dir,graph_filename)\\n                if not problem.generate_graph(graph_filepath):\\n                    graph_filename=None\\n\\n            problem_id=uuid.uuid4().hex\\n            date_created=datetime.now().strftime('%Y-%m-%d %H:%M:%S')\\n\\n            self.db_manager.save_problem(problem_id,date_created,problem_text,solution_text,problem_type)\\n\\n            return problem_id,date_created,problem_type,problem_text,solution_text,graph_filename\\n        except Exception as e:\\n            logging.error(f\\\"問題生成エラー: {e}\\\")\\n            logging.error(traceback.format_exc())\\n            return None\\n\"\n    },\n    {\n      \"path\": \"統計検定1級\\\\sympy_solver.py\",\n      \"overview\": \"このスクリプトはPythonコードです。\\n定義されているクラス: SympySolver。\\n定義されている関数: __init__, check_equivalence。\\n\",\n      \"content\": \"# sympy_solver.py\\nclass SympySolver:\\n    def __init__(self):\\n        pass\\n    def check_equivalence(self,user_input,correct_answer):\\n        # 簡略化:常にFalseでエラーなし\\n        return False,\\\"\\\"\"\n    },\n    {\n      \"path\": \"統計検定1級\\\\topics.json\",\n      \"overview\": \"このファイルはテキストファイルまたはその他の形式です。\\n行数: 28\\n\",\n      \"content\": \"{\\n  \\\"topics\\\": {\\n    \\\"確率論\\\": [\\n      \\\"probability_definition\\\",\\n      \\\"conditional_probability\\\",\\n      \\\"distribution_functions\\\",\\n      \\\"joint_distribution\\\",\\n      \\\"probability\\\"\\n    ],\\n    \\\"統計的推定\\\": [\\n      \\\"statistical_inference\\\",\\n      \\\"t_test\\\"\\n    ],\\n    \\\"統計的検定\\\": [\\n      \\\"statistical_inference\\\",\\n      \\\"t_test\\\"\\n    ],\\n    \\\"回帰分析\\\": [\\n      \\\"regression_analysis\\\"\\n    ],\\n    \\\"分散分析\\\": [\\n      \\\"variance_analysis\\\"\\n    ],\\n    \\\"ノンパラメトリック検定\\\": [\\n      \\\"nonparametric_test\\\"\\n    ]\\n  }\\n}\\n\"\n    },\n    {\n      \"path\": \"統計検定1級\\\\__init__.py\",\n      \"overview\": \"このスクリプトはPythonコードです。\\n\",\n      \"content\": \"\"\n    },\n    {\n      \"path\": \"統計検定1級\\\\.vscode\\\\launch.json\",\n      \"overview\": \"このファイルはテキストファイルまたはその他の形式です。\\n行数: 14\\n\",\n      \"content\": \"{\\n    \\\"version\\\": \\\"0.2.0\\\",\\n    \\\"configurations\\\": [\\n        {\\n            \\\"name\\\": \\\"Python: 現在のファイル\\\",\\n            \\\"type\\\": \\\"python\\\",\\n            \\\"request\\\": \\\"launch\\\",\\n            \\\"program\\\": \\\"${file}\\\",\\n            \\\"console\\\": \\\"integratedTerminal\\\",\\n            \\\"justMyCode\\\": true,\\n            \\\"args\\\": [\\\"--generate-pdf\\\"]\\n\\n        }\\n    ]\\n}\"\n    },\n    {\n      \"path\": \"統計検定1級\\\\problem_types\\\\conjugate_problems.py\",\n      \"overview\": \"このスクリプトはPythonコードです。\\n定義されているクラス: BetaBinomialConjugateProblem, GammaPoissonConjugateProblem, DirichletMultinomialConjugateProblem, BinomialPoissonApproxProblem, PoissonNormalApproxProblem。\\n定義されている関数: __init__, generate_parameters, generate_explanation, generate_graph, __init__, generate_parameters, generate_explanation, generate_graph, __init__, generate_parameters, generate_explanation, generate_graph, __init__, generate_parameters, generate_explanation, generate_graph, __init__, generate_parameters, generate_explanation, generate_graph, B, B。\\n\",\n      \"content\": \"# conjugate_problems.py\\nimport math,random\\nfrom problem_types.problem import Problem\\nfrom math import comb,factorial,exp,gamma\\nimport numpy as np\\n\\nclass BetaBinomialConjugateProblem(Problem):\\n    def __init__(self):\\n        super().__init__('beta_binomial_conjugate_problem.tex')\\n    def generate_parameters(self):\\n        self.params['alpha']=random.randint(1,5)\\n        self.params['beta']=random.randint(1,5)\\n        self.params['n']=random.randint(5,20)\\n        self.params['k']=random.randint(0,self.params['n'])\\n        def B(x,y):\\n            return (gamma(x)*gamma(y))/gamma(x+y)\\n        p_x=comb(self.params['n'],self.params['k'])*B(self.params['k']+self.params['alpha'],self.params['n']-self.params['k']+self.params['beta'])/B(self.params['alpha'],self.params['beta'])\\n        self.params['probability']=round(p_x,4)\\n    def generate_explanation(self):\\n        return \\\"Beta+Binomial->Beta-Binomial\\\"\\n    def generate_graph(self,o):return False\\n\\nclass GammaPoissonConjugateProblem(Problem):\\n    def __init__(self):\\n        super().__init__('gamma_poisson_conjugate_problem.tex')\\n    def generate_parameters(self):\\n        self.params['alpha']=random.randint(1,5)\\n        self.params['beta']=random.uniform(0.5,2.0)\\n        self.params['k']=random.randint(0,20)\\n        p=self.params['beta']/(self.params['beta']+1)\\n        q=1-p\\n        negbin_p=comb(self.params['k']+self.params['alpha']-1,self.params['k'])*(q**self.params['k'])*(p**self.params['alpha'])\\n        self.params['probability']=round(negbin_p,4)\\n    def generate_explanation(self):\\n        return \\\"Gamma+Poisson->Negative Binomial\\\"\\n    def generate_graph(self,o):return False\\n\\nclass DirichletMultinomialConjugateProblem(Problem):\\n    def __init__(self):\\n        super().__init__('dirichlet_multinomial_conjugate_problem.tex')\\n    def generate_parameters(self):\\n        m=random.randint(2,4)\\n        self.params['m']=m\\n        self.params['n']=random.randint(5,20)\\n        self.params['alpha_vec']=[random.uniform(1,3) for _ in range(m)]\\n        counts=[0]*m\\n        remain=self.params['n']\\n        for i in range(m-1):\\n            c=random.randint(0,remain)\\n            counts[i]=c\\n            remain-=c\\n        counts[-1]=remain\\n        self.params['counts']=counts\\n        def B(alpha):\\n            import numpy as np\\n            return (np.prod([gamma(a) for a in alpha]))/gamma(sum(alpha))\\n        alpha_x=[self.params['alpha_vec'][i]+counts[i] for i in range(m)]\\n        num=B(alpha_x)\\n        den=B(self.params['alpha_vec'])\\n        multinomial_coef=math.factorial(self.params['n'])\\n        for c in counts:\\n            multinomial_coef/=math.factorial(c)\\n        p_x=multinomial_coef*(num/den)\\n        self.params['probability']=round(p_x,4)\\n    def generate_explanation(self):\\n        return \\\"Dirichlet+Multinomial->Dirichlet-Multinomial\\\"\\n    def generate_graph(self,o):return False\\n\\nclass BinomialPoissonApproxProblem(Problem):\\n    def __init__(self):\\n        super().__init__('binomial_poisson_approx_problem.tex')\\n    def generate_parameters(self):\\n        lam=random.uniform(2,5)\\n        n=random.randint(50,200)\\n        p=lam/n\\n        k=random.randint(0,int(lam*3))\\n        binom_p=comb(n,k)*(p**k)*((1-p)**(n-k))\\n        poisson_p=(lam**k)*exp(-lam)/math.factorial(k)\\n        self.params['n']=n\\n        self.params['p']=round(p,6)\\n        self.params['k']=k\\n        self.params['lambda']=round(lam,3)\\n        self.params['binom_p']=round(binom_p,6)\\n        self.params['poisson_p']=round(poisson_p,6)\\n    def generate_explanation(self):\\n        return \\\"Binomial->Poisson近似条件\\\"\\n    def generate_graph(self,o):return False\\n\\nclass PoissonNormalApproxProblem(Problem):\\n    def __init__(self):\\n        super().__init__('poisson_normal_approx_problem.tex')\\n    def generate_parameters(self):\\n        lam=random.randint(30,100)\\n        low=max(0,int(lam-3*math.sqrt(lam)))\\n        high=int(lam+3*math.sqrt(lam))\\n        k=random.randint(low,high)\\n        poisson_p=(lam**k)*exp(-lam)/math.factorial(k)\\n        self.params['lambda']=lam\\n        self.params['k']=k\\n        self.params['poisson_p']=round(poisson_p,6)\\n        self.params['mean']=lam\\n        self.params['variance']=lam\\n    def generate_explanation(self):\\n        return \\\"Poisson->Normal近似(λ大)\\\"\\n    def generate_graph(self,o):return False\"\n    },\n    {\n      \"path\": \"統計検定1級\\\\problem_types\\\\problem.py\",\n      \"overview\": \"このスクリプトはPythonコードです。\\n定義されているクラス: Problem, ProbabilityProblem, StatisticalInferenceProblem, RegressionAnalysisProblem, TimeSeriesAnalysisProblem, EconometricsProblem, LinearCombinationProblem, DistributionPropertiesProblem, HighMomentProblem, MultivariateNormalProblem, VarianceAnalysisProblem, NonParametricTestProblem, ProbabilityDefinitionProblem, ConditionalProbabilityProblem, DistributionFunctionsProblem, JointDistributionProblem。\\n定義されている関数: __init__, generate_parameters, generate_problem_text, generate_solution_text, generate_explanation, generate_graph, __init__, generate_parameters, _variant_binomial, _variant_poisson, _variant_conditional_probability, generate_explanation, generate_graph, __init__, generate_parameters, generate_graph, __init__, generate_parameters, generate_graph, __init__, generate_parameters, generate_graph, __init__, generate_parameters, generate_graph, __init__, generate_parameters, generate_explanation, generate_graph, __init__, generate_parameters, generate_graph, __init__, generate_parameters, generate_graph, __init__, generate_parameters, generate_explanation, generate_graph, __init__, generate_parameters, generate_explanation, generate_graph, __init__, generate_parameters, generate_explanation, generate_graph, __init__, generate_parameters, generate_explanation, generate_graph, __init__, generate_parameters, generate_explanation, generate_graph, __init__, generate_parameters, generate_explanation, generate_graph, __init__, generate_parameters, generate_explanation, generate_graph。\\n\",\n      \"content\": \"# problem_types/problem.py\\n\\nfrom abc import ABC, abstractmethod\\nimport os\\nimport random\\nimport logging\\nfrom config import config\\nfrom jinja2 import Environment, FileSystemLoader\\nfrom graph import ProbabilityDistributionVisualizer\\nimport traceback\\nfrom math import comb,exp,factorial\\nfrom scipy.stats import norm,stats\\n\\nclass Problem(ABC):\\n    def __init__(self, template_name):\\n        self.params = {}\\n        self.template_name = template_name\\n        templates_dir = os.path.join(\\n            os.path.dirname(os.path.abspath(__file__)),\\n            '..',\\n            config.get('problem_templates_directory', default='templates')\\n        )\\n        self.env = Environment(loader=FileSystemLoader(templates_dir), autoescape=False)\\n        self.visualizer = ProbabilityDistributionVisualizer()\\n\\n    @abstractmethod\\n    def generate_parameters(self):\\n        pass\\n\\n    def generate_problem_text(self):\\n        template = self.env.get_template(self.template_name)\\n        return template.render(**self.params, show_solution=False)\\n\\n    def generate_solution_text(self):\\n        template = self.env.get_template(self.template_name)\\n        self.params['explanation'] = self.generate_explanation()\\n        return template.render(**self.params, show_solution=True)\\n\\n    def generate_explanation(self):\\n        return \\\"\\\"\\n\\n    @abstractmethod\\n    def generate_graph(self, output_path):\\n        pass\\n\\nclass ProbabilityProblem(Problem):\\n    def __init__(self):\\n        super().__init__('probability_problem.tex')\\n    def generate_parameters(self):\\n        variants=[self._variant_binomial,self._variant_poisson,self._variant_conditional_probability]\\n        v=random.choice(variants)\\n        v()\\n    def _variant_binomial(self):\\n        self.params['problem_type']='binomial'\\n        self.params['n']=random.randint(5,20)\\n        self.params['p']=round(random.uniform(0.1,0.9),2)\\n        self.params['k']=random.randint(0,self.params['n'])\\n        from math import comb\\n        prob=comb(self.params['n'],self.params['k'])*(self.params['p']**self.params['k'])*((1-self.params['p'])**(self.params['n']-self.params['k']))\\n        self.params['probability']=round(prob,6)\\n    def _variant_poisson(self):\\n        self.params['problem_type']='poisson'\\n        self.params['lambda']=round(random.uniform(0.5,5.0),2)\\n        self.params['k']=random.randint(0,10)\\n        lam=self.params['lambda']\\n        k=self.params['k']\\n        prob=(lam**k)*exp(-lam)/factorial(k)\\n        self.params['probability']=round(prob,6)\\n    def _variant_conditional_probability(self):\\n        self.params['problem_type']='conditional_probability'\\n        self.params['P_A']=round(random.uniform(0.1,0.9),2)\\n        self.params['P_B_given_A']=round(random.uniform(0.1,0.9),2)\\n        self.params['P_A_and_B']=round(self.params['P_A']*self.params['P_B_given_A'],6)\\n    def generate_explanation(self):\\n        t=self.params['problem_type']\\n        if t=='binomial':\\n            return \\\"二項分布の公式を使用\\\"\\n        elif t=='poisson':\\n            return \\\"ポアソン分布の公式を使用\\\"\\n        elif t=='conditional_probability':\\n            return \\\"条件付き確率P(A∩B)=P(A)*P(B|A)\\\"\\n    def generate_graph(self,output_path):\\n        return self.visualizer.plot_probability(self.params,output_path)\\n\\nclass StatisticalInferenceProblem(Problem):\\n    def __init__(self):\\n        super().__init__('statistical_inference_problem.tex')\\n    def generate_parameters(self):\\n        self.params['sample_mean']=round(random.uniform(50,100),2)\\n        self.params['sample_std']=round(random.uniform(5,15),2)\\n        self.params['n']=random.randint(30,100)\\n        self.params['population_mean']=round(random.uniform(50,100),2)\\n        self.params['alpha']=round(random.uniform(0.01,0.1),2)\\n        t_stat = (self.params['sample_mean']-self.params['population_mean'])/(self.params['sample_std']/(self.params['n']**0.5))\\n        t_stat=round(t_stat,4)\\n        df=self.params['n']-1\\n        cv=round(stats.t.ppf(1-self.params['alpha']/2,df=df),4)\\n        reject='棄却' if abs(t_stat)>cv else '棄却しない'\\n        self.params['t_stat']=t_stat\\n        self.params['critical_value']=cv\\n        self.params['reject_null']=reject\\n        self.params['df']=df\\n    def generate_graph(self,output_path):\\n        return self.visualizer.plot_t_test(self.params,output_path)\\n\\nclass RegressionAnalysisProblem(Problem):\\n    def __init__(self):\\n        super().__init__('regression_analysis_problem.tex')\\n    def generate_parameters(self):\\n        self.params['beta_0']=round(random.uniform(0,10),2)\\n        self.params['beta_1']=round(random.uniform(-5,5),2)\\n        self.params['n']=random.randint(50,200)\\n        self.params['x_values']=[round(random.uniform(0,100),2) for _ in range(self.params['n'])]\\n        self.params['epsilon']=[round(random.gauss(0,10),2) for _ in range(self.params['n'])]\\n        self.params['y_values']=[self.params['beta_0']+self.params['beta_1']*x+e for x,e in zip(self.params['x_values'],self.params['epsilon'])]\\n        import numpy as np\\n        X=np.array(self.params['x_values'])\\n        Y=np.array(self.params['y_values'])\\n        beta_1_hat=np.cov(X,Y,bias=True)[0,1]/np.var(X)\\n        beta_0_hat=np.mean(Y)-beta_1_hat*np.mean(X)\\n        self.params['beta_0_hat']=round(beta_0_hat,4)\\n        self.params['beta_1_hat']=round(beta_1_hat,4)\\n    def generate_graph(self,output_path):\\n        return self.visualizer.plot_regression(self.params['x_values'],self.params['y_values'],self.params['beta_0_hat'],self.params['beta_1_hat'],output_path)\\n\\nclass TimeSeriesAnalysisProblem(Problem):\\n    def __init__(self):\\n        super().__init__('time_series_analysis_problem.tex')\\n    def generate_parameters(self):\\n        self.params['phi']=round(random.uniform(0.5,0.9),2)\\n        self.params['theta']=round(random.uniform(-0.5,0.5),2)\\n        self.params['n']=100\\n        self.params['epsilon']=[random.gauss(0,1) for _ in range(self.params['n'])]\\n        self.params['time_series']=[0]*self.params['n']\\n        for t in range(1,self.params['n']):\\n            self.params['time_series'][t]=self.params['phi']*self.params['time_series'][t-1]+self.params['epsilon'][t]+self.params['theta']*self.params['epsilon'][t-1]\\n    def generate_graph(self,output_path):\\n        return self.visualizer.plot_time_series(self.params,output_path)\\n\\nclass EconometricsProblem(Problem):\\n    def __init__(self):\\n        super().__init__('econometrics_problem.tex')\\n    def generate_parameters(self):\\n        self.params['beta_0']=round(random.uniform(0,5),2)\\n        self.params['beta_1']=round(random.uniform(0,1),2)\\n        self.params['beta_2']=round(random.uniform(-1,0),2)\\n        self.params['n']=random.randint(50,200)\\n        self.params['x1_values']=[round(random.uniform(0,100),2) for _ in range(self.params['n'])]\\n        self.params['x2_values']=[round(random.uniform(0,100),2) for _ in range(self.params['n'])]\\n        self.params['epsilon']=[round(random.gauss(0,5),2) for _ in range(self.params['n'])]\\n        self.params['y_values']=[self.params['beta_0']+self.params['beta_1']*x1+self.params['beta_2']*x2+e for x1,x2,e in zip(self.params['x1_values'],self.params['x2_values'],self.params['epsilon'])]\\n        import numpy as np\\n        X=np.column_stack((np.ones(self.params['n']),self.params['x1_values'],self.params['x2_values']))\\n        Y=np.array(self.params['y_values'])\\n        beta_hat=np.linalg.inv(X.T@X)@X.T@Y\\n        self.params['beta_0_hat']=round(beta_hat[0],4)\\n        self.params['beta_1_hat']=round(beta_hat[1],4)\\n        self.params['beta_2_hat']=round(beta_hat[2],4)\\n    def generate_graph(self,output_path):\\n        return self.visualizer.plot_econometrics(self.params,output_path)\\n\\nclass LinearCombinationProblem(Problem):\\n    def __init__(self):\\n        super().__init__('linear_combination_problem.tex')\\n    def generate_parameters(self):\\n        self.params['a']=random.randint(1,5)\\n        self.params['b']=random.randint(1,5)\\n        self.params['mu1']=round(random.uniform(0,10),2)\\n        self.params['mu2']=round(random.uniform(0,10),2)\\n        self.params['sigma1_squared']=round(random.uniform(1,5),2)\\n        self.params['sigma2_squared']=round(random.uniform(1,5),2)\\n        E_Z=self.params['a']*self.params['mu1']+self.params['b']*self.params['mu2']\\n        Var_Z=(self.params['a']**2)*self.params['sigma1_squared']+(self.params['b']**2)*self.params['sigma2_squared']\\n        self.params['E_Z']=round(E_Z,4)\\n        self.params['Var_Z']=round(Var_Z,4)\\n    def generate_explanation(self):\\n        return \\\"線形結合の期待値・分散計算\\\"\\n    def generate_graph(self,output_path):\\n        return self.visualizer.plot_linear_combination(self.params,output_path)\\n\\nclass DistributionPropertiesProblem(Problem):\\n    def __init__(self):\\n        super().__init__('distribution_properties_problem.tex')\\n    def generate_parameters(self):\\n        dist_choice=random.choice(['正規分布','ポアソン分布','指数分布'])\\n        self.params['distribution']=dist_choice\\n        if dist_choice=='正規分布':\\n            self.params['properties']={'mean':'\\\\\\\\mu','variance':'\\\\\\\\sigma^2'}\\n        elif dist_choice=='ポアソン分布':\\n            self.params['properties']={'mean':'\\\\\\\\lambda','variance':'\\\\\\\\lambda'}\\n        else:\\n            self.params['properties']={'mean':'1/\\\\\\\\lambda','variance':'1/\\\\\\\\lambda^2'}\\n    def generate_graph(self,output_path):\\n        return self.visualizer.plot_distribution_properties(self.params,output_path)\\n\\nclass HighMomentProblem(Problem):\\n    def __init__(self):\\n        super().__init__('high_moment_problem.tex')\\n    def generate_parameters(self):\\n        self.params['n']=random.randint(3,5)\\n        self.params['mu']=round(random.uniform(0,10),2)\\n        self.params['sigma']=round(random.uniform(1,5),2)\\n        from scipy.stats import norm\\n        m=norm.moment(self.params['n'],loc=self.params['mu'],scale=self.params['sigma'])\\n        self.params['moment']=round(m,4)\\n    def generate_graph(self,output_path):\\n        return self.visualizer.plot_high_moment(self.params,output_path)\\n\\nclass MultivariateNormalProblem(Problem):\\n    def __init__(self):\\n        super().__init__('multivariate_normal_problem.tex')\\n    def generate_parameters(self):\\n        self.params['mu']=[round(random.uniform(0,10),2) for _ in range(2)]\\n        self.params['sigma']=[[round(random.uniform(1,5),2),round(random.uniform(0,2),2)],[round(random.uniform(0,2),2),round(random.uniform(1,5),2)]]\\n    def generate_explanation(self):\\n        return \\\"多変量正規分布の性質を利用\\\"\\n    def generate_graph(self,output_path):\\n        return self.visualizer.plot_multivariate_normal(self.params,output_path)\\n\\nclass VarianceAnalysisProblem(Problem):\\n    def __init__(self):\\n        super().__init__('variance_analysis_problem.tex')\\n    def generate_parameters(self):\\n        self.params['group_count']=random.randint(2,4)\\n        self.params['sample_sizes']=[random.randint(5,20) for _ in range(self.params['group_count'])]\\n        self.params['means']=[round(random.uniform(10,50),2) for _ in range(self.params['group_count'])]\\n        self.params['variances']=[round(random.uniform(1,5),2) for _ in range(self.params['group_count'])]\\n        total_n=sum(self.params['sample_sizes'])\\n        group_count=self.params['group_count']\\n        means=self.params['means']\\n        variances=self.params['variances']\\n        sample_sizes=self.params['sample_sizes']\\n        grand_mean=sum([means[i]*sample_sizes[i] for i in range(group_count)])/total_n\\n        ssb=sum([sample_sizes[i]*(means[i]-grand_mean)**2 for i in range(group_count)])\\n        ssw=sum([(sample_sizes[i]-1)*variances[i] for i in range(group_count)])\\n        df_between=group_count-1\\n        df_within=total_n-group_count\\n        msb=ssb/df_between\\n        msw=ssw/df_within\\n        F=msb/msw\\n        alpha=0.05\\n        from scipy.stats import f\\n        F_critical=f.ppf(1-alpha,df_between,df_within)\\n        reject='棄却する' if F>F_critical else '棄却しない'\\n        self.params['F_value']=round(F,4)\\n        self.params['F_critical']=round(F_critical,4)\\n        self.params['reject_null']=reject\\n        self.params['df_between']=df_between\\n        self.params['df_within']=df_within\\n    def generate_explanation(self):\\n        return \\\"一元配置分散分析による検定\\\"\\n    def generate_graph(self,output_path):\\n        return self.visualizer.plot_variance_analysis(self.params,output_path)\\n\\nclass NonParametricTestProblem(Problem):\\n    def __init__(self):\\n        super().__init__('nonparametric_test_problem.tex')\\n    def generate_parameters(self):\\n        self.params['test_type']=random.choice(['Mann-Whitney U','Kruskal-Wallis','Wilcoxon Signed-Rank'])\\n        self.params['sample1']=[round(random.uniform(10,100),2) for _ in range(random.randint(5,20))]\\n        self.params['sample2']=[round(random.uniform(10,100),2) for _ in range(random.randint(5,20))]\\n        self.params['test_result']='有意差なし(例)'\\n    def generate_explanation(self):\\n        return \\\"ノンパラ検定で中央値差を検定\\\"\\n    def generate_graph(self,output_path):\\n        return self.visualizer.plot_nonparametric_test(self.params,output_path)\\n\\nclass ProbabilityDefinitionProblem(Problem):\\n    def __init__(self):\\n        super().__init__('probability_definition_problem.tex')\\n    def generate_parameters(self):\\n        self.params['P_A']=round(random.uniform(0.1,0.9),2)\\n        self.params['P_B']=round(random.uniform(0.1,0.9),2)\\n        self.params['P_A_and_B']=round(self.params['P_A']*self.params['P_B'],4)\\n    def generate_explanation(self):\\n        return \\\"独立性利用 P(A∩B)=P(A)*P(B)\\\"\\n    def generate_graph(self,output_path):\\n        # 単純な棒グラフでP(A), P(B)とP(A∩B)を表示\\n        try:\\n            PA=self.params['P_A']\\n            PB=self.params['P_B']\\n            PAB=self.params['P_A_and_B']\\n            plt.figure()\\n            plt.bar(['P(A)','P(B)','P(A∩B)'],[PA,PB,PAB],color=['blue','green','red'])\\n            plt.title('Probability Definition')\\n            plt.tight_layout()\\n            plt.savefig(output_path)\\n            plt.close()\\n            return True\\n        except:\\n            return False\\n\\nclass ConditionalProbabilityProblem(Problem):\\n    def __init__(self):\\n        super().__init__('conditional_probability_problem.tex')\\n    def generate_parameters(self):\\n        self.params['P_A']=round(random.uniform(0.2,0.8),2)\\n        self.params['P_B_given_A']=round(random.uniform(0.2,0.8),2)\\n        self.params['P_A_and_B']=round(self.params['P_A']*self.params['P_B_given_A'],4)\\n    def generate_explanation(self):\\n        return \\\"P(A∩B)=P(A)*P(B|A)\\\"\\n    def generate_graph(self,output_path):\\n        # graph.pyでconditional_probabilityで実装済み\\n        from graph import ProbabilityDistributionVisualizer\\n        vis=ProbabilityDistributionVisualizer()\\n        return vis.plot_probability(self.params,output_path)\\n\\nclass DistributionFunctionsProblem(Problem):\\n    def __init__(self):\\n        super().__init__('distribution_functions_problem.tex')\\n    def generate_parameters(self):\\n        self.params['function_type']=random.choice(['pdf','cdf'])\\n        self.params['distribution']=random.choice(['正規分布','指数分布'])\\n        if self.params['distribution']=='正規分布':\\n            self.params['mean']=round(random.uniform(-5,5),2)\\n            self.params['std']=round(random.uniform(1,3),2)\\n        else:\\n            self.params['lambda']=round(random.uniform(0.5,2.0),2)\\n    def generate_explanation(self):\\n        return \\\"pdfやcdf定義式利用\\\"\\n    def generate_graph(self,output_path):\\n        # ここは実装なし、増やしてもよいが現状テンプレ通り\\n        return False\\n\\nclass JointDistributionProblem(Problem):\\n    def __init__(self):\\n        super().__init__('joint_distribution_problem.tex')\\n    def generate_parameters(self):\\n        A_and_B=round(random.uniform(0.05,0.2),2)\\n        A_and_notB=round(random.uniform(0.05,0.2),2)\\n        notA_and_B=round(random.uniform(0.05,0.2),2)\\n        notA_and_notB=round(random.uniform(0.05,0.2),2)\\n        total=A_and_B+A_and_notB+notA_and_B+notA_and_notB\\n        A_and_B/=total\\n        A_and_notB/=total\\n        notA_and_B/=total\\n        notA_and_notB/=total\\n        self.params['joint_probabilities']={'A_and_B':round(A_and_B,4),'A_and_not_B':round(A_and_notB,4),'not_A_and_B':round(notA_and_B,4),'not_A_and_not_B':round(notA_and_notB,4)}\\n        P_A=A_and_B+A_and_notB\\n        P_B=A_and_B+notA_and_B\\n        P_BA=A_and_B/P_A if P_A>0 else 0.0\\n        self.params['P_A']=round(P_A,4)\\n        self.params['P_B']=round(P_B,4)\\n        self.params['P_B_given_A']=round(P_BA,4)\\n    def generate_explanation(self):\\n        return \\\"同時→周辺→条件付き確率\\\"\\n    def generate_graph(self,output_path):\\n        # 簡易的にjoint分布表をHeatmapで可視化\\n        try:\\n            p=self.params['joint_probabilities']\\n            matrix=np.array([[p['A_and_B'],p['A_and_not_B']],[p['not_A_and_B'],p['not_A_and_not_B']]])\\n            plt.figure()\\n            plt.imshow(matrix,cmap='Blues',interpolation='nearest')\\n            plt.colorbar(label='Probability')\\n            plt.xticks([0,1],['B','not B'])\\n            plt.yticks([0,1],['A','not A'])\\n            plt.title('Joint Distribution Heatmap')\\n            plt.tight_layout()\\n            plt.savefig(output_path)\\n            plt.close()\\n            return True\\n        except:\\n            return False\\n\"\n    },\n    {\n      \"path\": \"統計検定1級\\\\problem_types\\\\problem_factory.py\",\n      \"overview\": \"このスクリプトはPythonコードです。\\n定義されているクラス: ProblemFactory。\\n定義されている関数: __init__, create_problem。\\n\",\n      \"content\": \"# problem_types/problem_factory.py\\n\\nimport logging,traceback\\nfrom problem_types.problem import ProbabilityProblem,StatisticalInferenceProblem,RegressionAnalysisProblem,TimeSeriesAnalysisProblem,EconometricsProblem,LinearCombinationProblem,DistributionPropertiesProblem,HighMomentProblem,MultivariateNormalProblem,VarianceAnalysisProblem,NonParametricTestProblem,ProbabilityDefinitionProblem,ConditionalProbabilityProblem,DistributionFunctionsProblem,JointDistributionProblem\\nfrom problem_types.conjugate_problems import BetaBinomialConjugateProblem,GammaPoissonConjugateProblem,DirichletMultinomialConjugateProblem,BinomialPoissonApproxProblem,PoissonNormalApproxProblem\\n\\nclass ProblemFactory:\\n    def __init__(self):\\n        self.problem_classes={\\n            'probability':ProbabilityProblem,\\n            'statistical_inference':StatisticalInferenceProblem,\\n            'regression_analysis':RegressionAnalysisProblem,\\n            'time_series_analysis':TimeSeriesAnalysisProblem,\\n            'econometrics':EconometricsProblem,\\n            'linear_combination':LinearCombinationProblem,\\n            'distribution_properties':DistributionPropertiesProblem,\\n            'high_moment':HighMomentProblem,\\n            'multivariate_normal':MultivariateNormalProblem,\\n            'probability_definition':ProbabilityDefinitionProblem,\\n            'conditional_probability':ConditionalProbabilityProblem,\\n            'distribution_functions':DistributionFunctionsProblem,\\n            'joint_distribution':JointDistributionProblem,\\n            't_test':StatisticalInferenceProblem,\\n            'variance_analysis':VarianceAnalysisProblem,\\n            'nonparametric_test':NonParametricTestProblem,\\n            'beta_binomial_conjugate':BetaBinomialConjugateProblem,\\n            'gamma_poisson_conjugate':GammaPoissonConjugateProblem,\\n            'dirichlet_multinomial_conjugate':DirichletMultinomialConjugateProblem,\\n            'binomial_poisson_approx':BinomialPoissonApproxProblem,\\n            'poisson_normal_approx':PoissonNormalApproxProblem\\n        }\\n    def create_problem(self,problem_type):\\n        pc=self.problem_classes.get(problem_type)\\n        if pc:\\n            try:\\n                return pc()\\n            except Exception as e:\\n                logging.error(f\\\"{problem_type} problem generation error:{e}\\\")\\n                logging.error(traceback.format_exc())\\n                raise\\n        else:\\n            raise ValueError(f\\\"Unknown problem type:{problem_type}\\\")\"\n    },\n    {\n      \"path\": \"統計検定1級\\\\problem_types\\\\__init__.py\",\n      \"overview\": \"このスクリプトはPythonコードです。\\n\",\n      \"content\": \"\"\n    },\n    {\n      \"path\": \"統計検定1級\\\\templates\\\\distribution_properties_problem.tex\",\n      \"overview\": \"このファイルはテキストファイルまたはその他の形式です。\\n行数: 7\\n\",\n      \"content\": \"% distribution_properties_problem.tex\\n{% if not show_solution %}\\n{{ distribution }} の平均と分散を求めよ。\\n{% else %}\\n平均: ${{ properties.mean }}$, 分散: ${{ properties.variance }}$\\n{{ explanation }}\\n{% endif %}\\n\"\n    },\n    {\n      \"path\": \"統計検定1級\\\\templates\\\\distribution_relations.tex\",\n      \"overview\": \"このファイルはテキストファイルまたはその他の形式です。\\n行数: 7\\n\",\n      \"content\": \"% distribution_relations.tex\\n\\\\section*{Distribution Relations}\\n- Beta+Binomial -> Beta-Binomial\\n- Gamma+Poisson -> Negative Binomial\\n- Dirichlet+Multinomial -> Dirichlet-Multinomial\\n- Binomial->Poisson approximation\\n- Poisson->Normal approximation\\n\"\n    },\n    {\n      \"path\": \"統計検定1級\\\\templates\\\\econometrics_problem.tex\",\n      \"overview\": \"このファイルはテキストファイルまたはその他の形式です。\\n行数: 6\\n\",\n      \"content\": \"% econometrics_problem.tex\\n{% if not show_solution %}\\n計量経済学モデルに関する問題\\n{% else %}\\n解答と推定量\\n{{ explanation }}\\n{% endif %}\"\n    },\n    {\n      \"path\": \"統計検定1級\\\\templates\\\\high_moment_problem.tex\",\n      \"overview\": \"このファイルはテキストファイルまたはその他の形式です。\\n行数: 6\\n\",\n      \"content\": \"% high_moment_problem.tex\\n{% if not show_solution %}\\n正規分布の{{ n }}次モーメントは？(例: $E[X^{n}]$ )\\n{% else %}\\n$E[X^{n}]={{ moment }}$\\n{{ explanation }}\\n{% endif %}\"\n    },\n    {\n      \"path\": \"統計検定1級\\\\templates\\\\linear_combination_problem.tex\",\n      \"overview\": \"このファイルはテキストファイルまたはその他の形式です。\\n行数: 6\\n\",\n      \"content\": \"% linear_combination_problem.tex\\n{% if not show_solution %}\\nZ=aX+bY のE[Z],Var[Z]\\n{% else %}\\nE[Z],Var[Z]\\n{{ explanation }}\\n{% endif %}\"\n    },\n    {\n      \"path\": \"統計検定1級\\\\templates\\\\multivariate_normal_problem.tex\",\n      \"overview\": \"このファイルはテキストファイルまたはその他の形式です。\\n行数: 6\\n\",\n      \"content\": \"% multivariate_normal_problem.tex\\n{% if not show_solution %}\\n多変量正規に関する問題\\n{% else %}\\n解答\\n{{ explanation }}\\n{% endif %}\"\n    },\n    {\n      \"path\": \"統計検定1級\\\\templates\\\\nonparametric_test_problem.tex\",\n      \"overview\": \"このファイルはテキストファイルまたはその他の形式です。\\n行数: 6\\n\",\n      \"content\": \"% nonparametric_test_problem.tex\\n{% if not show_solution %}\\nノンパラ検定問題\\n{% else %}\\n検定結果\\n{{ explanation }}\\n{% endif %}\"\n    },\n    {\n      \"path\": \"統計検定1級\\\\templates\\\\poisson_normal_approx_problem.tex\",\n      \"overview\": \"このファイルはテキストファイルまたはその他の形式です。\\n行数: 6\\n\",\n      \"content\": \"% poisson_normal_approx_problem.tex\\n{% if not show_solution %}\\nPoisson→Normal近似\\n{% else %}\\nPoisson P=${{ poisson_p }}$, mean=${{mean}}$ var=${{variance}}$\\n{{ explanation }}\\n{% endif %}\"\n    },\n    {\n      \"path\": \"統計検定1級\\\\templates\\\\probability_definition_problem.tex\",\n      \"overview\": \"このファイルはテキストファイルまたはその他の形式です。\\n行数: 6\\n\",\n      \"content\": \"% probability_definition_problem.tex\\n{% if not show_solution %}\\nP(A∩B)求めよ\\n{% else %}\\n結果\\n{{ explanation }}\\n{% endif %}\"\n    },\n    {\n      \"path\": \"統計検定1級\\\\templates\\\\probability_problem.tex\",\n      \"overview\": \"このファイルはテキストファイルまたはその他の形式です。\\n行数: 7\\n\",\n      \"content\": \"% probability_problem.tex\\n{% if not show_solution %}\\n確率計算問題（例）\\n問題タイプ: {{ problem_type }}\\n{% else %}\\n解答と説明: {{ explanation }}\\n計算結果: P = {{ probability }}\\n{% endif %}\"\n    },\n    {\n      \"path\": \"統計検定1級\\\\templates\\\\regression_analysis_problem.tex\",\n      \"overview\": \"このファイルはテキストファイルまたはその他の形式です。\\n行数: 6\\n\",\n      \"content\": \"% regression_analysis_problem.tex\\n{% if not show_solution %}\\n回帰分析問題\\n{% else %}\\n回帰係数結果\\n{{ explanation }}\\n{% endif %}\"\n    },\n    {\n      \"path\": \"統計検定1級\\\\templates\\\\statistical_inference_problem.tex\",\n      \"overview\": \"このファイルはテキストファイルまたはその他の形式です。\\n行数: 6\\n\",\n      \"content\": \"% statistical_inference_problem.tex\\n{% if not show_solution %}\\n統計的推定/検定問題\\n{% else %}\\n検定結果\\n{{ explanation }}\\n{% endif %}\"\n    },\n    {\n      \"path\": \"統計検定1級\\\\templates\\\\time_series_analysis_problem.tex\",\n      \"overview\": \"このファイルはテキストファイルまたはその他の形式です。\\n行数: 6\\n\",\n      \"content\": \"% time_series_analysis_problem.tex\\n{% if not show_solution %}\\n時系列分析問題\\n{% else %}\\n解答と説明\\n{{ explanation }}\\n{% endif %}\"\n    },\n    {\n      \"path\": \"統計検定1級\\\\templates\\\\variance_analysis_problem.tex\",\n      \"overview\": \"このファイルはテキストファイルまたはその他の形式です。\\n行数: 6\\n\",\n      \"content\": \"% variance_analysis_problem.tex\\n{% if not show_solution %}\\n分散分析問題\\n{% else %}\\nANOVA結果\\n{{ explanation }}\\n{% endif %}\"\n    }\n  ]\n}"
    },
    {
      "path": "collected_scripts - コピー (2).json",
      "overview": "JSONファイル (辞書)。キー: system_overview, settings, scripts\n",
      "content": "{\n  \"system_overview\": \"確率統計の網羅的理解\",\n  \"settings\": {\n    \"data_directory\": {\n      \"value\": \"data\",\n      \"description\": \"データディレクトリのパス。\"\n    },\n    \"log_level\": {\n      \"value\": \"INFO\",\n      \"description\": \"ログレベルの設定（例: INFO、DEBUG、ERROR）。\"\n    },\n    \"output_directory\": {\n      \"value\": \"outputs\",\n      \"description\": \"出力ディレクトリのパス。\"\n    },\n    \"database_path\": {\n      \"value\": \"data/data.db\",\n      \"description\": \"データベースのパス。\"\n    },\n    \"problem_templates_directory\": {\n      \"value\": \"templates\",\n      \"description\": \"問題テンプレートディレクトリのパス。\"\n    },\n    \"output_tex_file\": {\n      \"value\": \"practice_problems.tex\",\n      \"description\": \"生成されるLaTeXファイルの名前。\"\n    },\n    \"latex_path\": {\n      \"value\": \"C:/Users/KEN/Desktop/TAROML/texlive/2024/bin/windows/xelatex.exe\",\n      \"description\": \"LaTeXコンパイラのパス。\"\n    },\n    \"cjk_main_font\": {\n      \"value\": \"Yu Gothic\",\n      \"description\": \"使用するCJKフォント。\"\n    },\n    \"problem_types\": {\n      \"value\": {\n        \"probability_definition\": 0.05,\n        \"conditional_probability\": 0.05,\n        \"distribution_functions\": 0.05,\n        \"joint_distribution\": 0.05,\n        \"statistical_inference\": 0.1,\n        \"regression_analysis\": 0.1,\n        \"linear_combination\": 0.1,\n        \"distribution_properties\": 0.1,\n        \"high_moment\": 0.1,\n        \"variance_analysis\": 0.1,\n        \"nonparametric_test\": 0.1,\n        \"beta_binomial_conjugate\": 0.05,\n        \"gamma_poisson_conjugate\": 0.05,\n        \"dirichlet_multinomial_conjugate\": 0.05,\n        \"binomial_poisson_approx\": 0.05,\n        \"poisson_normal_approx\": 0.05\n      },\n      \"description\": \"問題タイプとその割合。\"\n    },\n    \"pdf_generation\": {\n      \"value\": {\n        \"problem_count\": 9\n      },\n      \"description\": \"PDF生成の設定。\"\n    },\n    \"gui_settings\": {\n      \"value\": {\n        \"window_title\": \"統計検定1級 インタラクティブ問題解答システム\",\n        \"font_size\": 14\n      },\n      \"description\": \"GUIの設定。\"\n    },\n    \"enable_visualization\": {\n      \"value\": true,\n      \"description\": \"可視化機能の有効化。\"\n    }\n  },\n  \"scripts\": [\n    {\n      \"path\": \".vscode\\\\launch.json\",\n      \"overview\": \"このファイルはテキストファイルまたはその他の形式です。\\n行数: 17\\n\",\n      \"content\": \"{\\n    // IntelliSense を使用して利用可能な属性を学べます。\\n    // 既存の属性の説明をホバーして表示します。\\n    // 詳細情報は次を確認してください: https://go.microsoft.com/fwlink/?linkid=830387\\n    \\\"version\\\": \\\"0.2.0\\\",\\n    \\\"configurations\\\": [\\n        {\\n            \\\"name\\\": \\\"Python: 現在のファイル\\\",\\n            \\\"type\\\": \\\"python\\\",\\n            \\\"request\\\": \\\"launch\\\",\\n            \\\"program\\\": \\\"${file}\\\",\\n            \\\"console\\\": \\\"integratedTerminal\\\",\\n            \\\"justMyCode\\\": true,\\n            \\\"args\\\": [\\\"--generate-pdf\\\"]\\n\\n        }\\n    ]\\n}\"\n    },\n    {\n      \"path\": \"統計検定1級\\\\config.json\",\n      \"overview\": \"このファイルはテキストファイルまたはその他の形式です。\\n行数: 37\\n\",\n      \"content\": \"\"\n    },\n    {\n      \"path\": \"統計検定1級\\\\config.py\",\n      \"overview\": \"このスクリプトはPythonコードです。\\n定義されているクラス: Config。\\n定義されている関数: __init__, load_config, get。\\n\",\n      \"content\": \"# config.py\\nimport json, os\\nclass Config:\\n    def __init__(self, config_file='config.json'):\\n        script_dir=os.path.dirname(os.path.abspath(__file__))\\n        self.config_file=os.path.join(script_dir,config_file)\\n        self.settings=self.load_config()\\n    def load_config(self):\\n        if not os.path.exists(self.config_file):\\n            # ファイルが無ければ空dict\\n            return {}\\n        try:\\n            with open(self.config_file,'r',encoding='utf-8')as f:\\n                content=f.read().strip()\\n                if not content:\\n                    # 空ファイルなら{}扱い\\n                    return {}\\n                return json.loads(content)\\n        except Exception:\\n            # JSONパース失敗時も{}\\n            return {}\\n    def get(self,*keys,default=None):\\n        data=self.settings\\n        for k in keys:\\n            if isinstance(data, dict) and k in data:\\n                data=data[k]\\n            else:\\n                return default\\n        return data\\n\\nconfig=Config()\"\n    },\n    {\n      \"path\": \"統計検定1級\\\\database.py\",\n      \"overview\": \"このスクリプトはPythonコードです。\\n定義されているクラス: DatabaseManager。\\n定義されている関数: __init__, setup_database, save_problem。\\n\",\n      \"content\": \"# database.py\\nimport sqlite3,os\\nfrom config import config\\nclass DatabaseManager:\\n    def __init__(self):\\n        script_dir=os.path.dirname(os.path.abspath(__file__))\\n        db_path=config.get('settings','database_path',default='data/data.db')\\n        if db_path is None:\\n            db_path='data/data.db'\\n        self.db_name=os.path.join(script_dir,db_path)\\n        os.makedirs(os.path.dirname(self.db_name), exist_ok=True)\\n    def setup_database(self):\\n        conn=sqlite3.connect(self.db_name)\\n        c=conn.cursor()\\n        c.execute('CREATE TABLE IF NOT EXISTS problems (id TEXT PRIMARY KEY,date_created TEXT,problem_text TEXT,solution_text TEXT,problem_type TEXT)')\\n        conn.commit()\\n        conn.close()\\n    def save_problem(self,problem_id,date_created,problem_text,solution_text,problem_type):\\n        conn=sqlite3.connect(self.db_name)\\n        c=conn.cursor()\\n        c.execute('INSERT INTO problems VALUES (?,?,?,?,?)',(problem_id,date_created,problem_text,solution_text,problem_type))\\n        conn.commit()\\n        conn.close()\"\n    },\n    {\n      \"path\": \"統計検定1級\\\\graph.py\",\n      \"overview\": \"このスクリプトはPythonコードです。\\n定義されているクラス: ProbabilityDistributionVisualizer。\\n定義されている関数: __init__, plot_probability, plot_t_test, plot_regression, plot_time_series, plot_econometrics, plot_multivariate_normal, plot_distribution_properties, plot_high_moment, plot_variance_analysis, plot_nonparametric_test, plot_linear_combination, confidence_ellipse, mvn_pdf, ecdf。\\n\",\n      \"content\": \"# graph.py\\nimport matplotlib\\nmatplotlib.use('Agg')\\nimport matplotlib.pyplot as plt\\nimport math\\nimport numpy as np\\nimport logging\\nimport os\\nfrom math import factorial, exp\\nfrom scipy.stats import norm, t as t_dist, f as f_dist, poisson, expon\\nfrom statsmodels.graphics.tsaplots import plot_acf\\nimport traceback\\n\\nclass ProbabilityDistributionVisualizer:\\n    def __init__(self):\\n        pass\\n\\n    def plot_probability(self, params, output_path):\\n        # PMF + CDF、kを強調\\n        ptype = params.get('problem_type')\\n        try:\\n            if ptype == 'binomial':\\n                p=params['p']\\n                n=params['n']\\n                k=params['k']\\n                x=range(n+1)\\n                from math import comb\\n                pmf=[comb(n,i)*(p**i)*((1-p)**(n-i)) for i in x]\\n                cdf=np.cumsum(pmf)\\n\\n                plt.figure(figsize=(10,5))\\n                plt.subplot(1,2,1)\\n                plt.bar(x,pmf,color='skyblue')\\n                plt.title(f'Binomial PMF n={n},p={p}')\\n                plt.xlabel('k')\\n                plt.ylabel('P(X=k)')\\n                plt.axvline(k,color='red',linestyle='--',label='Target k')\\n                plt.legend()\\n\\n                plt.subplot(1,2,2)\\n                plt.step(x,cdf,where='post',color='green')\\n                plt.title('CDF')\\n                plt.xlabel('k')\\n                plt.ylabel('P(X<=k)')\\n                plt.axvline(k,color='red',linestyle='--')\\n                plt.tight_layout()\\n                plt.savefig(output_path)\\n                plt.close()\\n                return True\\n            elif ptype == 'poisson':\\n                lam=params['lambda']\\n                k=params['k']\\n                x=range(k+10+1)\\n                pmf=[(lam**i)*exp(-lam)/factorial(i) for i in x]\\n                cdf=np.cumsum(pmf)\\n\\n                plt.figure(figsize=(10,5))\\n                plt.subplot(1,2,1)\\n                plt.bar(x,pmf,color='orange')\\n                plt.title(f'Poisson(lambda={lam}) PMF')\\n                plt.xlabel('k')\\n                plt.ylabel('P(X=k)')\\n                plt.axvline(k,color='red',linestyle='--',label='Target k')\\n                plt.legend()\\n\\n                plt.subplot(1,2,2)\\n                plt.step(x,cdf,where='post',color='green')\\n                plt.title('CDF')\\n                plt.xlabel('k')\\n                plt.ylabel('P(X<=k)')\\n                plt.axvline(k,color='red',linestyle='--')\\n                plt.tight_layout()\\n                plt.savefig(output_path)\\n                plt.close()\\n                return True\\n            elif ptype == 'conditional_probability':\\n                # 条件付き確率の場合は特別なグラフ(分割表)を可視化\\n                P_A=params['P_A']\\n                P_BA=params['P_B_given_A']\\n                P_AB=params['P_A_and_B']\\n                # 簡単な棒グラフでP(A),P(B|A),P(A∩B)の関係を表示\\n                plt.figure()\\n                vals=[P_A,P_BA,P_AB]\\n                labels=['P(A)','P(B|A)','P(A∩B)']\\n                plt.bar(labels,vals,color=['blue','green','red'])\\n                plt.title('Conditional Probability Visualization')\\n                plt.ylabel('Probability')\\n                plt.tight_layout()\\n                plt.savefig(output_path)\\n                plt.close()\\n                return True\\n            else:\\n                # 未知タイプには何もしない\\n                return False\\n        except Exception as e:\\n            logging.error(f\\\"Error in plot_probability: {e}\\\")\\n            return False\\n\\n    def plot_t_test(self, params, output_path):\\n        # t分布+棄却域+標本平均から計算されたt値など\\n        # 追加で、標準正規近似や、p値部分の色塗りなど\\n        try:\\n            alpha = params['alpha']\\n            df = params['df']\\n            t_stat = params['t_stat']\\n            critical_value = params['critical_value']\\n\\n            x = np.linspace(t_dist.ppf(0.001, df), t_dist.ppf(0.999, df), 1000)\\n            y = t_dist.pdf(x, df)\\n\\n            plt.figure(figsize=(10,5))\\n            plt.plot(x, y, 'b-', label=f't-dist(df={df})')\\n            # 臨界値に線\\n            plt.axvline(x=critical_value,color='r',linestyle='--',label='critical +')\\n            plt.axvline(x=-critical_value,color='r',linestyle='--',label='critical -')\\n            # t統計量\\n            plt.axvline(x=t_stat,color='g',label='t-stat')\\n\\n            # p値領域を色付け\\n            # 両側検定として|t|>crit\\n            p_area_x = x[x>critical_value]\\n            plt.fill_between(p_area_x,t_dist.pdf(p_area_x,df),color='red',alpha=0.3)\\n            p_area_x2 = x[x<-critical_value]\\n            plt.fill_between(p_area_x2,t_dist.pdf(p_area_x2,df),color='red',alpha=0.3)\\n\\n            plt.title('t-test visualization')\\n            plt.xlabel('t')\\n            plt.ylabel('pdf')\\n            plt.legend()\\n            plt.tight_layout()\\n\\n            # 保存\\n            plt.savefig(output_path)\\n            plt.close()\\n            return True\\n        except Exception as e:\\n            logging.error(f\\\"t検定グラフ生成エラー: {e}\\\")\\n            return False\\n\\n    def plot_regression(self, x_values, y_values, beta_0_hat, beta_1_hat, output_path):\\n        # 回帰直線+データ点 + 残差ヒスト + Q-Qプロットなど複数図\\n        try:\\n            import statsmodels.api as sm\\n            X = sm.add_constant(x_values)\\n            model = sm.OLS(y_values, X).fit()\\n            residuals = model.resid\\n\\n            fig,axes = plt.subplots(2,2,figsize=(10,10))\\n\\n            # Scatter + regression line\\n            ax=axes[0,0]\\n            ax.scatter(x_values,y_values,color='blue',label='data')\\n            x_line=np.linspace(min(x_values),max(x_values),100)\\n            y_line=beta_0_hat+beta_1_hat*x_line\\n            ax.plot(x_line,y_line,color='red',label='reg line')\\n            ax.set_title('Data & Regression Line')\\n            ax.set_xlabel('X')\\n            ax.set_ylabel('Y')\\n            ax.legend()\\n\\n            # Residual histogram\\n            ax=axes[0,1]\\n            ax.hist(residuals,bins=20,color='green',alpha=0.7)\\n            ax.set_title('Residual Histogram')\\n            ax.set_xlabel('Residual')\\n            ax.set_ylabel('Frequency')\\n\\n            # Q-Q plot of residuals\\n            sm.qqplot(residuals, line='45', ax=axes[1,0],color='purple')\\n            axes[1,0].set_title('Q-Q plot of Residuals')\\n\\n            # Residuals vs fitted\\n            fitted=model.fittedvalues\\n            ax=axes[1,1]\\n            ax.scatter(fitted,residuals,color='orange')\\n            ax.axhline(y=0,color='red',linestyle='--')\\n            ax.set_title('Residuals vs Fitted')\\n            ax.set_xlabel('Fitted')\\n            ax.set_ylabel('Residuals')\\n\\n            fig.tight_layout()\\n            fig.savefig(output_path)\\n            plt.close(fig)\\n            return True\\n        except Exception as e:\\n            logging.error(f\\\"回帰分析グラフ生成中にエラー: {e}\\\")\\n            return False\\n\\n    def plot_time_series(self, params, output_path):\\n        # 時系列データ + ACFプロット\\n        try:\\n            ts = params['time_series']\\n\\n            fig,axes=plt.subplots(2,1,figsize=(10,8))\\n            axes[0].plot(ts, color='blue')\\n            axes[0].set_title('Time Series Data')\\n            axes[0].set_xlabel('Time')\\n            axes[0].set_ylabel('Value')\\n\\n            plot_acf(ts,ax=axes[1])\\n            axes[1].set_title('Autocorrelation Function')\\n\\n            fig.tight_layout()\\n            fig.savefig(output_path)\\n            plt.close(fig)\\n            return True\\n        except Exception as e:\\n            logging.error(f\\\"時系列分析グラフエラー: {e}\\\")\\n            return False\\n\\n    def plot_econometrics(self, params, output_path):\\n        # X1,YやX2,Y 散布図 + 残差分析\\n        try:\\n            import statsmodels.api as sm\\n            X = np.column_stack((params['x1_values'], params['x2_values']))\\n            Y = np.array(params['y_values'])\\n            Xc = sm.add_constant(X)\\n            model = sm.OLS(Y,Xc).fit()\\n            residuals = model.resid\\n            fitted = model.fittedvalues\\n\\n            fig,axes=plt.subplots(2,2,figsize=(10,10))\\n\\n            # X1 vs Y\\n            ax=axes[0,0]\\n            ax.scatter(params['x1_values'],Y,color='blue',alpha=0.7,label='X1-Y')\\n            ax.set_title('X1 vs Y')\\n            ax.set_xlabel('X1')\\n            ax.set_ylabel('Y')\\n\\n            # X2 vs Y\\n            ax=axes[0,1]\\n            ax.scatter(params['x2_values'],Y,color='green',alpha=0.7,label='X2-Y')\\n            ax.set_title('X2 vs Y')\\n            ax.set_xlabel('X2')\\n            ax.set_ylabel('Y')\\n\\n            # Residuals histogram\\n            ax=axes[1,0]\\n            ax.hist(residuals,bins=20,color='gray',alpha=0.7)\\n            ax.set_title('Residuals Histogram')\\n            ax.set_xlabel('Residual')\\n\\n            # Residuals vs Fitted\\n            ax=axes[1,1]\\n            ax.scatter(fitted,residuals,color='red',alpha=0.7)\\n            ax.axhline(y=0,color='black',linestyle='--')\\n            ax.set_title('Residuals vs Fitted')\\n            ax.set_xlabel('Fitted')\\n            ax.set_ylabel('Residuals')\\n\\n            fig.tight_layout()\\n            fig.savefig(output_path)\\n            plt.close(fig)\\n            return True\\n        except Exception as e:\\n            logging.error(f\\\"計量経済学グラフエラー: {e}\\\")\\n            return False\\n\\n    def plot_multivariate_normal(self, params, output_path):\\n        # 信頼楕円 + 等高線 + 周辺分布\\n        try:\\n            mu = params['mu']\\n            sigma = np.array(params['sigma'])\\n\\n            fig = plt.figure(figsize=(10,10))\\n            from matplotlib.patches import Ellipse\\n            import matplotlib.transforms as transforms\\n\\n            def confidence_ellipse(mu,cov,ax,n_std=1.96,facecolor='none',**kwargs):\\n                pearson = cov[0,1]/np.sqrt(cov[0,0]*cov[1,1])\\n                ell_radius_x = np.sqrt(1+pearson)\\n                ell_radius_y = np.sqrt(1-pearson)\\n                ellipse=Ellipse((0,0),width=ell_radius_x*2,height=ell_radius_y*2,facecolor=facecolor,**kwargs)\\n                scale_x = np.sqrt(cov[0,0])*n_std\\n                scale_y = np.sqrt(cov[1,1])*n_std\\n                transf = transforms.Affine2D().rotate_deg(45).scale(scale_x,scale_y).translate(mu[0],mu[1])\\n                ellipse.set_transform(transf+ax.transData)\\n                return ax.add_patch(ellipse)\\n\\n            ax=fig.add_subplot(2,2,1)\\n            ax.set_title('Confidence Ellipse')\\n            confidence_ellipse(mu,sigma,ax,edgecolor='red')\\n            ax.scatter(mu[0],mu[1],c='blue',marker='x',label='mean')\\n            ax.legend()\\n            ax.set_xlabel('X1')\\n            ax.set_ylabel('X2')\\n\\n            # 等高線\\n            ax2=fig.add_subplot(2,2,2)\\n            x = np.linspace(mu[0]-4*np.sqrt(sigma[0,0]),mu[0]+4*np.sqrt(sigma[0,0]),100)\\n            y = np.linspace(mu[1]-4*np.sqrt(sigma[1,1]),mu[1]+4*np.sqrt(sigma[1,1]),100)\\n            X,Y = np.meshgrid(x,y)\\n            pos = np.dstack((X,Y))\\n            def mvn_pdf(x, mu, cov):\\n                n=2\\n                det=np.linalg.det(cov)\\n                inv=np.linalg.inv(cov)\\n                diff=(x - mu)\\n                return (1./(2*np.pi*np.sqrt(det)))*np.exp(-0.5*(diff@inv@diff.T))\\n            Z=np.empty(X.shape)\\n            for i in range(X.shape[0]):\\n                for j in range(X.shape[1]):\\n                    Z[i,j]=mvn_pdf(np.array([X[i,j],Y[i,j]]),np.array(mu),sigma)\\n            ax2.contour(X,Y,Z,levels=5,cmap='Blues')\\n            ax2.set_title('Contour')\\n\\n            # 周辺分布\\n            ax3=fig.add_subplot(2,2,3)\\n            # marginal X1\\n            X_marg = norm(loc=mu[0],scale=np.sqrt(sigma[0,0]))\\n            x_line = np.linspace(mu[0]-4*np.sqrt(sigma[0,0]),mu[0]+4*np.sqrt(sigma[0,0]),100)\\n            y_line = X_marg.pdf(x_line)\\n            ax3.plot(x_line,y_line,'r-')\\n            ax3.set_title('Marginal X1 distribution')\\n            ax3.set_xlabel('X1')\\n            ax3.set_ylabel('pdf')\\n\\n            ax4=fig.add_subplot(2,2,4)\\n            Y_marg=norm(loc=mu[1],scale=np.sqrt(sigma[1,1]))\\n            y_line=Y_marg.pdf(x_line)\\n            ax4.plot(x_line,y_line,'g-')\\n            ax4.set_title('Marginal X2 distribution')\\n            ax4.set_xlabel('X2')\\n            ax4.set_ylabel('pdf')\\n\\n            fig.tight_layout()\\n            fig.savefig(output_path)\\n            plt.close(fig)\\n            return True\\n        except Exception as e:\\n            logging.error(f\\\"多変量正規分布グラフエラー: {e}\\\")\\n            return False\\n\\n    def plot_distribution_properties(self, params, output_path):\\n        # 選択された分布に対し、PDF+平均分散表示ライン\\n        try:\\n            dist = params['distribution']\\n            plt.figure(figsize=(10,5))\\n            if dist=='正規分布':\\n                mu=0; sigma=1\\n                x=np.linspace(mu-4*sigma,mu+4*sigma,200)\\n                y=norm.pdf(x,mu,sigma)\\n                plt.plot(x,y,'b-')\\n                plt.axvline(mu,color='r',linestyle='--',label='mean')\\n                plt.axvline(mu+sigma,color='g',linestyle=':',label='mean+sigma')\\n                plt.axvline(mu-sigma,color='g',linestyle=':')\\n                plt.title('Normal Distribution (mu=0,sigma=1)')\\n                plt.legend()\\n            elif dist=='ポアソン分布':\\n                lam=3\\n                x=np.arange(0,15)\\n                y=poisson.pmf(x,lam)\\n                plt.bar(x,y,color='skyblue')\\n                plt.axvline(lam,color='r',linestyle='--',label='mean=lambda=3')\\n                plt.title('Poisson(lambda=3)')\\n                plt.legend()\\n            elif dist=='指数分布':\\n                lam=1\\n                x=np.linspace(0,5,200)\\n                y=expon.pdf(x,scale=1/lam)\\n                plt.plot(x,y,'b-')\\n                plt.axvline(1/lam,color='r',linestyle='--',label='mean=1/lambda')\\n                plt.title('Exponential(lambda=1)')\\n                plt.legend()\\n            plt.tight_layout()\\n            plt.savefig(output_path)\\n            plt.close()\\n            return True\\n        except Exception as e:\\n            logging.error(f\\\"分布性質グラフエラー: {e}\\\")\\n            return False\\n\\n    def plot_high_moment(self, params, output_path):\\n        # 正規分布pdf上に平均、±σ、n次モーメント近傍など\\n        try:\\n            mu=params['mu']\\n            sigma=params['sigma']\\n            n=params['n']\\n            moment=params['moment']\\n            x=np.linspace(mu-4*sigma,mu+4*sigma,200)\\n            y=norm.pdf(x,mu,sigma)\\n\\n            plt.figure(figsize=(10,5))\\n            plt.plot(x,y,'b-',label='Normal pdf')\\n            plt.axvline(mu,color='r',linestyle='--',label='mean')\\n            plt.axvline(mu+sigma,color='g',linestyle=':',label='mu±sigma')\\n            plt.axvline(mu-sigma,color='g',linestyle=':')\\n            plt.title(f'Normal distribution (mu={mu}, sigma={sigma}). n={n}-th moment={moment}')\\n            plt.legend()\\n            plt.tight_layout()\\n            plt.savefig(output_path)\\n            plt.close()\\n            return True\\n        except Exception as e:\\n            logging.error(f\\\"高次モーメントグラフエラー: {e}\\\")\\n            return False\\n\\n    def plot_variance_analysis(self, params, output_path):\\n        # 箱ひげ図など\\n        try:\\n            group_count=params['group_count']\\n            sample_sizes=params['sample_sizes']\\n            means=params['means']\\n            variances=params['variances']\\n\\n            # データを仮に正規生成して可視化\\n            # 単に箱ひげ図で分布の違いを視覚化\\n            data=[]\\n            for i in range(group_count):\\n                # 各グループ：mean, varから乱数生成\\n                np.random.seed(i)\\n                samples=np.random.normal(means[i],np.sqrt(variances[i]),sample_sizes[i])\\n                data.append(samples)\\n\\n            plt.figure(figsize=(8,6))\\n            plt.boxplot(data,labels=[f'Group{i+1}' for i in range(group_count)])\\n            plt.title('ANOVA: Boxplots of groups')\\n            plt.ylabel('Value')\\n            plt.tight_layout()\\n            plt.savefig(output_path)\\n            plt.close()\\n            return True\\n        except Exception as e:\\n            logging.error(f\\\"分散分析グラフエラー: {e}\\\")\\n            return False\\n\\n    def plot_nonparametric_test(self, params, output_path):\\n        # ノンパラ検定：2サンプルECDFなど\\n        try:\\n            s1=params['sample1']\\n            s2=params['sample2']\\n\\n            def ecdf(data):\\n                d_sorted = np.sort(data)\\n                y = np.arange(1,len(d_sorted)+1)/len(d_sorted)\\n                return d_sorted,y\\n\\n            x1,y1=ecdf(s1)\\n            x2,y2=ecdf(s2)\\n\\n            plt.figure(figsize=(8,6))\\n            plt.step(x1,y1,where='post',label='Sample1 ECDF',color='blue')\\n            plt.step(x2,y2,where='post',label='Sample2 ECDF',color='red')\\n            plt.title('Nonparametric Test Visualization')\\n            plt.xlabel('Value')\\n            plt.ylabel('ECDF')\\n            plt.legend()\\n            plt.tight_layout()\\n            plt.savefig(output_path)\\n            plt.close()\\n            return True\\n        except Exception as e:\\n            logging.error(f\\\"ノンパラ検定グラフエラー: {e}\\\")\\n            return False\\n\\n    def plot_linear_combination(self, params, output_path):\\n        # 合成Z=aX+bYをシミュレーション\\n        try:\\n            a=params['a']\\n            b=params['b']\\n            mu1=params['mu1']\\n            mu2=params['mu2']\\n            sig1=params['sigma1_squared']\\n            sig2=params['sigma2_squared']\\n            # シミュレーションでZ生成\\n            np.random.seed(123)\\n            x = np.random.normal(mu1, np.sqrt(sig1),1000)\\n            y = np.random.normal(mu2, np.sqrt(sig2),1000)\\n            Z=a*x+b*y\\n            plt.figure(figsize=(8,6))\\n            plt.hist(Z,bins=30,density=True,alpha=0.7,color='purple',label='Simulated Z')\\n            # 理論分布：N(a*mu1+b*mu2, a²sigma1+b²sigma2)\\n            E_Z=a*mu1+b*mu2\\n            Var_Z=a**2*sig1+b**2*sig2\\n            X_line=np.linspace(E_Z-4*np.sqrt(Var_Z),E_Z+4*np.sqrt(Var_Z),200)\\n            Y_line=norm.pdf(X_line,E_Z,np.sqrt(Var_Z))\\n            plt.plot(X_line,Y_line,'r-',label='Theoretical PDF')\\n            plt.title('Linear Combination Distribution')\\n            plt.xlabel('Z')\\n            plt.ylabel('Density')\\n            plt.legend()\\n            plt.tight_layout()\\n            plt.savefig(output_path)\\n            plt.close()\\n            return True\\n        except Exception as e:\\n            logging.error(f\\\"線形結合グラフエラー: {e}\\\")\\n            return False\\n\"\n    },\n    {\n      \"path\": \"統計検定1級\\\\gui.py\",\n      \"overview\": \"このスクリプトはPythonコードです。\\n定義されているクラス: InteractiveSolverGUI。\\n定義されている関数: __init__, run。\\n\",\n      \"content\": \"# gui.py\\nimport tkinter as tk\\nclass InteractiveSolverGUI:\\n    def __init__(self):\\n        self.root=tk.Tk()\\n        self.root.title('統計検定1級 インタラクティブ問題解答システム')\\n    def run(self):\\n        self.root.mainloop()\"\n    },\n    {\n      \"path\": \"統計検定1級\\\\main.py\",\n      \"overview\": \"このスクリプトはPythonコードです。\\n定義されている関数: main。\\n\",\n      \"content\": \"# main.py\\nimport sys\\nfrom main_app import MainApp\\nfrom gui import InteractiveSolverGUI\\nfrom config import config\\n\\ndef main():\\n    pdf_gen=config.get('settings','pdf_generation',default={})\\n    problem_count=pdf_gen.get('problem_count',9)\\n\\n    if len(sys.argv)>1 and sys.argv[1]=='--generate-pdf':\\n        app=MainApp()\\n        app.generate_and_compile(problem_count)\\n    else:\\n        gui=InteractiveSolverGUI()\\n        gui.run()\\n\\nif __name__=='__main__':\\n    main()\\n\"\n    },\n    {\n      \"path\": \"統計検定1級\\\\main_app.py\",\n      \"overview\": \"このスクリプトはPythonコードです。\\n定義されているクラス: MainApp。\\n定義されている関数: __init__, generate_latex_header, generate_and_compile, compile_latex。\\n\",\n      \"content\": \"# main_app.py\\nimport os,subprocess\\nfrom config import config\\nfrom problem_generator import ProblemGenerator\\nimport logging\\nimport traceback\\n\\nclass MainApp:\\n    def __init__(self):\\n        # 必要設定を__init__でまとめて取得\\n        self.output_tex_file = config.get('settings','output_tex_file',default='practice_problems.tex')\\n        if self.output_tex_file is None:\\n            self.output_tex_file='practice_problems.tex'\\n\\n        self.enable_visualization = config.get('settings','enable_visualization',default=True)\\n        self.problem_templates_dir = config.get('settings','problem_templates_directory',default='templates')\\n        self.output_directory = config.get('settings','output_directory',default='outputs')\\n        self.latex_path = config.get('settings','latex_path',default='xelatex')\\n\\n        # ProblemGeneratorの生成\\n        self.generator = ProblemGenerator()\\n\\n    def generate_latex_header(self):\\n        cjk_font=config.get('settings','cjk_main_font',default='Yu Gothic')\\n        if cjk_font is None:\\n            cjk_font='Yu Gothic'\\n        header=[\\n            '\\\\\\\\documentclass{article}',\\n            '\\\\\\\\usepackage{amsmath}',\\n            '\\\\\\\\usepackage{amssymb}',\\n            '\\\\\\\\usepackage{graphicx}',\\n            '\\\\\\\\usepackage{float}',\\n            '\\\\\\\\usepackage{geometry}',\\n            '\\\\\\\\usepackage{xeCJK}',\\n            '\\\\\\\\usepackage{fontspec}',\\n            '\\\\\\\\setmainfont{Times New Roman}',\\n            f'\\\\\\\\setCJKmainfont{{{cjk_font}}}',\\n            '\\\\\\\\geometry{a4paper, margin=1in}',\\n            '\\\\\\\\begin{document}'\\n        ]\\n        return header\\n\\n    def generate_and_compile(self,problem_count):\\n        script_dir=os.path.dirname(os.path.abspath(__file__))\\n        latex_content=self.generate_latex_header()\\n\\n        out_dir = self.output_directory\\n        if out_dir is None:\\n            out_dir='outputs'\\n        output_dir=os.path.join(script_dir,out_dir)\\n        os.makedirs(output_dir,exist_ok=True)\\n        tex_file_path=os.path.join(output_dir,self.output_tex_file)\\n\\n        for idx in range(1,problem_count+1):\\n            try:\\n                result=self.generator.generate_problem()\\n                if result:\\n                    problem_id,date_created,problem_type,problem_text,solution_text,graph_filename=result\\n\\n                    latex_content.append(f'\\\\\\\\section*{{問題 {idx}}}')\\n                    latex_content.append(problem_text)\\n\\n                    if self.enable_visualization and graph_filename:\\n                        latex_content.append('\\\\\\\\begin{figure}[H]')\\n                        latex_content.append('\\\\\\\\centering')\\n                        graph_relative_path=os.path.join('graphs',graph_filename).replace('\\\\\\\\','/')\\n                        latex_content.append(f'\\\\\\\\includegraphics[width=0.8\\\\\\\\textwidth]{{{graph_relative_path}}}')\\n                        latex_content.append('\\\\\\\\end{figure}')\\n\\n                    latex_content.append('\\\\\\\\subsection*{解答}')\\n                    latex_content.append(solution_text)\\n                    latex_content.append('\\\\\\\\newpage')\\n                else:\\n                    logging.warning(f\\\"問題 {idx} の生成に失敗しました。\\\")\\n                    print(f\\\"問題 {idx} の生成に失敗しました。\\\")\\n            except Exception as e:\\n                logging.error(f\\\"問題 {idx} の生成中にエラー: {e}\\\")\\n                logging.error(traceback.format_exc())\\n                print(f\\\"問題 {idx} 生成エラー。ログを確認\\\")\\n\\n        templates_dir = self.problem_templates_dir\\n        if templates_dir is None:\\n            templates_dir='templates'\\n        latex_content.append('\\\\\\\\clearpage')\\n        latex_content.append('\\\\\\\\input{../'+templates_dir+'/distribution_relations.tex}')\\n\\n        latex_content.append('\\\\\\\\end{document}')\\n\\n        with open(tex_file_path,'w',encoding='utf-8')as tex_file:\\n            tex_file.write('\\\\n'.join(latex_content))\\n\\n        self.compile_latex(tex_file_path)\\n\\n    def compile_latex(self,tex_file_path):\\n        tex_file_name=os.path.basename(tex_file_path)\\n        latex_path=self.latex_path\\n        if not latex_path or not os.path.exists(latex_path):\\n            print(f\\\"LaTeXコンパイラが見つかりません: {latex_path}\\\")\\n            return\\n        process=subprocess.run([latex_path,'-interaction=nonstopmode',tex_file_name],cwd=os.path.dirname(tex_file_path),stdout=subprocess.PIPE,stderr=subprocess.PIPE,text=True)\\n        log_file_path=os.path.join(os.path.dirname(tex_file_path),'latex_compile.log')\\n        with open(log_file_path,'w',encoding='utf-8')as f:\\n            f.write(process.stdout or '')\\n            f.write(process.stderr or '')\\n        if process.returncode!=0:\\n            print(\\\"LaTeXコンパイルでエラー\\\")\\n        else:\\n            pdf_file=tex_file_path.replace('.tex','.pdf')\\n            if os.path.exists(pdf_file):\\n                print(f\\\"PDF生成成功: {pdf_file}\\\")\\n            else:\\n                print(\\\"PDFファイル未発見\\\")\\n\"\n    },\n    {\n      \"path\": \"統計検定1級\\\\problem_generator.py\",\n      \"overview\": \"このスクリプトはPythonコードです。\\n定義されているクラス: ProblemGenerator。\\n定義されている関数: __init__, load_topics, get_problem_types_by_topic, generate_problem。\\n\",\n      \"content\": \"# problem_generator.py\\nimport random,os,logging,uuid\\nfrom datetime import datetime\\nfrom config import config\\nfrom database import DatabaseManager\\nfrom problem_types.problem_factory import ProblemFactory\\nimport json\\nimport traceback\\n\\nclass ProblemGenerator:\\n    def __init__(self):\\n        self.db_path = config.get('settings','database_path',default='data/data.db')\\n        self.output_directory = config.get('settings','output_directory',default='outputs')\\n        self.enable_visualization = config.get('settings','enable_visualization',default=True)\\n        self.problem_types_weights = config.get('settings','problem_types',default={})\\n\\n        self.factory=ProblemFactory()\\n        self.db_manager=DatabaseManager()\\n        self.db_manager.setup_database()\\n\\n        script_dir=os.path.dirname(os.path.abspath(__file__))\\n        out_dir=self.output_directory\\n        if out_dir is None:\\n            out_dir='outputs'\\n        self.output_dir=os.path.join(script_dir,out_dir,'graphs')\\n        os.makedirs(self.output_dir,exist_ok=True)\\n        self.topics_data=self.load_topics()\\n\\n    def load_topics(self):\\n        script_dir=os.path.dirname(os.path.abspath(__file__))\\n        topics_file=os.path.join(script_dir,'topics.json')\\n        if not os.path.exists(topics_file):\\n            raise FileNotFoundError(f\\\"'{topics_file}'がない\\\")\\n        with open(topics_file,'r',encoding='utf-8')as f:\\n            return json.load(f)\\n\\n    def get_problem_types_by_topic(self,topic):\\n        return self.topics_data['topics'].get(topic,[])\\n\\n    def generate_problem(self,selected_topic=None):\\n        try:\\n            ptypes=self.problem_types_weights\\n            if selected_topic:\\n                problem_types=self.get_problem_types_by_topic(selected_topic)\\n                if not problem_types:\\n                    return None\\n                problem_type=random.choice(problem_types)\\n            else:\\n                pts=list(ptypes.keys())\\n                pwt=list(ptypes.values())\\n                if not pts:\\n                    pts=['probability']\\n                    pwt=[1.0]\\n                problem_type=random.choices(pts,weights=pwt,k=1)[0]\\n\\n            problem=self.factory.create_problem(problem_type)\\n            problem.generate_parameters()\\n            problem_text=problem.generate_problem_text()\\n            solution_text=problem.generate_solution_text()\\n\\n            enable_vis=self.enable_visualization\\n            if enable_vis is None:\\n                enable_vis=True\\n\\n            graph_filename=None\\n            if enable_vis:\\n                graph_filename=f\\\"graph_{uuid.uuid4().hex}.png\\\"\\n                graph_filepath=os.path.join(self.output_dir,graph_filename)\\n                if not problem.generate_graph(graph_filepath):\\n                    graph_filename=None\\n\\n            problem_id=uuid.uuid4().hex\\n            date_created=datetime.now().strftime('%Y-%m-%d %H:%M:%S')\\n\\n            self.db_manager.save_problem(problem_id,date_created,problem_text,solution_text,problem_type)\\n\\n            return problem_id,date_created,problem_type,problem_text,solution_text,graph_filename\\n        except Exception as e:\\n            logging.error(f\\\"問題生成エラー: {e}\\\")\\n            logging.error(traceback.format_exc())\\n            return None\\n\"\n    },\n    {\n      \"path\": \"統計検定1級\\\\sympy_solver.py\",\n      \"overview\": \"このスクリプトはPythonコードです。\\n定義されているクラス: SympySolver。\\n定義されている関数: __init__, check_equivalence。\\n\",\n      \"content\": \"# sympy_solver.py\\nclass SympySolver:\\n    def __init__(self):\\n        pass\\n    def check_equivalence(self,user_input,correct_answer):\\n        # 簡略化:常にFalseでエラーなし\\n        return False,\\\"\\\"\"\n    },\n    {\n      \"path\": \"統計検定1級\\\\topics.json\",\n      \"overview\": \"このファイルはテキストファイルまたはその他の形式です。\\n行数: 28\\n\",\n      \"content\": \"{\\n  \\\"topics\\\": {\\n    \\\"確率論\\\": [\\n      \\\"probability_definition\\\",\\n      \\\"conditional_probability\\\",\\n      \\\"distribution_functions\\\",\\n      \\\"joint_distribution\\\",\\n      \\\"probability\\\"\\n    ],\\n    \\\"統計的推定\\\": [\\n      \\\"statistical_inference\\\",\\n      \\\"t_test\\\"\\n    ],\\n    \\\"統計的検定\\\": [\\n      \\\"statistical_inference\\\",\\n      \\\"t_test\\\"\\n    ],\\n    \\\"回帰分析\\\": [\\n      \\\"regression_analysis\\\"\\n    ],\\n    \\\"分散分析\\\": [\\n      \\\"variance_analysis\\\"\\n    ],\\n    \\\"ノンパラメトリック検定\\\": [\\n      \\\"nonparametric_test\\\"\\n    ]\\n  }\\n}\\n\"\n    },\n    {\n      \"path\": \"統計検定1級\\\\__init__.py\",\n      \"overview\": \"このスクリプトはPythonコードです。\\n\",\n      \"content\": \"\"\n    },\n    {\n      \"path\": \"統計検定1級\\\\.vscode\\\\launch.json\",\n      \"overview\": \"このファイルはテキストファイルまたはその他の形式です。\\n行数: 14\\n\",\n      \"content\": \"{\\n    \\\"version\\\": \\\"0.2.0\\\",\\n    \\\"configurations\\\": [\\n        {\\n            \\\"name\\\": \\\"Python: 現在のファイル\\\",\\n            \\\"type\\\": \\\"python\\\",\\n            \\\"request\\\": \\\"launch\\\",\\n            \\\"program\\\": \\\"${file}\\\",\\n            \\\"console\\\": \\\"integratedTerminal\\\",\\n            \\\"justMyCode\\\": true,\\n            \\\"args\\\": [\\\"--generate-pdf\\\"]\\n\\n        }\\n    ]\\n}\"\n    },\n    {\n      \"path\": \"統計検定1級\\\\problem_types\\\\conjugate_problems.py\",\n      \"overview\": \"このスクリプトはPythonコードです。\\n定義されているクラス: BetaBinomialConjugateProblem, GammaPoissonConjugateProblem, DirichletMultinomialConjugateProblem, BinomialPoissonApproxProblem, PoissonNormalApproxProblem。\\n定義されている関数: __init__, generate_parameters, generate_explanation, generate_graph, __init__, generate_parameters, generate_explanation, generate_graph, __init__, generate_parameters, generate_explanation, generate_graph, __init__, generate_parameters, generate_explanation, generate_graph, __init__, generate_parameters, generate_explanation, generate_graph, B, B。\\n\",\n      \"content\": \"# conjugate_problems.py\\nimport math,random\\nfrom problem_types.problem import Problem\\nfrom math import comb,factorial,exp,gamma\\nimport numpy as np\\n\\nclass BetaBinomialConjugateProblem(Problem):\\n    def __init__(self):\\n        super().__init__('beta_binomial_conjugate_problem.tex')\\n    def generate_parameters(self):\\n        self.params['alpha']=random.randint(1,5)\\n        self.params['beta']=random.randint(1,5)\\n        self.params['n']=random.randint(5,20)\\n        self.params['k']=random.randint(0,self.params['n'])\\n        def B(x,y):\\n            return (gamma(x)*gamma(y))/gamma(x+y)\\n        p_x=comb(self.params['n'],self.params['k'])*B(self.params['k']+self.params['alpha'],self.params['n']-self.params['k']+self.params['beta'])/B(self.params['alpha'],self.params['beta'])\\n        self.params['probability']=round(p_x,4)\\n    def generate_explanation(self):\\n        return \\\"Beta+Binomial->Beta-Binomial\\\"\\n    def generate_graph(self,o):return False\\n\\nclass GammaPoissonConjugateProblem(Problem):\\n    def __init__(self):\\n        super().__init__('gamma_poisson_conjugate_problem.tex')\\n    def generate_parameters(self):\\n        self.params['alpha']=random.randint(1,5)\\n        self.params['beta']=random.uniform(0.5,2.0)\\n        self.params['k']=random.randint(0,20)\\n        p=self.params['beta']/(self.params['beta']+1)\\n        q=1-p\\n        negbin_p=comb(self.params['k']+self.params['alpha']-1,self.params['k'])*(q**self.params['k'])*(p**self.params['alpha'])\\n        self.params['probability']=round(negbin_p,4)\\n    def generate_explanation(self):\\n        return \\\"Gamma+Poisson->Negative Binomial\\\"\\n    def generate_graph(self,o):return False\\n\\nclass DirichletMultinomialConjugateProblem(Problem):\\n    def __init__(self):\\n        super().__init__('dirichlet_multinomial_conjugate_problem.tex')\\n    def generate_parameters(self):\\n        m=random.randint(2,4)\\n        self.params['m']=m\\n        self.params['n']=random.randint(5,20)\\n        self.params['alpha_vec']=[random.uniform(1,3) for _ in range(m)]\\n        counts=[0]*m\\n        remain=self.params['n']\\n        for i in range(m-1):\\n            c=random.randint(0,remain)\\n            counts[i]=c\\n            remain-=c\\n        counts[-1]=remain\\n        self.params['counts']=counts\\n        def B(alpha):\\n            import numpy as np\\n            return (np.prod([gamma(a) for a in alpha]))/gamma(sum(alpha))\\n        alpha_x=[self.params['alpha_vec'][i]+counts[i] for i in range(m)]\\n        num=B(alpha_x)\\n        den=B(self.params['alpha_vec'])\\n        multinomial_coef=math.factorial(self.params['n'])\\n        for c in counts:\\n            multinomial_coef/=math.factorial(c)\\n        p_x=multinomial_coef*(num/den)\\n        self.params['probability']=round(p_x,4)\\n    def generate_explanation(self):\\n        return \\\"Dirichlet+Multinomial->Dirichlet-Multinomial\\\"\\n    def generate_graph(self,o):return False\\n\\nclass BinomialPoissonApproxProblem(Problem):\\n    def __init__(self):\\n        super().__init__('binomial_poisson_approx_problem.tex')\\n    def generate_parameters(self):\\n        lam=random.uniform(2,5)\\n        n=random.randint(50,200)\\n        p=lam/n\\n        k=random.randint(0,int(lam*3))\\n        binom_p=comb(n,k)*(p**k)*((1-p)**(n-k))\\n        poisson_p=(lam**k)*exp(-lam)/math.factorial(k)\\n        self.params['n']=n\\n        self.params['p']=round(p,6)\\n        self.params['k']=k\\n        self.params['lambda']=round(lam,3)\\n        self.params['binom_p']=round(binom_p,6)\\n        self.params['poisson_p']=round(poisson_p,6)\\n    def generate_explanation(self):\\n        return \\\"Binomial->Poisson近似条件\\\"\\n    def generate_graph(self,o):return False\\n\\nclass PoissonNormalApproxProblem(Problem):\\n    def __init__(self):\\n        super().__init__('poisson_normal_approx_problem.tex')\\n    def generate_parameters(self):\\n        lam=random.randint(30,100)\\n        low=max(0,int(lam-3*math.sqrt(lam)))\\n        high=int(lam+3*math.sqrt(lam))\\n        k=random.randint(low,high)\\n        poisson_p=(lam**k)*exp(-lam)/math.factorial(k)\\n        self.params['lambda']=lam\\n        self.params['k']=k\\n        self.params['poisson_p']=round(poisson_p,6)\\n        self.params['mean']=lam\\n        self.params['variance']=lam\\n    def generate_explanation(self):\\n        return \\\"Poisson->Normal近似(λ大)\\\"\\n    def generate_graph(self,o):return False\"\n    },\n    {\n      \"path\": \"統計検定1級\\\\problem_types\\\\problem.py\",\n      \"overview\": \"このスクリプトはPythonコードです。\\n定義されているクラス: Problem, ProbabilityProblem, StatisticalInferenceProblem, RegressionAnalysisProblem, TimeSeriesAnalysisProblem, EconometricsProblem, LinearCombinationProblem, DistributionPropertiesProblem, HighMomentProblem, MultivariateNormalProblem, VarianceAnalysisProblem, NonParametricTestProblem, ProbabilityDefinitionProblem, ConditionalProbabilityProblem, DistributionFunctionsProblem, JointDistributionProblem。\\n定義されている関数: __init__, generate_parameters, generate_problem_text, generate_solution_text, generate_explanation, generate_graph, __init__, generate_parameters, _variant_binomial, _variant_poisson, _variant_conditional_probability, generate_explanation, generate_graph, __init__, generate_parameters, generate_graph, __init__, generate_parameters, generate_graph, __init__, generate_parameters, generate_graph, __init__, generate_parameters, generate_graph, __init__, generate_parameters, generate_explanation, generate_graph, __init__, generate_parameters, generate_graph, __init__, generate_parameters, generate_graph, __init__, generate_parameters, generate_explanation, generate_graph, __init__, generate_parameters, generate_explanation, generate_graph, __init__, generate_parameters, generate_explanation, generate_graph, __init__, generate_parameters, generate_explanation, generate_graph, __init__, generate_parameters, generate_explanation, generate_graph, __init__, generate_parameters, generate_explanation, generate_graph, __init__, generate_parameters, generate_explanation, generate_graph。\\n\",\n      \"content\": \"# problem_types/problem.py\\n\\nfrom abc import ABC, abstractmethod\\nimport os\\nimport random\\nimport logging\\nfrom config import config\\nfrom jinja2 import Environment, FileSystemLoader\\nfrom graph import ProbabilityDistributionVisualizer\\nimport traceback\\nfrom math import comb,exp,factorial\\nfrom scipy.stats import norm,stats\\n\\nclass Problem(ABC):\\n    def __init__(self, template_name):\\n        self.params = {}\\n        self.template_name = template_name\\n        templates_dir = os.path.join(\\n            os.path.dirname(os.path.abspath(__file__)),\\n            '..',\\n            config.get('problem_templates_directory', default='templates')\\n        )\\n        self.env = Environment(loader=FileSystemLoader(templates_dir), autoescape=False)\\n        self.visualizer = ProbabilityDistributionVisualizer()\\n\\n    @abstractmethod\\n    def generate_parameters(self):\\n        pass\\n\\n    def generate_problem_text(self):\\n        template = self.env.get_template(self.template_name)\\n        return template.render(**self.params, show_solution=False)\\n\\n    def generate_solution_text(self):\\n        template = self.env.get_template(self.template_name)\\n        self.params['explanation'] = self.generate_explanation()\\n        return template.render(**self.params, show_solution=True)\\n\\n    def generate_explanation(self):\\n        return \\\"\\\"\\n\\n    @abstractmethod\\n    def generate_graph(self, output_path):\\n        pass\\n\\nclass ProbabilityProblem(Problem):\\n    def __init__(self):\\n        super().__init__('probability_problem.tex')\\n    def generate_parameters(self):\\n        variants=[self._variant_binomial,self._variant_poisson,self._variant_conditional_probability]\\n        v=random.choice(variants)\\n        v()\\n    def _variant_binomial(self):\\n        self.params['problem_type']='binomial'\\n        self.params['n']=random.randint(5,20)\\n        self.params['p']=round(random.uniform(0.1,0.9),2)\\n        self.params['k']=random.randint(0,self.params['n'])\\n        from math import comb\\n        prob=comb(self.params['n'],self.params['k'])*(self.params['p']**self.params['k'])*((1-self.params['p'])**(self.params['n']-self.params['k']))\\n        self.params['probability']=round(prob,6)\\n    def _variant_poisson(self):\\n        self.params['problem_type']='poisson'\\n        self.params['lambda']=round(random.uniform(0.5,5.0),2)\\n        self.params['k']=random.randint(0,10)\\n        lam=self.params['lambda']\\n        k=self.params['k']\\n        prob=(lam**k)*exp(-lam)/factorial(k)\\n        self.params['probability']=round(prob,6)\\n    def _variant_conditional_probability(self):\\n        self.params['problem_type']='conditional_probability'\\n        self.params['P_A']=round(random.uniform(0.1,0.9),2)\\n        self.params['P_B_given_A']=round(random.uniform(0.1,0.9),2)\\n        self.params['P_A_and_B']=round(self.params['P_A']*self.params['P_B_given_A'],6)\\n    def generate_explanation(self):\\n        t=self.params['problem_type']\\n        if t=='binomial':\\n            return \\\"二項分布の公式を使用\\\"\\n        elif t=='poisson':\\n            return \\\"ポアソン分布の公式を使用\\\"\\n        elif t=='conditional_probability':\\n            return \\\"条件付き確率P(A∩B)=P(A)*P(B|A)\\\"\\n    def generate_graph(self,output_path):\\n        return self.visualizer.plot_probability(self.params,output_path)\\n\\nclass StatisticalInferenceProblem(Problem):\\n    def __init__(self):\\n        super().__init__('statistical_inference_problem.tex')\\n    def generate_parameters(self):\\n        self.params['sample_mean']=round(random.uniform(50,100),2)\\n        self.params['sample_std']=round(random.uniform(5,15),2)\\n        self.params['n']=random.randint(30,100)\\n        self.params['population_mean']=round(random.uniform(50,100),2)\\n        self.params['alpha']=round(random.uniform(0.01,0.1),2)\\n        t_stat = (self.params['sample_mean']-self.params['population_mean'])/(self.params['sample_std']/(self.params['n']**0.5))\\n        t_stat=round(t_stat,4)\\n        df=self.params['n']-1\\n        cv=round(stats.t.ppf(1-self.params['alpha']/2,df=df),4)\\n        reject='棄却' if abs(t_stat)>cv else '棄却しない'\\n        self.params['t_stat']=t_stat\\n        self.params['critical_value']=cv\\n        self.params['reject_null']=reject\\n        self.params['df']=df\\n    def generate_graph(self,output_path):\\n        return self.visualizer.plot_t_test(self.params,output_path)\\n\\nclass RegressionAnalysisProblem(Problem):\\n    def __init__(self):\\n        super().__init__('regression_analysis_problem.tex')\\n    def generate_parameters(self):\\n        self.params['beta_0']=round(random.uniform(0,10),2)\\n        self.params['beta_1']=round(random.uniform(-5,5),2)\\n        self.params['n']=random.randint(50,200)\\n        self.params['x_values']=[round(random.uniform(0,100),2) for _ in range(self.params['n'])]\\n        self.params['epsilon']=[round(random.gauss(0,10),2) for _ in range(self.params['n'])]\\n        self.params['y_values']=[self.params['beta_0']+self.params['beta_1']*x+e for x,e in zip(self.params['x_values'],self.params['epsilon'])]\\n        import numpy as np\\n        X=np.array(self.params['x_values'])\\n        Y=np.array(self.params['y_values'])\\n        beta_1_hat=np.cov(X,Y,bias=True)[0,1]/np.var(X)\\n        beta_0_hat=np.mean(Y)-beta_1_hat*np.mean(X)\\n        self.params['beta_0_hat']=round(beta_0_hat,4)\\n        self.params['beta_1_hat']=round(beta_1_hat,4)\\n    def generate_graph(self,output_path):\\n        return self.visualizer.plot_regression(self.params['x_values'],self.params['y_values'],self.params['beta_0_hat'],self.params['beta_1_hat'],output_path)\\n\\nclass TimeSeriesAnalysisProblem(Problem):\\n    def __init__(self):\\n        super().__init__('time_series_analysis_problem.tex')\\n    def generate_parameters(self):\\n        self.params['phi']=round(random.uniform(0.5,0.9),2)\\n        self.params['theta']=round(random.uniform(-0.5,0.5),2)\\n        self.params['n']=100\\n        self.params['epsilon']=[random.gauss(0,1) for _ in range(self.params['n'])]\\n        self.params['time_series']=[0]*self.params['n']\\n        for t in range(1,self.params['n']):\\n            self.params['time_series'][t]=self.params['phi']*self.params['time_series'][t-1]+self.params['epsilon'][t]+self.params['theta']*self.params['epsilon'][t-1]\\n    def generate_graph(self,output_path):\\n        return self.visualizer.plot_time_series(self.params,output_path)\\n\\nclass EconometricsProblem(Problem):\\n    def __init__(self):\\n        super().__init__('econometrics_problem.tex')\\n    def generate_parameters(self):\\n        self.params['beta_0']=round(random.uniform(0,5),2)\\n        self.params['beta_1']=round(random.uniform(0,1),2)\\n        self.params['beta_2']=round(random.uniform(-1,0),2)\\n        self.params['n']=random.randint(50,200)\\n        self.params['x1_values']=[round(random.uniform(0,100),2) for _ in range(self.params['n'])]\\n        self.params['x2_values']=[round(random.uniform(0,100),2) for _ in range(self.params['n'])]\\n        self.params['epsilon']=[round(random.gauss(0,5),2) for _ in range(self.params['n'])]\\n        self.params['y_values']=[self.params['beta_0']+self.params['beta_1']*x1+self.params['beta_2']*x2+e for x1,x2,e in zip(self.params['x1_values'],self.params['x2_values'],self.params['epsilon'])]\\n        import numpy as np\\n        X=np.column_stack((np.ones(self.params['n']),self.params['x1_values'],self.params['x2_values']))\\n        Y=np.array(self.params['y_values'])\\n        beta_hat=np.linalg.inv(X.T@X)@X.T@Y\\n        self.params['beta_0_hat']=round(beta_hat[0],4)\\n        self.params['beta_1_hat']=round(beta_hat[1],4)\\n        self.params['beta_2_hat']=round(beta_hat[2],4)\\n    def generate_graph(self,output_path):\\n        return self.visualizer.plot_econometrics(self.params,output_path)\\n\\nclass LinearCombinationProblem(Problem):\\n    def __init__(self):\\n        super().__init__('linear_combination_problem.tex')\\n    def generate_parameters(self):\\n        self.params['a']=random.randint(1,5)\\n        self.params['b']=random.randint(1,5)\\n        self.params['mu1']=round(random.uniform(0,10),2)\\n        self.params['mu2']=round(random.uniform(0,10),2)\\n        self.params['sigma1_squared']=round(random.uniform(1,5),2)\\n        self.params['sigma2_squared']=round(random.uniform(1,5),2)\\n        E_Z=self.params['a']*self.params['mu1']+self.params['b']*self.params['mu2']\\n        Var_Z=(self.params['a']**2)*self.params['sigma1_squared']+(self.params['b']**2)*self.params['sigma2_squared']\\n        self.params['E_Z']=round(E_Z,4)\\n        self.params['Var_Z']=round(Var_Z,4)\\n    def generate_explanation(self):\\n        return \\\"線形結合の期待値・分散計算\\\"\\n    def generate_graph(self,output_path):\\n        return self.visualizer.plot_linear_combination(self.params,output_path)\\n\\nclass DistributionPropertiesProblem(Problem):\\n    def __init__(self):\\n        super().__init__('distribution_properties_problem.tex')\\n    def generate_parameters(self):\\n        dist_choice=random.choice(['正規分布','ポアソン分布','指数分布'])\\n        self.params['distribution']=dist_choice\\n        if dist_choice=='正規分布':\\n            self.params['properties']={'mean':'\\\\\\\\mu','variance':'\\\\\\\\sigma^2'}\\n        elif dist_choice=='ポアソン分布':\\n            self.params['properties']={'mean':'\\\\\\\\lambda','variance':'\\\\\\\\lambda'}\\n        else:\\n            self.params['properties']={'mean':'1/\\\\\\\\lambda','variance':'1/\\\\\\\\lambda^2'}\\n    def generate_graph(self,output_path):\\n        return self.visualizer.plot_distribution_properties(self.params,output_path)\\n\\nclass HighMomentProblem(Problem):\\n    def __init__(self):\\n        super().__init__('high_moment_problem.tex')\\n    def generate_parameters(self):\\n        self.params['n']=random.randint(3,5)\\n        self.params['mu']=round(random.uniform(0,10),2)\\n        self.params['sigma']=round(random.uniform(1,5),2)\\n        from scipy.stats import norm\\n        m=norm.moment(self.params['n'],loc=self.params['mu'],scale=self.params['sigma'])\\n        self.params['moment']=round(m,4)\\n    def generate_graph(self,output_path):\\n        return self.visualizer.plot_high_moment(self.params,output_path)\\n\\nclass MultivariateNormalProblem(Problem):\\n    def __init__(self):\\n        super().__init__('multivariate_normal_problem.tex')\\n    def generate_parameters(self):\\n        self.params['mu']=[round(random.uniform(0,10),2) for _ in range(2)]\\n        self.params['sigma']=[[round(random.uniform(1,5),2),round(random.uniform(0,2),2)],[round(random.uniform(0,2),2),round(random.uniform(1,5),2)]]\\n    def generate_explanation(self):\\n        return \\\"多変量正規分布の性質を利用\\\"\\n    def generate_graph(self,output_path):\\n        return self.visualizer.plot_multivariate_normal(self.params,output_path)\\n\\nclass VarianceAnalysisProblem(Problem):\\n    def __init__(self):\\n        super().__init__('variance_analysis_problem.tex')\\n    def generate_parameters(self):\\n        self.params['group_count']=random.randint(2,4)\\n        self.params['sample_sizes']=[random.randint(5,20) for _ in range(self.params['group_count'])]\\n        self.params['means']=[round(random.uniform(10,50),2) for _ in range(self.params['group_count'])]\\n        self.params['variances']=[round(random.uniform(1,5),2) for _ in range(self.params['group_count'])]\\n        total_n=sum(self.params['sample_sizes'])\\n        group_count=self.params['group_count']\\n        means=self.params['means']\\n        variances=self.params['variances']\\n        sample_sizes=self.params['sample_sizes']\\n        grand_mean=sum([means[i]*sample_sizes[i] for i in range(group_count)])/total_n\\n        ssb=sum([sample_sizes[i]*(means[i]-grand_mean)**2 for i in range(group_count)])\\n        ssw=sum([(sample_sizes[i]-1)*variances[i] for i in range(group_count)])\\n        df_between=group_count-1\\n        df_within=total_n-group_count\\n        msb=ssb/df_between\\n        msw=ssw/df_within\\n        F=msb/msw\\n        alpha=0.05\\n        from scipy.stats import f\\n        F_critical=f.ppf(1-alpha,df_between,df_within)\\n        reject='棄却する' if F>F_critical else '棄却しない'\\n        self.params['F_value']=round(F,4)\\n        self.params['F_critical']=round(F_critical,4)\\n        self.params['reject_null']=reject\\n        self.params['df_between']=df_between\\n        self.params['df_within']=df_within\\n    def generate_explanation(self):\\n        return \\\"一元配置分散分析による検定\\\"\\n    def generate_graph(self,output_path):\\n        return self.visualizer.plot_variance_analysis(self.params,output_path)\\n\\nclass NonParametricTestProblem(Problem):\\n    def __init__(self):\\n        super().__init__('nonparametric_test_problem.tex')\\n    def generate_parameters(self):\\n        self.params['test_type']=random.choice(['Mann-Whitney U','Kruskal-Wallis','Wilcoxon Signed-Rank'])\\n        self.params['sample1']=[round(random.uniform(10,100),2) for _ in range(random.randint(5,20))]\\n        self.params['sample2']=[round(random.uniform(10,100),2) for _ in range(random.randint(5,20))]\\n        self.params['test_result']='有意差なし(例)'\\n    def generate_explanation(self):\\n        return \\\"ノンパラ検定で中央値差を検定\\\"\\n    def generate_graph(self,output_path):\\n        return self.visualizer.plot_nonparametric_test(self.params,output_path)\\n\\nclass ProbabilityDefinitionProblem(Problem):\\n    def __init__(self):\\n        super().__init__('probability_definition_problem.tex')\\n    def generate_parameters(self):\\n        self.params['P_A']=round(random.uniform(0.1,0.9),2)\\n        self.params['P_B']=round(random.uniform(0.1,0.9),2)\\n        self.params['P_A_and_B']=round(self.params['P_A']*self.params['P_B'],4)\\n    def generate_explanation(self):\\n        return \\\"独立性利用 P(A∩B)=P(A)*P(B)\\\"\\n    def generate_graph(self,output_path):\\n        # 単純な棒グラフでP(A), P(B)とP(A∩B)を表示\\n        try:\\n            PA=self.params['P_A']\\n            PB=self.params['P_B']\\n            PAB=self.params['P_A_and_B']\\n            plt.figure()\\n            plt.bar(['P(A)','P(B)','P(A∩B)'],[PA,PB,PAB],color=['blue','green','red'])\\n            plt.title('Probability Definition')\\n            plt.tight_layout()\\n            plt.savefig(output_path)\\n            plt.close()\\n            return True\\n        except:\\n            return False\\n\\nclass ConditionalProbabilityProblem(Problem):\\n    def __init__(self):\\n        super().__init__('conditional_probability_problem.tex')\\n    def generate_parameters(self):\\n        self.params['P_A']=round(random.uniform(0.2,0.8),2)\\n        self.params['P_B_given_A']=round(random.uniform(0.2,0.8),2)\\n        self.params['P_A_and_B']=round(self.params['P_A']*self.params['P_B_given_A'],4)\\n    def generate_explanation(self):\\n        return \\\"P(A∩B)=P(A)*P(B|A)\\\"\\n    def generate_graph(self,output_path):\\n        # graph.pyでconditional_probabilityで実装済み\\n        from graph import ProbabilityDistributionVisualizer\\n        vis=ProbabilityDistributionVisualizer()\\n        return vis.plot_probability(self.params,output_path)\\n\\nclass DistributionFunctionsProblem(Problem):\\n    def __init__(self):\\n        super().__init__('distribution_functions_problem.tex')\\n    def generate_parameters(self):\\n        self.params['function_type']=random.choice(['pdf','cdf'])\\n        self.params['distribution']=random.choice(['正規分布','指数分布'])\\n        if self.params['distribution']=='正規分布':\\n            self.params['mean']=round(random.uniform(-5,5),2)\\n            self.params['std']=round(random.uniform(1,3),2)\\n        else:\\n            self.params['lambda']=round(random.uniform(0.5,2.0),2)\\n    def generate_explanation(self):\\n        return \\\"pdfやcdf定義式利用\\\"\\n    def generate_graph(self,output_path):\\n        # ここは実装なし、増やしてもよいが現状テンプレ通り\\n        return False\\n\\nclass JointDistributionProblem(Problem):\\n    def __init__(self):\\n        super().__init__('joint_distribution_problem.tex')\\n    def generate_parameters(self):\\n        A_and_B=round(random.uniform(0.05,0.2),2)\\n        A_and_notB=round(random.uniform(0.05,0.2),2)\\n        notA_and_B=round(random.uniform(0.05,0.2),2)\\n        notA_and_notB=round(random.uniform(0.05,0.2),2)\\n        total=A_and_B+A_and_notB+notA_and_B+notA_and_notB\\n        A_and_B/=total\\n        A_and_notB/=total\\n        notA_and_B/=total\\n        notA_and_notB/=total\\n        self.params['joint_probabilities']={'A_and_B':round(A_and_B,4),'A_and_not_B':round(A_and_notB,4),'not_A_and_B':round(notA_and_B,4),'not_A_and_not_B':round(notA_and_notB,4)}\\n        P_A=A_and_B+A_and_notB\\n        P_B=A_and_B+notA_and_B\\n        P_BA=A_and_B/P_A if P_A>0 else 0.0\\n        self.params['P_A']=round(P_A,4)\\n        self.params['P_B']=round(P_B,4)\\n        self.params['P_B_given_A']=round(P_BA,4)\\n    def generate_explanation(self):\\n        return \\\"同時→周辺→条件付き確率\\\"\\n    def generate_graph(self,output_path):\\n        # 簡易的にjoint分布表をHeatmapで可視化\\n        try:\\n            p=self.params['joint_probabilities']\\n            matrix=np.array([[p['A_and_B'],p['A_and_not_B']],[p['not_A_and_B'],p['not_A_and_not_B']]])\\n            plt.figure()\\n            plt.imshow(matrix,cmap='Blues',interpolation='nearest')\\n            plt.colorbar(label='Probability')\\n            plt.xticks([0,1],['B','not B'])\\n            plt.yticks([0,1],['A','not A'])\\n            plt.title('Joint Distribution Heatmap')\\n            plt.tight_layout()\\n            plt.savefig(output_path)\\n            plt.close()\\n            return True\\n        except:\\n            return False\\n\"\n    },\n    {\n      \"path\": \"統計検定1級\\\\problem_types\\\\problem_factory.py\",\n      \"overview\": \"このスクリプトはPythonコードです。\\n定義されているクラス: ProblemFactory。\\n定義されている関数: __init__, create_problem。\\n\",\n      \"content\": \"# problem_types/problem_factory.py\\n\\nimport logging,traceback\\nfrom problem_types.problem import ProbabilityProblem,StatisticalInferenceProblem,RegressionAnalysisProblem,TimeSeriesAnalysisProblem,EconometricsProblem,LinearCombinationProblem,DistributionPropertiesProblem,HighMomentProblem,MultivariateNormalProblem,VarianceAnalysisProblem,NonParametricTestProblem,ProbabilityDefinitionProblem,ConditionalProbabilityProblem,DistributionFunctionsProblem,JointDistributionProblem\\nfrom problem_types.conjugate_problems import BetaBinomialConjugateProblem,GammaPoissonConjugateProblem,DirichletMultinomialConjugateProblem,BinomialPoissonApproxProblem,PoissonNormalApproxProblem\\n\\nclass ProblemFactory:\\n    def __init__(self):\\n        self.problem_classes={\\n            'probability':ProbabilityProblem,\\n            'statistical_inference':StatisticalInferenceProblem,\\n            'regression_analysis':RegressionAnalysisProblem,\\n            'time_series_analysis':TimeSeriesAnalysisProblem,\\n            'econometrics':EconometricsProblem,\\n            'linear_combination':LinearCombinationProblem,\\n            'distribution_properties':DistributionPropertiesProblem,\\n            'high_moment':HighMomentProblem,\\n            'multivariate_normal':MultivariateNormalProblem,\\n            'probability_definition':ProbabilityDefinitionProblem,\\n            'conditional_probability':ConditionalProbabilityProblem,\\n            'distribution_functions':DistributionFunctionsProblem,\\n            'joint_distribution':JointDistributionProblem,\\n            't_test':StatisticalInferenceProblem,\\n            'variance_analysis':VarianceAnalysisProblem,\\n            'nonparametric_test':NonParametricTestProblem,\\n            'beta_binomial_conjugate':BetaBinomialConjugateProblem,\\n            'gamma_poisson_conjugate':GammaPoissonConjugateProblem,\\n            'dirichlet_multinomial_conjugate':DirichletMultinomialConjugateProblem,\\n            'binomial_poisson_approx':BinomialPoissonApproxProblem,\\n            'poisson_normal_approx':PoissonNormalApproxProblem\\n        }\\n    def create_problem(self,problem_type):\\n        pc=self.problem_classes.get(problem_type)\\n        if pc:\\n            try:\\n                return pc()\\n            except Exception as e:\\n                logging.error(f\\\"{problem_type} problem generation error:{e}\\\")\\n                logging.error(traceback.format_exc())\\n                raise\\n        else:\\n            raise ValueError(f\\\"Unknown problem type:{problem_type}\\\")\"\n    },\n    {\n      \"path\": \"統計検定1級\\\\problem_types\\\\__init__.py\",\n      \"overview\": \"このスクリプトはPythonコードです。\\n\",\n      \"content\": \"\"\n    },\n    {\n      \"path\": \"統計検定1級\\\\templates\\\\distribution_properties_problem.tex\",\n      \"overview\": \"このファイルはテキストファイルまたはその他の形式です。\\n行数: 7\\n\",\n      \"content\": \"% distribution_properties_problem.tex\\n{% if not show_solution %}\\n{{ distribution }} の平均と分散を求めよ。\\n{% else %}\\n平均: ${{ properties.mean }}$, 分散: ${{ properties.variance }}$\\n{{ explanation }}\\n{% endif %}\\n\"\n    },\n    {\n      \"path\": \"統計検定1級\\\\templates\\\\distribution_relations.tex\",\n      \"overview\": \"このファイルはテキストファイルまたはその他の形式です。\\n行数: 7\\n\",\n      \"content\": \"% distribution_relations.tex\\n\\\\section*{Distribution Relations}\\n- Beta+Binomial -> Beta-Binomial\\n- Gamma+Poisson -> Negative Binomial\\n- Dirichlet+Multinomial -> Dirichlet-Multinomial\\n- Binomial->Poisson approximation\\n- Poisson->Normal approximation\\n\"\n    },\n    {\n      \"path\": \"統計検定1級\\\\templates\\\\econometrics_problem.tex\",\n      \"overview\": \"このファイルはテキストファイルまたはその他の形式です。\\n行数: 6\\n\",\n      \"content\": \"% econometrics_problem.tex\\n{% if not show_solution %}\\n計量経済学モデルに関する問題\\n{% else %}\\n解答と推定量\\n{{ explanation }}\\n{% endif %}\"\n    },\n    {\n      \"path\": \"統計検定1級\\\\templates\\\\high_moment_problem.tex\",\n      \"overview\": \"このファイルはテキストファイルまたはその他の形式です。\\n行数: 6\\n\",\n      \"content\": \"% high_moment_problem.tex\\n{% if not show_solution %}\\n正規分布の{{ n }}次モーメントは？(例: $E[X^{n}]$ )\\n{% else %}\\n$E[X^{n}]={{ moment }}$\\n{{ explanation }}\\n{% endif %}\"\n    },\n    {\n      \"path\": \"統計検定1級\\\\templates\\\\linear_combination_problem.tex\",\n      \"overview\": \"このファイルはテキストファイルまたはその他の形式です。\\n行数: 6\\n\",\n      \"content\": \"% linear_combination_problem.tex\\n{% if not show_solution %}\\nZ=aX+bY のE[Z],Var[Z]\\n{% else %}\\nE[Z],Var[Z]\\n{{ explanation }}\\n{% endif %}\"\n    },\n    {\n      \"path\": \"統計検定1級\\\\templates\\\\multivariate_normal_problem.tex\",\n      \"overview\": \"このファイルはテキストファイルまたはその他の形式です。\\n行数: 6\\n\",\n      \"content\": \"% multivariate_normal_problem.tex\\n{% if not show_solution %}\\n多変量正規に関する問題\\n{% else %}\\n解答\\n{{ explanation }}\\n{% endif %}\"\n    },\n    {\n      \"path\": \"統計検定1級\\\\templates\\\\nonparametric_test_problem.tex\",\n      \"overview\": \"このファイルはテキストファイルまたはその他の形式です。\\n行数: 6\\n\",\n      \"content\": \"% nonparametric_test_problem.tex\\n{% if not show_solution %}\\nノンパラ検定問題\\n{% else %}\\n検定結果\\n{{ explanation }}\\n{% endif %}\"\n    },\n    {\n      \"path\": \"統計検定1級\\\\templates\\\\poisson_normal_approx_problem.tex\",\n      \"overview\": \"このファイルはテキストファイルまたはその他の形式です。\\n行数: 6\\n\",\n      \"content\": \"% poisson_normal_approx_problem.tex\\n{% if not show_solution %}\\nPoisson→Normal近似\\n{% else %}\\nPoisson P=${{ poisson_p }}$, mean=${{mean}}$ var=${{variance}}$\\n{{ explanation }}\\n{% endif %}\"\n    },\n    {\n      \"path\": \"統計検定1級\\\\templates\\\\probability_definition_problem.tex\",\n      \"overview\": \"このファイルはテキストファイルまたはその他の形式です。\\n行数: 6\\n\",\n      \"content\": \"% probability_definition_problem.tex\\n{% if not show_solution %}\\nP(A∩B)求めよ\\n{% else %}\\n結果\\n{{ explanation }}\\n{% endif %}\"\n    },\n    {\n      \"path\": \"統計検定1級\\\\templates\\\\probability_problem.tex\",\n      \"overview\": \"このファイルはテキストファイルまたはその他の形式です。\\n行数: 7\\n\",\n      \"content\": \"% probability_problem.tex\\n{% if not show_solution %}\\n確率計算問題（例）\\n問題タイプ: {{ problem_type }}\\n{% else %}\\n解答と説明: {{ explanation }}\\n計算結果: P = {{ probability }}\\n{% endif %}\"\n    },\n    {\n      \"path\": \"統計検定1級\\\\templates\\\\regression_analysis_problem.tex\",\n      \"overview\": \"このファイルはテキストファイルまたはその他の形式です。\\n行数: 6\\n\",\n      \"content\": \"% regression_analysis_problem.tex\\n{% if not show_solution %}\\n回帰分析問題\\n{% else %}\\n回帰係数結果\\n{{ explanation }}\\n{% endif %}\"\n    },\n    {\n      \"path\": \"統計検定1級\\\\templates\\\\statistical_inference_problem.tex\",\n      \"overview\": \"このファイルはテキストファイルまたはその他の形式です。\\n行数: 6\\n\",\n      \"content\": \"% statistical_inference_problem.tex\\n{% if not show_solution %}\\n統計的推定/検定問題\\n{% else %}\\n検定結果\\n{{ explanation }}\\n{% endif %}\"\n    },\n    {\n      \"path\": \"統計検定1級\\\\templates\\\\time_series_analysis_problem.tex\",\n      \"overview\": \"このファイルはテキストファイルまたはその他の形式です。\\n行数: 6\\n\",\n      \"content\": \"% time_series_analysis_problem.tex\\n{% if not show_solution %}\\n時系列分析問題\\n{% else %}\\n解答と説明\\n{{ explanation }}\\n{% endif %}\"\n    },\n    {\n      \"path\": \"統計検定1級\\\\templates\\\\variance_analysis_problem.tex\",\n      \"overview\": \"このファイルはテキストファイルまたはその他の形式です。\\n行数: 6\\n\",\n      \"content\": \"% variance_analysis_problem.tex\\n{% if not show_solution %}\\n分散分析問題\\n{% else %}\\nANOVA結果\\n{{ explanation }}\\n{% endif %}\"\n    }\n  ]\n}"
    },
    {
      "path": "collected_scripts - コピー.json",
      "overview": "JSONファイル (辞書)。キー: system_overview, settings, scripts\n",
      "content": "{\n  \"system_overview\": \"\",\n  \"settings\": {},\n  \"scripts\": [\n    {\n      \"path\": \"feedback.json\",\n      \"overview\": \"JSONファイル (辞書)。キー: timestamp, status, remarks, possible_improvements\\n\",\n      \"content\": \"{\\n  \\\"timestamp\\\": \\\"20250109_142942\\\",\\n  \\\"status\\\": \\\"Completed\\\",\\n  \\\"remarks\\\": \\\"\\\",\\n  \\\"possible_improvements\\\": [\\n    \\\"ログレベルをDEBUGにして詳細確認\\\",\\n    \\\"追加パラメータを考慮した解析を実装する\\\",\\n    \\\"AIプロンプトの自動生成を検討\\\"\\n  ]\\n}\"\n    },\n    {\n      \"path\": \".vscode\\\\launch.json\",\n      \"overview\": \"JSON解析エラー: Expecting property name enclosed in double quotes: line 2 column 5 (char 6)\\n\",\n      \"content\": \"{\\n    // IntelliSense を使用して利用可能な属性を学べます。\\n    // 既存の属性の説明をホバーして表示します。\\n    // 詳細情報は次を確認してください: https://go.microsoft.com/fwlink/?linkid=830387\\n    \\\"version\\\": \\\"0.2.0\\\",\\n    \\\"configurations\\\": [\\n        {\\n            \\\"name\\\": \\\"Python: 現在のファイル\\\",\\n            \\\"type\\\": \\\"python\\\",\\n            \\\"request\\\": \\\"launch\\\",\\n            \\\"program\\\": \\\"${file}\\\",\\n            \\\"console\\\": \\\"integratedTerminal\\\",\\n            \\\"justMyCode\\\": true,\\n        }\\n    ]\\n}\"\n    },\n    {\n      \"path\": \"your_project\\\\credential_manager.py\",\n      \"overview\": \"Pythonコード。\\nクラス: CredentialManager。\\n関数: decrypt_credentials。\\n\",\n      \"content\": \"import logging\\nfrom typing import Dict\\n\\nclass CredentialManager:\\n\\n    @staticmethod\\n    def decrypt_credentials(encrypted_credentials: Dict) -> Dict:\\n        \\\"\\\"\\\"\\n        入力:\\n          encrypted_credentials (dict): {\\\"user\\\": \\\"encrypted_user\\\", \\\"password\\\": \\\"encrypted_password\\\"}\\n        出力:\\n          dict – 復号化された認証情報、例: {\\\"user\\\": \\\"decrypted_user\\\", \\\"password\\\": \\\"decrypted_password\\\"}\\n        ※ 本実装はダミーで、実際には復号化処理を実装すべき箇所です。\\n        \\\"\\\"\\\"\\n        decrypted = {}\\n        for (key, value) in encrypted_credentials.items():\\n            decrypted[key] = 'decrypted_' + str(value)\\n        logging.info('CredentialManager: Credentials decrypted (dummy implementation).')\\n        return decrypted\"\n    },\n    {\n      \"path\": \"your_project\\\\data_pipeline.py\",\n      \"overview\": \"Pythonコード。\\nクラス: DataPipeline。\\n関数: run_pipeline。\\n\",\n      \"content\": \"import asyncio\\nimport logging\\nfrom typing import Union, List, Dict\\nimport pandas as pd\\nfrom data_preparation.data_loader import DataLoader\\nfrom data_preparation.data_preprocessor import DataPreprocessor\\n\\nclass DataPipeline:\\n\\n    def run_pipeline(self, input_source: Union[str, dict, list], config: Dict, data_types: List[str]) -> Dict:\\n        \\\"\\\"\\\"\\n        入力:\\n          input_source (Union[str, dict, list]): DataLoader に渡す入力ソース\\n          config (dict): 全体の設定情報\\n          data_types (List[str]): 例: ['text', 'html', 'csv', 'json', 'xls']\\n        出力:\\n          dict – {\\\"files\\\": <List[str]>, \\\"processed_data\\\": <DataFrame/その他>}\\n        \\\"\\\"\\\"\\n        loader = DataLoader(input_source, data_types)\\n        files = loader.collect_files()\\n        preprocessor = DataPreprocessor()\\n        raw_data = asyncio.run(loader.file_processor.process_files_in_parallel(files))\\n        processed_data = preprocessor.preprocess_data(raw_data)\\n        logging.info('DataPipeline: Pipeline run completed.')\\n        return {'files': files, 'processed_data': processed_data}\"\n    },\n    {\n      \"path\": \"your_project\\\\feedback.json\",\n      \"overview\": \"JSONファイル (辞書)。キー: timestamp, status, remarks, possible_improvements\\n\",\n      \"content\": \"{\\n  \\\"timestamp\\\": \\\"20250109_142942\\\",\\n  \\\"status\\\": \\\"Completed\\\",\\n  \\\"remarks\\\": \\\"\\\",\\n  \\\"possible_improvements\\\": [\\n    \\\"ログレベルをDEBUGにして詳細確認\\\",\\n    \\\"追加パラメータを考慮した解析を実装する\\\",\\n    \\\"AIプロンプトの自動生成を検討\\\"\\n  ]\\n}\"\n    },\n    {\n      \"path\": \"your_project\\\\input_parser.py\",\n      \"overview\": \"Pythonコード。\\nクラス: InputParser。\\n関数: parse_input_source。\\n\",\n      \"content\": \"import json\\nimport logging\\nfrom typing import Union\\n\\nclass InputParser:\\n\\n    @staticmethod\\n    def parse_input_source(raw_input: str) -> Union[str, dict, list]:\\n        \\\"\\\"\\\"\\n        入力:\\n          raw_input (str): コマンドラインから渡される入力文字列。JSON形式なら解析する。\\n        出力:\\n          Union[str, dict, list] – JSON形式の場合は dict または list、そうでなければそのままの文字列。\\n        \\\"\\\"\\\"\\n        try:\\n            parsed = json.loads(raw_input)\\n            logging.info('InputParser: Input parsed as JSON.')\\n            return parsed\\n        except json.JSONDecodeError:\\n            logging.info('InputParser: Input is a plain string.')\\n            return raw_input\"\n    },\n    {\n      \"path\": \"your_project\\\\main.py\",\n      \"overview\": \"Pythonコード。\\n関数: debug_show_sibling_folders, parse_arguments, validate_input_path, load_and_preprocess_data, apply_analysis_methods, save_performance_metrics, load_config, main。\\n\",\n      \"content\": \"import sys\\nimport os\\nimport logging\\nimport argparse\\nimport asyncio\\nimport datetime\\nimport json\\nfrom typing import Any, Dict, List\\nsys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))\\nfrom data_preparation.data_loader import DataLoader\\nfrom data_preparation.data_preprocessor import DataPreprocessor\\nfrom models.model_manager import ModelManager\\nfrom utils.output_manager import OutputManager\\nfrom utils.logger import Logger\\nfrom utils.db_utils import DBUtils\\nfrom performance.performance_tracker import PerformanceTracker\\nfrom visualization.visualizer import Visualizer\\nfrom utils.config_manager import ConfigManager\\nfrom input_parser import InputParser\\nfrom core.finalize import finalize_with_feedback\\n\\ndef debug_show_sibling_folders(base_dir: str) -> None:\\n    \\\"\\\"\\\"\\n    指定ディレクトリの親フォルダにあるファイル・フォルダをログ出力\\n    \\\"\\\"\\\"\\n    parent_dir = os.path.dirname(base_dir)\\n    logging.info('=== Debug: Sibling folders/files ===')\\n    try:\\n        for item in os.listdir(parent_dir):\\n            full_path = os.path.join(parent_dir, item)\\n            if os.path.isdir(full_path):\\n                logging.info(f'Dir: {item}')\\n            else:\\n                logging.info(f'File: {item}')\\n    except Exception as e:\\n        logging.warning(f'Error listing siblings: {e}', exc_info=True)\\n    logging.info('=== End of listing ===')\\n\\ndef parse_arguments(default_input: str):\\n    parser = argparse.ArgumentParser(description='汎用データ分析基盤ツール')\\n    parser.add_argument('--input', type=str, default=default_input, help='Input data source (folder path or media descriptor in JSON)')\\n    return parser.parse_args()\\n\\ndef validate_input_path(input_path: str) -> None:\\n    if not os.path.exists(input_path):\\n        raise FileNotFoundError(f'Input path not found: {input_path}')\\n\\ndef load_and_preprocess_data(input_source: Any, data_types: List[str]) -> Any:\\n    from performance.performance_tracker import PerformanceTracker\\n    performance_local = PerformanceTracker()\\n    performance_local.start_timer('data_loading')\\n    loader = DataLoader(input_source, data_types)\\n    raw_data = loader.load_data()\\n    performance_local.end_timer('data_loading')\\n    logging.info('Data loading completed.')\\n    performance_local.start_timer('data_preprocessing')\\n    from data_preparation.data_preprocessor import DataPreprocessor\\n    preprocessor = DataPreprocessor()\\n    df_data = preprocessor.preprocess_data(raw_data)\\n    performance_local.end_timer('data_preprocessing')\\n    logging.info('Data preprocessing completed.')\\n    return df_data\\n\\ndef apply_analysis_methods(df_data: Any, base_dir: str, output_dir: str, enable_visualization: bool) -> None:\\n    from performance.performance_tracker import PerformanceTracker\\n    performance_local = PerformanceTracker()\\n    from models.model_manager import ModelManager\\n    from utils.output_manager import OutputManager\\n    from visualization.visualizer import Visualizer\\n    model_manager = ModelManager()\\n    output_manager = OutputManager()\\n    available_methods = model_manager.get_available_methods()\\n    logging.info(f'Available analysis methods: {available_methods}')\\n    for method_name in available_methods:\\n        logging.info(f\\\"Applying analysis method '{method_name}'\\\")\\n        try:\\n            performance_local.start_timer(method_name)\\n            result = model_manager.apply_method(method_name, df_data)\\n            performance_local.end_timer(method_name)\\n            if result.empty:\\n                logging.info(f\\\"No valid result for '{method_name}'.\\\")\\n                continue\\n            output_manager.save_results(result, method_name, output_dir)\\n            asyncio.run(output_manager.save_to_database(result, method_name))\\n            logging.info(f\\\"Result saved for '{method_name}'.\\\")\\n            if enable_visualization:\\n                vis_dir = os.path.join(output_dir, 'visualizations')\\n                os.makedirs(vis_dir, exist_ok=True)\\n                visualizer = Visualizer()\\n                visualizer.visualize(method_name, result, vis_dir)\\n                logging.info(f\\\"Visualization saved for '{method_name}'.\\\")\\n            report_dir = os.path.join(output_dir, 'reports')\\n            os.makedirs(report_dir, exist_ok=True)\\n            try:\\n                output_manager.save_html_report(result, method_name, report_dir)\\n                logging.info(f\\\"HTML report saved for '{method_name}'.\\\")\\n            except Exception as err:\\n                logging.error(f\\\"Error generating HTML report for '{method_name}': {err}\\\", exc_info=True)\\n        except Exception as e:\\n            logging.error(f\\\"Error executing '{method_name}': {e}\\\", exc_info=True)\\n\\ndef save_performance_metrics(performance: PerformanceTracker, base_dir: str, output_dir: str) -> None:\\n    from utils.output_manager import OutputManager\\n    perf_data = performance.get_metrics()\\n    output_manager = OutputManager()\\n    csv_path = os.path.join(output_dir, 'performance_metrics.csv')\\n    output_manager.save_performance_metrics(perf_data, csv_path)\\n    logging.info(f'Performance metrics saved at: {csv_path}')\\n\\ndef load_config() -> ConfigManager:\\n    from utils.config_manager import ConfigManager\\n    config_manager = ConfigManager()\\n    config_path = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), 'config.json')\\n    config_manager.load_config(config_path)\\n    return config_manager\\n\\ndef main() -> None:\\n    run_timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\\n    base_dir = os.path.dirname(os.path.abspath(__file__))\\n    run_output_dir = os.path.join(base_dir, f'run_{run_timestamp}')\\n    os.makedirs(run_output_dir, exist_ok=True)\\n    log_dir = os.path.join(run_output_dir, 'logs')\\n    os.makedirs(log_dir, exist_ok=True)\\n    initial_log_level = 'INFO'\\n    Logger.setup_logging(log_dir, initial_log_level)\\n    logging.info('===== Execution Started =====')\\n    debug_show_sibling_folders(base_dir)\\n    config = load_config()\\n    log_level = config.get('log_level', default='INFO')\\n    data_directory = config.get('data_directory', default='data')\\n    output_directory = config.get('output_directory', default='outputs')\\n    database_path = config.get('database_path', default='data.db')\\n    data_types = config.get('data_types', default=['text', 'html', 'csv', 'json'])\\n    enable_visualization = config.get('enable_visualization', default=True)\\n    Logger.setup_logging(log_dir, log_level)\\n    actual_output_dir = os.path.join(run_output_dir, output_directory)\\n    os.makedirs(actual_output_dir, exist_ok=True)\\n    db_full_path = os.path.join(run_output_dir, os.path.basename(database_path))\\n    logging.info(f'Configuration loaded. log_level={log_level}, data_directory={data_directory}, output_directory={output_directory}')\\n    logging.info(f'run_output_dir={run_output_dir}, db_full_path={db_full_path}')\\n    performance = PerformanceTracker()\\n    performance.start_timer('total_execution')\\n    try:\\n        asyncio.run(DBUtils.initialize_database(db_full_path))\\n    except Exception as init_db_err:\\n        logging.error(f'Error initializing DB: {init_db_err}', exc_info=True)\\n        sys.exit(1)\\n    args = parse_arguments(data_directory)\\n    parent_dir = os.path.dirname(base_dir)\\n    input_path = os.path.join(parent_dir, args.input)\\n    try:\\n        validate_input_path(input_path)\\n    except FileNotFoundError as fnf_err:\\n        logging.error(fnf_err, exc_info=True)\\n        finalize_with_feedback(run_output_dir, run_timestamp, 'DB初期化失敗', extra_remarks=str(fnf_err))\\n        sys.exit(1)\\n    except Exception as val_err:\\n        logging.error(val_err, exc_info=True)\\n        finalize_with_feedback(run_output_dir, run_timestamp, '入力パス検証失敗', extra_remarks=str(val_err))\\n        sys.exit(1)\\n    try:\\n        df_data = load_and_preprocess_data(input_path, data_types)\\n    except Exception as load_err:\\n        logging.error(f'Error in data loading/preprocessing: {load_err}', exc_info=True)\\n        finalize_with_feedback(run_output_dir, run_timestamp, 'データ読み込み or 前処理失敗', extra_remarks=str(load_err))\\n        sys.exit(1)\\n    try:\\n        apply_analysis_methods(df_data, base_dir, actual_output_dir, enable_visualization)\\n    except Exception as analyze_err:\\n        logging.error(f'Error during analysis: {analyze_err}', exc_info=True)\\n        finalize_with_feedback(run_output_dir, run_timestamp, '解析失敗', extra_remarks=str(analyze_err))\\n        sys.exit(1)\\n    performance.end_timer('total_execution')\\n    save_performance_metrics(performance, base_dir, actual_output_dir)\\n    try:\\n        asyncio.run(DBUtils.close_connection())\\n    except Exception as close_err:\\n        logging.error(f'Error closing DB: {close_err}', exc_info=True)\\n    logging.info('Analysis completed. Results saved in output folder.')\\n    finalize_with_feedback(run_output_dir, run_timestamp, 'Completed')\\n    sys.exit(0)\\nif __name__ == '__main__':\\n    main()\"\n    },\n    {\n      \"path\": \"your_project\\\\media_manager.py\",\n      \"overview\": \"Pythonコード。\\nクラス: MediaManager。\\n\",\n      \"content\": \"import asyncio\\nimport logging\\nfrom typing import List, Dict\\nfrom media_scanner import scan_media\\n\\nclass MediaManager:\\n    \\\"\\\"\\\"\\n    複数の媒体ディスクリプタを管理し、各媒体からのファイル収集結果を統合するクラス。\\n    \\\"\\\"\\\"\\n\\n    async def scan_all_media(self, media_list: List[Dict]) -> List[str]:\\n        \\\"\\\"\\\"\\n        入力:\\n          media_list (List[dict]): 各媒体の設定情報リスト\\n        出力:\\n          List[str] – 各媒体から取得したファイルパスを統合したリスト\\n        \\\"\\\"\\\"\\n        loop = asyncio.get_event_loop()\\n        tasks = [loop.run_in_executor(None, scan_media, media) for media in media_list]\\n        results = await asyncio.gather(*tasks)\\n        aggregated = [item for sublist in results for item in sublist]\\n        logging.info(f'MediaManager: Aggregated {len(aggregated)} files from {len(media_list)} media sources.')\\n        return aggregated\"\n    },\n    {\n      \"path\": \"your_project\\\\media_scanner.py\",\n      \"overview\": \"Pythonコード。\\n関数: scan_media。\\n\",\n      \"content\": \"import os\\nimport logging\\nfrom typing import List, Dict\\n\\ndef scan_media(media_descriptor: Dict) -> List[str]:\\n    \\\"\\\"\\\"\\n    入力:\\n      media_descriptor (dict): 例\\n        {\\n          \\\"type\\\": \\\"local\\\" or \\\"network\\\",             // 現在は単純にマウント済みとして扱う\\n          \\\"mount_point\\\": \\\"/mnt/data\\\",                 // マウント済みのパス\\n          \\\"credentials\\\": {                            // ダミー実装（認証は未実装）\\n              \\\"user\\\": \\\"dummy_user\\\",\\n              \\\"password\\\": \\\"dummy_password\\\"\\n          },\\n          \\\"include_subdirs\\\": True,                    // サブディレクトリも探索するか\\n          \\\"file_extensions\\\": [\\\".txt\\\", \\\".csv\\\", \\\".json\\\", \\\".xls\\\", \\\".html\\\"]\\n        }\\n    出力:\\n      List[str] – 指定媒体内から収集されたファイルパスのリスト\\n    \\\"\\\"\\\"\\n    mount_point = media_descriptor.get('mount_point')\\n    include_subdirs = media_descriptor.get('include_subdirs', True)\\n    file_extensions = media_descriptor.get('file_extensions', [])\\n    if not mount_point or not os.path.exists(mount_point):\\n        logging.error(f'Mount point does not exist: {mount_point}')\\n        return []\\n    file_list = []\\n    if include_subdirs:\\n        for (root, _, files) in os.walk(mount_point):\\n            for file in files:\\n                ext = os.path.splitext(file)[1].lower()\\n                if not file_extensions or ext in file_extensions:\\n                    file_list.append(os.path.join(root, file))\\n    else:\\n        try:\\n            for file in os.listdir(mount_point):\\n                file_path = os.path.join(mount_point, file)\\n                if os.path.isfile(file_path):\\n                    ext = os.path.splitext(file)[1].lower()\\n                    if not file_extensions or ext in file_extensions:\\n                        file_list.append(file_path)\\n        except Exception as e:\\n            logging.error(f'Error listing files in {mount_point}: {e}', exc_info=True)\\n    logging.info(f'scan_media: Found {len(file_list)} files in {mount_point}')\\n    return file_list\"\n    },\n    {\n      \"path\": \"your_project\\\\__init__.py\",\n      \"overview\": \"Pythonコード。\\nDocstring(冒頭60文字): プロジェクトのルート __init__.py\\nここでは特に処理はなく、パッケージとして認識させるためだけの空ファイル。...\\n\",\n      \"content\": \"\\\"\\\"\\\"\\nプロジェクトのルート __init__.py\\nここでは特に処理はなく、パッケージとして認識させるためだけの空ファイル。\\n\\\"\\\"\\\"\"\n    },\n    {\n      \"path\": \"your_project\\\\.vscode\\\\launch.json\",\n      \"overview\": \"JSON解析エラー: Expecting property name enclosed in double quotes: line 2 column 5 (char 6)\\n\",\n      \"content\": \"{\\n    // IntelliSense を使用して利用可能な属性を学べます。\\n    // 既存の属性の説明をホバーして表示します。\\n    // 詳細情報は次を確認してください: https://go.microsoft.com/fwlink/?linkid=830387\\n    \\\"version\\\": \\\"0.2.0\\\",\\n    \\\"configurations\\\": [\\n        {\\n            \\\"name\\\": \\\"Python: 現在のファイル\\\",\\n            \\\"type\\\": \\\"python\\\",\\n            \\\"request\\\": \\\"launch\\\",\\n            \\\"program\\\": \\\"${file}\\\",\\n            \\\"console\\\": \\\"integratedTerminal\\\",\\n            \\\"justMyCode\\\": true,\\n        }\\n    ]\\n}\"\n    },\n    {\n      \"path\": \"your_project\\\\adapters\\\\db\\\\db_utils.py\",\n      \"overview\": \"Pythonコード。\\nクラス: DBUtils。\\n\",\n      \"content\": \"import aiosqlite\\nimport logging\\nimport os\\n\\nclass DBUtils:\\n    _connection = None\\n    _db_path = None\\n\\n    @staticmethod\\n    async def initialize_database(db_path='data.db'):\\n        DBUtils._db_path = db_path\\n        if not os.path.exists(db_path):\\n            conn = await aiosqlite.connect(db_path)\\n            await conn.execute('\\\\n                CREATE TABLE IF NOT EXISTS analysis_results (\\\\n                    id INTEGER PRIMARY KEY AUTOINCREMENT,\\\\n                    method_name TEXT,\\\\n                    result_data TEXT,\\\\n                    timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP\\\\n                )\\\\n            ')\\n            await conn.commit()\\n            await conn.close()\\n            logging.info(f'DB初期化完了: {db_path}')\\n        else:\\n            logging.info(f'既存DB使用: {db_path}')\\n        await DBUtils.get_connection()\\n\\n    @staticmethod\\n    async def get_connection():\\n        if DBUtils._connection is None:\\n            if DBUtils._db_path is None:\\n                raise ValueError('DBパス未設定。先にinitialize_databaseを呼ぶ必要があります。')\\n            DBUtils._connection = await aiosqlite.connect(DBUtils._db_path)\\n        return DBUtils._connection\\n\\n    @staticmethod\\n    async def close_connection():\\n        if DBUtils._connection:\\n            await DBUtils._connection.close()\\n            DBUtils._connection = None\\n            logging.info('DB接続を閉じました。')\"\n    },\n    {\n      \"path\": \"your_project\\\\adapters\\\\parsers\\\\base_parser.py\",\n      \"overview\": \"Pythonコード。\\nクラス: BaseParser。\\n関数: parse, supported_extensions, data_type。\\n\",\n      \"content\": \"from abc import ABC, abstractmethod\\nfrom typing import Any, List, Dict\\n\\nclass BaseParser(ABC):\\n\\n    @abstractmethod\\n    def parse(self, file_path: str, encoding: str='utf-8') -> List[Dict[str, Any]]:\\n        pass\\n\\n    @abstractmethod\\n    def supported_extensions(self) -> List[str]:\\n        pass\\n\\n    @abstractmethod\\n    def data_type(self) -> str:\\n        pass\"\n    },\n    {\n      \"path\": \"your_project\\\\adapters\\\\parsers\\\\csv_parser.py\",\n      \"overview\": \"Pythonコード。\\nクラス: CsvParser。\\n関数: parse, supported_extensions, data_type。\\n\",\n      \"content\": \"import pandas as pd\\nimport logging\\nfrom typing import Any, List, Dict\\nfrom .base_parser import BaseParser\\n\\nclass CsvParser(BaseParser):\\n\\n    def parse(self, file_path: str, encoding: str='utf-8') -> List[Dict[str, Any]]:\\n        try:\\n            df = pd.read_csv(file_path, encoding=encoding, errors='ignore')\\n            return df.to_dict(orient='records')\\n        except Exception as e:\\n            logging.error(f'CSVパース中エラー: {str(e)}', exc_info=True)\\n            return []\\n\\n    def supported_extensions(self) -> List[str]:\\n        return ['.csv']\\n\\n    def data_type(self) -> str:\\n        return 'csv'\"\n    },\n    {\n      \"path\": \"your_project\\\\adapters\\\\parsers\\\\html_parser.py\",\n      \"overview\": \"Pythonコード。\\nクラス: HtmlParser。\\n関数: parse, supported_extensions, data_type。\\n\",\n      \"content\": \"import logging\\nfrom typing import Any, List, Dict\\nfrom bs4 import BeautifulSoup\\nfrom .base_parser import BaseParser\\n\\nclass HtmlParser(BaseParser):\\n\\n    def parse(self, file_path: str, encoding: str='utf-8') -> List[Dict[str, Any]]:\\n        try:\\n            with open(file_path, 'r', encoding=encoding, errors='ignore') as f:\\n                soup = BeautifulSoup(f, 'html.parser')\\n            text = soup.get_text()\\n            return [{'content': text}]\\n        except Exception as e:\\n            logging.error(f'HTMLパース中エラー: {str(e)}', exc_info=True)\\n            return []\\n\\n    def supported_extensions(self) -> List[str]:\\n        return ['.html', '.htm', '.mhtml']\\n\\n    def data_type(self) -> str:\\n        return 'html'\"\n    },\n    {\n      \"path\": \"your_project\\\\adapters\\\\parsers\\\\json_parser.py\",\n      \"overview\": \"Pythonコード。\\nクラス: JsonParser。\\n関数: parse, supported_extensions, data_type。\\n\",\n      \"content\": \"import json\\nimport logging\\nfrom typing import Any, List, Dict\\nfrom .base_parser import BaseParser\\n\\nclass JsonParser(BaseParser):\\n\\n    def parse(self, file_path: str, encoding: str='utf-8') -> List[Dict[str, Any]]:\\n        try:\\n            with open(file_path, 'r', encoding=encoding, errors='ignore') as f:\\n                data = json.load(f)\\n            return [{'content': json.dumps(data, ensure_ascii=False)}]\\n        except Exception as e:\\n            logging.error(f'JSONパース中エラー: {str(e)}', exc_info=True)\\n            return []\\n\\n    def supported_extensions(self) -> List[str]:\\n        return ['.json']\\n\\n    def data_type(self) -> str:\\n        return 'json'\"\n    },\n    {\n      \"path\": \"your_project\\\\adapters\\\\parsers\\\\parser_manager.py\",\n      \"overview\": \"Pythonコード。\\nクラス: ParserManager。\\n関数: __init__, load_parsers, get_parser。\\n\",\n      \"content\": \"import importlib\\nimport pkgutil\\nimport logging\\nfrom typing import Dict\\nfrom adapters.parsers.base_parser import BaseParser\\nfrom adapters.parsers import csv_parser, json_parser, xls_parser, text_parser, html_parser\\n\\nclass ParserManager:\\n\\n    def __init__(self):\\n        self.parsers = self.load_parsers()\\n\\n    def load_parsers(self) -> Dict[str, BaseParser]:\\n        parsers_dict = {}\\n        csvp = csv_parser.CsvParser()\\n        parsers_dict[csvp.data_type()] = csvp\\n        jsonp = json_parser.JsonParser()\\n        parsers_dict[jsonp.data_type()] = jsonp\\n        xlsp = xls_parser.XlsParser()\\n        parsers_dict[xlsp.data_type()] = xlsp\\n        txtp = text_parser.TextParser()\\n        parsers_dict[txtp.data_type()] = txtp\\n        htm = html_parser.HtmlParser()\\n        parsers_dict[htm.data_type()] = htm\\n        logging.info(f'ParserManager: {len(parsers_dict)}個のパーサを登録済')\\n        return parsers_dict\\n\\n    def get_parser(self, data_type: str) -> BaseParser:\\n        return self.parsers.get(data_type)\"\n    },\n    {\n      \"path\": \"your_project\\\\adapters\\\\parsers\\\\text_parser.py\",\n      \"overview\": \"Pythonコード。\\nクラス: TextParser。\\n関数: parse, supported_extensions, data_type。\\n\",\n      \"content\": \"import logging\\nfrom typing import Any, List, Dict\\nfrom .base_parser import BaseParser\\n\\nclass TextParser(BaseParser):\\n\\n    def parse(self, file_path: str, encoding: str='utf-8') -> List[Dict[str, Any]]:\\n        try:\\n            with open(file_path, 'r', encoding=encoding, errors='ignore') as f:\\n                content = f.read()\\n            return [{'content': content}]\\n        except Exception as e:\\n            logging.error(f'テキストパース中エラー: {str(e)}', exc_info=True)\\n            return []\\n\\n    def supported_extensions(self) -> List[str]:\\n        return ['.txt', '.md']\\n\\n    def data_type(self) -> str:\\n        return 'text'\"\n    },\n    {\n      \"path\": \"your_project\\\\adapters\\\\parsers\\\\xls_parser.py\",\n      \"overview\": \"Pythonコード。\\nクラス: XlsParser。\\n関数: parse, supported_extensions, data_type。\\n\",\n      \"content\": \"import pandas as pd\\nimport logging\\nfrom typing import Any, List, Dict\\nfrom .base_parser import BaseParser\\n\\nclass XlsParser(BaseParser):\\n\\n    def parse(self, file_path: str, encoding: str='utf-8') -> List[Dict[str, Any]]:\\n        try:\\n            df = pd.read_excel(file_path, engine='xlrd')\\n            return df.to_dict(orient='records')\\n        except Exception as e:\\n            logging.error(f'XLSデータパース中エラー: {str(e)}', exc_info=True)\\n            return []\\n\\n    def supported_extensions(self) -> List[str]:\\n        return ['.xls']\\n\\n    def data_type(self) -> str:\\n        return 'xls'\"\n    },\n    {\n      \"path\": \"your_project\\\\adapters\\\\parsers\\\\__init__.py\",\n      \"overview\": \"Pythonコード。\\nDocstring(冒頭60文字): パーサーアダプタのパッケージ認識用...\\n\",\n      \"content\": \"\\\"\\\"\\\"\\nパーサーアダプタのパッケージ認識用\\n\\\"\\\"\\\"\"\n    },\n    {\n      \"path\": \"your_project\\\\application\\\\api\\\\main_api.py\",\n      \"overview\": \"Pythonコード。\\n\",\n      \"content\": \"import uvicorn\\nfrom fastapi import FastAPI\\nfrom application.services.data_service import DataService\\nfrom application.services.analysis_service import AnalysisService\\napp = FastAPI()\\n\\n@app.get('/')\\nasync def root():\\n    return {'message': 'Hello from API'}\\n\\n@app.post('/analyze')\\nasync def analyze_endpoint(input_path: str):\\n    data_service = DataService()\\n    analysis_service = AnalysisService()\\n    df_data = data_service.load_and_preprocess(input_path)\\n    analysis_service.run_analysis(df_data, 'outputs')\\n    return {'status': 'OK'}\\nif __name__ == '__main__':\\n    uvicorn.run(app, host='0.0.0.0', port=8000)\"\n    },\n    {\n      \"path\": \"your_project\\\\application\\\\api\\\\__init__.py\",\n      \"overview\": \"Pythonコード。\\nDocstring(冒頭60文字): API関連のパッケージ...\\n\",\n      \"content\": \"\\\"\\\"\\\"\\nAPI関連のパッケージ\\n\\\"\\\"\\\"\"\n    },\n    {\n      \"path\": \"your_project\\\\application\\\\cli\\\\main_cli.py\",\n      \"overview\": \"Pythonコード。\\n関数: parse_args, main。\\n\",\n      \"content\": \"import sys\\nimport logging\\nimport argparse\\nfrom application.services.data_service import DataService\\nfrom application.services.analysis_service import AnalysisService\\nfrom core.environment import prepare_environment\\nfrom core.finalize import finalize_with_feedback\\n\\ndef parse_args():\\n    parser = argparse.ArgumentParser(description='CLIツール サンプル')\\n    parser.add_argument('--input', type=str, required=True, help='入力データのパス（ファイルまたはディレクトリ）')\\n    return parser.parse_args()\\n\\ndef main():\\n    (run_timestamp, run_output_dir) = prepare_environment()\\n    args = parse_args()\\n    data_service = DataService()\\n    analysis_service = AnalysisService()\\n    try:\\n        df_data = data_service.load_and_preprocess(args.input)\\n        analysis_service.run_analysis(df_data, run_output_dir)\\n    except Exception as e:\\n        logging.error(f'CLIエラー: {e}', exc_info=True)\\n        finalize_with_feedback(run_output_dir, run_timestamp, 'Failed', str(e))\\n        sys.exit(1)\\n    finalize_with_feedback(run_output_dir, run_timestamp, 'Completed')\\n    sys.exit(0)\\nif __name__ == '__main__':\\n    main()\"\n    },\n    {\n      \"path\": \"your_project\\\\application\\\\cli\\\\__init__.py\",\n      \"overview\": \"Pythonコード。\\nDocstring(冒頭60文字): CLI関連のパッケージ...\\n\",\n      \"content\": \"\\\"\\\"\\\"\\nCLI関連のパッケージ\\n\\\"\\\"\\\"\"\n    },\n    {\n      \"path\": \"your_project\\\\application\\\\services\\\\analysis_service.py\",\n      \"overview\": \"Pythonコード。\\nクラス: AnalysisService。\\n関数: __init__, run_analysis。\\n\",\n      \"content\": \"import os\\nimport logging\\nimport asyncio\\nfrom domain.models.model_manager import ModelManager\\nfrom utils.output_manager import OutputManager\\nfrom domain.visualization.visualizer import Visualizer\\n\\nclass AnalysisService:\\n\\n    def __init__(self):\\n        self.model_manager = ModelManager()\\n        self.output_manager = OutputManager()\\n        self.visualizer = Visualizer()\\n\\n    def run_analysis(self, df_data, output_dir):\\n        methods = self.model_manager.get_available_methods()\\n        logging.info(f'利用可能な解析手法: {methods}')\\n        for m in methods:\\n            logging.info(f'解析手法 {m} 実行')\\n            result = self.model_manager.apply_method(m, df_data)\\n            if not result.empty:\\n                self.output_manager.save_results(result, m, output_dir)\\n                asyncio.run(self.output_manager.save_to_database(result, m))\\n                vis_dir = os.path.join(output_dir, 'visualizations')\\n                os.makedirs(vis_dir, exist_ok=True)\\n                self.visualizer.visualize(m, result, vis_dir)\\n            else:\\n                logging.info(f'{m} の結果は空')\"\n    },\n    {\n      \"path\": \"your_project\\\\application\\\\services\\\\data_service.py\",\n      \"overview\": \"Pythonコード。\\nクラス: DataService。\\n関数: load_and_preprocess。\\n\",\n      \"content\": \"import logging\\nimport os\\nfrom domain.data_preparation.data_loader import DataLoader\\nfrom domain.data_preparation.data_preprocessor import DataPreprocessor\\n\\nclass DataService:\\n\\n    def load_and_preprocess(self, input_path: str):\\n        loader = DataLoader(input_path)\\n        raw_data = loader.load_data()\\n        processor = DataPreprocessor()\\n        df_data = processor.preprocess_data(raw_data)\\n        if df_data.empty:\\n            logging.warning('DataService: 前処理結果が空')\\n        return df_data\"\n    },\n    {\n      \"path\": \"your_project\\\\application\\\\services\\\\__init__.py\",\n      \"overview\": \"Pythonコード。\\nDocstring(冒頭60文字): ユースケースやビジネスロジックを実装するサービス群...\\n\",\n      \"content\": \"\\\"\\\"\\\"\\nユースケースやビジネスロジックを実装するサービス群\\n\\\"\\\"\\\"\"\n    },\n    {\n      \"path\": \"your_project\\\\core\\\\environment.py\",\n      \"overview\": \"Pythonコード。\\n関数: prepare_environment。\\n\",\n      \"content\": \"import os\\nimport datetime\\nimport logging\\nfrom utils.logger import Logger\\n\\ndef prepare_environment():\\n    run_timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\\n    base_dir = os.path.dirname(os.path.abspath(__file__))\\n    project_dir = os.path.abspath(os.path.join(base_dir, '..'))\\n    run_output_dir = os.path.join(project_dir, f'run_{run_timestamp}')\\n    os.makedirs(run_output_dir, exist_ok=True)\\n    log_dir = os.path.join(run_output_dir, 'logs')\\n    os.makedirs(log_dir, exist_ok=True)\\n    Logger.setup_logging(log_dir, 'INFO')\\n    logging.info('===== 環境準備完了 =====')\\n    return (run_timestamp, run_output_dir)\"\n    },\n    {\n      \"path\": \"your_project\\\\core\\\\finalize.py\",\n      \"overview\": \"Pythonコード。\\n関数: finalize_with_feedback。\\n\",\n      \"content\": \"import os\\nimport json\\nimport logging\\n\\ndef finalize_with_feedback(run_output_dir: str, run_timestamp: str, status_msg: str, extra_remarks: str='') -> None:\\n    feedback_data = {'timestamp': run_timestamp, 'status': status_msg, 'remarks': extra_remarks, 'possible_improvements': ['ログレベルをDEBUGにして詳細確認', '追加パラメータを考慮した解析を実装する', 'AIプロンプトの自動生成を検討']}\\n    feedback_path = os.path.join(run_output_dir, 'feedback.json')\\n    try:\\n        with open(feedback_path, 'w', encoding='utf-8') as fw:\\n            json.dump(feedback_data, fw, ensure_ascii=False, indent=2)\\n        logging.info(f'フィードバック情報を {feedback_path} に保存しました。(status={status_msg})')\\n    except Exception as e:\\n        logging.error(f'フィードバック情報の保存に失敗: {str(e)}', exc_info=True)\\n    logging.info('===== 終了処理完了 =====')\"\n    },\n    {\n      \"path\": \"your_project\\\\core\\\\__init__.py\",\n      \"overview\": \"Pythonコード。\\nDocstring(冒頭60文字): コア機能(環境準備や終了処理など横断的役割)...\\n\",\n      \"content\": \"\\\"\\\"\\\"\\nコア機能(環境準備や終了処理など横断的役割)\\n\\\"\\\"\\\"\"\n    },\n    {\n      \"path\": \"your_project\\\\data_preparation\\\\data_loader.py\",\n      \"overview\": \"Pythonコード。\\nクラス: DataLoader。\\n関数: __init__, load_data, collect_files, extract_zip, is_supported_type, filter_by_data_types。\\n\",\n      \"content\": \"import os\\nimport logging\\nimport asyncio\\nimport zipfile\\nfrom typing import Any, Dict, List, Union\\nfrom utils.file_processor import FileProcessor\\nfrom utils.file_identifier import FileIdentifier\\nfrom media_scanner import scan_media\\nfrom media_manager import MediaManager\\n\\nclass DataLoader:\\n    \\\"\\\"\\\"\\n    入力ソース（文字列: 従来のフォルダパス、または dict / list: 媒体ディスクリプタ）から対象ファイルを収集し、\\n    FileProcessor を用いて並列パースを実施する。\\n    \\\"\\\"\\\"\\n\\n    def __init__(self, input_source: Union[str, dict, list], data_types: List[str]):\\n        self.input_source = input_source\\n        self.data_types = data_types if data_types is not None else ['text', 'html', 'csv', 'json', 'xls']\\n        self.file_processor = FileProcessor(self.data_types)\\n        self.file_identifier = FileIdentifier(self.data_types)\\n\\n    def load_data(self) -> Dict[str, Any]:\\n        files = self.collect_files()\\n        if not files:\\n            logging.warning('入力パスにファイルがありません。')\\n            return {}\\n        contents = asyncio.run(self.file_processor.process_files_in_parallel(files))\\n        return contents\\n\\n    def collect_files(self) -> List[str]:\\n        file_list = []\\n        if isinstance(self.input_source, str):\\n            if not os.path.exists(self.input_source):\\n                logging.warning(f'Input directory does not exist: {self.input_source}')\\n                return []\\n            for (root, _, files) in os.walk(self.input_source):\\n                for file in files:\\n                    file_path = os.path.join(root, file)\\n                    if zipfile.is_zipfile(file_path):\\n                        extracted = self.extract_zip(file_path)\\n                        file_list.extend(self.filter_by_data_types(extracted))\\n                    elif self.is_supported_type(file_path):\\n                        file_list.append(file_path)\\n        elif isinstance(self.input_source, dict):\\n            file_list = scan_media(self.input_source)\\n        elif isinstance(self.input_source, list):\\n            media_manager = MediaManager()\\n            file_list = asyncio.run(media_manager.scan_all_media(self.input_source))\\n        else:\\n            logging.error('Invalid input_source type. Must be str, dict, or list.')\\n        logging.info(f'DataLoader.collect_files: Collected {len(file_list)} files.')\\n        return file_list\\n\\n    def extract_zip(self, zip_path: str) -> List[str]:\\n        extracted_files = []\\n        try:\\n            with zipfile.ZipFile(zip_path, 'r') as zip_ref:\\n                extract_path = os.path.join(os.path.dirname(zip_path), os.path.splitext(os.path.basename(zip_path))[0])\\n                zip_ref.extractall(extract_path)\\n                for (root, _, files) in os.walk(extract_path):\\n                    for file in files:\\n                        extracted_files.append(os.path.join(root, file))\\n            logging.info(f'ZIPファイル展開完了: {zip_path}')\\n        except Exception as e:\\n            logging.error(f'ZIP展開中にエラー: {str(e)}', exc_info=True)\\n        return extracted_files\\n\\n    def is_supported_type(self, file_path: str) -> bool:\\n        ext = os.path.splitext(file_path)[1].lower()\\n        return ext in ['.txt', '.md', '.html', '.htm', '.mhtml', '.csv', '.json', '.xls']\\n\\n    def filter_by_data_types(self, file_paths: List[str]) -> List[str]:\\n        return [fp for fp in file_paths if self.is_supported_type(fp)]\"\n    },\n    {\n      \"path\": \"your_project\\\\data_preparation\\\\data_preprocessor.py\",\n      \"overview\": \"Pythonコード。\\nクラス: DataPreprocessor。\\n関数: __init__, load_stopwords, preprocess_data, normalize_text, filter_token。\\n\",\n      \"content\": \"import os\\nimport pandas as pd\\nimport logging\\nimport re\\nfrom typing import Any, Dict, Optional\\nfrom janome.tokenizer import Tokenizer\\n\\nclass DataPreprocessor:\\n    \\\"\\\"\\\"\\n    前処理クラス\\n      - テキストの小文字化、記号除去\\n      - ストップワード除去\\n      - トークナイズと基本形変換\\n    \\\"\\\"\\\"\\n\\n    def __init__(self, stopwords_path: Optional[str]=None):\\n        self.tokenizer = Tokenizer()\\n        self.stopwords = self.load_stopwords(stopwords_path)\\n\\n    def load_stopwords(self, path: Optional[str]) -> set:\\n        stopwords = set()\\n        if path and os.path.exists(path):\\n            try:\\n                with open(path, 'r', encoding='utf-8') as f:\\n                    for line in f:\\n                        w = line.strip().lower()\\n                        if w:\\n                            stopwords.add(w)\\n                logging.info(f'ストップワードファイル読み込み完了: {path} (単語数={len(stopwords)})')\\n            except Exception as e:\\n                logging.warning(f'ストップワード読込中のエラー: {str(e)}', exc_info=True)\\n        else:\\n            logging.info('ストップワードファイル未指定または見つからず。フィルタなしで進行。')\\n        return stopwords\\n\\n    def preprocess_data(self, contents: Dict[str, Any]) -> pd.DataFrame:\\n        data_frames = []\\n        for (dtype, data_list) in contents.items():\\n            if data_list:\\n                df = pd.DataFrame(data_list)\\n                df['type'] = dtype\\n                if 'content' in df.columns:\\n                    df['normalized_content'] = df['content'].apply(self.normalize_text)\\n                    tokens_list = df['normalized_content'].apply(lambda text: list(self.tokenizer.tokenize(str(text))))\\n                    df['tokens'] = tokens_list.apply(lambda tokens: [t.surface for t in tokens if self.filter_token(t.surface)])\\n                    df['lemmas'] = tokens_list.apply(lambda tokens: [t.base_form if t.base_form != '*' else t.surface for t in tokens if self.filter_token(t.base_form if t.base_form != '*' else t.surface)])\\n                    df['pos_tags'] = tokens_list.apply(lambda tokens: [t.part_of_speech for t in tokens])\\n                data_frames.append(df)\\n        if data_frames:\\n            combined = pd.concat(data_frames, ignore_index=True)\\n            logging.info(f'前処理後データ件数: {len(combined)}')\\n            return combined\\n        else:\\n            logging.warning('前処理後データが空です。')\\n            return pd.DataFrame()\\n\\n    def normalize_text(self, text: str) -> str:\\n        text = text.lower()\\n        text = re.sub('[^a-z0-9\\\\\\\\s]+', '', text)\\n        text = re.sub('\\\\\\\\s+', ' ', text).strip()\\n        return text\\n\\n    def filter_token(self, token: str) -> bool:\\n        return token not in self.stopwords and token != ''\"\n    },\n    {\n      \"path\": \"your_project\\\\data_preparation\\\\__init__.py\",\n      \"overview\": \"Pythonコード。\\n\",\n      \"content\": \"from .data_loader import DataLoader\\nfrom .data_preprocessor import DataPreprocessor\"\n    },\n    {\n      \"path\": \"your_project\\\\domain\\\\__init__.py\",\n      \"overview\": \"Pythonコード。\\nDocstring(冒頭60文字): ビジネスルール・ドメインモデル・ロジック集約層...\\n\",\n      \"content\": \"\\\"\\\"\\\"\\nビジネスルール・ドメインモデル・ロジック集約層\\n\\\"\\\"\\\"\"\n    },\n    {\n      \"path\": \"your_project\\\\domain\\\\data_preparation\\\\data_loader.py\",\n      \"overview\": \"Pythonコード。\\nクラス: DataLoader。\\n関数: __init__, load_data, collect_files, extract_zip, is_supported_type, filter_by_data_types。\\n\",\n      \"content\": \"import os\\nimport logging\\nimport asyncio\\nimport zipfile\\nfrom typing import Any, Dict, List\\nfrom utils.file_processor import FileProcessor\\n\\nclass DataLoader:\\n\\n    def __init__(self, input_path: str):\\n        self.input_path = input_path\\n        self.data_types = ['text', 'html', 'csv', 'json', 'xls']\\n        self.file_processor = FileProcessor(self.data_types)\\n\\n    def load_data(self) -> Dict[str, Any]:\\n        files = self.collect_files()\\n        if not files:\\n            logging.warning('入力パスにファイルがありません。')\\n            return {}\\n        contents = asyncio.run(self.file_processor.process_files_in_parallel(files))\\n        return contents\\n\\n    def collect_files(self) -> List[str]:\\n        if not os.path.exists(self.input_path):\\n            logging.warning(f'ディレクトリ/ファイルが存在しません: {self.input_path}')\\n            return []\\n        file_list = []\\n        if os.path.isdir(self.input_path):\\n            for (root, _, files) in os.walk(self.input_path):\\n                for file in files:\\n                    file_path = os.path.join(root, file)\\n                    if zipfile.is_zipfile(file_path):\\n                        extracted = self.extract_zip(file_path)\\n                        file_list.extend(self.filter_by_data_types(extracted))\\n                    elif self.is_supported_type(file_path):\\n                        file_list.append(file_path)\\n        elif zipfile.is_zipfile(self.input_path):\\n            extracted = self.extract_zip(self.input_path)\\n            file_list.extend(self.filter_by_data_types(extracted))\\n        elif self.is_supported_type(self.input_path):\\n            file_list.append(self.input_path)\\n        return file_list\\n\\n    def extract_zip(self, zip_path: str) -> List[str]:\\n        extracted_files = []\\n        try:\\n            with zipfile.ZipFile(zip_path, 'r') as zip_ref:\\n                extract_path = os.path.join(os.path.dirname(zip_path), os.path.splitext(os.path.basename(zip_path))[0])\\n                zip_ref.extractall(extract_path)\\n                for (root, _, files) in os.walk(extract_path):\\n                    for file in files:\\n                        extracted_files.append(os.path.join(root, file))\\n            logging.info(f'ZIPファイル展開完了: {zip_path}')\\n        except Exception as e:\\n            logging.error(f'ZIP展開中にエラー: {str(e)}', exc_info=True)\\n        return extracted_files\\n\\n    def is_supported_type(self, file_path: str) -> bool:\\n        ext = os.path.splitext(file_path)[1].lower()\\n        return ext in ['.txt', '.md', '.html', '.htm', '.mhtml', '.csv', '.json', '.xls']\\n\\n    def filter_by_data_types(self, file_paths: List[str]) -> List[str]:\\n        filtered = []\\n        for fp in file_paths:\\n            if self.is_supported_type(fp):\\n                filtered.append(fp)\\n        return filtered\"\n    },\n    {\n      \"path\": \"your_project\\\\domain\\\\data_preparation\\\\data_preprocessor.py\",\n      \"overview\": \"Pythonコード。\\nクラス: DataPreprocessor。\\n関数: __init__, load_stopwords, preprocess_data, normalize_text, filter_token。\\n\",\n      \"content\": \"import os\\nimport pandas as pd\\nimport logging\\nimport re\\nfrom typing import Any, Dict, Optional\\nfrom janome.tokenizer import Tokenizer\\n\\nclass DataPreprocessor:\\n\\n    def __init__(self, stopwords_path: Optional[str]=None):\\n        \\\"\\\"\\\"\\n        stopwords_path: 英単語のストップワードを格納したファイルのパス (例: \\\"english_stopwords.txt\\\")\\n        \\\"\\\"\\\"\\n        self.tokenizer = Tokenizer()\\n        self.stopwords = self.load_stopwords(stopwords_path)\\n\\n    def load_stopwords(self, path: Optional[str]) -> set:\\n        stopwords = set()\\n        if path and os.path.exists(path):\\n            try:\\n                with open(path, 'r', encoding='utf-8') as f:\\n                    for line in f:\\n                        w = line.strip().lower()\\n                        if w:\\n                            stopwords.add(w)\\n                logging.info(f'ストップワードファイル読み込み完了: {path} (単語数={len(stopwords)})')\\n            except Exception as e:\\n                logging.warning(f'ストップワードファイル読込中にエラー: {str(e)}', exc_info=True)\\n        else:\\n            logging.info('ストップワードファイル未指定 or 見つからず。フィルタなしで進行。')\\n        return stopwords\\n\\n    def preprocess_data(self, contents: Dict[str, Any]) -> pd.DataFrame:\\n        data_frames = []\\n        for (dtype, data_list) in contents.items():\\n            if data_list:\\n                df = pd.DataFrame(data_list)\\n                df['type'] = dtype\\n                if 'content' in df.columns:\\n                    df['normalized_content'] = df['content'].apply(self.normalize_text)\\n                    tokens_list = df['normalized_content'].apply(lambda text: list(self.tokenizer.tokenize(str(text))))\\n                    df['tokens'] = tokens_list.apply(lambda tokens: [t.surface for t in tokens if self.filter_token(t.surface)])\\n                    df['lemmas'] = tokens_list.apply(lambda tokens: [t.base_form if t.base_form != '*' else t.surface for t in tokens if self.filter_token(t.base_form if t.base_form != '*' else t.surface)])\\n                    df['pos_tags'] = tokens_list.apply(lambda tokens: [t.part_of_speech for t in tokens])\\n                data_frames.append(df)\\n        if data_frames:\\n            combined = pd.concat(data_frames, ignore_index=True)\\n            logging.info(f'前処理後データ件数: {len(combined)}')\\n            return combined\\n        else:\\n            logging.warning('前処理後データが空です。')\\n            return pd.DataFrame()\\n\\n    def normalize_text(self, text: str) -> str:\\n        text = text.lower()\\n        text = re.sub('[^a-z0-9\\\\\\\\s]+', '', text)\\n        text = re.sub('\\\\\\\\s+', ' ', text).strip()\\n        return text\\n\\n    def filter_token(self, token: str) -> bool:\\n        return token not in self.stopwords and token != ''\"\n    },\n    {\n      \"path\": \"your_project\\\\domain\\\\models\\\\model_manager.py\",\n      \"overview\": \"Pythonコード。\\nクラス: ModelManager。\\n関数: __init__, get_available_methods, apply_method。\\n\",\n      \"content\": \"import logging\\nfrom typing import Any\\nfrom .analysis_methods.bigram_analysis import BigramAnalysis\\nfrom .analysis_methods.word_frequency_analysis import WordFrequencyAnalysis\\n\\nclass ModelManager:\\n\\n    def __init__(self):\\n        self.methods = {'BigramAnalysis': BigramAnalysis(), 'WordFrequencyAnalysis': WordFrequencyAnalysis()}\\n\\n    def get_available_methods(self):\\n        return list(self.methods.keys())\\n\\n    def apply_method(self, method_name: str, data: Any, *args, **kwargs):\\n        if method_name not in self.methods:\\n            raise ValueError(f\\\"解析手法 '{method_name}' は未登録です。\\\")\\n        method_instance = self.methods[method_name]\\n        return method_instance.analyze(data, *args, **kwargs)\"\n    },\n    {\n      \"path\": \"your_project\\\\domain\\\\models\\\\__init__.py\",\n      \"overview\": \"Pythonコード。\\nDocstring(冒頭60文字): モデルやエンティティなど...\\n\",\n      \"content\": \"\\\"\\\"\\\"\\nモデルやエンティティなど\\n\\\"\\\"\\\"\"\n    },\n    {\n      \"path\": \"your_project\\\\domain\\\\models\\\\analysis_methods\\\\analysis_method_base.py\",\n      \"overview\": \"Pythonコード。\\nクラス: AnalysisMethodBase。\\n関数: analyze, get_name。\\n\",\n      \"content\": \"from abc import ABC, abstractmethod\\nimport pandas as pd\\n\\nclass AnalysisMethodBase(ABC):\\n\\n    @abstractmethod\\n    def analyze(self, data: pd.DataFrame, **kwargs) -> pd.DataFrame:\\n        pass\\n\\n    @abstractmethod\\n    def get_name(self) -> str:\\n        pass\"\n    },\n    {\n      \"path\": \"your_project\\\\domain\\\\models\\\\analysis_methods\\\\bigram_analysis.py\",\n      \"overview\": \"Pythonコード。\\nクラス: BigramAnalysis。\\n関数: __init__, analyze, get_name。\\n\",\n      \"content\": \"import logging\\nimport pandas as pd\\nfrom collections import Counter\\nfrom janome.tokenizer import Tokenizer\\nfrom itertools import islice\\nfrom .analysis_method_base import AnalysisMethodBase\\n\\nclass BigramAnalysis(AnalysisMethodBase):\\n\\n    def __init__(self):\\n        self.tokenizer = Tokenizer()\\n\\n    def analyze(self, data: pd.DataFrame, **kwargs) -> pd.DataFrame:\\n        text_data = data['normalized_content'] if 'normalized_content' in data.columns else data['content'] if 'content' in data.columns else pd.Series([])\\n        if text_data.empty:\\n            logging.info('BigramAnalysis: テキストデータなし。')\\n            return pd.DataFrame()\\n        try:\\n            all_bigrams = []\\n            if 'tokens' in data.columns:\\n                for words in data['tokens']:\\n                    bigrams = zip(words, islice(words, 1, None))\\n                    all_bigrams.extend(bigrams)\\n            else:\\n                for txt in text_data:\\n                    tokens = [t.surface for t in self.tokenizer.tokenize(txt)]\\n                    bigrams = zip(tokens, islice(tokens, 1, None))\\n                    all_bigrams.extend(bigrams)\\n            freq = Counter(all_bigrams)\\n            df_result = pd.DataFrame(freq.items(), columns=['bigram', 'frequency'])\\n            df_result['bigram'] = df_result['bigram'].apply(' '.join)\\n            total = sum(df_result['frequency'])\\n            df_result['probability'] = df_result['frequency'] / total if total > 0 else 0\\n            return df_result.sort_values(by='frequency', ascending=False)\\n        except Exception as e:\\n            logging.error(f'BigramAnalysisエラー: {str(e)}', exc_info=True)\\n            return pd.DataFrame()\\n\\n    def get_name(self) -> str:\\n        return 'BigramAnalysis'\"\n    },\n    {\n      \"path\": \"your_project\\\\domain\\\\models\\\\analysis_methods\\\\word_frequency_analysis.py\",\n      \"overview\": \"Pythonコード。\\nクラス: WordFrequencyAnalysis。\\n関数: __init__, analyze, get_name。\\n\",\n      \"content\": \"import logging\\nimport pandas as pd\\nfrom collections import Counter\\nfrom janome.tokenizer import Tokenizer\\nfrom .analysis_method_base import AnalysisMethodBase\\n\\nclass WordFrequencyAnalysis(AnalysisMethodBase):\\n\\n    def __init__(self):\\n        self.tokenizer = Tokenizer()\\n\\n    def analyze(self, data: pd.DataFrame, **kwargs) -> pd.DataFrame:\\n        text_data = data['normalized_content'] if 'normalized_content' in data.columns else data['content'] if 'content' in data.columns else pd.Series([])\\n        if text_data.empty:\\n            logging.info('WordFrequencyAnalysis: テキストデータなし。')\\n            return pd.DataFrame()\\n        try:\\n            if 'tokens' in data.columns:\\n                all_words = [w for words in data['tokens'] for w in words]\\n            else:\\n                all_words = []\\n                for txt in text_data:\\n                    tokens = [t.surface for t in self.tokenizer.tokenize(txt)]\\n                    all_words.extend(tokens)\\n            freq = Counter(all_words)\\n            df_result = pd.DataFrame(freq.items(), columns=['word', 'frequency'])\\n            total = sum(df_result['frequency'])\\n            df_result['probability'] = df_result['frequency'] / total if total > 0 else 0\\n            return df_result.sort_values(by='frequency', ascending=False)\\n        except Exception as e:\\n            logging.error(f'WordFrequencyAnalysisエラー: {str(e)}', exc_info=True)\\n            return pd.DataFrame()\\n\\n    def get_name(self) -> str:\\n        return 'WordFrequencyAnalysis'\"\n    },\n    {\n      \"path\": \"your_project\\\\domain\\\\performance\\\\performance_tracker.py\",\n      \"overview\": \"Pythonコード。\\nクラス: PerformanceTracker。\\n関数: __init__, start_timer, end_timer, get_metrics。\\n\",\n      \"content\": \"import time\\nfrom typing import Dict, List\\n\\nclass PerformanceTracker:\\n\\n    def __init__(self):\\n        self.start_times: Dict[str, float] = {}\\n        self.metrics: List[Dict[str, float]] = []\\n\\n    def start_timer(self, name: str) -> None:\\n        self.start_times[name] = time.time()\\n\\n    def end_timer(self, name: str) -> None:\\n        if name in self.start_times:\\n            elapsed = time.time() - self.start_times.pop(name)\\n            self.metrics.append({'process': name, 'elapsed_time': elapsed})\\n\\n    def get_metrics(self) -> List[Dict[str, float]]:\\n        return self.metrics\"\n    },\n    {\n      \"path\": \"your_project\\\\domain\\\\visualization\\\\visualizer.py\",\n      \"overview\": \"Pythonコード。\\nクラス: Visualizer。\\n関数: __init__, visualize, _visualize_word_frequency, _visualize_bigram_frequency。\\n\",\n      \"content\": \"import os\\nimport logging\\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\n\\nclass Visualizer:\\n\\n    def __init__(self):\\n        sns.set(style='whitegrid')\\n\\n    def visualize(self, method_name: str, data: pd.DataFrame, output_path: str) -> None:\\n        try:\\n            if method_name == 'WordFrequencyAnalysis':\\n                self._visualize_word_frequency(data, output_path)\\n            elif method_name == 'BigramAnalysis':\\n                self._visualize_bigram_frequency(data, output_path)\\n            else:\\n                logging.warning(f\\\"'{method_name}'は可視化未サポート\\\")\\n        except Exception as e:\\n            logging.error(f'可視化エラー: {str(e)}', exc_info=True)\\n\\n    def _visualize_word_frequency(self, df_data: pd.DataFrame, output_path: str):\\n        top_words = df_data.head(20)\\n        if top_words.empty:\\n            logging.info('単語頻度可視化: データ無し')\\n            return\\n        (fig, ax) = plt.subplots(figsize=(10, 6))\\n        sns.barplot(x='frequency', y='word', data=top_words, ax=ax)\\n        ax.set_title('Top 20 Words by Frequency')\\n        plt.tight_layout()\\n        os.makedirs(output_path, exist_ok=True)\\n        out_file = os.path.join(output_path, 'word_frequency.png')\\n        fig.savefig(out_file)\\n        plt.close(fig)\\n        logging.info(f'単語頻度可視化: {out_file}')\\n\\n    def _visualize_bigram_frequency(self, df_data: pd.DataFrame, output_path: str):\\n        top_bigrams = df_data.head(20)\\n        if top_bigrams.empty:\\n            logging.info('バイグラム可視化: データ無し')\\n            return\\n        (fig, ax) = plt.subplots(figsize=(10, 6))\\n        sns.barplot(x='frequency', y='bigram', data=top_bigrams, ax=ax)\\n        ax.set_title('Top 20 Bigrams by Frequency')\\n        plt.tight_layout()\\n        os.makedirs(output_path, exist_ok=True)\\n        out_file = os.path.join(output_path, 'bigram_frequency.png')\\n        fig.savefig(out_file)\\n        plt.close(fig)\\n        logging.info(f'バイグラム可視化: {out_file}')\"\n    },\n    {\n      \"path\": \"your_project\\\\models\\\\model_manager.py\",\n      \"overview\": \"Pythonコード。\\nクラス: ModelManager。\\n関数: __init__, load_methods, get_available_methods, apply_method。\\n\",\n      \"content\": \"import importlib\\nimport pkgutil\\nimport logging\\nfrom typing import List, Any, Dict\\nfrom models.analysis_methods.analysis_method_base import AnalysisMethodBase\\nimport models.analysis_methods\\n\\nclass ModelManager:\\n\\n    def __init__(self):\\n        self.methods = self.load_methods()\\n\\n    def load_methods(self) -> Dict[str, AnalysisMethodBase]:\\n        methods = {}\\n        for (_, module_name, _) in pkgutil.iter_modules(models.analysis_methods.__path__):\\n            if module_name != '__init__':\\n                module = importlib.import_module(f'models.analysis_methods.{module_name}')\\n                for attr_name in dir(module):\\n                    attr = getattr(module, attr_name)\\n                    if isinstance(attr, type) and issubclass(attr, AnalysisMethodBase) and (attr != AnalysisMethodBase):\\n                        instance = attr()\\n                        methods[instance.get_name()] = instance\\n        if not methods:\\n            logging.warning('解析手法がロードされませんでした。')\\n        return methods\\n\\n    def get_available_methods(self) -> List[str]:\\n        return list(self.methods.keys())\\n\\n    def apply_method(self, method_name: str, data: Any, *args, **kwargs) -> Any:\\n        if method_name in self.methods:\\n            return self.methods[method_name].analyze(data, *args, **kwargs)\\n        else:\\n            raise ValueError(f\\\"解析手法 '{method_name}'が見つかりません。\\\")\"\n    },\n    {\n      \"path\": \"your_project\\\\models\\\\__init__.py\",\n      \"overview\": \"Pythonコード。\\n\",\n      \"content\": \"from .model_manager import ModelManager\"\n    },\n    {\n      \"path\": \"your_project\\\\models\\\\analysis_methods\\\\analysis_method_base.py\",\n      \"overview\": \"Pythonコード。\\nクラス: AnalysisMethodBase。\\n関数: analyze, get_name。\\n\",\n      \"content\": \"from abc import ABC, abstractmethod\\nimport pandas as pd\\n\\nclass AnalysisMethodBase(ABC):\\n\\n    @abstractmethod\\n    def analyze(self, data: pd.DataFrame, **kwargs) -> pd.DataFrame:\\n        pass\\n\\n    @abstractmethod\\n    def get_name(self) -> str:\\n        pass\"\n    },\n    {\n      \"path\": \"your_project\\\\models\\\\analysis_methods\\\\bigram_analysis.py\",\n      \"overview\": \"Pythonコード。\\nクラス: BigramAnalysis。\\n関数: __init__, analyze, get_name。\\n\",\n      \"content\": \"import logging\\nimport pandas as pd\\nfrom collections import Counter\\nfrom janome.tokenizer import Tokenizer\\nfrom itertools import islice\\nfrom models.analysis_methods.analysis_method_base import AnalysisMethodBase\\n\\nclass BigramAnalysis(AnalysisMethodBase):\\n\\n    def __init__(self):\\n        self.tokenizer = Tokenizer()\\n\\n    def analyze(self, data: pd.DataFrame, **kwargs) -> pd.DataFrame:\\n        text_data = data['normalized_content'] if 'normalized_content' in data.columns else data['content'] if 'content' in data.columns else pd.Series([])\\n        if text_data.empty:\\n            logging.info('BigramAnalysis: テキストデータなし。')\\n            return pd.DataFrame()\\n        try:\\n            all_bigrams = []\\n            if 'tokens' in data.columns:\\n                for words in data['tokens']:\\n                    bigrams = zip(words, islice(words, 1, None))\\n                    all_bigrams.extend(bigrams)\\n            else:\\n                for txt in text_data:\\n                    tokens = [t.surface for t in self.tokenizer.tokenize(txt)]\\n                    bigrams = zip(tokens, islice(tokens, 1, None))\\n                    all_bigrams.extend(bigrams)\\n            freq = Counter(all_bigrams)\\n            df_result = pd.DataFrame(freq.items(), columns=['bigram', 'frequency'])\\n            df_result['bigram'] = df_result['bigram'].apply(lambda tup: ' '.join(tup))\\n            total = sum(df_result['frequency'])\\n            df_result['probability'] = df_result['frequency'] / total if total > 0 else 0\\n            return df_result.sort_values(by='frequency', ascending=False)\\n        except Exception as e:\\n            logging.error(f'BigramAnalysisエラー: {str(e)}', exc_info=True)\\n            return pd.DataFrame()\\n\\n    def get_name(self) -> str:\\n        return 'BigramAnalysis'\"\n    },\n    {\n      \"path\": \"your_project\\\\models\\\\analysis_methods\\\\word_frequency_analysis.py\",\n      \"overview\": \"Pythonコード。\\nクラス: WordFrequencyAnalysis。\\n関数: __init__, analyze, get_name。\\n\",\n      \"content\": \"import logging\\nimport pandas as pd\\nfrom collections import Counter\\nfrom janome.tokenizer import Tokenizer\\nfrom models.analysis_methods.analysis_method_base import AnalysisMethodBase\\n\\nclass WordFrequencyAnalysis(AnalysisMethodBase):\\n\\n    def __init__(self):\\n        self.tokenizer = Tokenizer()\\n\\n    def analyze(self, data: pd.DataFrame, **kwargs) -> pd.DataFrame:\\n        text_data = data['normalized_content'] if 'normalized_content' in data.columns else data['content'] if 'content' in data.columns else pd.Series([])\\n        if text_data.empty:\\n            logging.info('WordFrequencyAnalysis: テキストデータなし。')\\n            return pd.DataFrame()\\n        try:\\n            if 'tokens' in data.columns:\\n                all_words = [w for words in data['tokens'] for w in words]\\n            else:\\n                all_words = [t.surface for text in text_data for t in self.tokenizer.tokenize(text)]\\n            word_freq = Counter(all_words)\\n            df_result = pd.DataFrame(word_freq.items(), columns=['word', 'frequency'])\\n            df_result['probability'] = df_result['frequency'] / sum(df_result['frequency'])\\n            return df_result.sort_values(by='frequency', ascending=False)\\n        except Exception as e:\\n            logging.error(f'WordFrequencyAnalysisエラー: {str(e)}', exc_info=True)\\n            return pd.DataFrame()\\n\\n    def get_name(self) -> str:\\n        return 'WordFrequencyAnalysis'\"\n    },\n    {\n      \"path\": \"your_project\\\\parsers\\\\base_parser.py\",\n      \"overview\": \"Pythonコード。\\nクラス: BaseParser。\\n関数: parse, supported_extensions, data_type。\\n\",\n      \"content\": \"from abc import ABC, abstractmethod\\nfrom typing import Any, List, Dict\\n\\nclass BaseParser(ABC):\\n\\n    @abstractmethod\\n    def parse(self, file_path: str, encoding: str='utf-8') -> List[Dict[str, Any]]:\\n        pass\\n\\n    @abstractmethod\\n    def supported_extensions(self) -> List[str]:\\n        pass\\n\\n    @abstractmethod\\n    def data_type(self) -> str:\\n        pass\"\n    },\n    {\n      \"path\": \"your_project\\\\parsers\\\\csv_parser.py\",\n      \"overview\": \"Pythonコード。\\nクラス: CsvParser。\\n関数: parse, supported_extensions, data_type。\\n\",\n      \"content\": \"from parsers.base_parser import BaseParser\\nfrom typing import Any, List, Dict\\nimport pandas as pd\\nimport logging\\n\\nclass CsvParser(BaseParser):\\n\\n    def parse(self, file_path: str, encoding: str='utf-8') -> List[Dict[str, Any]]:\\n        try:\\n            df = pd.read_csv(file_path, encoding=encoding, errors='ignore')\\n            return df.to_dict(orient='records')\\n        except Exception as e:\\n            logging.error(f'CSVパース中エラー: {str(e)}', exc_info=True)\\n            return []\\n\\n    def supported_extensions(self) -> List[str]:\\n        return ['.csv']\\n\\n    def data_type(self) -> str:\\n        return 'csv'\"\n    },\n    {\n      \"path\": \"your_project\\\\parsers\\\\html_parser.py\",\n      \"overview\": \"Pythonコード。\\nクラス: HtmlParser。\\n関数: parse, supported_extensions, data_type。\\n\",\n      \"content\": \"from parsers.base_parser import BaseParser\\nfrom typing import Any, List, Dict\\nimport logging\\nfrom bs4 import BeautifulSoup\\n\\nclass HtmlParser(BaseParser):\\n\\n    def parse(self, file_path: str, encoding: str='utf-8') -> List[Dict[str, Any]]:\\n        try:\\n            with open(file_path, 'r', encoding=encoding, errors='ignore') as f:\\n                soup = BeautifulSoup(f, 'html.parser')\\n            text = soup.get_text()\\n            return [{'content': text}]\\n        except Exception as e:\\n            logging.error(f'HTMLパース中エラー: {str(e)}', exc_info=True)\\n            return []\\n\\n    def supported_extensions(self) -> List[str]:\\n        return ['.html', '.htm', '.mhtml']\\n\\n    def data_type(self) -> str:\\n        return 'html'\"\n    },\n    {\n      \"path\": \"your_project\\\\parsers\\\\json_parser.py\",\n      \"overview\": \"Pythonコード。\\nクラス: JsonParser。\\n関数: parse, supported_extensions, data_type。\\n\",\n      \"content\": \"from parsers.base_parser import BaseParser\\nfrom typing import Any, List, Dict\\nimport json\\nimport logging\\n\\nclass JsonParser(BaseParser):\\n\\n    def parse(self, file_path: str, encoding: str='utf-8') -> List[Dict[str, Any]]:\\n        try:\\n            with open(file_path, 'r', encoding=encoding, errors='ignore') as f:\\n                data = json.load(f)\\n            return [{'content': json.dumps(data, ensure_ascii=False)}]\\n        except Exception as e:\\n            logging.error(f'JSONパース中エラー: {str(e)}', exc_info=True)\\n            return []\\n\\n    def supported_extensions(self) -> List[str]:\\n        return ['.json']\\n\\n    def data_type(self) -> str:\\n        return 'json'\"\n    },\n    {\n      \"path\": \"your_project\\\\parsers\\\\parser_manager.py\",\n      \"overview\": \"Pythonコード。\\nクラス: ParserManager。\\n関数: __init__, load_parsers, get_parser。\\n\",\n      \"content\": \"import importlib\\nimport pkgutil\\nimport logging\\nfrom typing import Dict\\nfrom parsers.base_parser import BaseParser\\nimport parsers\\n\\nclass ParserManager:\\n\\n    def __init__(self):\\n        self.parsers = self.load_parsers()\\n\\n    def load_parsers(self) -> Dict[str, BaseParser]:\\n        parsers_dict = {}\\n        for (_, module_name, _) in pkgutil.iter_modules(parsers.__path__):\\n            if module_name.endswith('_parser') and module_name != 'base_parser':\\n                module = importlib.import_module(f'parsers.{module_name}')\\n                class_name = ''.join([word.capitalize() for word in module_name.split('_')])\\n                parser_class = getattr(module, class_name, None)\\n                if parser_class is None:\\n                    logging.warning(f'{module_name}に対応クラスなし')\\n                    continue\\n                parser_instance = parser_class()\\n                data_type = parser_instance.data_type()\\n                parsers_dict[data_type] = parser_instance\\n        return parsers_dict\\n\\n    def get_parser(self, data_type: str) -> BaseParser:\\n        return self.parsers.get(data_type)\"\n    },\n    {\n      \"path\": \"your_project\\\\parsers\\\\text_parser.py\",\n      \"overview\": \"Pythonコード。\\nクラス: TextParser。\\n関数: parse, supported_extensions, data_type。\\n\",\n      \"content\": \"from parsers.base_parser import BaseParser\\nfrom typing import Any, List, Dict\\nimport logging\\n\\nclass TextParser(BaseParser):\\n\\n    def parse(self, file_path: str, encoding: str='utf-8') -> List[Dict[str, Any]]:\\n        try:\\n            with open(file_path, 'r', encoding=encoding, errors='ignore') as f:\\n                content = f.read()\\n            return [{'content': content}]\\n        except Exception as e:\\n            logging.error(f'テキストパース中エラー: {str(e)}', exc_info=True)\\n            return []\\n\\n    def supported_extensions(self) -> List[str]:\\n        return ['.txt', '.md']\\n\\n    def data_type(self) -> str:\\n        return 'text'\"\n    },\n    {\n      \"path\": \"your_project\\\\parsers\\\\xls_parser.py\",\n      \"overview\": \"Pythonコード。\\nクラス: XlsParser。\\n関数: parse, supported_extensions, data_type。\\n\",\n      \"content\": \"from parsers.base_parser import BaseParser\\nfrom typing import Any, List, Dict\\nimport pandas as pd\\nimport logging\\n\\nclass XlsParser(BaseParser):\\n\\n    def parse(self, file_path: str, encoding: str='utf-8') -> List[Dict[str, Any]]:\\n        try:\\n            df = pd.read_excel(file_path, engine='xlrd')\\n            return df.to_dict(orient='records')\\n        except Exception as e:\\n            logging.error(f'XLSデータパース中エラー: {str(e)}', exc_info=True)\\n            return []\\n\\n    def supported_extensions(self) -> List[str]:\\n        return ['.xls']\\n\\n    def data_type(self) -> str:\\n        return 'xls'\"\n    },\n    {\n      \"path\": \"your_project\\\\performance\\\\performance_tracker.py\",\n      \"overview\": \"Pythonコード。\\nクラス: PerformanceTracker。\\n関数: __init__, start_timer, end_timer, get_metrics。\\n\",\n      \"content\": \"import time\\nfrom typing import Dict, List\\n\\nclass PerformanceTracker:\\n    \\\"\\\"\\\"各処理ステップの実行時間を記録するクラス\\\"\\\"\\\"\\n\\n    def __init__(self):\\n        self.start_times: Dict[str, float] = {}\\n        self.metrics: List[Dict[str, float]] = []\\n\\n    def start_timer(self, name: str) -> None:\\n        self.start_times[name] = time.time()\\n\\n    def end_timer(self, name: str) -> None:\\n        if name in self.start_times:\\n            elapsed = time.time() - self.start_times.pop(name)\\n            self.metrics.append({'process': name, 'elapsed_time': elapsed})\\n\\n    def get_metrics(self) -> List[Dict[str, float]]:\\n        return self.metrics\"\n    },\n    {\n      \"path\": \"your_project\\\\performance\\\\__init__.py\",\n      \"overview\": \"Pythonコード。\\nDocstring(冒頭60文字): プロジェクトのルート __init__.py\\nここでは特に処理はなく、パッケージとして認識させるためだけの空ファイル。...\\n\",\n      \"content\": \"\\\"\\\"\\\"\\nプロジェクトのルート __init__.py\\nここでは特に処理はなく、パッケージとして認識させるためだけの空ファイル。\\n\\\"\\\"\\\"\"\n    },\n    {\n      \"path\": \"your_project\\\\run_20250223_133141\\\\feedback.json\",\n      \"overview\": \"JSONファイル (辞書)。キー: timestamp, status, remarks, possible_improvements\\n\",\n      \"content\": \"{\\n  \\\"timestamp\\\": \\\"20250223_133141\\\",\\n  \\\"status\\\": \\\"Completed\\\",\\n  \\\"remarks\\\": \\\"\\\",\\n  \\\"possible_improvements\\\": [\\n    \\\"ログレベルをDEBUGにして詳細確認\\\",\\n    \\\"追加パラメータを考慮した解析を実装する\\\",\\n    \\\"AIプロンプトの自動生成を検討\\\"\\n  ]\\n}\"\n    },\n    {\n      \"path\": \"your_project\\\\run_20250223_151500\\\\feedback.json\",\n      \"overview\": \"JSONファイル (辞書)。キー: timestamp, status, remarks, possible_improvements\\n\",\n      \"content\": \"{\\n  \\\"timestamp\\\": \\\"20250223_151500\\\",\\n  \\\"status\\\": \\\"DB初期化失敗\\\",\\n  \\\"remarks\\\": \\\"Input path not found: c:\\\\\\\\Users\\\\\\\\KEN\\\\\\\\Desktop\\\\\\\\チェックアウト\\\\\\\\汎用分析基盤コード\\\\\\\\../../data\\\",\\n  \\\"possible_improvements\\\": [\\n    \\\"ログレベルをDEBUGにして詳細確認\\\",\\n    \\\"追加パラメータを考慮した解析を実装する\\\",\\n    \\\"AIプロンプトの自動生成を検討\\\"\\n  ]\\n}\"\n    },\n    {\n      \"path\": \"your_project\\\\run_20250223_153737\\\\feedback.json\",\n      \"overview\": \"JSONファイル (辞書)。キー: timestamp, status, remarks, possible_improvements\\n\",\n      \"content\": \"{\\n  \\\"timestamp\\\": \\\"20250223_153737\\\",\\n  \\\"status\\\": \\\"Completed\\\",\\n  \\\"remarks\\\": \\\"\\\",\\n  \\\"possible_improvements\\\": [\\n    \\\"ログレベルをDEBUGにして詳細確認\\\",\\n    \\\"追加パラメータを考慮した解析を実装する\\\",\\n    \\\"AIプロンプトの自動生成を検討\\\"\\n  ]\\n}\"\n    },\n    {\n      \"path\": \"your_project\\\\run_20250224_221720\\\\feedback.json\",\n      \"overview\": \"JSONファイル (辞書)。キー: timestamp, status, remarks, possible_improvements\\n\",\n      \"content\": \"{\\n  \\\"timestamp\\\": \\\"20250224_221720\\\",\\n  \\\"status\\\": \\\"DB初期化失敗\\\",\\n  \\\"remarks\\\": \\\"Input path not found: c:\\\\\\\\Users\\\\\\\\KEN\\\\\\\\Desktop\\\\\\\\チェックアウト\\\\\\\\汎用分析基盤コード\\\\\\\\../../data\\\",\\n  \\\"possible_improvements\\\": [\\n    \\\"ログレベルをDEBUGにして詳細確認\\\",\\n    \\\"追加パラメータを考慮した解析を実装する\\\",\\n    \\\"AIプロンプトの自動生成を検討\\\"\\n  ]\\n}\"\n    },\n    {\n      \"path\": \"your_project\\\\run_20250224_222224\\\\feedback.json\",\n      \"overview\": \"JSONファイル (辞書)。キー: timestamp, status, remarks, possible_improvements\\n\",\n      \"content\": \"{\\n  \\\"timestamp\\\": \\\"20250224_222224\\\",\\n  \\\"status\\\": \\\"Completed\\\",\\n  \\\"remarks\\\": \\\"\\\",\\n  \\\"possible_improvements\\\": [\\n    \\\"ログレベルをDEBUGにして詳細確認\\\",\\n    \\\"追加パラメータを考慮した解析を実装する\\\",\\n    \\\"AIプロンプトの自動生成を検討\\\"\\n  ]\\n}\"\n    },\n    {\n      \"path\": \"your_project\\\\run_20250224_232145\\\\feedback.json\",\n      \"overview\": \"JSONファイル (辞書)。キー: timestamp, status, remarks, possible_improvements\\n\",\n      \"content\": \"{\\n  \\\"timestamp\\\": \\\"20250224_232145\\\",\\n  \\\"status\\\": \\\"DB初期化失敗\\\",\\n  \\\"remarks\\\": \\\"Input path not found: c:\\\\\\\\Users\\\\\\\\KEN\\\\\\\\Desktop\\\\\\\\チェックアウト\\\\\\\\汎用分析基盤コード\\\\\\\\../../data\\\",\\n  \\\"possible_improvements\\\": [\\n    \\\"ログレベルをDEBUGにして詳細確認\\\",\\n    \\\"追加パラメータを考慮した解析を実装する\\\",\\n    \\\"AIプロンプトの自動生成を検討\\\"\\n  ]\\n}\"\n    },\n    {\n      \"path\": \"your_project\\\\run_20250308_025755\\\\feedback.json\",\n      \"overview\": \"JSONファイル (辞書)。キー: timestamp, status, remarks, possible_improvements\\n\",\n      \"content\": \"{\\n  \\\"timestamp\\\": \\\"20250308_025755\\\",\\n  \\\"status\\\": \\\"DB初期化失敗\\\",\\n  \\\"remarks\\\": \\\"Input path not found: C:\\\\\\\\Users\\\\\\\\KEN\\\\\\\\Desktop\\\\\\\\チェックアウト\\\\\\\\汎用分析基盤コード\\\\\\\\../../data\\\",\\n  \\\"possible_improvements\\\": [\\n    \\\"ログレベルをDEBUGにして詳細確認\\\",\\n    \\\"追加パラメータを考慮した解析を実装する\\\",\\n    \\\"AIプロンプトの自動生成を検討\\\"\\n  ]\\n}\"\n    },\n    {\n      \"path\": \"your_project\\\\run_20250308_033843\\\\feedback.json\",\n      \"overview\": \"JSONファイル (辞書)。キー: timestamp, status, remarks, possible_improvements\\n\",\n      \"content\": \"{\\n  \\\"timestamp\\\": \\\"20250308_033843\\\",\\n  \\\"status\\\": \\\"DB初期化失敗\\\",\\n  \\\"remarks\\\": \\\"Input path not found: C:\\\\\\\\Users\\\\\\\\KEN\\\\\\\\Desktop\\\\\\\\チェックアウト\\\\\\\\汎用分析基盤コード\\\\\\\\../../data\\\",\\n  \\\"possible_improvements\\\": [\\n    \\\"ログレベルをDEBUGにして詳細確認\\\",\\n    \\\"追加パラメータを考慮した解析を実装する\\\",\\n    \\\"AIプロンプトの自動生成を検討\\\"\\n  ]\\n}\"\n    },\n    {\n      \"path\": \"your_project\\\\utils\\\\config_manager.py\",\n      \"overview\": \"Pythonコード。\\nクラス: ConfigManager。\\n関数: __new__, load_config, get。\\n\",\n      \"content\": \"import json\\nimport os\\nimport logging\\nfrom typing import Any\\n\\nclass ConfigManager:\\n    _instance = None\\n\\n    def __new__(cls):\\n        if cls._instance is None:\\n            cls._instance = super(ConfigManager, cls).__new__(cls)\\n            cls._instance._config = {}\\n        return cls._instance\\n\\n    def load_config(self, config_path: str) -> None:\\n        if not os.path.exists(config_path):\\n            logging.warning(f'設定ファイルが見つかりません: {config_path}')\\n            self._config = {}\\n        else:\\n            try:\\n                with open(config_path, 'r', encoding='utf-8') as f:\\n                    content = f.read().strip()\\n                    if not content:\\n                        self._config = {}\\n                    else:\\n                        self._config = json.loads(content)\\n                logging.info(f'設定ファイル読込完了: {config_path}')\\n            except Exception as e:\\n                logging.warning(f'設定ファイル読込中にエラー: {str(e)}', exc_info=True)\\n                self._config = {}\\n\\n    def get(self, *keys: str, default: Any=None) -> Any:\\n        data = self._config\\n        for k in keys:\\n            if isinstance(data, dict) and k in data:\\n                data = data[k]\\n            else:\\n                return default\\n        if isinstance(data, dict) and 'value' in data:\\n            return data['value']\\n        return data\"\n    },\n    {\n      \"path\": \"your_project\\\\utils\\\\db_utils.py\",\n      \"overview\": \"Pythonコード。\\nクラス: DBUtils。\\n\",\n      \"content\": \"import aiosqlite\\nimport logging\\nimport os\\n\\nclass DBUtils:\\n    _connection = None\\n    _db_path = None\\n\\n    @staticmethod\\n    async def initialize_database(db_path='data.db'):\\n        DBUtils._db_path = db_path\\n        if not os.path.exists(db_path):\\n            conn = await aiosqlite.connect(db_path)\\n            await conn.execute('\\\\n                CREATE TABLE IF NOT EXISTS analysis_results (\\\\n                    id INTEGER PRIMARY KEY AUTOINCREMENT,\\\\n                    method_name TEXT,\\\\n                    result_data TEXT,\\\\n                    timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP\\\\n                )\\\\n            ')\\n            await conn.commit()\\n            await conn.close()\\n            logging.info(f'DB初期化完了: {db_path}')\\n        else:\\n            logging.info(f'既存DB使用: {db_path}')\\n        await DBUtils.get_connection()\\n\\n    @staticmethod\\n    async def get_connection():\\n        if DBUtils._connection is None:\\n            if DBUtils._db_path is None:\\n                raise ValueError('DBパス未設定。先にinitialize_databaseを呼ぶ必要があります。')\\n            DBUtils._connection = await aiosqlite.connect(DBUtils._db_path)\\n        return DBUtils._connection\\n\\n    @staticmethod\\n    async def close_connection():\\n        if DBUtils._connection:\\n            await DBUtils._connection.close()\\n            DBUtils._connection = None\\n            logging.info('DB接続を閉じました。')\"\n    },\n    {\n      \"path\": \"your_project\\\\utils\\\\file_identifier.py\",\n      \"overview\": \"Pythonコード。\\nクラス: FileIdentifier。\\n関数: __init__, identify_file_type, _identify_base_type_by_extension。\\n\",\n      \"content\": \"import os\\nimport logging\\nfrom utils.platform_identifier import PlatformIdentifier\\n\\nclass FileIdentifier:\\n\\n    def __init__(self, supported_data_types=None):\\n        self.supported_data_types = supported_data_types if supported_data_types else []\\n        self.platform_identifier = PlatformIdentifier()\\n\\n    def identify_file_type(self, file_path: str) -> str:\\n        extension = os.path.splitext(file_path)[1].lower()\\n        base_type = self._identify_base_type_by_extension(extension)\\n        if base_type in ['csv', 'json']:\\n            platform_type = self.platform_identifier.identify_platform(file_path, base_type)\\n            if platform_type in self.supported_data_types:\\n                return platform_type\\n        return base_type\\n\\n    def _identify_base_type_by_extension(self, extension: str) -> str:\\n        if extension in ['.txt', '.md']:\\n            return 'text'\\n        elif extension in ['.html', '.htm', '.mhtml']:\\n            return 'html'\\n        elif extension == '.json':\\n            return 'json'\\n        elif extension == '.csv':\\n            return 'csv'\\n        elif extension == '.xls':\\n            return 'xls'\\n        else:\\n            return 'unknown'\"\n    },\n    {\n      \"path\": \"your_project\\\\utils\\\\file_processor.py\",\n      \"overview\": \"Pythonコード。\\nクラス: FileProcessor。\\n関数: __init__, detect_encoding。\\n\",\n      \"content\": \"import os\\nimport logging\\nimport asyncio\\nfrom typing import List, Dict, Any\\nimport chardet\\nfrom utils.file_identifier import FileIdentifier\\nfrom parsers.parser_manager import ParserManager\\n\\nclass FileProcessor:\\n\\n    def __init__(self, data_types: List[str]):\\n        self.file_identifier = FileIdentifier(data_types)\\n        self.parser_manager = ParserManager()\\n\\n    async def process_files_in_parallel(self, files: List[str]) -> Dict[str, Any]:\\n        if not files:\\n            logging.info('処理対象ファイルなし。')\\n            return {}\\n        all_contents: Dict[str, List[Any]] = {}\\n        tasks = [self.process_file(file) for file in files]\\n        results = await asyncio.gather(*tasks, return_exceptions=True)\\n        for response in results:\\n            if isinstance(response, dict):\\n                for (key, value) in response.items():\\n                    all_contents.setdefault(key, []).extend(value)\\n            else:\\n                logging.error(f'ファイル処理中に例外: {response}', exc_info=True)\\n        return all_contents\\n\\n    async def process_file(self, file_path: str) -> Dict[str, List[Any]]:\\n        data_type = self.file_identifier.identify_file_type(file_path)\\n        parser = self.parser_manager.get_parser(data_type)\\n        if parser:\\n            encoding = self.detect_encoding(file_path)\\n            content = parser.parse(file_path, encoding)\\n            return {data_type: content}\\n        else:\\n            logging.warning(f\\\"未対応データタイプ '{data_type}' スキップ: {file_path}\\\")\\n            return {'unknown': []}\\n\\n    def detect_encoding(self, file_path: str) -> str:\\n        try:\\n            with open(file_path, 'rb') as f:\\n                raw_data = f.read(4096)\\n            result = chardet.detect(raw_data)\\n            encoding = result['encoding'] if result['encoding'] else 'utf-8'\\n            return encoding\\n        except Exception as e:\\n            logging.error(f'エンコーディング検出中にエラー: {str(e)}', exc_info=True)\\n            return 'utf-8'\"\n    },\n    {\n      \"path\": \"your_project\\\\utils\\\\html_report_generator.py\",\n      \"overview\": \"Pythonコード。\\nクラス: HtmlReportGenerator。\\n関数: __init__, save_html_report。\\n\",\n      \"content\": \"import os\\nimport logging\\nimport pandas as pd\\nfrom jinja2 import Environment, FileSystemLoader\\n\\nclass HtmlReportGenerator:\\n    \\\"\\\"\\\"\\n    HTMLレポート生成のみを担当し、OutputManagerとは責務を分離。\\n    \\\"\\\"\\\"\\n\\n    def __init__(self, template_dir: str='templates'):\\n        self.template_dir = template_dir\\n\\n    def save_html_report(self, data: pd.DataFrame, method_name: str, output_path: str) -> None:\\n        try:\\n            env = Environment(loader=FileSystemLoader(self.template_dir))\\n            template = env.get_template('report_template.html')\\n        except Exception as tmpl_err:\\n            logging.error(f'HTMLテンプレート読み込み失敗: {str(tmpl_err)}', exc_info=True)\\n            return\\n        try:\\n            html_content = template.render(title=method_name, data=data.to_dict(orient='records'), method_name=method_name)\\n            report_path = os.path.join(output_path, f'{method_name}.html')\\n            os.makedirs(os.path.dirname(report_path), exist_ok=True)\\n            with open(report_path, 'w', encoding='utf-8') as f:\\n                f.write(html_content)\\n            logging.info(f'HTMLレポート出力: {report_path}')\\n        except Exception as e:\\n            logging.error(f'HTMLレポート生成中エラー: {str(e)}', exc_info=True)\"\n    },\n    {\n      \"path\": \"your_project\\\\utils\\\\logger.py\",\n      \"overview\": \"Pythonコード。\\nクラス: Logger。\\n関数: setup_logging。\\n\",\n      \"content\": \"import logging\\nimport os\\nimport datetime\\n\\nclass Logger:\\n    _initialized = False\\n\\n    @staticmethod\\n    def setup_logging(log_dir: str, log_level: str) -> None:\\n        \\\"\\\"\\\"\\n        log_dir: ログファイル保存先\\n        log_level: ログレベル（例: INFO, DEBUG）\\n        \\\"\\\"\\\"\\n        if Logger._initialized:\\n            logging.getLogger(__name__).warning('Loggerは既に初期化済み。再初期化試行。')\\n        os.makedirs(log_dir, exist_ok=True)\\n        log_file = os.path.join(log_dir, 'app.log')\\n        if not hasattr(logging, log_level.upper()):\\n            logging.getLogger(__name__).warning(f\\\"無効なログレベル '{log_level}' -> 'INFO'を使用\\\")\\n            log_level = 'INFO'\\n        for handler in logging.root.handlers[:]:\\n            logging.root.removeHandler(handler)\\n        logging.basicConfig(level=getattr(logging, log_level.upper()), format='%(asctime)s - %(levelname)s - %(message)s', datefmt='%Y-%m-%d %H:%M:%S', handlers=[logging.FileHandler(log_file, encoding='utf-8'), logging.StreamHandler()])\\n        Logger._initialized = True\\n        logging.info('ロギング初期化完了。')\"\n    },\n    {\n      \"path\": \"your_project\\\\utils\\\\output_manager.py\",\n      \"overview\": \"Pythonコード。\\nクラス: OutputManager。\\n関数: save_results, save_to_csv, save_to_json, save_html_report, save_performance_metrics。\\n\",\n      \"content\": \"import os\\nimport pandas as pd\\nimport logging\\nimport json\\nfrom typing import Any, List, Dict\\nimport asyncio\\nfrom jinja2 import Environment, FileSystemLoader\\nfrom utils.db_utils import DBUtils\\n\\nclass OutputManager:\\n\\n    def save_results(self, data: pd.DataFrame, method_name: str, output_path: str) -> None:\\n        csv_output_path = os.path.join(output_path, f'{method_name}.csv')\\n        json_output_path = os.path.join(output_path, f'{method_name}.json')\\n        self.save_to_csv(data, csv_output_path)\\n        self.save_to_json(data, json_output_path)\\n\\n    def save_to_csv(self, data: pd.DataFrame, file_path: str) -> None:\\n        try:\\n            os.makedirs(os.path.dirname(file_path), exist_ok=True)\\n            data.to_csv(file_path, index=False)\\n            logging.info(f'CSV出力: {file_path}')\\n        except Exception as e:\\n            logging.error(f'CSV保存中エラー: {str(e)}', exc_info=True)\\n\\n    def save_to_json(self, data: pd.DataFrame, file_path: str) -> None:\\n        try:\\n            os.makedirs(os.path.dirname(file_path), exist_ok=True)\\n            data.to_json(file_path, orient='records', force_ascii=False)\\n            logging.info(f'JSON出力: {file_path}')\\n        except Exception as e:\\n            logging.error(f'JSON保存中エラー: {str(e)}', exc_info=True)\\n\\n    async def save_to_database(self, data: pd.DataFrame, method_name: str) -> None:\\n        try:\\n            conn = await DBUtils.get_connection()\\n            cursor = await conn.cursor()\\n            result_json = data.to_json(orient='records', force_ascii=False)\\n            await cursor.execute('INSERT INTO analysis_results (method_name, result_data) VALUES (?, ?)', (method_name, result_json))\\n            await conn.commit()\\n            logging.info(f'DB保存成功: {method_name}')\\n        except Exception as e:\\n            logging.error(f'DB保存中エラー: {str(e)}', exc_info=True)\\n\\n    def save_html_report(self, data: pd.DataFrame, method_name: str, output_path: str) -> None:\\n        try:\\n            env = Environment(loader=FileSystemLoader('templates'))\\n            template = env.get_template('report_template.html')\\n        except Exception as tmpl_err:\\n            logging.error(f'HTMLテンプレート読み込み失敗: {str(tmpl_err)}', exc_info=True)\\n            return\\n        try:\\n            html_content = template.render(title=method_name, data=data.to_dict(orient='records'), method_name=method_name)\\n            report_path = os.path.join(output_path, f'{method_name}.html')\\n            os.makedirs(os.path.dirname(report_path), exist_ok=True)\\n            with open(report_path, 'w', encoding='utf-8') as f:\\n                f.write(html_content)\\n            logging.info(f'HTMLレポート出力: {report_path}')\\n        except Exception as e:\\n            logging.error(f'HTMLレポート生成中エラー: {str(e)}', exc_info=True)\\n\\n    def save_performance_metrics(self, metrics: List[Dict[str, Any]], file_path: str) -> None:\\n        try:\\n            os.makedirs(os.path.dirname(file_path), exist_ok=True)\\n            df = pd.DataFrame(metrics)\\n            df.to_csv(file_path, index=False)\\n            logging.info(f'パフォーマンス結果CSV出力: {file_path}')\\n        except Exception as e:\\n            logging.error(f'パフォーマンス結果保存中エラー: {str(e)}', exc_info=True)\"\n    },\n    {\n      \"path\": \"your_project\\\\utils\\\\platform_feature_extractors.py\",\n      \"overview\": \"Pythonコード。\\nクラス: IPlatformFeatureExtractor, CsvPlatformFeatureExtractor, JsonPlatformFeatureExtractor。\\n関数: extract_features, extract_features, extract_features, extract_keys。\\n\",\n      \"content\": \"import pandas as pd\\nimport json\\nimport logging\\nimport os\\nfrom typing import Set, Any\\n\\nclass IPlatformFeatureExtractor:\\n    \\\"\\\"\\\"\\n    プラットフォーム特徴抽出インターフェース。\\n    ファイルからプラットフォーム判別用の特徴（カラム名やJSONキー）を抽出する。\\n    \\\"\\\"\\\"\\n\\n    def extract_features(self, file_path: str) -> Set[str]:\\n        raise NotImplementedError\\n\\nclass CsvPlatformFeatureExtractor(IPlatformFeatureExtractor):\\n\\n    def extract_features(self, file_path: str) -> Set[str]:\\n        features = set()\\n        try:\\n            df = pd.read_csv(file_path, nrows=5)\\n            features.update(df.columns.tolist())\\n        except Exception as e:\\n            logging.warning(f'CSV特徴抽出中エラー({file_path}): {str(e)}', exc_info=True)\\n        return features\\n\\nclass JsonPlatformFeatureExtractor(IPlatformFeatureExtractor):\\n\\n    def extract_features(self, file_path: str) -> Set[str]:\\n        features = set()\\n        try:\\n            with open(file_path, 'r', encoding='utf-8', errors='ignore') as jf:\\n                data = json.load(jf)\\n                features = self.extract_keys(data)\\n        except Exception as e:\\n            logging.warning(f'JSON特徴抽出中エラー({file_path}): {str(e)}', exc_info=True)\\n        return features\\n\\n    def extract_keys(self, data: Any, prefix: str='') -> Set[str]:\\n        keys = set()\\n        if isinstance(data, dict):\\n            for (k, v) in data.items():\\n                new_key = f'{prefix}.{k}' if prefix else k\\n                keys.add(new_key)\\n                keys |= self.extract_keys(v, new_key)\\n        elif isinstance(data, list):\\n            for (i, item) in enumerate(data):\\n                new_key = f'{prefix}[{i}]' if prefix else f'[{i}]'\\n                keys |= self.extract_keys(item, new_key)\\n        return keys\"\n    },\n    {\n      \"path\": \"your_project\\\\utils\\\\platform_identifier.py\",\n      \"overview\": \"Pythonコード。\\nクラス: PlatformIdentifier。\\n関数: __init__, identify_platform。\\n\",\n      \"content\": \"import logging\\nimport pandas as pd\\nimport json\\nfrom utils.platform_sample_manager import PlatformSampleManager\\n\\nclass PlatformIdentifier:\\n\\n    def __init__(self):\\n        self.sample_manager = PlatformSampleManager()\\n\\n    def identify_platform(self, file_path: str, base_type: str) -> str:\\n        try:\\n            if base_type == 'csv':\\n                df = pd.read_csv(file_path, nrows=5)\\n                return self.sample_manager.match_platform_for_csv(df)\\n            elif base_type == 'json':\\n                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\\n                    data = json.load(f)\\n                return self.sample_manager.match_platform_for_json(data)\\n        except Exception as e:\\n            logging.error(f'プラットフォーム識別中エラー: {str(e)}', exc_info=True)\\n        return base_type\"\n    },\n    {\n      \"path\": \"your_project\\\\utils\\\\platform_sample_manager.py\",\n      \"overview\": \"Pythonコード。\\nクラス: PlatformSampleManager。\\n関数: __init__, load_samples, load_platform_samples, match_platform_for_csv, match_platform_for_json。\\n\",\n      \"content\": \"import os\\nimport logging\\nimport pandas as pd\\nimport json\\nfrom utils.config_manager import ConfigManager\\nfrom utils.platform_feature_extractors import CsvPlatformFeatureExtractor, JsonPlatformFeatureExtractor\\n\\nclass PlatformSampleManager:\\n    \\\"\\\"\\\"\\n    プラットフォームごとのサンプルデータをロードし、\\n    CSV/JSON用の特徴抽出器で特徴セットを構築。\\n    解析時の類似度マッチングに利用する。\\n    \\\"\\\"\\\"\\n\\n    def __init__(self):\\n        config = ConfigManager()\\n        self.platform_samples_dir = config.get('platform_samples_directory', default='')\\n        if not self.platform_samples_dir:\\n            logging.warning('platform_samples_directory が設定されていません。サンプルロードはスキップします。')\\n            self.platform_samples_dir = ''\\n        logging.debug(f\\\"PlatformSampleManager: platform_samples_dir = '{self.platform_samples_dir}'\\\")\\n        self.platform_csv_patterns = {}\\n        self.platform_json_patterns = {}\\n        self.extractors = {'csv': CsvPlatformFeatureExtractor(), 'json': JsonPlatformFeatureExtractor()}\\n        self.load_samples()\\n\\n    def load_samples(self):\\n        if not self.platform_samples_dir:\\n            logging.warning('platform_samples_directory が空文字です。サンプルディレクトリ未設定扱い。')\\n            return\\n        if not os.path.exists(self.platform_samples_dir):\\n            logging.warning(f'プラットフォームサンプルディレクトリが存在しません: {self.platform_samples_dir}')\\n            return\\n        for platform in os.listdir(self.platform_samples_dir):\\n            p_dir = os.path.join(self.platform_samples_dir, platform)\\n            if os.path.isdir(p_dir):\\n                self.load_platform_samples(platform, p_dir)\\n\\n    def load_platform_samples(self, platform: str, p_dir: str):\\n        csv_cols = set()\\n        json_keys = set()\\n        for (root, _, files) in os.walk(p_dir):\\n            for f in files:\\n                fpath = os.path.join(root, f)\\n                ext = os.path.splitext(fpath)[1].lower()\\n                if ext == '.csv':\\n                    csv_cols |= self.extractors['csv'].extract_features(fpath)\\n                elif ext == '.json':\\n                    json_keys |= self.extractors['json'].extract_features(fpath)\\n        if csv_cols:\\n            self.platform_csv_patterns[platform] = {'columns': csv_cols}\\n        if json_keys:\\n            self.platform_json_patterns[platform] = {'keys': json_keys}\\n\\n    def match_platform_for_csv(self, df: pd.DataFrame) -> str:\\n        input_cols = set(df.columns.tolist())\\n        best_match = None\\n        best_score = 0\\n        for (platform, pattern) in self.platform_csv_patterns.items():\\n            common = len(input_cols & pattern['columns'])\\n            score = common / (len(pattern['columns']) + 1)\\n            if score > best_score:\\n                best_score = score\\n                best_match = platform\\n        return best_match if best_match else 'csv'\\n\\n    def match_platform_for_json(self, data: dict) -> str:\\n        input_keys = self.extractors['json'].extract_keys(data)\\n        best_match = None\\n        best_score = 0\\n        for (platform, pattern) in self.platform_json_patterns.items():\\n            common = len(input_keys & pattern['keys'])\\n            score = common / (len(pattern['keys']) + 1)\\n            if score > best_score:\\n                best_score = score\\n                best_match = platform\\n        return best_match if best_match else 'json'\"\n    },\n    {\n      \"path\": \"your_project\\\\utils\\\\__init__.py\",\n      \"overview\": \"Pythonコード。\\nDocstring(冒頭60文字): プロジェクトのルート __init__.py\\nここでは特に処理はなく、パッケージとして認識させるためだけの空ファイル。...\\n\",\n      \"content\": \"\\\"\\\"\\\"\\nプロジェクトのルート __init__.py\\nここでは特に処理はなく、パッケージとして認識させるためだけの空ファイル。\\n\\\"\\\"\\\"\"\n    },\n    {\n      \"path\": \"your_project\\\\visualization\\\\visualizer.py\",\n      \"overview\": \"Pythonコード。\\nクラス: Visualizer。\\n関数: __init__, visualize, _visualize_word_frequency, _visualize_bigram_frequency。\\n\",\n      \"content\": \"import os\\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\nimport logging\\n\\nclass Visualizer:\\n\\n    def __init__(self):\\n        sns.set(style='whitegrid')\\n\\n    def visualize(self, method_name: str, data: pd.DataFrame, output_path: str) -> None:\\n        try:\\n            if method_name == 'WordFrequencyAnalysis':\\n                self._visualize_word_frequency(data, output_path)\\n            elif method_name == 'BigramAnalysis':\\n                self._visualize_bigram_frequency(data, output_path)\\n            else:\\n                logging.warning(f\\\"'{method_name}'は可視化未サポート。\\\")\\n                return\\n        except Exception as e:\\n            logging.error(f'可視化中エラー: {str(e)}', exc_info=True)\\n\\n    def _visualize_word_frequency(self, data: pd.DataFrame, output_path: str) -> None:\\n        (fig, ax) = plt.subplots(figsize=(10, 6))\\n        top_words = data.head(20)\\n        sns.barplot(x='frequency', y='word', data=top_words, ax=ax)\\n        ax.set_title('Top 20 Words by Frequency')\\n        plt.tight_layout()\\n        output_file = os.path.join(output_path, 'word_frequency.png')\\n        fig.savefig(output_file)\\n        plt.close(fig)\\n        logging.info(f'単語頻度可視化保存: {output_file}')\\n\\n    def _visualize_bigram_frequency(self, data: pd.DataFrame, output_path: str) -> None:\\n        (fig, ax) = plt.subplots(figsize=(10, 6))\\n        top_bigrams = data.head(20)\\n        sns.barplot(x='frequency', y='bigram', data=top_bigrams, ax=ax)\\n        ax.set_title('Top 20 Bigrams by Frequency')\\n        plt.tight_layout()\\n        output_file = os.path.join(output_path, 'bigram_frequency.png')\\n        fig.savefig(output_file)\\n        plt.close(fig)\\n        logging.info(f'バイグラム頻度可視化保存: {output_file}')\"\n    },\n    {\n      \"path\": \"your_project\\\\visualization\\\\__init__.py\",\n      \"overview\": \"Pythonコード。\\nDocstring(冒頭60文字): プロジェクトのルート __init__.py\\nここでは特に処理はなく、パッケージとして認識させるためだけの空ファイル。...\\n\",\n      \"content\": \"\\\"\\\"\\\"\\nプロジェクトのルート __init__.py\\nここでは特に処理はなく、パッケージとして認識させるためだけの空ファイル。\\n\\\"\\\"\\\"\"\n    },\n    {\n      \"path\": \"your_project\\\\run_20250308_053814\\\\feedback.json\",\n      \"overview\": \"JSONファイル (辞書)。キー: timestamp, status, remarks, possible_improvements\\n\",\n      \"content\": \"{\\n  \\\"timestamp\\\": \\\"20250308_053814\\\",\\n  \\\"status\\\": \\\"Completed\\\",\\n  \\\"remarks\\\": \\\"\\\",\\n  \\\"possible_improvements\\\": [\\n    \\\"ログレベルをDEBUGにして詳細確認\\\",\\n    \\\"追加パラメータを考慮した解析を実装する\\\",\\n    \\\"AIプロンプトの自動生成を検討\\\"\\n  ]\\n}\"\n    },\n    {\n      \"path\": \"your_project\\\\run_20250308_054140\\\\feedback.json\",\n      \"overview\": \"JSONファイル (辞書)。キー: timestamp, status, remarks, possible_improvements\\n\",\n      \"content\": \"{\\n  \\\"timestamp\\\": \\\"20250308_054140\\\",\\n  \\\"status\\\": \\\"Completed\\\",\\n  \\\"remarks\\\": \\\"\\\",\\n  \\\"possible_improvements\\\": [\\n    \\\"ログレベルをDEBUGにして詳細確認\\\",\\n    \\\"追加パラメータを考慮した解析を実装する\\\",\\n    \\\"AIプロンプトの自動生成を検討\\\"\\n  ]\\n}\"\n    }\n  ]\n}"
    },
    {
      "path": "理解度チェック.py",
      "overview": "Pythonコード。\nクラス: LearningAnalyzer。\n関数: __init__, get_file_hash, is_binary_file, extract_text, load_documents, analyze_topics, visualize, process_file。\n",
      "content": "import os\nimport hashlib\nimport pandas as pd\nimport nltk\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import NMF\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport concurrent.futures\nimport gzip\nfrom bs4 import BeautifulSoup\nimport sqlite3\nfrom threading import Lock\n\n# 必要なNLTKデータのダウンロード\nnltk.download('punkt')\n\n\nclass LearningAnalyzer:\n    def __init__(self, folder_path):\n        self.folder_path = folder_path\n        self.documents = []\n        self.file_hashes = set()\n        self.processed_files = set()\n        self.topics = [\"確率分布\", \"推定・検定\", \"ベイズ統計\",\n                       \"回帰分析\", \"機械学習\", \"最適化\", \"データ可視化\"]\n        # SQLiteデータベースの初期化\n        self.conn = sqlite3.connect(\n            'processed_files.db', check_same_thread=False)\n        self.cursor = self.conn.cursor()\n        self.cursor.execute('''CREATE TABLE IF NOT EXISTS processed_files (\n                                file_path TEXT PRIMARY KEY,\n                                file_hash TEXT\n                               )''')\n        self.conn.commit()\n        # ロックオブジェクトの作成\n        self.db_lock = Lock()\n        self.log_lock = Lock()\n\n    def get_file_hash(self, file_path):\n        \"\"\"ファイル内容のSHA-256ハッシュ値を計算\"\"\"\n        hasher = hashlib.sha256()\n        with open(file_path, 'rb') as f:\n            for chunk in iter(lambda: f.read(65536), b''):\n                hasher.update(chunk)\n        return hasher.hexdigest()\n\n    def is_binary_file(self, file_path):\n        \"\"\"先頭数百バイトを確認し、バイナリファイルか推定する\"\"\"\n        try:\n            with open(file_path, 'rb') as f:\n                chunk = f.read(512)\n        except Exception:\n            # 開けないファイルはバイナリ扱いでスキップ\n            return True\n        if not chunk:\n            # 空ファイルはテキストとみなす\n            return False\n        if b'\\x00' in chunk:\n            return True\n        # テキストと許容するバイト集合（印字可能ASCIIと一部制御文字）\n        text_bytes = set(range(32, 127)) | {9, 10, 13, 8, 12}\n        nontext_count = sum(1 for b in chunk if b not in text_bytes)\n        return (nontext_count / len(chunk) > 0.30)\n\n    def extract_text(self, file_path):\n        \"\"\"ファイルからテキストを抽出（HTML, MHTML, GZ対応）\"\"\"\n        try:\n            if file_path.endswith('.html', '.htm', '.mhtml','.txt', '.ipynb', '.py'):\n                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n                    soup = BeautifulSoup(f, 'html.parser')\n                    return soup.get_text()\n            elif file_path.endswith('.gz'):\n                # .gzファイルをメモリ上で解凍してテキスト化\n                with gzip.open(file_path, 'rb') as f:\n                    data = f.read()\n                if not data:\n                    return \"\"  # 空ファイル\n                # バイナリデータが含まれるかチェック\n                sample = data[:512]\n                text_bytes = set(range(32, 127)) | {9, 10, 13, 8, 12}\n                if b'\\x00' in sample or (sum(1 for b in sample if b not in text_bytes) / len(sample) > 0.30):\n                    return None\n                # UTF-8デコード（不正なバイトは無視）\n                return data.decode('utf-8', errors='ignore')\n            else:\n                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n                    return f.read()\n        except Exception:\n            # 読み込みやパースでエラー発生\n            return None\n\n    def load_documents(self):\n        \"\"\"フォルダ内の全ファイルを探索し、並列にテキスト読み込み\"\"\"\n        def process_file(path):\n            # 1) バイナリ拡張子のファイルをスキップ\n            if path.lower().endswith(('.bin', '.exe', '.dll', '.dat')):\n                with self.log_lock:\n                    with open(\"skipped_files.log\", \"a\", encoding=\"utf-8\") as log:\n                        log.write(f\"{path}: Skipped (binary file extension)\\n\")\n                return None\n            # 2) 内容的にバイナリっぽいファイルもスキップ\n            if self.is_binary_file(path):\n                with self.log_lock:\n                    with open(\"skipped_files.log\", \"a\", encoding=\"utf-8\") as log:\n                        log.write(f\"{path}: Skipped (binary content)\\n\")\n                return None\n            # 3) ハッシュ値計算（この段階まで来たのはテキストファイルの可能性が高い）\n            try:\n                file_hash = self.get_file_hash(path)\n            except Exception as e:\n                with self.log_lock:\n                    with open(\"skipped_files.log\", \"a\", encoding=\"utf-8\") as log:\n                        log.write(f\"{path}: Failed to compute hash ({e})\\n\")\n                return None\n            # 4) データベースやセットで処理済みか確認\n            with self.db_lock:\n                self.cursor.execute(\n                    \"SELECT 1 FROM processed_files WHERE file_path=? OR file_hash=?\", (path, file_hash))\n                found = self.cursor.fetchone()\n                if found or file_hash in self.file_hashes or path in self.processed_files:\n                    reason = \"already processed\" if found else \"duplicate in current run\"\n                    # ログに記録\n                    with open(\"skipped_files.log\", \"a\", encoding=\"utf-8\") as log:\n                        log.write(f\"{path}: Skipped ({reason})\\n\")\n                    return None\n                # 処理対象として予約（重複防止のため先にセットに追加）\n                self.file_hashes.add(file_hash)\n                self.processed_files.add(path)\n            # 5) テキスト抽出\n            text = self.extract_text(path)\n            if text is None or text.strip() == \"\":\n                # 抽出失敗や空テキストの場合は予約解除してスキップ\n                with self.db_lock:\n                    self.file_hashes.discard(file_hash)\n                    self.processed_files.discard(path)\n                with self.log_lock:\n                    with open(\"skipped_files.log\", \"a\", encoding=\"utf-8\") as log:\n                        reason = \"no text\" if text == \"\" else \"extract error\"\n                        log.write(f\"{path}: Skipped ({reason})\\n\")\n                return None\n            # 6) 処理済みとしてDB登録\n            with self.db_lock:\n                try:\n                    self.cursor.execute(\"INSERT OR IGNORE INTO processed_files (file_path, file_hash) VALUES (?, ?)\",\n                                        (path, file_hash))\n                    self.conn.commit()\n                except Exception as e:\n                    with open(\"skipped_files.log\", \"a\", encoding=\"utf-8\") as log:\n                        log.write(\n                            f\"{path}: Warning - DB insert failed ({e})\\n\")\n            return text\n\n        # フォルダ内の全ファイルパスを再帰取得\n        file_list = [os.path.join(root, f)\n                     for root, _, files in os.walk(self.folder_path) for f in files]\n        # スレッドプールでファイル処理を並列実行\n        with concurrent.futures.ThreadPoolExecutor() as executor:\n            results = list(executor.map(process_file, file_list))\n        # 抽出成功したテキストだけを保存\n        self.documents = [doc for doc in results if doc]\n        return len(self.documents)\n\n    def analyze_topics(self):\n        \"\"\"NMFを使用してトピック分析を行う\"\"\"\n        if not self.documents:\n            raise ValueError(\"文書リストが空です。先にload_documentsを実行してください。\")\n        vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)\n        tfidf_matrix = vectorizer.fit_transform(self.documents)\n        nmf = NMF(n_components=len(self.topics), random_state=42)\n        topic_matrix = nmf.fit_transform(tfidf_matrix)\n        df = pd.DataFrame(topic_matrix, columns=self.topics)\n        df[\"Document\"] = [f\"Doc_{i+1}\" for i in range(len(self.documents))]\n        return df\n\n    def visualize(self, df):\n        \"\"\"ヒートマップでトピック分布を可視化\"\"\"\n        plt.figure(figsize=(10, 6))\n        sns.heatmap(df.set_index(\"Document\"), annot=True, cmap=\"coolwarm\")\n        plt.title(\"トピックごとの理解度マッピング\")\n        plt.show()\n\n\n# 使用例\nfolder_path = \"p:\"\nanalyzer = LearningAnalyzer(folder_path)\n\nprint(\"文書を読み込み中...\")\nanalyzer.load_documents()\nprint(f\"読み込んだ文書数: {len(analyzer.documents)}\")\n\nprint(\"トピック分析中...\")\ndf_topics = analyzer.analyze_topics()\nprint(df_topics.head())\n\nanalyzer.visualize(df_topics)\n"
    },
    {
      "path": "ｃ環境インストール補助.py",
      "overview": "Pythonコード。\n関数: list_envs, choose_env_hard, run_command_in_env, main。\n",
      "content": "import os\nimport sys\nimport subprocess\nfrom pathlib import Path\n\n# ===== ハードコードされた設定 =====\n# Anaconda のインストール先（必要に応じて変更してください）\nCONDA_PREFIX = r\"C:\\Users\\KEN\\anaconda3\"\n\n# 実行するコマンド（例: 必要なパッケージのインストール）\nCOMMAND = \"pip install pymc arviz\"\n\n# 仮想環境として使用する環境名が \"python3-10-8\" が存在すればそれを使い、\n# 存在しなければ最初に見つかった環境を使用する\nTARGET_ENV_NAME = \"python3-10-8\"\n# ===================================\n\ndef list_envs(envs_path):\n    \"\"\"指定された envs フォルダ内の仮想環境（ディレクトリ）をリストアップする\"\"\"\n    try:\n        envs = [d.name for d in Path(envs_path).iterdir() if d.is_dir()]\n        return envs\n    except Exception as e:\n        print(f\"仮想環境のリスト取得に失敗: {e}\")\n        sys.exit(1)\n\ndef choose_env_hard(envs):\n    \"\"\"ハードコードされた TARGET_ENV_NAME が存在するかチェックし、\n    なければ最初の環境を返す\"\"\"\n    if TARGET_ENV_NAME in envs:\n        return TARGET_ENV_NAME\n    else:\n        print(f\"指定された環境 '{TARGET_ENV_NAME}' が見つかりません。代わりに '{envs[0]}' を使用します。\")\n        return envs[0]\n\ndef run_command_in_env(env_name, command, conda_prefix):\n    \"\"\"\n    Windows の cmd.exe を利用して、指定した仮想環境をアクティベートした上で\n    コマンドを実行します。activate.bat を用いるので、システム環境変数は変更しません。\n    \"\"\"\n    activate_script = os.path.join(conda_prefix, \"Scripts\", \"activate.bat\")\n    if not os.path.exists(activate_script):\n        print(\"activate.bat が見つかりません。Anaconda のパスを確認してください。\")\n        sys.exit(1)\n    \n    # cmd.exe を使って activate.bat を呼び出し、その後にコマンドを実行\n    full_command = f'cmd /c \"{activate_script} {env_name} && {command}\"'\n    print(\"実行コマンド:\")\n    print(full_command)\n    \n    try:\n        result = subprocess.run(full_command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n        print(\"標準出力:\")\n        print(result.stdout)\n        if result.stderr:\n            print(\"標準エラー出力:\")\n            print(result.stderr)\n    except Exception as e:\n        print(f\"コマンド実行中にエラーが発生しました: {e}\")\n        sys.exit(1)\n\ndef main():\n    # envs フォルダのパス\n    envs_path = os.path.join(CONDA_PREFIX, \"envs\")\n    \n    if not os.path.isdir(envs_path):\n        print(f\"指定された envs パスが存在しません: {envs_path}\")\n        sys.exit(1)\n    \n    available_envs = list_envs(envs_path)\n    if not available_envs:\n        print(\"利用可能な仮想環境が見つかりませんでした。\")\n        sys.exit(1)\n    \n    # ハードコードされた TARGET_ENV_NAME が存在するかチェック\n    chosen_env = choose_env_hard(available_envs)\n    \n    print(f\"選択された仮想環境: {chosen_env}\")\n    run_command_in_env(chosen_env, COMMAND, CONDA_PREFIX)\n\nif __name__ == \"__main__\":\n    main()\n"
    },
    {
      "path": "obfuscated_latest\\統計検定1級\\config.py",
      "overview": "Pythonコード。\nクラス: c_sym。\n関数: __sym, l_sym, g_sym。\n",
      "content": "import json, os\n\nclass c_sym:\n\n    def __sym(self, config_file='config.json'):\n        script_dir = os.path.dirname(os.path.abspath(__file__))\n        self.config_file = os.path.join(script_dir, config_file)\n        self.settings = self.load_config()\n\n    def l_sym(self):\n        if not os.path.exists(self.config_file):\n            return {}\n        try:\n            with open(self.config_file, 'r', encoding='utf-8') as f:\n                content = f.read().strip()\n                if not content:\n                    return {}\n                return json.loads(content)\n        except Exception:\n            return {}\n\n    def g_sym(self, *keys, default=None):\n        data = self.settings\n        for k in keys:\n            if isinstance(data, dict) and k in data:\n                data = data[k]\n            else:\n                return default\n        return data\nconfig = c_sym()"
    },
    {
      "path": "obfuscated_latest\\統計検定1級\\database.py",
      "overview": "Pythonコード。\nクラス: d_sym。\n関数: __sym, s_sym, s_sym。\n",
      "content": "import sqlite3, os\nfrom config import config\n\nclass d_sym:\n\n    def __sym(self):\n        script_dir = os.path.dirname(os.path.abspath(__file__))\n        db_path = config.get('database_path', default='data/data.db')\n        if db_path is None:\n            db_path = 'data/data.db'\n        self.db_name = os.path.join(script_dir, db_path)\n        os.makedirs(os.path.dirname(self.db_name), exist_ok=True)\n\n    def s_sym(self):\n        conn = sqlite3.connect(self.db_name)\n        c = conn.cursor()\n        c.execute('CREATE TABLE IF NOT EXISTS problems (id TEXT PRIMARY KEY,date_created TEXT,problem_text TEXT,solution_text TEXT,problem_type TEXT)')\n        conn.commit()\n        conn.close()\n\n    def s_sym(self, problem_id, date_created, problem_text, solution_text, problem_type):\n        conn = sqlite3.connect(self.db_name)\n        c = conn.cursor()\n        c.execute('INSERT INTO problems VALUES (?,?,?,?,?)', (problem_id, date_created, problem_text, solution_text, problem_type))\n        conn.commit()\n        conn.close()"
    },
    {
      "path": "obfuscated_latest\\統計検定1級\\graph.py",
      "overview": "Pythonコード。\nクラス: p_sym。\n関数: __sym, __sym, p_sym, p_sym, p_sym, p_sym, p_sym, p_sym, p_sym, p_sym, p_sym, p_sym, p_sym, c_sym, m_sym, e_sym。\n",
      "content": "import matplotlib\nmatplotlib.use('Agg')\nimport matplotlib.pyplot as plt\nimport math\nimport numpy as np\nimport logging\nimport os\nfrom math import factorial, exp\nfrom scipy.stats import norm, t as t_dist, f as f_dist, poisson, expon\nfrom statsmodels.graphics.tsaplots import plot_acf\nimport traceback\n\nclass p_sym:\n\n    def __sym(self):\n        pass\n\n    def __sym(self, output_path):\n        \"\"\"\n        画像を確実にPNGで出力し、ファイルサイズなどをログするヘルパー関数。\n        \"\"\"\n        try:\n            (_, ext) = os.path.splitext(output_path)\n            if ext.lower() == '.png':\n                plt.savefig(output_path, format='png')\n            else:\n                plt.savefig(output_path, format='png')\n            plt.close()\n            if os.path.exists(output_path):\n                fsize = os.path.getsize(output_path)\n                logging.info(f'Saved figure: {output_path} (size: {fsize} bytes)')\n            else:\n                logging.warning(f'File not found after saving: {output_path}')\n        except Exception as e:\n            logging.error(f'Failed to save figure to {output_path}, error={e}')\n            plt.close()\n\n    def p_sym(self, params, output_path):\n        ptype = params.get('problem_type')\n        try:\n            if ptype == 'binomial':\n                p = params['p']\n                n = params['n']\n                k = params['k']\n                x = range(n + 1)\n                from math import comb\n                pmf = [comb(n, i) * p ** i * (1 - p) ** (n - i) for i in x]\n                cdf = np.cumsum(pmf)\n                plt.figure(figsize=(10, 5))\n                plt.subplot(1, 2, 1)\n                plt.bar(x, pmf, color='skyblue')\n                plt.title(f'Binomial PMF n={n}, p={p}')\n                plt.xlabel('k')\n                plt.ylabel('P(X=k)')\n                plt.axvline(k, color='red', linestyle='--', label='Target k')\n                plt.legend()\n                plt.subplot(1, 2, 2)\n                plt.step(x, cdf, where='post', color='green')\n                plt.title('CDF')\n                plt.xlabel('k')\n                plt.ylabel('P(X<=k)')\n                plt.axvline(k, color='red', linestyle='--')\n                plt.tight_layout()\n                self._safe_savefig(output_path)\n                return True\n            elif ptype == 'poisson':\n                lam = params['lambda']\n                k = params['k']\n                x = range(k + 10 + 1)\n                pmf = [lam ** i * exp(-lam) / factorial(i) for i in x]\n                cdf = np.cumsum(pmf)\n                plt.figure(figsize=(10, 5))\n                plt.subplot(1, 2, 1)\n                plt.bar(x, pmf, color='orange')\n                plt.title(f'Poisson(lambda={lam}) PMF')\n                plt.xlabel('k')\n                plt.ylabel('P(X=k)')\n                plt.axvline(k, color='red', linestyle='--', label='Target k')\n                plt.legend()\n                plt.subplot(1, 2, 2)\n                plt.step(x, cdf, where='post', color='green')\n                plt.title('CDF')\n                plt.xlabel('k')\n                plt.ylabel('P(X<=k)')\n                plt.axvline(k, color='red', linestyle='--')\n                plt.tight_layout()\n                self._safe_savefig(output_path)\n                return True\n            elif ptype == 'conditional_probability':\n                P_A = params['P_A']\n                P_BA = params['P_B_given_A']\n                P_AB = params['P_A_and_B']\n                plt.figure()\n                vals = [P_A, P_BA, P_AB]\n                labels = ['P(A)', 'P(B|A)', 'P(A∩B)']\n                plt.bar(labels, vals, color=['blue', 'green', 'red'])\n                plt.title('Conditional Probability Visualization')\n                plt.ylabel('Probability')\n                plt.tight_layout()\n                self._safe_savefig(output_path)\n                return True\n            else:\n                return False\n        except Exception as e:\n            logging.error(f'Error in plot_probability: {e}')\n            plt.close()\n            return False\n\n    def p_sym(self, params, output_path):\n        try:\n            alpha = params['alpha']\n            df = params['df']\n            t_stat = params['t_stat']\n            critical_value = params['critical_value']\n            x = np.linspace(t_dist.ppf(0.001, df), t_dist.ppf(0.999, df), 1000)\n            y = t_dist.pdf(x, df)\n            plt.figure(figsize=(10, 5))\n            plt.plot(x, y, 'b-', label=f't-dist(df={df})')\n            plt.axvline(x=critical_value, color='r', linestyle='--', label='critical +')\n            plt.axvline(x=-critical_value, color='r', linestyle='--', label='critical -')\n            plt.axvline(x=t_stat, color='g', label='t-stat')\n            p_area_x = x[x > critical_value]\n            plt.fill_between(p_area_x, t_dist.pdf(p_area_x, df), color='red', alpha=0.3)\n            p_area_x2 = x[x < -critical_value]\n            plt.fill_between(p_area_x2, t_dist.pdf(p_area_x2, df), color='red', alpha=0.3)\n            plt.title('t-test visualization')\n            plt.xlabel('t')\n            plt.ylabel('pdf')\n            plt.legend()\n            plt.tight_layout()\n            self._safe_savefig(output_path)\n            return True\n        except Exception as e:\n            logging.error(f't検定グラフ生成エラー: {e}')\n            plt.close()\n            return False\n\n    def p_sym(self, x_values, y_values, beta_0_hat, beta_1_hat, output_path):\n        try:\n            import statsmodels.api as sm\n            X = sm.add_constant(x_values)\n            model = sm.OLS(y_values, X).fit()\n            residuals = model.resid\n            (fig, axes) = plt.subplots(2, 2, figsize=(10, 10))\n            ax = axes[0, 0]\n            ax.scatter(x_values, y_values, color='blue', label='data')\n            x_line = np.linspace(min(x_values), max(x_values), 100)\n            y_line = beta_0_hat + beta_1_hat * x_line\n            ax.plot(x_line, y_line, color='red', label='reg line')\n            ax.set_title('Data & Regression Line')\n            ax.set_xlabel('X')\n            ax.set_ylabel('Y')\n            ax.legend()\n            ax = axes[0, 1]\n            ax.hist(residuals, bins=20, color='green', alpha=0.7)\n            ax.set_title('Residual Histogram')\n            ax.set_xlabel('Residual')\n            ax.set_ylabel('Frequency')\n            sm.qqplot(residuals, line='45', ax=axes[1, 0], color='purple')\n            axes[1, 0].set_title('Q-Q plot of Residuals')\n            fitted = model.fittedvalues\n            ax = axes[1, 1]\n            ax.scatter(fitted, residuals, color='orange')\n            ax.axhline(y=0, color='red', linestyle='--')\n            ax.set_title('Residuals vs Fitted')\n            ax.set_xlabel('Fitted')\n            ax.set_ylabel('Residuals')\n            fig.tight_layout()\n            self._safe_savefig(output_path)\n            return True\n        except Exception as e:\n            logging.error(f'回帰分析グラフ生成中にエラー: {e}')\n            plt.close()\n            return False\n\n    def p_sym(self, params, output_path):\n        try:\n            ts = params['time_series']\n            (fig, axes) = plt.subplots(2, 1, figsize=(10, 8))\n            axes[0].plot(ts, color='blue')\n            axes[0].set_title('Time Series Data')\n            axes[0].set_xlabel('Time')\n            axes[0].set_ylabel('Value')\n            plot_acf(ts, ax=axes[1])\n            axes[1].set_title('Autocorrelation Function')\n            fig.tight_layout()\n            self._safe_savefig(output_path)\n            return True\n        except Exception as e:\n            logging.error(f'時系列分析グラフエラー: {e}')\n            plt.close()\n            return False\n\n    def p_sym(self, params, output_path):\n        try:\n            import statsmodels.api as sm\n            X = np.column_stack((params['x1_values'], params['x2_values']))\n            Y = np.array(params['y_values'])\n            Xc = sm.add_constant(X)\n            model = sm.OLS(Y, Xc).fit()\n            residuals = model.resid\n            fitted = model.fittedvalues\n            (fig, axes) = plt.subplots(2, 2, figsize=(10, 10))\n            ax = axes[0, 0]\n            ax.scatter(params['x1_values'], Y, color='blue', alpha=0.7, label='X1-Y')\n            ax.set_title('X1 vs Y')\n            ax.set_xlabel('X1')\n            ax.set_ylabel('Y')\n            ax = axes[0, 1]\n            ax.scatter(params['x2_values'], Y, color='green', alpha=0.7, label='X2-Y')\n            ax.set_title('X2 vs Y')\n            ax.set_xlabel('X2')\n            ax.set_ylabel('Y')\n            ax = axes[1, 0]\n            ax.hist(residuals, bins=20, color='gray', alpha=0.7)\n            ax.set_title('Residuals Histogram')\n            ax.set_xlabel('Residual')\n            ax = axes[1, 1]\n            ax.scatter(fitted, residuals, color='red', alpha=0.7)\n            ax.axhline(y=0, color='black', linestyle='--')\n            ax.set_title('Residuals vs Fitted')\n            ax.set_xlabel('Fitted')\n            ax.set_ylabel('Residuals')\n            fig.tight_layout()\n            self._safe_savefig(output_path)\n            return True\n        except Exception as e:\n            logging.error(f'計量経済学グラフエラー: {e}')\n            plt.close()\n            return False\n\n    def p_sym(self, params, output_path):\n        try:\n            mu = params['mu']\n            sigma = np.array(params['sigma'])\n            fig = plt.figure(figsize=(10, 10))\n            from matplotlib.patches import Ellipse\n            import matplotlib.transforms as transforms\n\n            def c_sym(mu, cov, ax, n_std=1.96, facecolor='none', **kwargs):\n                pearson = cov[0, 1] / np.sqrt(cov[0, 0] * cov[1, 1])\n                ell_radius_x = np.sqrt(1 + pearson)\n                ell_radius_y = np.sqrt(1 - pearson)\n                ellipse = Ellipse((0, 0), width=ell_radius_x * 2, height=ell_radius_y * 2, facecolor=facecolor, **kwargs)\n                scale_x = np.sqrt(cov[0, 0]) * n_std\n                scale_y = np.sqrt(cov[1, 1]) * n_std\n                transf = transforms.Affine2D().rotate_deg(45).scale(scale_x, scale_y).translate(mu[0], mu[1])\n                ellipse.set_transform(transf + ax.transData)\n                return ax.add_patch(ellipse)\n            ax = fig.add_subplot(2, 2, 1)\n            ax.set_title('Confidence Ellipse')\n            c_sym(mu, sigma, ax, edgecolor='red')\n            ax.scatter(mu[0], mu[1], c='blue', marker='x', label='mean')\n            ax.legend()\n            ax.set_xlabel('X1')\n            ax.set_ylabel('X2')\n            ax2 = fig.add_subplot(2, 2, 2)\n            x = np.linspace(mu[0] - 4 * np.sqrt(sigma[0, 0]), mu[0] + 4 * np.sqrt(sigma[0, 0]), 100)\n            y = np.linspace(mu[1] - 4 * np.sqrt(sigma[1, 1]), mu[1] + 4 * np.sqrt(sigma[1, 1]), 100)\n            (X, Y) = np.meshgrid(x, y)\n            pos = np.dstack((X, Y))\n\n            def m_sym(xarr, muarr, cov):\n                det = np.linalg.det(cov)\n                inv = np.linalg.inv(cov)\n                diff = xarr - muarr\n                return 1.0 / (2 * np.pi * np.sqrt(det)) * np.exp(-0.5 * (diff @ inv @ diff.T))\n            Z = np.empty(X.shape)\n            for i in range(X.shape[0]):\n                for j in range(X.shape[1]):\n                    Z[i, j] = m_sym(np.array([X[i, j], Y[i, j]]), np.array(mu), sigma)\n            ax2.contour(X, Y, Z, levels=5, cmap='Blues')\n            ax2.set_title('Contour')\n            ax3 = fig.add_subplot(2, 2, 3)\n            X_marg = norm(loc=mu[0], scale=np.sqrt(sigma[0, 0]))\n            x_line = np.linspace(mu[0] - 4 * np.sqrt(sigma[0, 0]), mu[0] + 4 * np.sqrt(sigma[0, 0]), 100)\n            y_line = X_marg.pdf(x_line)\n            ax3.plot(x_line, y_line, 'r-')\n            ax3.set_title('Marginal X1 distribution')\n            ax3.set_xlabel('X1')\n            ax3.set_ylabel('pdf')\n            ax4 = fig.add_subplot(2, 2, 4)\n            Y_marg = norm(loc=mu[1], scale=np.sqrt(sigma[1, 1]))\n            y_line = Y_marg.pdf(x_line)\n            ax4.plot(x_line, y_line, 'g-')\n            ax4.set_title('Marginal X2 distribution')\n            ax4.set_xlabel('X2')\n            ax4.set_ylabel('pdf')\n            fig.tight_layout()\n            self._safe_savefig(output_path)\n            return True\n        except Exception as e:\n            logging.error(f'多変量正規分布グラフエラー: {e}')\n            plt.close()\n            return False\n\n    def p_sym(self, params, output_path):\n        try:\n            dist = params['distribution']\n            plt.figure(figsize=(10, 5))\n            if dist == '正規分布':\n                mu = 0\n                sigma = 1\n                x = np.linspace(mu - 4 * sigma, mu + 4 * sigma, 200)\n                y = norm.pdf(x, mu, sigma)\n                plt.plot(x, y, 'b-')\n                plt.axvline(mu, color='r', linestyle='--', label='mean')\n                plt.axvline(mu + sigma, color='g', linestyle=':', label='mean+sigma')\n                plt.axvline(mu - sigma, color='g', linestyle=':')\n                plt.title('Normal Distribution (mu=0, sigma=1)')\n                plt.legend()\n            elif dist == 'ポアソン分布':\n                lam = 3\n                x = np.arange(0, 15)\n                y = poisson.pmf(x, lam)\n                plt.bar(x, y, color='skyblue')\n                plt.axvline(lam, color='r', linestyle='--', label='mean=lambda=3')\n                plt.title('Poisson(lambda=3)')\n                plt.legend()\n            elif dist == '指数分布':\n                lam = 1\n                x = np.linspace(0, 5, 200)\n                y = expon.pdf(x, scale=1 / lam)\n                plt.plot(x, y, 'b-')\n                plt.axvline(1 / lam, color='r', linestyle='--', label='mean=1/lambda')\n                plt.title('Exponential(lambda=1)')\n                plt.legend()\n            plt.tight_layout()\n            self._safe_savefig(output_path)\n            return True\n        except Exception as e:\n            logging.error(f'分布性質グラフエラー: {e}')\n            plt.close()\n            return False\n\n    def p_sym(self, params, output_path):\n        try:\n            mu = params['mu']\n            sigma = params['sigma']\n            n = params['n']\n            moment = params['moment']\n            x = np.linspace(mu - 4 * sigma, mu + 4 * sigma, 200)\n            y = norm.pdf(x, mu, sigma)\n            plt.figure(figsize=(10, 5))\n            plt.plot(x, y, 'b-', label='Normal pdf')\n            plt.axvline(mu, color='r', linestyle='--', label='mean')\n            plt.axvline(mu + sigma, color='g', linestyle=':', label='mu±sigma')\n            plt.axvline(mu - sigma, color='g', linestyle=':')\n            plt.title(f'Normal distribution (mu={mu}, sigma={sigma}). n={n}-th moment={moment}')\n            plt.legend()\n            plt.tight_layout()\n            self._safe_savefig(output_path)\n            return True\n        except Exception as e:\n            logging.error(f'高次モーメントグラフエラー: {e}')\n            plt.close()\n            return False\n\n    def p_sym(self, params, output_path):\n        try:\n            group_count = params['group_count']\n            sample_sizes = params['sample_sizes']\n            means = params['means']\n            variances = params['variances']\n            data = []\n            for i in range(group_count):\n                np.random.seed(i)\n                samples = np.random.normal(means[i], np.sqrt(variances[i]), sample_sizes[i])\n                data.append(samples)\n            plt.figure(figsize=(8, 6))\n            plt.boxplot(data, labels=[f'Group{i + 1}' for i in range(group_count)])\n            plt.title('ANOVA: Boxplots of groups')\n            plt.ylabel('Value')\n            plt.tight_layout()\n            self._safe_savefig(output_path)\n            return True\n        except Exception as e:\n            logging.error(f'分散分析グラフエラー: {e}')\n            plt.close()\n            return False\n\n    def p_sym(self, params, output_path):\n        try:\n            s1 = params['sample1']\n            s2 = params['sample2']\n\n            def e_sym(data):\n                d_sorted = np.sort(data)\n                y = np.arange(1, len(d_sorted) + 1) / len(d_sorted)\n                return (d_sorted, y)\n            (x1, y1) = e_sym(s1)\n            (x2, y2) = e_sym(s2)\n            plt.figure(figsize=(8, 6))\n            plt.step(x1, y1, where='post', label='Sample1 ECDF', color='blue')\n            plt.step(x2, y2, where='post', label='Sample2 ECDF', color='red')\n            plt.title('Nonparametric Test Visualization')\n            plt.xlabel('Value')\n            plt.ylabel('ECDF')\n            plt.legend()\n            plt.tight_layout()\n            self._safe_savefig(output_path)\n            return True\n        except Exception as e:\n            logging.error(f'ノンパラ検定グラフエラー: {e}')\n            plt.close()\n            return False\n\n    def p_sym(self, params, output_path):\n        try:\n            a = params['a']\n            b = params['b']\n            mu1 = params['mu1']\n            mu2 = params['mu2']\n            sig1 = params['sigma1_squared']\n            sig2 = params['sigma2_squared']\n            np.random.seed(123)\n            x = np.random.normal(mu1, np.sqrt(sig1), 1000)\n            y = np.random.normal(mu2, np.sqrt(sig2), 1000)\n            Z = a * x + b * y\n            plt.figure(figsize=(8, 6))\n            plt.hist(Z, bins=30, density=True, alpha=0.7, color='purple', label='Simulated Z')\n            E_Z = a * mu1 + b * mu2\n            Var_Z = a ** 2 * sig1 + b ** 2 * sig2\n            X_line = np.linspace(E_Z - 4 * np.sqrt(Var_Z), E_Z + 4 * np.sqrt(Var_Z), 200)\n            Y_line = norm.pdf(X_line, E_Z, np.sqrt(Var_Z))\n            plt.plot(X_line, Y_line, 'r-', label='Theoretical PDF')\n            plt.title('Linear Combination Distribution')\n            plt.xlabel('Z')\n            plt.ylabel('Density')\n            plt.legend()\n            plt.tight_layout()\n            self._safe_savefig(output_path)\n            return True\n        except Exception as e:\n            logging.error(f'線形結合グラフエラー: {e}')\n            plt.close()\n            return False"
    },
    {
      "path": "obfuscated_latest\\統計検定1級\\gui.py",
      "overview": "Pythonコード。\nクラス: i_sym。\n関数: __sym, r_sym。\n",
      "content": "import tkinter as tk\n\nclass i_sym:\n\n    def __sym(self):\n        self.root = tk.Tk()\n        self.root.title('統計検定1級 インタラクティブ問題解答システム')\n\n    def r_sym(self):\n        self.root.mainloop()"
    },
    {
      "path": "obfuscated_latest\\統計検定1級\\main.py",
      "overview": "Pythonコード。\n関数: m_sym。\n",
      "content": "import sys\nfrom main_app import MainApp\nfrom gui import InteractiveSolverGUI\nfrom config import config\n\ndef m_sym():\n    pdf_gen = config.get('pdf_generation', default={})\n    problem_count = pdf_gen.get('problem_count', 9)\n    if len(sys.argv) > 1 and sys.argv[1] == '--generate-pdf':\n        app = m_sym()\n        app.generate_and_compile(problem_count)\n    else:\n        gui = i_sym()\n        gui.run()\nif __name__ == '__main__':\n    m_sym()"
    },
    {
      "path": "obfuscated_latest\\統計検定1級\\main_app.py",
      "overview": "Pythonコード。\nクラス: m_sym。\n関数: __sym, g_sym, g_sym, c_sym。\n",
      "content": "import os, subprocess\nfrom config import config\nfrom problem_generator import ProblemGenerator\nimport logging\nimport traceback\n\nclass m_sym:\n\n    def __sym(self):\n        self.output_tex_file = config.get('output_tex_file', default='practice_problems.tex')\n        if self.output_tex_file is None:\n            self.output_tex_file = 'practice_problems.tex'\n        self.enable_visualization = config.get('enable_visualization', default=True)\n        self.problem_templates_dir = config.get('problem_templates_directory', default='templates')\n        self.output_directory = config.get('output_directory', default='outputs')\n        self.latex_path = config.get('latex_path', default='xelatex')\n        self.generator = p_sym()\n\n    def g_sym(self):\n        cjk_font = config.get('cjk_main_font', default='Yu Gothic')\n        if cjk_font is None:\n            cjk_font = 'Yu Gothic'\n        header = ['\\\\documentclass{article}', '\\\\usepackage{amsmath}', '\\\\usepackage{amssymb}', '\\\\usepackage{graphicx}', '\\\\usepackage{float}', '\\\\usepackage{geometry}', '\\\\usepackage{xeCJK}', '\\\\usepackage{fontspec}', '\\\\setmainfont{Times New Roman}', f'\\\\setCJKmainfont{{{cjk_font}}}', '\\\\geometry{a4paper, margin=1in}', '\\\\begin{document}']\n        return header\n\n    def g_sym(self, problem_count):\n        script_dir = os.path.dirname(os.path.abspath(__file__))\n        latex_content = self.generate_latex_header()\n        out_dir = self.output_directory\n        if out_dir is None:\n            out_dir = 'outputs'\n        output_dir = os.path.join(script_dir, out_dir)\n        os.makedirs(output_dir, exist_ok=True)\n        tex_file_path = os.path.join(output_dir, self.output_tex_file)\n        for idx in range(1, problem_count + 1):\n            try:\n                result = self.generator.generate_problem()\n                if result:\n                    (problem_id, date_created, problem_type, problem_text, solution_text, graph_filename) = result\n                    latex_content.append(f'\\\\section*{{問題 {idx}}}')\n                    latex_content.append(problem_text)\n                    if self.enable_visualization and graph_filename:\n                        latex_content.append('\\\\begin{figure}[H]')\n                        latex_content.append('\\\\centering')\n                        graph_relative_path = os.path.join('graphs', graph_filename).replace('\\\\', '/')\n                        latex_content.append(f'\\\\includegraphics[width=0.8\\\\textwidth]{{{graph_relative_path}}}')\n                        latex_content.append('\\\\end{figure}')\n                    latex_content.append('\\\\subsection*{解答}')\n                    latex_content.append(solution_text)\n                    latex_content.append('\\\\newpage')\n                else:\n                    logging.warning(f'問題 {idx} の生成に失敗しました。')\n                    print(f'問題 {idx} の生成に失敗しました。')\n            except Exception as e:\n                logging.error(f'問題 {idx} の生成中にエラー: {e}')\n                logging.error(traceback.format_exc())\n                print(f'問題 {idx} 生成エラー。ログを確認')\n        templates_dir = self.problem_templates_dir\n        if templates_dir is None:\n            templates_dir = 'templates'\n        latex_content.append('\\\\clearpage')\n        latex_content.append('\\\\input{../' + templates_dir + '/distribution_relations.tex}')\n        latex_content.append('\\\\end{document}')\n        with open(tex_file_path, 'w', encoding='utf-8') as tex_file:\n            tex_file.write('\\n'.join(latex_content))\n        self.compile_latex(tex_file_path)\n\n    def c_sym(self, tex_file_path):\n        tex_file_name = os.path.basename(tex_file_path)\n        latex_path = self.latex_path\n        if not latex_path or not os.path.exists(latex_path):\n            print(f'LaTeXコンパイラが見つかりません: {latex_path}')\n            return\n        process = subprocess.run([latex_path, '-interaction=nonstopmode', tex_file_name], cwd=os.path.dirname(tex_file_path), stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n        log_file_path = os.path.join(os.path.dirname(tex_file_path), 'latex_compile.log')\n        with open(log_file_path, 'w', encoding='utf-8') as f:\n            f.write(process.stdout or '')\n            f.write(process.stderr or '')\n        if process.returncode != 0:\n            print('LaTeXコンパイルでエラー')\n        else:\n            pdf_file = tex_file_path.replace('.tex', '.pdf')\n            if os.path.exists(pdf_file):\n                print(f'PDF生成成功: {pdf_file}')\n            else:\n                print('PDFファイル未発見')"
    },
    {
      "path": "obfuscated_latest\\統計検定1級\\problem_generator.py",
      "overview": "Pythonコード。\nクラス: p_sym。\n関数: __sym, l_sym, g_sym, g_sym。\n",
      "content": "import random, os, logging, uuid\nfrom datetime import datetime\nfrom config import config\nfrom database import DatabaseManager\nfrom problem_types.problem_factory import ProblemFactory\nimport json\nimport traceback\n\nclass p_sym:\n\n    def __sym(self):\n        self.db_path = config.get('database_path', default='data/data.db')\n        self.output_directory = config.get('output_directory', default='outputs')\n        self.enable_visualization = config.get('enable_visualization', default=True)\n        self.problem_types_weights = config.get('problem_types', default={})\n        self.factory = p_sym()\n        self.db_manager = d_sym()\n        self.db_manager.setup_database()\n        script_dir = os.path.dirname(os.path.abspath(__file__))\n        out_dir = self.output_directory\n        if out_dir is None:\n            out_dir = 'outputs'\n        self.output_dir = os.path.join(script_dir, out_dir, 'graphs')\n        os.makedirs(self.output_dir, exist_ok=True)\n        self.topics_data = self.load_topics()\n\n    def l_sym(self):\n        script_dir = os.path.dirname(os.path.abspath(__file__))\n        topics_file = os.path.join(script_dir, 'topics.json')\n        if not os.path.exists(topics_file):\n            raise FileNotFoundError(f\"'{topics_file}'がない\")\n        with open(topics_file, 'r', encoding='utf-8') as f:\n            return json.load(f)\n\n    def g_sym(self, topic):\n        return self.topics_data['topics'].get(topic, [])\n\n    def g_sym(self, selected_topic=None):\n        try:\n            ptypes = self.problem_types_weights\n            if selected_topic:\n                problem_types = self.get_problem_types_by_topic(selected_topic)\n                if not problem_types:\n                    return None\n                problem_type = random.choice(problem_types)\n            else:\n                pts = list(ptypes.keys())\n                pwt = list(ptypes.values())\n                if not pts:\n                    pts = ['probability']\n                    pwt = [1.0]\n                problem_type = random.choices(pts, weights=pwt, k=1)[0]\n            problem = self.factory.create_problem(problem_type)\n            problem.generate_parameters()\n            problem_text = problem.generate_problem_text()\n            solution_text = problem.generate_solution_text()\n            enable_vis = self.enable_visualization\n            if enable_vis is None:\n                enable_vis = True\n            graph_filename = None\n            if enable_vis:\n                graph_filename = f'graph_{uuid.uuid4().hex}.png'\n                graph_filepath = os.path.join(self.output_dir, graph_filename)\n                if not problem.generate_graph(graph_filepath):\n                    graph_filename = None\n            problem_id = uuid.uuid4().hex\n            date_created = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n            self.db_manager.save_problem(problem_id, date_created, problem_text, solution_text, problem_type)\n            return (problem_id, date_created, problem_type, problem_text, solution_text, graph_filename)\n        except Exception as e:\n            logging.error(f'問題生成エラー: {e}')\n            logging.error(traceback.format_exc())\n            return None"
    },
    {
      "path": "obfuscated_latest\\統計検定1級\\sympy_solver.py",
      "overview": "Pythonコード。\nクラス: s_sym。\n関数: __sym, c_sym。\n",
      "content": "class s_sym:\n\n    def __sym(self):\n        pass\n\n    def c_sym(self, user_input, correct_answer):\n        return (False, '')"
    },
    {
      "path": "obfuscated_latest\\統計検定1級\\topics.json",
      "overview": "JSONファイル (辞書)。キー: topics\n",
      "content": "{\n  \"topics\": {\n    \"確率論\": [\n      \"probability_definition\",\n      \"conditional_probability\",\n      \"distribution_functions\",\n      \"joint_distribution\",\n      \"probability\"\n    ],\n    \"統計的推定\": [\n      \"statistical_inference\",\n      \"t_test\"\n    ],\n    \"統計的検定\": [\n      \"statistical_inference\",\n      \"t_test\"\n    ],\n    \"回帰分析\": [\n      \"regression_analysis\"\n    ],\n    \"分散分析\": [\n      \"variance_analysis\"\n    ],\n    \"ノンパラメトリック検定\": [\n      \"nonparametric_test\"\n    ]\n  }\n}\n"
    },
    {
      "path": "obfuscated_latest\\統計検定1級\\.vscode\\launch.json",
      "overview": "JSONファイル (辞書)。キー: version, configurations\n",
      "content": "{\n    \"version\": \"0.2.0\",\n    \"configurations\": [\n        {\n            \"name\": \"Python: 現在のファイル\",\n            \"type\": \"python\",\n            \"request\": \"launch\",\n            \"program\": \"${file}\",\n            \"console\": \"integratedTerminal\",\n            \"justMyCode\": true,\n            \"args\": [\"--generate-pdf\"]\n\n        }\n    ]\n}"
    },
    {
      "path": "obfuscated_latest\\統計検定1級\\problem_types\\conjugate_problems.py",
      "overview": "Pythonコード。\nクラス: b_sym, g_sym, d_sym, b_sym, p_sym。\n関数: __sym, g_sym, g_sym, g_sym, __sym, g_sym, g_sym, g_sym, __sym, g_sym, g_sym, g_sym, __sym, g_sym, g_sym, g_sym, __sym, g_sym, g_sym, g_sym, b_sym, b_sym。\n",
      "content": "import math, random\nfrom problem_types.problem import Problem\nfrom math import comb, factorial, exp, gamma\nimport numpy as np\n\nclass b_sym(p_sym):\n\n    def __sym(self):\n        super().__init__('beta_binomial_conjugate_problem.tex')\n\n    def g_sym(self):\n        self.params['alpha'] = random.randint(1, 5)\n        self.params['beta'] = random.randint(1, 5)\n        self.params['n'] = random.randint(5, 20)\n        self.params['k'] = random.randint(0, self.params['n'])\n\n        def b_sym(x, y):\n            return gamma(x) * gamma(y) / gamma(x + y)\n        p_x = comb(self.params['n'], self.params['k']) * b_sym(self.params['k'] + self.params['alpha'], self.params['n'] - self.params['k'] + self.params['beta']) / b_sym(self.params['alpha'], self.params['beta'])\n        self.params['probability'] = round(p_x, 4)\n\n    def g_sym(self):\n        return 'Beta+Binomial->Beta-Binomial'\n\n    def g_sym(self, o):\n        return False\n\nclass g_sym(p_sym):\n\n    def __sym(self):\n        super().__init__('gamma_poisson_conjugate_problem.tex')\n\n    def g_sym(self):\n        self.params['alpha'] = random.randint(1, 5)\n        self.params['beta'] = random.uniform(0.5, 2.0)\n        self.params['k'] = random.randint(0, 20)\n        p = self.params['beta'] / (self.params['beta'] + 1)\n        q = 1 - p\n        negbin_p = comb(self.params['k'] + self.params['alpha'] - 1, self.params['k']) * q ** self.params['k'] * p ** self.params['alpha']\n        self.params['probability'] = round(negbin_p, 4)\n\n    def g_sym(self):\n        return 'Gamma+Poisson->Negative Binomial'\n\n    def g_sym(self, o):\n        return False\n\nclass d_sym(p_sym):\n\n    def __sym(self):\n        super().__init__('dirichlet_multinomial_conjugate_problem.tex')\n\n    def g_sym(self):\n        m = random.randint(2, 4)\n        self.params['m'] = m\n        self.params['n'] = random.randint(5, 20)\n        self.params['alpha_vec'] = [random.uniform(1, 3) for _ in range(m)]\n        counts = [0] * m\n        remain = self.params['n']\n        for i in range(m - 1):\n            c = random.randint(0, remain)\n            counts[i] = c\n            remain -= c\n        counts[-1] = remain\n        self.params['counts'] = counts\n\n        def b_sym(alpha):\n            import numpy as np\n            return np.prod([gamma(a) for a in alpha]) / gamma(sum(alpha))\n        alpha_x = [self.params['alpha_vec'][i] + counts[i] for i in range(m)]\n        num = b_sym(alpha_x)\n        den = b_sym(self.params['alpha_vec'])\n        multinomial_coef = math.factorial(self.params['n'])\n        for c in counts:\n            multinomial_coef /= math.factorial(c)\n        p_x = multinomial_coef * (num / den)\n        self.params['probability'] = round(p_x, 4)\n\n    def g_sym(self):\n        return 'Dirichlet+Multinomial->Dirichlet-Multinomial'\n\n    def g_sym(self, o):\n        return False\n\nclass b_sym(p_sym):\n\n    def __sym(self):\n        super().__init__('binomial_poisson_approx_problem.tex')\n\n    def g_sym(self):\n        lam = random.uniform(2, 5)\n        n = random.randint(50, 200)\n        p = lam / n\n        k = random.randint(0, int(lam * 3))\n        binom_p = comb(n, k) * p ** k * (1 - p) ** (n - k)\n        poisson_p = lam ** k * exp(-lam) / math.factorial(k)\n        self.params['n'] = n\n        self.params['p'] = round(p, 6)\n        self.params['k'] = k\n        self.params['lambda'] = round(lam, 3)\n        self.params['binom_p'] = round(binom_p, 6)\n        self.params['poisson_p'] = round(poisson_p, 6)\n\n    def g_sym(self):\n        return 'Binomial->Poisson近似条件'\n\n    def g_sym(self, o):\n        return False\n\nclass p_sym(p_sym):\n\n    def __sym(self):\n        super().__init__('poisson_normal_approx_problem.tex')\n\n    def g_sym(self):\n        lam = random.randint(30, 100)\n        low = max(0, int(lam - 3 * math.sqrt(lam)))\n        high = int(lam + 3 * math.sqrt(lam))\n        k = random.randint(low, high)\n        poisson_p = lam ** k * exp(-lam) / math.factorial(k)\n        self.params['lambda'] = lam\n        self.params['k'] = k\n        self.params['poisson_p'] = round(poisson_p, 6)\n        self.params['mean'] = lam\n        self.params['variance'] = lam\n\n    def g_sym(self):\n        return 'Poisson->Normal近似(λ大)'\n\n    def g_sym(self, o):\n        return False"
    },
    {
      "path": "obfuscated_latest\\統計検定1級\\problem_types\\problem.py",
      "overview": "Pythonコード。\nクラス: p_sym, p_sym, s_sym, r_sym, t_sym, e_sym, l_sym, d_sym, h_sym, m_sym, v_sym, n_sym, p_sym, c_sym, d_sym, j_sym。\n関数: __sym, g_sym, g_sym, g_sym, g_sym, g_sym, __sym, g_sym, __sym, __sym, __sym, g_sym, g_sym, __sym, g_sym, g_sym, __sym, g_sym, g_sym, __sym, g_sym, g_sym, __sym, g_sym, g_sym, __sym, g_sym, g_sym, g_sym, __sym, g_sym, g_sym, __sym, g_sym, g_sym, __sym, g_sym, g_sym, g_sym, __sym, g_sym, g_sym, g_sym, __sym, g_sym, g_sym, g_sym, __sym, g_sym, g_sym, g_sym, __sym, g_sym, g_sym, g_sym, __sym, g_sym, g_sym, g_sym, __sym, g_sym, g_sym, g_sym。\n",
      "content": "from abc import ABC, abstractmethod\nimport os\nimport random\nimport logging\nfrom config import config\nfrom jinja2 import Environment, FileSystemLoader\nfrom graph import ProbabilityDistributionVisualizer\nimport traceback\nfrom math import comb, exp, factorial\nfrom scipy.stats import norm, stats\n\nclass p_sym(ABC):\n\n    def __sym(self, template_name):\n        self.params = {}\n        self.template_name = template_name\n        templates_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), '..', config.get('problem_templates_directory', default='templates'))\n        self.env = Environment(loader=FileSystemLoader(templates_dir), autoescape=False)\n        self.visualizer = p_sym()\n\n    @abstractmethod\n    def g_sym(self):\n        pass\n\n    def g_sym(self):\n        template = self.env.get_template(self.template_name)\n        return template.render(**self.params, show_solution=False)\n\n    def g_sym(self):\n        template = self.env.get_template(self.template_name)\n        self.params['explanation'] = self.generate_explanation()\n        return template.render(**self.params, show_solution=True)\n\n    def g_sym(self):\n        return ''\n\n    @abstractmethod\n    def g_sym(self, output_path):\n        pass\n\nclass p_sym(p_sym):\n\n    def __sym(self):\n        super().__init__('probability_problem.tex')\n\n    def g_sym(self):\n        variants = [self._variant_binomial, self._variant_poisson, self._variant_conditional_probability]\n        v = random.choice(variants)\n        v()\n\n    def __sym(self):\n        self.params['problem_type'] = 'binomial'\n        self.params['n'] = random.randint(5, 20)\n        self.params['p'] = round(random.uniform(0.1, 0.9), 2)\n        self.params['k'] = random.randint(0, self.params['n'])\n        from math import comb\n        prob = comb(self.params['n'], self.params['k']) * self.params['p'] ** self.params['k'] * (1 - self.params['p']) ** (self.params['n'] - self.params['k'])\n        self.params['probability'] = round(prob, 6)\n\n    def __sym(self):\n        self.params['problem_type'] = 'poisson'\n        self.params['lambda'] = round(random.uniform(0.5, 5.0), 2)\n        self.params['k'] = random.randint(0, 10)\n        lam = self.params['lambda']\n        k = self.params['k']\n        prob = lam ** k * exp(-lam) / factorial(k)\n        self.params['probability'] = round(prob, 6)\n\n    def __sym(self):\n        self.params['problem_type'] = 'conditional_probability'\n        self.params['P_A'] = round(random.uniform(0.1, 0.9), 2)\n        self.params['P_B_given_A'] = round(random.uniform(0.1, 0.9), 2)\n        self.params['P_A_and_B'] = round(self.params['P_A'] * self.params['P_B_given_A'], 6)\n\n    def g_sym(self):\n        t = self.params['problem_type']\n        if t == 'binomial':\n            return '二項分布の公式を使用'\n        elif t == 'poisson':\n            return 'ポアソン分布の公式を使用'\n        elif t == 'conditional_probability':\n            return '条件付き確率P(A∩B)=P(A)*P(B|A)'\n\n    def g_sym(self, output_path):\n        return self.visualizer.plot_probability(self.params, output_path)\n\nclass s_sym(p_sym):\n\n    def __sym(self):\n        super().__init__('statistical_inference_problem.tex')\n\n    def g_sym(self):\n        self.params['sample_mean'] = round(random.uniform(50, 100), 2)\n        self.params['sample_std'] = round(random.uniform(5, 15), 2)\n        self.params['n'] = random.randint(30, 100)\n        self.params['population_mean'] = round(random.uniform(50, 100), 2)\n        self.params['alpha'] = round(random.uniform(0.01, 0.1), 2)\n        t_stat = (self.params['sample_mean'] - self.params['population_mean']) / (self.params['sample_std'] / self.params['n'] ** 0.5)\n        t_stat = round(t_stat, 4)\n        df = self.params['n'] - 1\n        cv = round(stats.t.ppf(1 - self.params['alpha'] / 2, df=df), 4)\n        reject = '棄却' if abs(t_stat) > cv else '棄却しない'\n        self.params['t_stat'] = t_stat\n        self.params['critical_value'] = cv\n        self.params['reject_null'] = reject\n        self.params['df'] = df\n\n    def g_sym(self, output_path):\n        return self.visualizer.plot_t_test(self.params, output_path)\n\nclass r_sym(p_sym):\n\n    def __sym(self):\n        super().__init__('regression_analysis_problem.tex')\n\n    def g_sym(self):\n        self.params['beta_0'] = round(random.uniform(0, 10), 2)\n        self.params['beta_1'] = round(random.uniform(-5, 5), 2)\n        self.params['n'] = random.randint(50, 200)\n        self.params['x_values'] = [round(random.uniform(0, 100), 2) for _ in range(self.params['n'])]\n        self.params['epsilon'] = [round(random.gauss(0, 10), 2) for _ in range(self.params['n'])]\n        self.params['y_values'] = [self.params['beta_0'] + self.params['beta_1'] * x + e for (x, e) in zip(self.params['x_values'], self.params['epsilon'])]\n        import numpy as np\n        X = np.array(self.params['x_values'])\n        Y = np.array(self.params['y_values'])\n        beta_1_hat = np.cov(X, Y, bias=True)[0, 1] / np.var(X)\n        beta_0_hat = np.mean(Y) - beta_1_hat * np.mean(X)\n        self.params['beta_0_hat'] = round(beta_0_hat, 4)\n        self.params['beta_1_hat'] = round(beta_1_hat, 4)\n\n    def g_sym(self, output_path):\n        return self.visualizer.plot_regression(self.params['x_values'], self.params['y_values'], self.params['beta_0_hat'], self.params['beta_1_hat'], output_path)\n\nclass t_sym(p_sym):\n\n    def __sym(self):\n        super().__init__('time_series_analysis_problem.tex')\n\n    def g_sym(self):\n        self.params['phi'] = round(random.uniform(0.5, 0.9), 2)\n        self.params['theta'] = round(random.uniform(-0.5, 0.5), 2)\n        self.params['n'] = 100\n        self.params['epsilon'] = [random.gauss(0, 1) for _ in range(self.params['n'])]\n        self.params['time_series'] = [0] * self.params['n']\n        for t in range(1, self.params['n']):\n            self.params['time_series'][t] = self.params['phi'] * self.params['time_series'][t - 1] + self.params['epsilon'][t] + self.params['theta'] * self.params['epsilon'][t - 1]\n\n    def g_sym(self, output_path):\n        return self.visualizer.plot_time_series(self.params, output_path)\n\nclass e_sym(p_sym):\n\n    def __sym(self):\n        super().__init__('econometrics_problem.tex')\n\n    def g_sym(self):\n        self.params['beta_0'] = round(random.uniform(0, 5), 2)\n        self.params['beta_1'] = round(random.uniform(0, 1), 2)\n        self.params['beta_2'] = round(random.uniform(-1, 0), 2)\n        self.params['n'] = random.randint(50, 200)\n        self.params['x1_values'] = [round(random.uniform(0, 100), 2) for _ in range(self.params['n'])]\n        self.params['x2_values'] = [round(random.uniform(0, 100), 2) for _ in range(self.params['n'])]\n        self.params['epsilon'] = [round(random.gauss(0, 5), 2) for _ in range(self.params['n'])]\n        self.params['y_values'] = [self.params['beta_0'] + self.params['beta_1'] * x1 + self.params['beta_2'] * x2 + e for (x1, x2, e) in zip(self.params['x1_values'], self.params['x2_values'], self.params['epsilon'])]\n        import numpy as np\n        X = np.column_stack((np.ones(self.params['n']), self.params['x1_values'], self.params['x2_values']))\n        Y = np.array(self.params['y_values'])\n        beta_hat = np.linalg.inv(X.T @ X) @ X.T @ Y\n        self.params['beta_0_hat'] = round(beta_hat[0], 4)\n        self.params['beta_1_hat'] = round(beta_hat[1], 4)\n        self.params['beta_2_hat'] = round(beta_hat[2], 4)\n\n    def g_sym(self, output_path):\n        return self.visualizer.plot_econometrics(self.params, output_path)\n\nclass l_sym(p_sym):\n\n    def __sym(self):\n        super().__init__('linear_combination_problem.tex')\n\n    def g_sym(self):\n        self.params['a'] = random.randint(1, 5)\n        self.params['b'] = random.randint(1, 5)\n        self.params['mu1'] = round(random.uniform(0, 10), 2)\n        self.params['mu2'] = round(random.uniform(0, 10), 2)\n        self.params['sigma1_squared'] = round(random.uniform(1, 5), 2)\n        self.params['sigma2_squared'] = round(random.uniform(1, 5), 2)\n        E_Z = self.params['a'] * self.params['mu1'] + self.params['b'] * self.params['mu2']\n        Var_Z = self.params['a'] ** 2 * self.params['sigma1_squared'] + self.params['b'] ** 2 * self.params['sigma2_squared']\n        self.params['E_Z'] = round(E_Z, 4)\n        self.params['Var_Z'] = round(Var_Z, 4)\n\n    def g_sym(self):\n        return '線形結合の期待値・分散計算'\n\n    def g_sym(self, output_path):\n        return self.visualizer.plot_linear_combination(self.params, output_path)\n\nclass d_sym(p_sym):\n\n    def __sym(self):\n        super().__init__('distribution_properties_problem.tex')\n\n    def g_sym(self):\n        dist_choice = random.choice(['正規分布', 'ポアソン分布', '指数分布'])\n        self.params['distribution'] = dist_choice\n        if dist_choice == '正規分布':\n            self.params['properties'] = {'mean': '\\\\mu', 'variance': '\\\\sigma^2'}\n        elif dist_choice == 'ポアソン分布':\n            self.params['properties'] = {'mean': '\\\\lambda', 'variance': '\\\\lambda'}\n        else:\n            self.params['properties'] = {'mean': '1/\\\\lambda', 'variance': '1/\\\\lambda^2'}\n\n    def g_sym(self, output_path):\n        return self.visualizer.plot_distribution_properties(self.params, output_path)\n\nclass h_sym(p_sym):\n\n    def __sym(self):\n        super().__init__('high_moment_problem.tex')\n\n    def g_sym(self):\n        self.params['n'] = random.randint(3, 5)\n        self.params['mu'] = round(random.uniform(0, 10), 2)\n        self.params['sigma'] = round(random.uniform(1, 5), 2)\n        from scipy.stats import norm\n        m = norm.moment(self.params['n'], loc=self.params['mu'], scale=self.params['sigma'])\n        self.params['moment'] = round(m, 4)\n\n    def g_sym(self, output_path):\n        return self.visualizer.plot_high_moment(self.params, output_path)\n\nclass m_sym(p_sym):\n\n    def __sym(self):\n        super().__init__('multivariate_normal_problem.tex')\n\n    def g_sym(self):\n        self.params['mu'] = [round(random.uniform(0, 10), 2) for _ in range(2)]\n        self.params['sigma'] = [[round(random.uniform(1, 5), 2), round(random.uniform(0, 2), 2)], [round(random.uniform(0, 2), 2), round(random.uniform(1, 5), 2)]]\n\n    def g_sym(self):\n        return '多変量正規分布の性質を利用'\n\n    def g_sym(self, output_path):\n        return self.visualizer.plot_multivariate_normal(self.params, output_path)\n\nclass v_sym(p_sym):\n\n    def __sym(self):\n        super().__init__('variance_analysis_problem.tex')\n\n    def g_sym(self):\n        self.params['group_count'] = random.randint(2, 4)\n        self.params['sample_sizes'] = [random.randint(5, 20) for _ in range(self.params['group_count'])]\n        self.params['means'] = [round(random.uniform(10, 50), 2) for _ in range(self.params['group_count'])]\n        self.params['variances'] = [round(random.uniform(1, 5), 2) for _ in range(self.params['group_count'])]\n        total_n = sum(self.params['sample_sizes'])\n        group_count = self.params['group_count']\n        means = self.params['means']\n        variances = self.params['variances']\n        sample_sizes = self.params['sample_sizes']\n        grand_mean = sum([means[i] * sample_sizes[i] for i in range(group_count)]) / total_n\n        ssb = sum([sample_sizes[i] * (means[i] - grand_mean) ** 2 for i in range(group_count)])\n        ssw = sum([(sample_sizes[i] - 1) * variances[i] for i in range(group_count)])\n        df_between = group_count - 1\n        df_within = total_n - group_count\n        msb = ssb / df_between\n        msw = ssw / df_within\n        F = msb / msw\n        alpha = 0.05\n        from scipy.stats import f\n        F_critical = f.ppf(1 - alpha, df_between, df_within)\n        reject = '棄却する' if F > F_critical else '棄却しない'\n        self.params['F_value'] = round(F, 4)\n        self.params['F_critical'] = round(F_critical, 4)\n        self.params['reject_null'] = reject\n        self.params['df_between'] = df_between\n        self.params['df_within'] = df_within\n\n    def g_sym(self):\n        return '一元配置分散分析による検定'\n\n    def g_sym(self, output_path):\n        return self.visualizer.plot_variance_analysis(self.params, output_path)\n\nclass n_sym(p_sym):\n\n    def __sym(self):\n        super().__init__('nonparametric_test_problem.tex')\n\n    def g_sym(self):\n        self.params['test_type'] = random.choice(['Mann-Whitney U', 'Kruskal-Wallis', 'Wilcoxon Signed-Rank'])\n        self.params['sample1'] = [round(random.uniform(10, 100), 2) for _ in range(random.randint(5, 20))]\n        self.params['sample2'] = [round(random.uniform(10, 100), 2) for _ in range(random.randint(5, 20))]\n        self.params['test_result'] = '有意差なし(例)'\n\n    def g_sym(self):\n        return 'ノンパラ検定で中央値差を検定'\n\n    def g_sym(self, output_path):\n        return self.visualizer.plot_nonparametric_test(self.params, output_path)\n\nclass p_sym(p_sym):\n\n    def __sym(self):\n        super().__init__('probability_definition_problem.tex')\n\n    def g_sym(self):\n        self.params['P_A'] = round(random.uniform(0.1, 0.9), 2)\n        self.params['P_B'] = round(random.uniform(0.1, 0.9), 2)\n        self.params['P_A_and_B'] = round(self.params['P_A'] * self.params['P_B'], 4)\n\n    def g_sym(self):\n        return '独立性利用 P(A∩B)=P(A)*P(B)'\n\n    def g_sym(self, output_path):\n        try:\n            PA = self.params['P_A']\n            PB = self.params['P_B']\n            PAB = self.params['P_A_and_B']\n            plt.figure()\n            plt.bar(['P(A)', 'P(B)', 'P(A∩B)'], [PA, PB, PAB], color=['blue', 'green', 'red'])\n            plt.title('Probability Definition')\n            plt.tight_layout()\n            plt.savefig(output_path)\n            plt.close()\n            return True\n        except:\n            return False\n\nclass c_sym(p_sym):\n\n    def __sym(self):\n        super().__init__('conditional_probability_problem.tex')\n\n    def g_sym(self):\n        self.params['P_A'] = round(random.uniform(0.2, 0.8), 2)\n        self.params['P_B_given_A'] = round(random.uniform(0.2, 0.8), 2)\n        self.params['P_A_and_B'] = round(self.params['P_A'] * self.params['P_B_given_A'], 4)\n\n    def g_sym(self):\n        return 'P(A∩B)=P(A)*P(B|A)'\n\n    def g_sym(self, output_path):\n        from graph import ProbabilityDistributionVisualizer\n        vis = p_sym()\n        return vis.plot_probability(self.params, output_path)\n\nclass d_sym(p_sym):\n\n    def __sym(self):\n        super().__init__('distribution_functions_problem.tex')\n\n    def g_sym(self):\n        self.params['function_type'] = random.choice(['pdf', 'cdf'])\n        self.params['distribution'] = random.choice(['正規分布', '指数分布'])\n        if self.params['distribution'] == '正規分布':\n            self.params['mean'] = round(random.uniform(-5, 5), 2)\n            self.params['std'] = round(random.uniform(1, 3), 2)\n        else:\n            self.params['lambda'] = round(random.uniform(0.5, 2.0), 2)\n\n    def g_sym(self):\n        return 'pdfやcdf定義式利用'\n\n    def g_sym(self, output_path):\n        return False\n\nclass j_sym(p_sym):\n\n    def __sym(self):\n        super().__init__('joint_distribution_problem.tex')\n\n    def g_sym(self):\n        A_and_B = round(random.uniform(0.05, 0.2), 2)\n        A_and_notB = round(random.uniform(0.05, 0.2), 2)\n        notA_and_B = round(random.uniform(0.05, 0.2), 2)\n        notA_and_notB = round(random.uniform(0.05, 0.2), 2)\n        total = A_and_B + A_and_notB + notA_and_B + notA_and_notB\n        A_and_B /= total\n        A_and_notB /= total\n        notA_and_B /= total\n        notA_and_notB /= total\n        self.params['joint_probabilities'] = {'A_and_B': round(A_and_B, 4), 'A_and_not_B': round(A_and_notB, 4), 'not_A_and_B': round(notA_and_B, 4), 'not_A_and_not_B': round(notA_and_notB, 4)}\n        P_A = A_and_B + A_and_notB\n        P_B = A_and_B + notA_and_B\n        P_BA = A_and_B / P_A if P_A > 0 else 0.0\n        self.params['P_A'] = round(P_A, 4)\n        self.params['P_B'] = round(P_B, 4)\n        self.params['P_B_given_A'] = round(P_BA, 4)\n\n    def g_sym(self):\n        return '同時→周辺→条件付き確率'\n\n    def g_sym(self, output_path):\n        try:\n            p = self.params['joint_probabilities']\n            matrix = np.array([[p['A_and_B'], p['A_and_not_B']], [p['not_A_and_B'], p['not_A_and_not_B']]])\n            plt.figure()\n            plt.imshow(matrix, cmap='Blues', interpolation='nearest')\n            plt.colorbar(label='Probability')\n            plt.xticks([0, 1], ['B', 'not B'])\n            plt.yticks([0, 1], ['A', 'not A'])\n            plt.title('Joint Distribution Heatmap')\n            plt.tight_layout()\n            plt.savefig(output_path)\n            plt.close()\n            return True\n        except:\n            return False"
    },
    {
      "path": "obfuscated_latest\\統計検定1級\\problem_types\\problem_factory.py",
      "overview": "Pythonコード。\nクラス: p_sym。\n関数: __sym, c_sym。\n",
      "content": "import logging, traceback\nfrom problem_types.problem import ProbabilityProblem, StatisticalInferenceProblem, RegressionAnalysisProblem, TimeSeriesAnalysisProblem, EconometricsProblem, LinearCombinationProblem, DistributionPropertiesProblem, HighMomentProblem, MultivariateNormalProblem, VarianceAnalysisProblem, NonParametricTestProblem, ProbabilityDefinitionProblem, ConditionalProbabilityProblem, DistributionFunctionsProblem, JointDistributionProblem\nfrom problem_types.conjugate_problems import BetaBinomialConjugateProblem, GammaPoissonConjugateProblem, DirichletMultinomialConjugateProblem, BinomialPoissonApproxProblem, PoissonNormalApproxProblem\n\nclass p_sym:\n\n    def __sym(self):\n        self.problem_classes = {'probability': p_sym, 'statistical_inference': s_sym, 'regression_analysis': r_sym, 'time_series_analysis': t_sym, 'econometrics': e_sym, 'linear_combination': l_sym, 'distribution_properties': d_sym, 'high_moment': h_sym, 'multivariate_normal': m_sym, 'probability_definition': p_sym, 'conditional_probability': c_sym, 'distribution_functions': d_sym, 'joint_distribution': j_sym, 't_test': s_sym, 'variance_analysis': v_sym, 'nonparametric_test': n_sym, 'beta_binomial_conjugate': b_sym, 'gamma_poisson_conjugate': g_sym, 'dirichlet_multinomial_conjugate': d_sym, 'binomial_poisson_approx': b_sym, 'poisson_normal_approx': p_sym}\n\n    def c_sym(self, problem_type):\n        pc = self.problem_classes.get(problem_type)\n        if pc:\n            try:\n                return pc()\n            except Exception as e:\n                logging.error(f'{problem_type} problem generation error:{e}')\n                logging.error(traceback.format_exc())\n                raise\n        else:\n            raise ValueError(f'Unknown problem type:{problem_type}')"
    },
    {
      "path": "obfuscated_latest\\統計検定1級\\templates\\distribution_properties_problem.tex",
      "overview": "TeXファイル。行数: 7\n",
      "content": "% distribution_properties_problem.tex\n{% if not show_solution %}\n{{ distribution }} の平均と分散を求めよ。\n{% else %}\n平均: ${{ properties.mean }}$, 分散: ${{ properties.variance }}$\n{{ explanation }}\n{% endif %}\n"
    },
    {
      "path": "obfuscated_latest\\統計検定1級\\templates\\distribution_relations.tex",
      "overview": "TeXファイル。行数: 7\n",
      "content": "% distribution_relations.tex\n\\section*{Distribution Relations}\n- Beta+Binomial -> Beta-Binomial\n- Gamma+Poisson -> Negative Binomial\n- Dirichlet+Multinomial -> Dirichlet-Multinomial\n- Binomial->Poisson approximation\n- Poisson->Normal approximation\n"
    },
    {
      "path": "obfuscated_latest\\統計検定1級\\templates\\econometrics_problem.tex",
      "overview": "TeXファイル。行数: 6\n",
      "content": "% econometrics_problem.tex\n{% if not show_solution %}\n計量経済学モデルに関する問題\n{% else %}\n解答と推定量\n{{ explanation }}\n{% endif %}"
    },
    {
      "path": "obfuscated_latest\\統計検定1級\\templates\\high_moment_problem.tex",
      "overview": "TeXファイル。行数: 6\n",
      "content": "% high_moment_problem.tex\n{% if not show_solution %}\n正規分布の{{ n }}次モーメントは？(例: $E[X^{n}]$ )\n{% else %}\n$E[X^{n}]={{ moment }}$\n{{ explanation }}\n{% endif %}"
    },
    {
      "path": "obfuscated_latest\\統計検定1級\\templates\\linear_combination_problem.tex",
      "overview": "TeXファイル。行数: 6\n",
      "content": "% linear_combination_problem.tex\n{% if not show_solution %}\nZ=aX+bY のE[Z],Var[Z]\n{% else %}\nE[Z],Var[Z]\n{{ explanation }}\n{% endif %}"
    },
    {
      "path": "obfuscated_latest\\統計検定1級\\templates\\multivariate_normal_problem.tex",
      "overview": "TeXファイル。行数: 6\n",
      "content": "% multivariate_normal_problem.tex\n{% if not show_solution %}\n多変量正規に関する問題\n{% else %}\n解答\n{{ explanation }}\n{% endif %}"
    },
    {
      "path": "obfuscated_latest\\統計検定1級\\templates\\nonparametric_test_problem.tex",
      "overview": "TeXファイル。行数: 6\n",
      "content": "% nonparametric_test_problem.tex\n{% if not show_solution %}\nノンパラ検定問題\n{% else %}\n検定結果\n{{ explanation }}\n{% endif %}"
    },
    {
      "path": "obfuscated_latest\\統計検定1級\\templates\\poisson_normal_approx_problem.tex",
      "overview": "TeXファイル。行数: 6\n",
      "content": "% poisson_normal_approx_problem.tex\n{% if not show_solution %}\nPoisson→Normal近似\n{% else %}\nPoisson P=${{ poisson_p }}$, mean=${{mean}}$ var=${{variance}}$\n{{ explanation }}\n{% endif %}"
    },
    {
      "path": "obfuscated_latest\\統計検定1級\\templates\\probability_definition_problem.tex",
      "overview": "TeXファイル。行数: 6\n",
      "content": "% probability_definition_problem.tex\n{% if not show_solution %}\nP(A∩B)求めよ\n{% else %}\n結果\n{{ explanation }}\n{% endif %}"
    },
    {
      "path": "obfuscated_latest\\統計検定1級\\templates\\probability_problem.tex",
      "overview": "TeXファイル。行数: 7\n",
      "content": "% probability_problem.tex\n{% if not show_solution %}\n確率計算問題（例）\n問題タイプ: {{ problem_type }}\n{% else %}\n解答と説明: {{ explanation }}\n計算結果: P = {{ probability }}\n{% endif %}"
    },
    {
      "path": "obfuscated_latest\\統計検定1級\\templates\\regression_analysis_problem.tex",
      "overview": "TeXファイル。行数: 6\n",
      "content": "% regression_analysis_problem.tex\n{% if not show_solution %}\n回帰分析問題\n{% else %}\n回帰係数結果\n{{ explanation }}\n{% endif %}"
    },
    {
      "path": "obfuscated_latest\\統計検定1級\\templates\\statistical_inference_problem.tex",
      "overview": "TeXファイル。行数: 6\n",
      "content": "% statistical_inference_problem.tex\n{% if not show_solution %}\n統計的推定/検定問題\n{% else %}\n検定結果\n{{ explanation }}\n{% endif %}"
    },
    {
      "path": "obfuscated_latest\\統計検定1級\\templates\\time_series_analysis_problem.tex",
      "overview": "TeXファイル。行数: 6\n",
      "content": "% time_series_analysis_problem.tex\n{% if not show_solution %}\n時系列分析問題\n{% else %}\n解答と説明\n{{ explanation }}\n{% endif %}"
    },
    {
      "path": "obfuscated_latest\\統計検定1級\\templates\\variance_analysis_problem.tex",
      "overview": "TeXファイル。行数: 6\n",
      "content": "% variance_analysis_problem.tex\n{% if not show_solution %}\n分散分析問題\n{% else %}\nANOVA結果\n{{ explanation }}\n{% endif %}"
    },
    {
      "path": "restored_latest\\統計検定1級\\config.py",
      "overview": "Pythonコード。\nクラス: Config。\n関数: __init__, load_config, get。\n",
      "content": "import json, os\n\nclass Config:\n\n    def __init__(self, config_file='config.json'):\n        script_dir = os.path.dirname(os.path.abspath(__file__))\n        self.config_file = os.path.join(script_dir, config_file)\n        self.settings = self.load_config()\n\n    def load_config(self):\n        if not os.path.exists(self.config_file):\n            return {}\n        try:\n            with open(self.config_file, 'r', encoding='utf-8') as f:\n                content = f.read().strip()\n                if not content:\n                    return {}\n                return json.loads(content)\n        except Exception:\n            return {}\n\n    def get(self, *keys, default=None):\n        data = self.settings\n        for k in keys:\n            if isinstance(data, dict) and k in data:\n                data = data[k]\n            else:\n                return default\n        return data\nconfig = Config()"
    },
    {
      "path": "restored_latest\\統計検定1級\\database.py",
      "overview": "Pythonコード。\nクラス: DatabaseManager。\n関数: __init__, setup_database, save_problem。\n",
      "content": "import sqlite3, os\nfrom config import config\n\nclass DatabaseManager:\n\n    def __init__(self):\n        script_dir = os.path.dirname(os.path.abspath(__file__))\n        db_path = config.get('database_path', default='data/data.db')\n        if db_path is None:\n            db_path = 'data/data.db'\n        self.db_name = os.path.join(script_dir, db_path)\n        os.makedirs(os.path.dirname(self.db_name), exist_ok=True)\n\n    def setup_database(self):\n        conn = sqlite3.connect(self.db_name)\n        c = conn.cursor()\n        c.execute('CREATE TABLE IF NOT EXISTS problems (id TEXT PRIMARY KEY,date_created TEXT,problem_text TEXT,solution_text TEXT,problem_type TEXT)')\n        conn.commit()\n        conn.close()\n\n    def save_problem(self, problem_id, date_created, problem_text, solution_text, problem_type):\n        conn = sqlite3.connect(self.db_name)\n        c = conn.cursor()\n        c.execute('INSERT INTO problems VALUES (?,?,?,?,?)', (problem_id, date_created, problem_text, solution_text, problem_type))\n        conn.commit()\n        conn.close()"
    },
    {
      "path": "restored_latest\\統計検定1級\\graph.py",
      "overview": "Pythonコード。\nクラス: ProbabilityDistributionVisualizer。\n関数: __init__, _safe_savefig, plot_probability, plot_t_test, plot_regression, plot_time_series, plot_econometrics, plot_multivariate_normal, plot_distribution_properties, plot_high_moment, plot_variance_analysis, plot_nonparametric_test, plot_linear_combination, confidence_ellipse, mvn_pdf, ecdf。\n",
      "content": "import matplotlib\nmatplotlib.use('Agg')\nimport matplotlib.pyplot as plt\nimport math\nimport numpy as np\nimport logging\nimport os\nfrom math import factorial, exp\nfrom scipy.stats import norm, t as t_dist, f as f_dist, poisson, expon\nfrom statsmodels.graphics.tsaplots import plot_acf\nimport traceback\n\nclass ProbabilityDistributionVisualizer:\n\n    def __init__(self):\n        pass\n\n    def _safe_savefig(self, output_path):\n        \"\"\"\n        画像を確実にPNGで出力し、ファイルサイズなどをログするヘルパー関数。\n        \"\"\"\n        try:\n            (_, ext) = os.path.splitext(output_path)\n            if ext.lower() == '.png':\n                plt.savefig(output_path, format='png')\n            else:\n                plt.savefig(output_path, format='png')\n            plt.close()\n            if os.path.exists(output_path):\n                fsize = os.path.getsize(output_path)\n                logging.info(f'Saved figure: {output_path} (size: {fsize} bytes)')\n            else:\n                logging.warning(f'File not found after saving: {output_path}')\n        except Exception as e:\n            logging.error(f'Failed to save figure to {output_path}, error={e}')\n            plt.close()\n\n    def plot_probability(self, params, output_path):\n        ptype = params.get('problem_type')\n        try:\n            if ptype == 'binomial':\n                p = params['p']\n                n = params['n']\n                k = params['k']\n                x = range(n + 1)\n                from math import comb\n                pmf = [comb(n, i) * p ** i * (1 - p) ** (n - i) for i in x]\n                cdf = np.cumsum(pmf)\n                plt.figure(figsize=(10, 5))\n                plt.subplot(1, 2, 1)\n                plt.bar(x, pmf, color='skyblue')\n                plt.title(f'Binomial PMF n={n}, p={p}')\n                plt.xlabel('k')\n                plt.ylabel('P(X=k)')\n                plt.axvline(k, color='red', linestyle='--', label='Target k')\n                plt.legend()\n                plt.subplot(1, 2, 2)\n                plt.step(x, cdf, where='post', color='green')\n                plt.title('CDF')\n                plt.xlabel('k')\n                plt.ylabel('P(X<=k)')\n                plt.axvline(k, color='red', linestyle='--')\n                plt.tight_layout()\n                self._safe_savefig(output_path)\n                return True\n            elif ptype == 'poisson':\n                lam = params['lambda']\n                k = params['k']\n                x = range(k + 10 + 1)\n                pmf = [lam ** i * exp(-lam) / factorial(i) for i in x]\n                cdf = np.cumsum(pmf)\n                plt.figure(figsize=(10, 5))\n                plt.subplot(1, 2, 1)\n                plt.bar(x, pmf, color='orange')\n                plt.title(f'Poisson(lambda={lam}) PMF')\n                plt.xlabel('k')\n                plt.ylabel('P(X=k)')\n                plt.axvline(k, color='red', linestyle='--', label='Target k')\n                plt.legend()\n                plt.subplot(1, 2, 2)\n                plt.step(x, cdf, where='post', color='green')\n                plt.title('CDF')\n                plt.xlabel('k')\n                plt.ylabel('P(X<=k)')\n                plt.axvline(k, color='red', linestyle='--')\n                plt.tight_layout()\n                self._safe_savefig(output_path)\n                return True\n            elif ptype == 'conditional_probability':\n                P_A = params['P_A']\n                P_BA = params['P_B_given_A']\n                P_AB = params['P_A_and_B']\n                plt.figure()\n                vals = [P_A, P_BA, P_AB]\n                labels = ['P(A)', 'P(B|A)', 'P(A∩B)']\n                plt.bar(labels, vals, color=['blue', 'green', 'red'])\n                plt.title('Conditional Probability Visualization')\n                plt.ylabel('Probability')\n                plt.tight_layout()\n                self._safe_savefig(output_path)\n                return True\n            else:\n                return False\n        except Exception as e:\n            logging.error(f'Error in plot_probability: {e}')\n            plt.close()\n            return False\n\n    def plot_t_test(self, params, output_path):\n        try:\n            alpha = params['alpha']\n            df = params['df']\n            t_stat = params['t_stat']\n            critical_value = params['critical_value']\n            x = np.linspace(t_dist.ppf(0.001, df), t_dist.ppf(0.999, df), 1000)\n            y = t_dist.pdf(x, df)\n            plt.figure(figsize=(10, 5))\n            plt.plot(x, y, 'b-', label=f't-dist(df={df})')\n            plt.axvline(x=critical_value, color='r', linestyle='--', label='critical +')\n            plt.axvline(x=-critical_value, color='r', linestyle='--', label='critical -')\n            plt.axvline(x=t_stat, color='g', label='t-stat')\n            p_area_x = x[x > critical_value]\n            plt.fill_between(p_area_x, t_dist.pdf(p_area_x, df), color='red', alpha=0.3)\n            p_area_x2 = x[x < -critical_value]\n            plt.fill_between(p_area_x2, t_dist.pdf(p_area_x2, df), color='red', alpha=0.3)\n            plt.title('t-test visualization')\n            plt.xlabel('t')\n            plt.ylabel('pdf')\n            plt.legend()\n            plt.tight_layout()\n            self._safe_savefig(output_path)\n            return True\n        except Exception as e:\n            logging.error(f't検定グラフ生成エラー: {e}')\n            plt.close()\n            return False\n\n    def plot_regression(self, x_values, y_values, beta_0_hat, beta_1_hat, output_path):\n        try:\n            import statsmodels.api as sm\n            X = sm.add_constant(x_values)\n            model = sm.OLS(y_values, X).fit()\n            residuals = model.resid\n            (fig, axes) = plt.subplots(2, 2, figsize=(10, 10))\n            ax = axes[0, 0]\n            ax.scatter(x_values, y_values, color='blue', label='data')\n            x_line = np.linspace(min(x_values), max(x_values), 100)\n            y_line = beta_0_hat + beta_1_hat * x_line\n            ax.plot(x_line, y_line, color='red', label='reg line')\n            ax.set_title('Data & Regression Line')\n            ax.set_xlabel('X')\n            ax.set_ylabel('Y')\n            ax.legend()\n            ax = axes[0, 1]\n            ax.hist(residuals, bins=20, color='green', alpha=0.7)\n            ax.set_title('Residual Histogram')\n            ax.set_xlabel('Residual')\n            ax.set_ylabel('Frequency')\n            sm.qqplot(residuals, line='45', ax=axes[1, 0], color='purple')\n            axes[1, 0].set_title('Q-Q plot of Residuals')\n            fitted = model.fittedvalues\n            ax = axes[1, 1]\n            ax.scatter(fitted, residuals, color='orange')\n            ax.axhline(y=0, color='red', linestyle='--')\n            ax.set_title('Residuals vs Fitted')\n            ax.set_xlabel('Fitted')\n            ax.set_ylabel('Residuals')\n            fig.tight_layout()\n            self._safe_savefig(output_path)\n            return True\n        except Exception as e:\n            logging.error(f'回帰分析グラフ生成中にエラー: {e}')\n            plt.close()\n            return False\n\n    def plot_time_series(self, params, output_path):\n        try:\n            ts = params['time_series']\n            (fig, axes) = plt.subplots(2, 1, figsize=(10, 8))\n            axes[0].plot(ts, color='blue')\n            axes[0].set_title('Time Series Data')\n            axes[0].set_xlabel('Time')\n            axes[0].set_ylabel('Value')\n            plot_acf(ts, ax=axes[1])\n            axes[1].set_title('Autocorrelation Function')\n            fig.tight_layout()\n            self._safe_savefig(output_path)\n            return True\n        except Exception as e:\n            logging.error(f'時系列分析グラフエラー: {e}')\n            plt.close()\n            return False\n\n    def plot_econometrics(self, params, output_path):\n        try:\n            import statsmodels.api as sm\n            X = np.column_stack((params['x1_values'], params['x2_values']))\n            Y = np.array(params['y_values'])\n            Xc = sm.add_constant(X)\n            model = sm.OLS(Y, Xc).fit()\n            residuals = model.resid\n            fitted = model.fittedvalues\n            (fig, axes) = plt.subplots(2, 2, figsize=(10, 10))\n            ax = axes[0, 0]\n            ax.scatter(params['x1_values'], Y, color='blue', alpha=0.7, label='X1-Y')\n            ax.set_title('X1 vs Y')\n            ax.set_xlabel('X1')\n            ax.set_ylabel('Y')\n            ax = axes[0, 1]\n            ax.scatter(params['x2_values'], Y, color='green', alpha=0.7, label='X2-Y')\n            ax.set_title('X2 vs Y')\n            ax.set_xlabel('X2')\n            ax.set_ylabel('Y')\n            ax = axes[1, 0]\n            ax.hist(residuals, bins=20, color='gray', alpha=0.7)\n            ax.set_title('Residuals Histogram')\n            ax.set_xlabel('Residual')\n            ax = axes[1, 1]\n            ax.scatter(fitted, residuals, color='red', alpha=0.7)\n            ax.axhline(y=0, color='black', linestyle='--')\n            ax.set_title('Residuals vs Fitted')\n            ax.set_xlabel('Fitted')\n            ax.set_ylabel('Residuals')\n            fig.tight_layout()\n            self._safe_savefig(output_path)\n            return True\n        except Exception as e:\n            logging.error(f'計量経済学グラフエラー: {e}')\n            plt.close()\n            return False\n\n    def plot_multivariate_normal(self, params, output_path):\n        try:\n            mu = params['mu']\n            sigma = np.array(params['sigma'])\n            fig = plt.figure(figsize=(10, 10))\n            from matplotlib.patches import Ellipse\n            import matplotlib.transforms as transforms\n\n            def confidence_ellipse(mu, cov, ax, n_std=1.96, facecolor='none', **kwargs):\n                pearson = cov[0, 1] / np.sqrt(cov[0, 0] * cov[1, 1])\n                ell_radius_x = np.sqrt(1 + pearson)\n                ell_radius_y = np.sqrt(1 - pearson)\n                ellipse = Ellipse((0, 0), width=ell_radius_x * 2, height=ell_radius_y * 2, facecolor=facecolor, **kwargs)\n                scale_x = np.sqrt(cov[0, 0]) * n_std\n                scale_y = np.sqrt(cov[1, 1]) * n_std\n                transf = transforms.Affine2D().rotate_deg(45).scale(scale_x, scale_y).translate(mu[0], mu[1])\n                ellipse.set_transform(transf + ax.transData)\n                return ax.add_patch(ellipse)\n            ax = fig.add_subplot(2, 2, 1)\n            ax.set_title('Confidence Ellipse')\n            confidence_ellipse(mu, sigma, ax, edgecolor='red')\n            ax.scatter(mu[0], mu[1], c='blue', marker='x', label='mean')\n            ax.legend()\n            ax.set_xlabel('X1')\n            ax.set_ylabel('X2')\n            ax2 = fig.add_subplot(2, 2, 2)\n            x = np.linspace(mu[0] - 4 * np.sqrt(sigma[0, 0]), mu[0] + 4 * np.sqrt(sigma[0, 0]), 100)\n            y = np.linspace(mu[1] - 4 * np.sqrt(sigma[1, 1]), mu[1] + 4 * np.sqrt(sigma[1, 1]), 100)\n            (X, Y) = np.meshgrid(x, y)\n            pos = np.dstack((X, Y))\n\n            def mvn_pdf(xarr, muarr, cov):\n                det = np.linalg.det(cov)\n                inv = np.linalg.inv(cov)\n                diff = xarr - muarr\n                return 1.0 / (2 * np.pi * np.sqrt(det)) * np.exp(-0.5 * (diff @ inv @ diff.T))\n            Z = np.empty(X.shape)\n            for i in range(X.shape[0]):\n                for j in range(X.shape[1]):\n                    Z[i, j] = mvn_pdf(np.array([X[i, j], Y[i, j]]), np.array(mu), sigma)\n            ax2.contour(X, Y, Z, levels=5, cmap='Blues')\n            ax2.set_title('Contour')\n            ax3 = fig.add_subplot(2, 2, 3)\n            X_marg = norm(loc=mu[0], scale=np.sqrt(sigma[0, 0]))\n            x_line = np.linspace(mu[0] - 4 * np.sqrt(sigma[0, 0]), mu[0] + 4 * np.sqrt(sigma[0, 0]), 100)\n            y_line = X_marg.pdf(x_line)\n            ax3.plot(x_line, y_line, 'r-')\n            ax3.set_title('Marginal X1 distribution')\n            ax3.set_xlabel('X1')\n            ax3.set_ylabel('pdf')\n            ax4 = fig.add_subplot(2, 2, 4)\n            Y_marg = norm(loc=mu[1], scale=np.sqrt(sigma[1, 1]))\n            y_line = Y_marg.pdf(x_line)\n            ax4.plot(x_line, y_line, 'g-')\n            ax4.set_title('Marginal X2 distribution')\n            ax4.set_xlabel('X2')\n            ax4.set_ylabel('pdf')\n            fig.tight_layout()\n            self._safe_savefig(output_path)\n            return True\n        except Exception as e:\n            logging.error(f'多変量正規分布グラフエラー: {e}')\n            plt.close()\n            return False\n\n    def plot_distribution_properties(self, params, output_path):\n        try:\n            dist = params['distribution']\n            plt.figure(figsize=(10, 5))\n            if dist == '正規分布':\n                mu = 0\n                sigma = 1\n                x = np.linspace(mu - 4 * sigma, mu + 4 * sigma, 200)\n                y = norm.pdf(x, mu, sigma)\n                plt.plot(x, y, 'b-')\n                plt.axvline(mu, color='r', linestyle='--', label='mean')\n                plt.axvline(mu + sigma, color='g', linestyle=':', label='mean+sigma')\n                plt.axvline(mu - sigma, color='g', linestyle=':')\n                plt.title('Normal Distribution (mu=0, sigma=1)')\n                plt.legend()\n            elif dist == 'ポアソン分布':\n                lam = 3\n                x = np.arange(0, 15)\n                y = poisson.pmf(x, lam)\n                plt.bar(x, y, color='skyblue')\n                plt.axvline(lam, color='r', linestyle='--', label='mean=lambda=3')\n                plt.title('Poisson(lambda=3)')\n                plt.legend()\n            elif dist == '指数分布':\n                lam = 1\n                x = np.linspace(0, 5, 200)\n                y = expon.pdf(x, scale=1 / lam)\n                plt.plot(x, y, 'b-')\n                plt.axvline(1 / lam, color='r', linestyle='--', label='mean=1/lambda')\n                plt.title('Exponential(lambda=1)')\n                plt.legend()\n            plt.tight_layout()\n            self._safe_savefig(output_path)\n            return True\n        except Exception as e:\n            logging.error(f'分布性質グラフエラー: {e}')\n            plt.close()\n            return False\n\n    def plot_high_moment(self, params, output_path):\n        try:\n            mu = params['mu']\n            sigma = params['sigma']\n            n = params['n']\n            moment = params['moment']\n            x = np.linspace(mu - 4 * sigma, mu + 4 * sigma, 200)\n            y = norm.pdf(x, mu, sigma)\n            plt.figure(figsize=(10, 5))\n            plt.plot(x, y, 'b-', label='Normal pdf')\n            plt.axvline(mu, color='r', linestyle='--', label='mean')\n            plt.axvline(mu + sigma, color='g', linestyle=':', label='mu±sigma')\n            plt.axvline(mu - sigma, color='g', linestyle=':')\n            plt.title(f'Normal distribution (mu={mu}, sigma={sigma}). n={n}-th moment={moment}')\n            plt.legend()\n            plt.tight_layout()\n            self._safe_savefig(output_path)\n            return True\n        except Exception as e:\n            logging.error(f'高次モーメントグラフエラー: {e}')\n            plt.close()\n            return False\n\n    def plot_variance_analysis(self, params, output_path):\n        try:\n            group_count = params['group_count']\n            sample_sizes = params['sample_sizes']\n            means = params['means']\n            variances = params['variances']\n            data = []\n            for i in range(group_count):\n                np.random.seed(i)\n                samples = np.random.normal(means[i], np.sqrt(variances[i]), sample_sizes[i])\n                data.append(samples)\n            plt.figure(figsize=(8, 6))\n            plt.boxplot(data, labels=[f'Group{i + 1}' for i in range(group_count)])\n            plt.title('ANOVA: Boxplots of groups')\n            plt.ylabel('Value')\n            plt.tight_layout()\n            self._safe_savefig(output_path)\n            return True\n        except Exception as e:\n            logging.error(f'分散分析グラフエラー: {e}')\n            plt.close()\n            return False\n\n    def plot_nonparametric_test(self, params, output_path):\n        try:\n            s1 = params['sample1']\n            s2 = params['sample2']\n\n            def ecdf(data):\n                d_sorted = np.sort(data)\n                y = np.arange(1, len(d_sorted) + 1) / len(d_sorted)\n                return (d_sorted, y)\n            (x1, y1) = ecdf(s1)\n            (x2, y2) = ecdf(s2)\n            plt.figure(figsize=(8, 6))\n            plt.step(x1, y1, where='post', label='Sample1 ECDF', color='blue')\n            plt.step(x2, y2, where='post', label='Sample2 ECDF', color='red')\n            plt.title('Nonparametric Test Visualization')\n            plt.xlabel('Value')\n            plt.ylabel('ECDF')\n            plt.legend()\n            plt.tight_layout()\n            self._safe_savefig(output_path)\n            return True\n        except Exception as e:\n            logging.error(f'ノンパラ検定グラフエラー: {e}')\n            plt.close()\n            return False\n\n    def plot_linear_combination(self, params, output_path):\n        try:\n            a = params['a']\n            b = params['b']\n            mu1 = params['mu1']\n            mu2 = params['mu2']\n            sig1 = params['sigma1_squared']\n            sig2 = params['sigma2_squared']\n            np.random.seed(123)\n            x = np.random.normal(mu1, np.sqrt(sig1), 1000)\n            y = np.random.normal(mu2, np.sqrt(sig2), 1000)\n            Z = a * x + b * y\n            plt.figure(figsize=(8, 6))\n            plt.hist(Z, bins=30, density=True, alpha=0.7, color='purple', label='Simulated Z')\n            E_Z = a * mu1 + b * mu2\n            Var_Z = a ** 2 * sig1 + b ** 2 * sig2\n            X_line = np.linspace(E_Z - 4 * np.sqrt(Var_Z), E_Z + 4 * np.sqrt(Var_Z), 200)\n            Y_line = norm.pdf(X_line, E_Z, np.sqrt(Var_Z))\n            plt.plot(X_line, Y_line, 'r-', label='Theoretical PDF')\n            plt.title('Linear Combination Distribution')\n            plt.xlabel('Z')\n            plt.ylabel('Density')\n            plt.legend()\n            plt.tight_layout()\n            self._safe_savefig(output_path)\n            return True\n        except Exception as e:\n            logging.error(f'線形結合グラフエラー: {e}')\n            plt.close()\n            return False"
    },
    {
      "path": "restored_latest\\統計検定1級\\gui.py",
      "overview": "Pythonコード。\nクラス: InteractiveSolverGUI。\n関数: __init__, run。\n",
      "content": "import tkinter as tk\n\nclass InteractiveSolverGUI:\n\n    def __init__(self):\n        self.root = tk.Tk()\n        self.root.title('統計検定1級 インタラクティブ問題解答システム')\n\n    def run(self):\n        self.root.mainloop()"
    },
    {
      "path": "restored_latest\\統計検定1級\\main.py",
      "overview": "Pythonコード。\n関数: main。\n",
      "content": "import sys\nfrom main_app import MainApp\nfrom gui import InteractiveSolverGUI\nfrom config import config\n\ndef main():\n    pdf_gen = config.get('pdf_generation', default={})\n    problem_count = pdf_gen.get('problem_count', 9)\n    if len(sys.argv) > 1 and sys.argv[1] == '--generate-pdf':\n        app = MainApp()\n        app.generate_and_compile(problem_count)\n    else:\n        gui = InteractiveSolverGUI()\n        gui.run()\nif __name__ == '__main__':\n    main()"
    },
    {
      "path": "restored_latest\\統計検定1級\\main_app.py",
      "overview": "Pythonコード。\nクラス: MainApp。\n関数: __init__, generate_latex_header, generate_and_compile, compile_latex。\n",
      "content": "import os, subprocess\nfrom config import config\nfrom problem_generator import ProblemGenerator\nimport logging\nimport traceback\n\nclass MainApp:\n\n    def __init__(self):\n        self.output_tex_file = config.get('output_tex_file', default='practice_problems.tex')\n        if self.output_tex_file is None:\n            self.output_tex_file = 'practice_problems.tex'\n        self.enable_visualization = config.get('enable_visualization', default=True)\n        self.problem_templates_dir = config.get('problem_templates_directory', default='templates')\n        self.output_directory = config.get('output_directory', default='outputs')\n        self.latex_path = config.get('latex_path', default='xelatex')\n        self.generator = ProblemGenerator()\n\n    def generate_latex_header(self):\n        cjk_font = config.get('cjk_main_font', default='Yu Gothic')\n        if cjk_font is None:\n            cjk_font = 'Yu Gothic'\n        header = ['\\\\documentclass{article}', '\\\\usepackage{amsmath}', '\\\\usepackage{amssymb}', '\\\\usepackage{graphicx}', '\\\\usepackage{float}', '\\\\usepackage{geometry}', '\\\\usepackage{xeCJK}', '\\\\usepackage{fontspec}', '\\\\setmainfont{Times New Roman}', f'\\\\setCJKmainfont{{{cjk_font}}}', '\\\\geometry{a4paper, margin=1in}', '\\\\begin{document}']\n        return header\n\n    def generate_and_compile(self, problem_count):\n        script_dir = os.path.dirname(os.path.abspath(__file__))\n        latex_content = self.generate_latex_header()\n        out_dir = self.output_directory\n        if out_dir is None:\n            out_dir = 'outputs'\n        output_dir = os.path.join(script_dir, out_dir)\n        os.makedirs(output_dir, exist_ok=True)\n        tex_file_path = os.path.join(output_dir, self.output_tex_file)\n        for idx in range(1, problem_count + 1):\n            try:\n                result = self.generator.generate_problem()\n                if result:\n                    (problem_id, date_created, problem_type, problem_text, solution_text, graph_filename) = result\n                    latex_content.append(f'\\\\section*{{問題 {idx}}}')\n                    latex_content.append(problem_text)\n                    if self.enable_visualization and graph_filename:\n                        latex_content.append('\\\\begin{figure}[H]')\n                        latex_content.append('\\\\centering')\n                        graph_relative_path = os.path.join('graphs', graph_filename).replace('\\\\', '/')\n                        latex_content.append(f'\\\\includegraphics[width=0.8\\\\textwidth]{{{graph_relative_path}}}')\n                        latex_content.append('\\\\end{figure}')\n                    latex_content.append('\\\\subsection*{解答}')\n                    latex_content.append(solution_text)\n                    latex_content.append('\\\\newpage')\n                else:\n                    logging.warning(f'問題 {idx} の生成に失敗しました。')\n                    print(f'問題 {idx} の生成に失敗しました。')\n            except Exception as e:\n                logging.error(f'問題 {idx} の生成中にエラー: {e}')\n                logging.error(traceback.format_exc())\n                print(f'問題 {idx} 生成エラー。ログを確認')\n        templates_dir = self.problem_templates_dir\n        if templates_dir is None:\n            templates_dir = 'templates'\n        latex_content.append('\\\\clearpage')\n        latex_content.append('\\\\input{../' + templates_dir + '/distribution_relations.tex}')\n        latex_content.append('\\\\end{document}')\n        with open(tex_file_path, 'w', encoding='utf-8') as tex_file:\n            tex_file.write('\\n'.join(latex_content))\n        self.compile_latex(tex_file_path)\n\n    def compile_latex(self, tex_file_path):\n        tex_file_name = os.path.basename(tex_file_path)\n        latex_path = self.latex_path\n        if not latex_path or not os.path.exists(latex_path):\n            print(f'LaTeXコンパイラが見つかりません: {latex_path}')\n            return\n        process = subprocess.run([latex_path, '-interaction=nonstopmode', tex_file_name], cwd=os.path.dirname(tex_file_path), stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n        log_file_path = os.path.join(os.path.dirname(tex_file_path), 'latex_compile.log')\n        with open(log_file_path, 'w', encoding='utf-8') as f:\n            f.write(process.stdout or '')\n            f.write(process.stderr or '')\n        if process.returncode != 0:\n            print('LaTeXコンパイルでエラー')\n        else:\n            pdf_file = tex_file_path.replace('.tex', '.pdf')\n            if os.path.exists(pdf_file):\n                print(f'PDF生成成功: {pdf_file}')\n            else:\n                print('PDFファイル未発見')"
    },
    {
      "path": "restored_latest\\統計検定1級\\problem_generator.py",
      "overview": "Pythonコード。\nクラス: ProblemGenerator。\n関数: __init__, load_topics, get_problem_types_by_topic, generate_problem。\n",
      "content": "import random, os, logging, uuid\nfrom datetime import datetime\nfrom config import config\nfrom database import DatabaseManager\nfrom problem_types.problem_factory import ProblemFactory\nimport json\nimport traceback\n\nclass ProblemGenerator:\n\n    def __init__(self):\n        self.db_path = config.get('database_path', default='data/data.db')\n        self.output_directory = config.get('output_directory', default='outputs')\n        self.enable_visualization = config.get('enable_visualization', default=True)\n        self.problem_types_weights = config.get('problem_types', default={})\n        self.factory = ProblemFactory()\n        self.db_manager = DatabaseManager()\n        self.db_manager.setup_database()\n        script_dir = os.path.dirname(os.path.abspath(__file__))\n        out_dir = self.output_directory\n        if out_dir is None:\n            out_dir = 'outputs'\n        self.output_dir = os.path.join(script_dir, out_dir, 'graphs')\n        os.makedirs(self.output_dir, exist_ok=True)\n        self.topics_data = self.load_topics()\n\n    def load_topics(self):\n        script_dir = os.path.dirname(os.path.abspath(__file__))\n        topics_file = os.path.join(script_dir, 'topics.json')\n        if not os.path.exists(topics_file):\n            raise FileNotFoundError(f\"'{topics_file}'がない\")\n        with open(topics_file, 'r', encoding='utf-8') as f:\n            return json.load(f)\n\n    def get_problem_types_by_topic(self, topic):\n        return self.topics_data['topics'].get(topic, [])\n\n    def generate_problem(self, selected_topic=None):\n        try:\n            ptypes = self.problem_types_weights\n            if selected_topic:\n                problem_types = self.get_problem_types_by_topic(selected_topic)\n                if not problem_types:\n                    return None\n                problem_type = random.choice(problem_types)\n            else:\n                pts = list(ptypes.keys())\n                pwt = list(ptypes.values())\n                if not pts:\n                    pts = ['probability']\n                    pwt = [1.0]\n                problem_type = random.choices(pts, weights=pwt, k=1)[0]\n            problem = self.factory.create_problem(problem_type)\n            problem.generate_parameters()\n            problem_text = problem.generate_problem_text()\n            solution_text = problem.generate_solution_text()\n            enable_vis = self.enable_visualization\n            if enable_vis is None:\n                enable_vis = True\n            graph_filename = None\n            if enable_vis:\n                graph_filename = f'graph_{uuid.uuid4().hex}.png'\n                graph_filepath = os.path.join(self.output_dir, graph_filename)\n                if not problem.generate_graph(graph_filepath):\n                    graph_filename = None\n            problem_id = uuid.uuid4().hex\n            date_created = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n            self.db_manager.save_problem(problem_id, date_created, problem_text, solution_text, problem_type)\n            return (problem_id, date_created, problem_type, problem_text, solution_text, graph_filename)\n        except Exception as e:\n            logging.error(f'問題生成エラー: {e}')\n            logging.error(traceback.format_exc())\n            return None"
    },
    {
      "path": "restored_latest\\統計検定1級\\sympy_solver.py",
      "overview": "Pythonコード。\nクラス: SympySolver。\n関数: __init__, check_equivalence。\n",
      "content": "class SympySolver:\n\n    def __init__(self):\n        pass\n\n    def check_equivalence(self, user_input, correct_answer):\n        return (False, '')"
    },
    {
      "path": "restored_latest\\統計検定1級\\topics.json",
      "overview": "JSONファイル (辞書)。キー: topics\n",
      "content": "{\n  \"topics\": {\n    \"確率論\": [\n      \"probability_definition\",\n      \"conditional_probability\",\n      \"distribution_functions\",\n      \"joint_distribution\",\n      \"probability\"\n    ],\n    \"統計的推定\": [\n      \"statistical_inference\",\n      \"t_test\"\n    ],\n    \"統計的検定\": [\n      \"statistical_inference\",\n      \"t_test\"\n    ],\n    \"回帰分析\": [\n      \"regression_analysis\"\n    ],\n    \"分散分析\": [\n      \"variance_analysis\"\n    ],\n    \"ノンパラメトリック検定\": [\n      \"nonparametric_test\"\n    ]\n  }\n}\n"
    },
    {
      "path": "restored_latest\\統計検定1級\\.vscode\\launch.json",
      "overview": "JSONファイル (辞書)。キー: version, configurations\n",
      "content": "{\n    \"version\": \"0.2.0\",\n    \"configurations\": [\n        {\n            \"name\": \"Python: 現在のファイル\",\n            \"type\": \"python\",\n            \"request\": \"launch\",\n            \"program\": \"${file}\",\n            \"console\": \"integratedTerminal\",\n            \"justMyCode\": true,\n            \"args\": [\"--generate-pdf\"]\n\n        }\n    ]\n}"
    },
    {
      "path": "restored_latest\\統計検定1級\\problem_types\\conjugate_problems.py",
      "overview": "Pythonコード。\nクラス: BetaBinomialConjugateProblem, GammaPoissonConjugateProblem, DirichletMultinomialConjugateProblem, BinomialPoissonApproxProblem, PoissonNormalApproxProblem。\n関数: __init__, generate_parameters, generate_explanation, generate_graph, __init__, generate_parameters, generate_explanation, generate_graph, __init__, generate_parameters, generate_explanation, generate_graph, __init__, generate_parameters, generate_explanation, generate_graph, __init__, generate_parameters, generate_explanation, generate_graph, B, B。\n",
      "content": "import math, random\nfrom problem_types.problem import Problem\nfrom math import comb, factorial, exp, gamma\nimport numpy as np\n\nclass BetaBinomialConjugateProblem(Problem):\n\n    def __init__(self):\n        super().__init__('beta_binomial_conjugate_problem.tex')\n\n    def generate_parameters(self):\n        self.params['alpha'] = random.randint(1, 5)\n        self.params['beta'] = random.randint(1, 5)\n        self.params['n'] = random.randint(5, 20)\n        self.params['k'] = random.randint(0, self.params['n'])\n\n        def B(x, y):\n            return gamma(x) * gamma(y) / gamma(x + y)\n        p_x = comb(self.params['n'], self.params['k']) * B(self.params['k'] + self.params['alpha'], self.params['n'] - self.params['k'] + self.params['beta']) / B(self.params['alpha'], self.params['beta'])\n        self.params['probability'] = round(p_x, 4)\n\n    def generate_explanation(self):\n        return 'Beta+Binomial->Beta-Binomial'\n\n    def generate_graph(self, o):\n        return False\n\nclass GammaPoissonConjugateProblem(Problem):\n\n    def __init__(self):\n        super().__init__('gamma_poisson_conjugate_problem.tex')\n\n    def generate_parameters(self):\n        self.params['alpha'] = random.randint(1, 5)\n        self.params['beta'] = random.uniform(0.5, 2.0)\n        self.params['k'] = random.randint(0, 20)\n        p = self.params['beta'] / (self.params['beta'] + 1)\n        q = 1 - p\n        negbin_p = comb(self.params['k'] + self.params['alpha'] - 1, self.params['k']) * q ** self.params['k'] * p ** self.params['alpha']\n        self.params['probability'] = round(negbin_p, 4)\n\n    def generate_explanation(self):\n        return 'Gamma+Poisson->Negative Binomial'\n\n    def generate_graph(self, o):\n        return False\n\nclass DirichletMultinomialConjugateProblem(Problem):\n\n    def __init__(self):\n        super().__init__('dirichlet_multinomial_conjugate_problem.tex')\n\n    def generate_parameters(self):\n        m = random.randint(2, 4)\n        self.params['m'] = m\n        self.params['n'] = random.randint(5, 20)\n        self.params['alpha_vec'] = [random.uniform(1, 3) for _ in range(m)]\n        counts = [0] * m\n        remain = self.params['n']\n        for i in range(m - 1):\n            c = random.randint(0, remain)\n            counts[i] = c\n            remain -= c\n        counts[-1] = remain\n        self.params['counts'] = counts\n\n        def B(alpha):\n            import numpy as np\n            return np.prod([gamma(a) for a in alpha]) / gamma(sum(alpha))\n        alpha_x = [self.params['alpha_vec'][i] + counts[i] for i in range(m)]\n        num = B(alpha_x)\n        den = B(self.params['alpha_vec'])\n        multinomial_coef = math.factorial(self.params['n'])\n        for c in counts:\n            multinomial_coef /= math.factorial(c)\n        p_x = multinomial_coef * (num / den)\n        self.params['probability'] = round(p_x, 4)\n\n    def generate_explanation(self):\n        return 'Dirichlet+Multinomial->Dirichlet-Multinomial'\n\n    def generate_graph(self, o):\n        return False\n\nclass BinomialPoissonApproxProblem(Problem):\n\n    def __init__(self):\n        super().__init__('binomial_poisson_approx_problem.tex')\n\n    def generate_parameters(self):\n        lam = random.uniform(2, 5)\n        n = random.randint(50, 200)\n        p = lam / n\n        k = random.randint(0, int(lam * 3))\n        binom_p = comb(n, k) * p ** k * (1 - p) ** (n - k)\n        poisson_p = lam ** k * exp(-lam) / math.factorial(k)\n        self.params['n'] = n\n        self.params['p'] = round(p, 6)\n        self.params['k'] = k\n        self.params['lambda'] = round(lam, 3)\n        self.params['binom_p'] = round(binom_p, 6)\n        self.params['poisson_p'] = round(poisson_p, 6)\n\n    def generate_explanation(self):\n        return 'Binomial->Poisson近似条件'\n\n    def generate_graph(self, o):\n        return False\n\nclass PoissonNormalApproxProblem(Problem):\n\n    def __init__(self):\n        super().__init__('poisson_normal_approx_problem.tex')\n\n    def generate_parameters(self):\n        lam = random.randint(30, 100)\n        low = max(0, int(lam - 3 * math.sqrt(lam)))\n        high = int(lam + 3 * math.sqrt(lam))\n        k = random.randint(low, high)\n        poisson_p = lam ** k * exp(-lam) / math.factorial(k)\n        self.params['lambda'] = lam\n        self.params['k'] = k\n        self.params['poisson_p'] = round(poisson_p, 6)\n        self.params['mean'] = lam\n        self.params['variance'] = lam\n\n    def generate_explanation(self):\n        return 'Poisson->Normal近似(λ大)'\n\n    def generate_graph(self, o):\n        return False"
    },
    {
      "path": "restored_latest\\統計検定1級\\problem_types\\problem.py",
      "overview": "Pythonコード。\nクラス: Problem, ProbabilityProblem, StatisticalInferenceProblem, RegressionAnalysisProblem, TimeSeriesAnalysisProblem, EconometricsProblem, LinearCombinationProblem, DistributionPropertiesProblem, HighMomentProblem, MultivariateNormalProblem, VarianceAnalysisProblem, NonParametricTestProblem, ProbabilityDefinitionProblem, ConditionalProbabilityProblem, DistributionFunctionsProblem, JointDistributionProblem。\n関数: __init__, generate_parameters, generate_problem_text, generate_solution_text, generate_explanation, generate_graph, __init__, generate_parameters, _variant_binomial, _variant_poisson, _variant_conditional_probability, generate_explanation, generate_graph, __init__, generate_parameters, generate_graph, __init__, generate_parameters, generate_graph, __init__, generate_parameters, generate_graph, __init__, generate_parameters, generate_graph, __init__, generate_parameters, generate_explanation, generate_graph, __init__, generate_parameters, generate_graph, __init__, generate_parameters, generate_graph, __init__, generate_parameters, generate_explanation, generate_graph, __init__, generate_parameters, generate_explanation, generate_graph, __init__, generate_parameters, generate_explanation, generate_graph, __init__, generate_parameters, generate_explanation, generate_graph, __init__, generate_parameters, generate_explanation, generate_graph, __init__, generate_parameters, generate_explanation, generate_graph, __init__, generate_parameters, generate_explanation, generate_graph。\n",
      "content": "from abc import ABC, abstractmethod\nimport os\nimport random\nimport logging\nfrom config import config\nfrom jinja2 import Environment, FileSystemLoader\nfrom graph import ProbabilityDistributionVisualizer\nimport traceback\nfrom math import comb, exp, factorial\nfrom scipy.stats import norm, stats\n\nclass Problem(ABC):\n\n    def __init__(self, template_name):\n        self.params = {}\n        self.template_name = template_name\n        templates_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), '..', config.get('problem_templates_directory', default='templates'))\n        self.env = Environment(loader=FileSystemLoader(templates_dir), autoescape=False)\n        self.visualizer = ProbabilityDistributionVisualizer()\n\n    @abstractmethod\n    def generate_parameters(self):\n        pass\n\n    def generate_problem_text(self):\n        template = self.env.get_template(self.template_name)\n        return template.render(**self.params, show_solution=False)\n\n    def generate_solution_text(self):\n        template = self.env.get_template(self.template_name)\n        self.params['explanation'] = self.generate_explanation()\n        return template.render(**self.params, show_solution=True)\n\n    def generate_explanation(self):\n        return ''\n\n    @abstractmethod\n    def generate_graph(self, output_path):\n        pass\n\nclass ProbabilityProblem(Problem):\n\n    def __init__(self):\n        super().__init__('probability_problem.tex')\n\n    def generate_parameters(self):\n        variants = [self._variant_binomial, self._variant_poisson, self._variant_conditional_probability]\n        v = random.choice(variants)\n        v()\n\n    def _variant_binomial(self):\n        self.params['problem_type'] = 'binomial'\n        self.params['n'] = random.randint(5, 20)\n        self.params['p'] = round(random.uniform(0.1, 0.9), 2)\n        self.params['k'] = random.randint(0, self.params['n'])\n        from math import comb\n        prob = comb(self.params['n'], self.params['k']) * self.params['p'] ** self.params['k'] * (1 - self.params['p']) ** (self.params['n'] - self.params['k'])\n        self.params['probability'] = round(prob, 6)\n\n    def _variant_poisson(self):\n        self.params['problem_type'] = 'poisson'\n        self.params['lambda'] = round(random.uniform(0.5, 5.0), 2)\n        self.params['k'] = random.randint(0, 10)\n        lam = self.params['lambda']\n        k = self.params['k']\n        prob = lam ** k * exp(-lam) / factorial(k)\n        self.params['probability'] = round(prob, 6)\n\n    def _variant_conditional_probability(self):\n        self.params['problem_type'] = 'conditional_probability'\n        self.params['P_A'] = round(random.uniform(0.1, 0.9), 2)\n        self.params['P_B_given_A'] = round(random.uniform(0.1, 0.9), 2)\n        self.params['P_A_and_B'] = round(self.params['P_A'] * self.params['P_B_given_A'], 6)\n\n    def generate_explanation(self):\n        t = self.params['problem_type']\n        if t == 'binomial':\n            return '二項分布の公式を使用'\n        elif t == 'poisson':\n            return 'ポアソン分布の公式を使用'\n        elif t == 'conditional_probability':\n            return '条件付き確率P(A∩B)=P(A)*P(B|A)'\n\n    def generate_graph(self, output_path):\n        return self.visualizer.plot_probability(self.params, output_path)\n\nclass StatisticalInferenceProblem(Problem):\n\n    def __init__(self):\n        super().__init__('statistical_inference_problem.tex')\n\n    def generate_parameters(self):\n        self.params['sample_mean'] = round(random.uniform(50, 100), 2)\n        self.params['sample_std'] = round(random.uniform(5, 15), 2)\n        self.params['n'] = random.randint(30, 100)\n        self.params['population_mean'] = round(random.uniform(50, 100), 2)\n        self.params['alpha'] = round(random.uniform(0.01, 0.1), 2)\n        t_stat = (self.params['sample_mean'] - self.params['population_mean']) / (self.params['sample_std'] / self.params['n'] ** 0.5)\n        t_stat = round(t_stat, 4)\n        df = self.params['n'] - 1\n        cv = round(stats.t.ppf(1 - self.params['alpha'] / 2, df=df), 4)\n        reject = '棄却' if abs(t_stat) > cv else '棄却しない'\n        self.params['t_stat'] = t_stat\n        self.params['critical_value'] = cv\n        self.params['reject_null'] = reject\n        self.params['df'] = df\n\n    def generate_graph(self, output_path):\n        return self.visualizer.plot_t_test(self.params, output_path)\n\nclass RegressionAnalysisProblem(Problem):\n\n    def __init__(self):\n        super().__init__('regression_analysis_problem.tex')\n\n    def generate_parameters(self):\n        self.params['beta_0'] = round(random.uniform(0, 10), 2)\n        self.params['beta_1'] = round(random.uniform(-5, 5), 2)\n        self.params['n'] = random.randint(50, 200)\n        self.params['x_values'] = [round(random.uniform(0, 100), 2) for _ in range(self.params['n'])]\n        self.params['epsilon'] = [round(random.gauss(0, 10), 2) for _ in range(self.params['n'])]\n        self.params['y_values'] = [self.params['beta_0'] + self.params['beta_1'] * x + e for (x, e) in zip(self.params['x_values'], self.params['epsilon'])]\n        import numpy as np\n        X = np.array(self.params['x_values'])\n        Y = np.array(self.params['y_values'])\n        beta_1_hat = np.cov(X, Y, bias=True)[0, 1] / np.var(X)\n        beta_0_hat = np.mean(Y) - beta_1_hat * np.mean(X)\n        self.params['beta_0_hat'] = round(beta_0_hat, 4)\n        self.params['beta_1_hat'] = round(beta_1_hat, 4)\n\n    def generate_graph(self, output_path):\n        return self.visualizer.plot_regression(self.params['x_values'], self.params['y_values'], self.params['beta_0_hat'], self.params['beta_1_hat'], output_path)\n\nclass TimeSeriesAnalysisProblem(Problem):\n\n    def __init__(self):\n        super().__init__('time_series_analysis_problem.tex')\n\n    def generate_parameters(self):\n        self.params['phi'] = round(random.uniform(0.5, 0.9), 2)\n        self.params['theta'] = round(random.uniform(-0.5, 0.5), 2)\n        self.params['n'] = 100\n        self.params['epsilon'] = [random.gauss(0, 1) for _ in range(self.params['n'])]\n        self.params['time_series'] = [0] * self.params['n']\n        for t in range(1, self.params['n']):\n            self.params['time_series'][t] = self.params['phi'] * self.params['time_series'][t - 1] + self.params['epsilon'][t] + self.params['theta'] * self.params['epsilon'][t - 1]\n\n    def generate_graph(self, output_path):\n        return self.visualizer.plot_time_series(self.params, output_path)\n\nclass EconometricsProblem(Problem):\n\n    def __init__(self):\n        super().__init__('econometrics_problem.tex')\n\n    def generate_parameters(self):\n        self.params['beta_0'] = round(random.uniform(0, 5), 2)\n        self.params['beta_1'] = round(random.uniform(0, 1), 2)\n        self.params['beta_2'] = round(random.uniform(-1, 0), 2)\n        self.params['n'] = random.randint(50, 200)\n        self.params['x1_values'] = [round(random.uniform(0, 100), 2) for _ in range(self.params['n'])]\n        self.params['x2_values'] = [round(random.uniform(0, 100), 2) for _ in range(self.params['n'])]\n        self.params['epsilon'] = [round(random.gauss(0, 5), 2) for _ in range(self.params['n'])]\n        self.params['y_values'] = [self.params['beta_0'] + self.params['beta_1'] * x1 + self.params['beta_2'] * x2 + e for (x1, x2, e) in zip(self.params['x1_values'], self.params['x2_values'], self.params['epsilon'])]\n        import numpy as np\n        X = np.column_stack((np.ones(self.params['n']), self.params['x1_values'], self.params['x2_values']))\n        Y = np.array(self.params['y_values'])\n        beta_hat = np.linalg.inv(X.T @ X) @ X.T @ Y\n        self.params['beta_0_hat'] = round(beta_hat[0], 4)\n        self.params['beta_1_hat'] = round(beta_hat[1], 4)\n        self.params['beta_2_hat'] = round(beta_hat[2], 4)\n\n    def generate_graph(self, output_path):\n        return self.visualizer.plot_econometrics(self.params, output_path)\n\nclass LinearCombinationProblem(Problem):\n\n    def __init__(self):\n        super().__init__('linear_combination_problem.tex')\n\n    def generate_parameters(self):\n        self.params['a'] = random.randint(1, 5)\n        self.params['b'] = random.randint(1, 5)\n        self.params['mu1'] = round(random.uniform(0, 10), 2)\n        self.params['mu2'] = round(random.uniform(0, 10), 2)\n        self.params['sigma1_squared'] = round(random.uniform(1, 5), 2)\n        self.params['sigma2_squared'] = round(random.uniform(1, 5), 2)\n        E_Z = self.params['a'] * self.params['mu1'] + self.params['b'] * self.params['mu2']\n        Var_Z = self.params['a'] ** 2 * self.params['sigma1_squared'] + self.params['b'] ** 2 * self.params['sigma2_squared']\n        self.params['E_Z'] = round(E_Z, 4)\n        self.params['Var_Z'] = round(Var_Z, 4)\n\n    def generate_explanation(self):\n        return '線形結合の期待値・分散計算'\n\n    def generate_graph(self, output_path):\n        return self.visualizer.plot_linear_combination(self.params, output_path)\n\nclass DistributionPropertiesProblem(Problem):\n\n    def __init__(self):\n        super().__init__('distribution_properties_problem.tex')\n\n    def generate_parameters(self):\n        dist_choice = random.choice(['正規分布', 'ポアソン分布', '指数分布'])\n        self.params['distribution'] = dist_choice\n        if dist_choice == '正規分布':\n            self.params['properties'] = {'mean': '\\\\mu', 'variance': '\\\\sigma^2'}\n        elif dist_choice == 'ポアソン分布':\n            self.params['properties'] = {'mean': '\\\\lambda', 'variance': '\\\\lambda'}\n        else:\n            self.params['properties'] = {'mean': '1/\\\\lambda', 'variance': '1/\\\\lambda^2'}\n\n    def generate_graph(self, output_path):\n        return self.visualizer.plot_distribution_properties(self.params, output_path)\n\nclass HighMomentProblem(Problem):\n\n    def __init__(self):\n        super().__init__('high_moment_problem.tex')\n\n    def generate_parameters(self):\n        self.params['n'] = random.randint(3, 5)\n        self.params['mu'] = round(random.uniform(0, 10), 2)\n        self.params['sigma'] = round(random.uniform(1, 5), 2)\n        from scipy.stats import norm\n        m = norm.moment(self.params['n'], loc=self.params['mu'], scale=self.params['sigma'])\n        self.params['moment'] = round(m, 4)\n\n    def generate_graph(self, output_path):\n        return self.visualizer.plot_high_moment(self.params, output_path)\n\nclass MultivariateNormalProblem(Problem):\n\n    def __init__(self):\n        super().__init__('multivariate_normal_problem.tex')\n\n    def generate_parameters(self):\n        self.params['mu'] = [round(random.uniform(0, 10), 2) for _ in range(2)]\n        self.params['sigma'] = [[round(random.uniform(1, 5), 2), round(random.uniform(0, 2), 2)], [round(random.uniform(0, 2), 2), round(random.uniform(1, 5), 2)]]\n\n    def generate_explanation(self):\n        return '多変量正規分布の性質を利用'\n\n    def generate_graph(self, output_path):\n        return self.visualizer.plot_multivariate_normal(self.params, output_path)\n\nclass VarianceAnalysisProblem(Problem):\n\n    def __init__(self):\n        super().__init__('variance_analysis_problem.tex')\n\n    def generate_parameters(self):\n        self.params['group_count'] = random.randint(2, 4)\n        self.params['sample_sizes'] = [random.randint(5, 20) for _ in range(self.params['group_count'])]\n        self.params['means'] = [round(random.uniform(10, 50), 2) for _ in range(self.params['group_count'])]\n        self.params['variances'] = [round(random.uniform(1, 5), 2) for _ in range(self.params['group_count'])]\n        total_n = sum(self.params['sample_sizes'])\n        group_count = self.params['group_count']\n        means = self.params['means']\n        variances = self.params['variances']\n        sample_sizes = self.params['sample_sizes']\n        grand_mean = sum([means[i] * sample_sizes[i] for i in range(group_count)]) / total_n\n        ssb = sum([sample_sizes[i] * (means[i] - grand_mean) ** 2 for i in range(group_count)])\n        ssw = sum([(sample_sizes[i] - 1) * variances[i] for i in range(group_count)])\n        df_between = group_count - 1\n        df_within = total_n - group_count\n        msb = ssb / df_between\n        msw = ssw / df_within\n        F = msb / msw\n        alpha = 0.05\n        from scipy.stats import f\n        F_critical = f.ppf(1 - alpha, df_between, df_within)\n        reject = '棄却する' if F > F_critical else '棄却しない'\n        self.params['F_value'] = round(F, 4)\n        self.params['F_critical'] = round(F_critical, 4)\n        self.params['reject_null'] = reject\n        self.params['df_between'] = df_between\n        self.params['df_within'] = df_within\n\n    def generate_explanation(self):\n        return '一元配置分散分析による検定'\n\n    def generate_graph(self, output_path):\n        return self.visualizer.plot_variance_analysis(self.params, output_path)\n\nclass NonParametricTestProblem(Problem):\n\n    def __init__(self):\n        super().__init__('nonparametric_test_problem.tex')\n\n    def generate_parameters(self):\n        self.params['test_type'] = random.choice(['Mann-Whitney U', 'Kruskal-Wallis', 'Wilcoxon Signed-Rank'])\n        self.params['sample1'] = [round(random.uniform(10, 100), 2) for _ in range(random.randint(5, 20))]\n        self.params['sample2'] = [round(random.uniform(10, 100), 2) for _ in range(random.randint(5, 20))]\n        self.params['test_result'] = '有意差なし(例)'\n\n    def generate_explanation(self):\n        return 'ノンパラ検定で中央値差を検定'\n\n    def generate_graph(self, output_path):\n        return self.visualizer.plot_nonparametric_test(self.params, output_path)\n\nclass ProbabilityDefinitionProblem(Problem):\n\n    def __init__(self):\n        super().__init__('probability_definition_problem.tex')\n\n    def generate_parameters(self):\n        self.params['P_A'] = round(random.uniform(0.1, 0.9), 2)\n        self.params['P_B'] = round(random.uniform(0.1, 0.9), 2)\n        self.params['P_A_and_B'] = round(self.params['P_A'] * self.params['P_B'], 4)\n\n    def generate_explanation(self):\n        return '独立性利用 P(A∩B)=P(A)*P(B)'\n\n    def generate_graph(self, output_path):\n        try:\n            PA = self.params['P_A']\n            PB = self.params['P_B']\n            PAB = self.params['P_A_and_B']\n            plt.figure()\n            plt.bar(['P(A)', 'P(B)', 'P(A∩B)'], [PA, PB, PAB], color=['blue', 'green', 'red'])\n            plt.title('Probability Definition')\n            plt.tight_layout()\n            plt.savefig(output_path)\n            plt.close()\n            return True\n        except:\n            return False\n\nclass ConditionalProbabilityProblem(Problem):\n\n    def __init__(self):\n        super().__init__('conditional_probability_problem.tex')\n\n    def generate_parameters(self):\n        self.params['P_A'] = round(random.uniform(0.2, 0.8), 2)\n        self.params['P_B_given_A'] = round(random.uniform(0.2, 0.8), 2)\n        self.params['P_A_and_B'] = round(self.params['P_A'] * self.params['P_B_given_A'], 4)\n\n    def generate_explanation(self):\n        return 'P(A∩B)=P(A)*P(B|A)'\n\n    def generate_graph(self, output_path):\n        from graph import ProbabilityDistributionVisualizer\n        vis = ProbabilityDistributionVisualizer()\n        return vis.plot_probability(self.params, output_path)\n\nclass DistributionFunctionsProblem(Problem):\n\n    def __init__(self):\n        super().__init__('distribution_functions_problem.tex')\n\n    def generate_parameters(self):\n        self.params['function_type'] = random.choice(['pdf', 'cdf'])\n        self.params['distribution'] = random.choice(['正規分布', '指数分布'])\n        if self.params['distribution'] == '正規分布':\n            self.params['mean'] = round(random.uniform(-5, 5), 2)\n            self.params['std'] = round(random.uniform(1, 3), 2)\n        else:\n            self.params['lambda'] = round(random.uniform(0.5, 2.0), 2)\n\n    def generate_explanation(self):\n        return 'pdfやcdf定義式利用'\n\n    def generate_graph(self, output_path):\n        return False\n\nclass JointDistributionProblem(Problem):\n\n    def __init__(self):\n        super().__init__('joint_distribution_problem.tex')\n\n    def generate_parameters(self):\n        A_and_B = round(random.uniform(0.05, 0.2), 2)\n        A_and_notB = round(random.uniform(0.05, 0.2), 2)\n        notA_and_B = round(random.uniform(0.05, 0.2), 2)\n        notA_and_notB = round(random.uniform(0.05, 0.2), 2)\n        total = A_and_B + A_and_notB + notA_and_B + notA_and_notB\n        A_and_B /= total\n        A_and_notB /= total\n        notA_and_B /= total\n        notA_and_notB /= total\n        self.params['joint_probabilities'] = {'A_and_B': round(A_and_B, 4), 'A_and_not_B': round(A_and_notB, 4), 'not_A_and_B': round(notA_and_B, 4), 'not_A_and_not_B': round(notA_and_notB, 4)}\n        P_A = A_and_B + A_and_notB\n        P_B = A_and_B + notA_and_B\n        P_BA = A_and_B / P_A if P_A > 0 else 0.0\n        self.params['P_A'] = round(P_A, 4)\n        self.params['P_B'] = round(P_B, 4)\n        self.params['P_B_given_A'] = round(P_BA, 4)\n\n    def generate_explanation(self):\n        return '同時→周辺→条件付き確率'\n\n    def generate_graph(self, output_path):\n        try:\n            p = self.params['joint_probabilities']\n            matrix = np.array([[p['A_and_B'], p['A_and_not_B']], [p['not_A_and_B'], p['not_A_and_not_B']]])\n            plt.figure()\n            plt.imshow(matrix, cmap='Blues', interpolation='nearest')\n            plt.colorbar(label='Probability')\n            plt.xticks([0, 1], ['B', 'not B'])\n            plt.yticks([0, 1], ['A', 'not A'])\n            plt.title('Joint Distribution Heatmap')\n            plt.tight_layout()\n            plt.savefig(output_path)\n            plt.close()\n            return True\n        except:\n            return False"
    },
    {
      "path": "restored_latest\\統計検定1級\\problem_types\\problem_factory.py",
      "overview": "Pythonコード。\nクラス: ProblemFactory。\n関数: __init__, create_problem。\n",
      "content": "import logging, traceback\nfrom problem_types.problem import ProbabilityProblem, StatisticalInferenceProblem, RegressionAnalysisProblem, TimeSeriesAnalysisProblem, EconometricsProblem, LinearCombinationProblem, DistributionPropertiesProblem, HighMomentProblem, MultivariateNormalProblem, VarianceAnalysisProblem, NonParametricTestProblem, ProbabilityDefinitionProblem, ConditionalProbabilityProblem, DistributionFunctionsProblem, JointDistributionProblem\nfrom problem_types.conjugate_problems import BetaBinomialConjugateProblem, GammaPoissonConjugateProblem, DirichletMultinomialConjugateProblem, BinomialPoissonApproxProblem, PoissonNormalApproxProblem\n\nclass ProblemFactory:\n\n    def __init__(self):\n        self.problem_classes = {'probability': ProbabilityProblem, 'statistical_inference': StatisticalInferenceProblem, 'regression_analysis': RegressionAnalysisProblem, 'time_series_analysis': TimeSeriesAnalysisProblem, 'econometrics': EconometricsProblem, 'linear_combination': LinearCombinationProblem, 'distribution_properties': DistributionPropertiesProblem, 'high_moment': HighMomentProblem, 'multivariate_normal': MultivariateNormalProblem, 'probability_definition': ProbabilityDefinitionProblem, 'conditional_probability': ConditionalProbabilityProblem, 'distribution_functions': DistributionFunctionsProblem, 'joint_distribution': JointDistributionProblem, 't_test': StatisticalInferenceProblem, 'variance_analysis': VarianceAnalysisProblem, 'nonparametric_test': NonParametricTestProblem, 'beta_binomial_conjugate': BetaBinomialConjugateProblem, 'gamma_poisson_conjugate': GammaPoissonConjugateProblem, 'dirichlet_multinomial_conjugate': DirichletMultinomialConjugateProblem, 'binomial_poisson_approx': BinomialPoissonApproxProblem, 'poisson_normal_approx': PoissonNormalApproxProblem}\n\n    def create_problem(self, problem_type):\n        pc = self.problem_classes.get(problem_type)\n        if pc:\n            try:\n                return pc()\n            except Exception as e:\n                logging.error(f'{problem_type} problem generation error:{e}')\n                logging.error(traceback.format_exc())\n                raise\n        else:\n            raise ValueError(f'Unknown problem type:{problem_type}')"
    },
    {
      "path": "restored_latest\\統計検定1級\\templates\\distribution_properties_problem.tex",
      "overview": "TeXファイル。行数: 7\n",
      "content": "% distribution_properties_problem.tex\n{% if not show_solution %}\n{{ distribution }} の平均と分散を求めよ。\n{% else %}\n平均: ${{ properties.mean }}$, 分散: ${{ properties.variance }}$\n{{ explanation }}\n{% endif %}\n"
    },
    {
      "path": "restored_latest\\統計検定1級\\templates\\distribution_relations.tex",
      "overview": "TeXファイル。行数: 7\n",
      "content": "% distribution_relations.tex\n\\section*{Distribution Relations}\n- Beta+Binomial -> Beta-Binomial\n- Gamma+Poisson -> Negative Binomial\n- Dirichlet+Multinomial -> Dirichlet-Multinomial\n- Binomial->Poisson approximation\n- Poisson->Normal approximation\n"
    },
    {
      "path": "restored_latest\\統計検定1級\\templates\\econometrics_problem.tex",
      "overview": "TeXファイル。行数: 6\n",
      "content": "% econometrics_problem.tex\n{% if not show_solution %}\n計量経済学モデルに関する問題\n{% else %}\n解答と推定量\n{{ explanation }}\n{% endif %}"
    },
    {
      "path": "restored_latest\\統計検定1級\\templates\\high_moment_problem.tex",
      "overview": "TeXファイル。行数: 6\n",
      "content": "% high_moment_problem.tex\n{% if not show_solution %}\n正規分布の{{ n }}次モーメントは？(例: $E[X^{n}]$ )\n{% else %}\n$E[X^{n}]={{ moment }}$\n{{ explanation }}\n{% endif %}"
    },
    {
      "path": "restored_latest\\統計検定1級\\templates\\linear_combination_problem.tex",
      "overview": "TeXファイル。行数: 6\n",
      "content": "% linear_combination_problem.tex\n{% if not show_solution %}\nZ=aX+bY のE[Z],Var[Z]\n{% else %}\nE[Z],Var[Z]\n{{ explanation }}\n{% endif %}"
    },
    {
      "path": "restored_latest\\統計検定1級\\templates\\multivariate_normal_problem.tex",
      "overview": "TeXファイル。行数: 6\n",
      "content": "% multivariate_normal_problem.tex\n{% if not show_solution %}\n多変量正規に関する問題\n{% else %}\n解答\n{{ explanation }}\n{% endif %}"
    },
    {
      "path": "restored_latest\\統計検定1級\\templates\\nonparametric_test_problem.tex",
      "overview": "TeXファイル。行数: 6\n",
      "content": "% nonparametric_test_problem.tex\n{% if not show_solution %}\nノンパラ検定問題\n{% else %}\n検定結果\n{{ explanation }}\n{% endif %}"
    },
    {
      "path": "restored_latest\\統計検定1級\\templates\\poisson_normal_approx_problem.tex",
      "overview": "TeXファイル。行数: 6\n",
      "content": "% poisson_normal_approx_problem.tex\n{% if not show_solution %}\nPoisson→Normal近似\n{% else %}\nPoisson P=${{ poisson_p }}$, mean=${{mean}}$ var=${{variance}}$\n{{ explanation }}\n{% endif %}"
    },
    {
      "path": "restored_latest\\統計検定1級\\templates\\probability_definition_problem.tex",
      "overview": "TeXファイル。行数: 6\n",
      "content": "% probability_definition_problem.tex\n{% if not show_solution %}\nP(A∩B)求めよ\n{% else %}\n結果\n{{ explanation }}\n{% endif %}"
    },
    {
      "path": "restored_latest\\統計検定1級\\templates\\probability_problem.tex",
      "overview": "TeXファイル。行数: 7\n",
      "content": "% probability_problem.tex\n{% if not show_solution %}\n確率計算問題（例）\n問題タイプ: {{ problem_type }}\n{% else %}\n解答と説明: {{ explanation }}\n計算結果: P = {{ probability }}\n{% endif %}"
    },
    {
      "path": "restored_latest\\統計検定1級\\templates\\regression_analysis_problem.tex",
      "overview": "TeXファイル。行数: 6\n",
      "content": "% regression_analysis_problem.tex\n{% if not show_solution %}\n回帰分析問題\n{% else %}\n回帰係数結果\n{{ explanation }}\n{% endif %}"
    },
    {
      "path": "restored_latest\\統計検定1級\\templates\\statistical_inference_problem.tex",
      "overview": "TeXファイル。行数: 6\n",
      "content": "% statistical_inference_problem.tex\n{% if not show_solution %}\n統計的推定/検定問題\n{% else %}\n検定結果\n{{ explanation }}\n{% endif %}"
    },
    {
      "path": "restored_latest\\統計検定1級\\templates\\time_series_analysis_problem.tex",
      "overview": "TeXファイル。行数: 6\n",
      "content": "% time_series_analysis_problem.tex\n{% if not show_solution %}\n時系列分析問題\n{% else %}\n解答と説明\n{{ explanation }}\n{% endif %}"
    },
    {
      "path": "restored_latest\\統計検定1級\\templates\\variance_analysis_problem.tex",
      "overview": "TeXファイル。行数: 6\n",
      "content": "% variance_analysis_problem.tex\n{% if not show_solution %}\n分散分析問題\n{% else %}\nANOVA結果\n{{ explanation }}\n{% endif %}"
    },
    {
      "path": "your_project\\.vscode\\launch (2).json",
      "overview": "JSON解析エラー: Expecting property name enclosed in double quotes: line 2 column 5 (char 6)\n",
      "content": "{\n    // IntelliSense を使用して利用可能な属性を学べます。\n    // 既存の属性の説明をホバーして表示します。\n    // 詳細情報は次を確認してください: https://go.microsoft.com/fwlink/?linkid=830387\n    \"version\": \"0.2.0\",\n    \"configurations\": [\n        {\n            \"name\": \"Python: 現在のファイル\",\n            \"type\": \"python\",\n            \"request\": \"launch\",\n            \"program\": \"${file}\",\n            \"console\": \"integratedTerminal\",\n            \"justMyCode\": true,\n            \"args\": [\"--generate-pdf\"]\n\n        }\n    ]\n}"
    },
    {
      "path": "your_project\\統計検定1級\\config.py",
      "overview": "Pythonコード。\nクラス: ConfigManager。\n関数: __new__, load_config, get。\n",
      "content": "import json, os, logging\nfrom typing import Any\n\nclass ConfigManager:\n    _instance = None\n\n    def __new__(cls):\n        if cls._instance is None:\n            cls._instance = super(ConfigManager, cls).__new__(cls)\n            cls._instance._config = {}\n        return cls._instance\n\n    def load_config(self, config_path: str) -> None:\n        if not os.path.exists(config_path):\n            logging.warning(f'設定ファイルが見つかりません: {config_path}')\n            self._config = {}\n        else:\n            try:\n                with open(config_path, 'r', encoding='utf-8') as f:\n                    content = f.read().strip()\n                    if not content:\n                        self._config = {}\n                    else:\n                        self._config = json.loads(content)\n                logging.info(f'設定ファイル読込完了: {config_path}')\n            except Exception as e:\n                logging.warning(f'設定ファイル読込中にエラー: {str(e)}', exc_info=True)\n                self._config = {}\n\n    def get(self, *keys: str, default: Any=None) -> Any:\n        data = self._config\n        for k in keys:\n            if isinstance(data, dict) and k in data:\n                data = data[k]\n            else:\n                return default\n        if isinstance(data, dict) and 'value' in data:\n            return data['value']\n        return data\nconfig = ConfigManager()"
    },
    {
      "path": "your_project\\統計検定1級\\database.py",
      "overview": "Pythonコード。\nクラス: DatabaseManager。\n関数: __init__, setup_database, save_problem。\n",
      "content": "import sqlite3, os\nfrom config import config\n\nclass DatabaseManager:\n\n    def __init__(self):\n        script_dir = os.path.dirname(os.path.abspath(__file__))\n        db_path = config.get('database_path', default='data/data.db')\n        if db_path is None:\n            db_path = 'data/data.db'\n        self.db_name = os.path.join(script_dir, db_path)\n        os.makedirs(os.path.dirname(self.db_name), exist_ok=True)\n\n    def setup_database(self):\n        conn = sqlite3.connect(self.db_name)\n        c = conn.cursor()\n        c.execute('CREATE TABLE IF NOT EXISTS problems (id TEXT PRIMARY KEY,date_created TEXT,problem_text TEXT,solution_text TEXT,problem_type TEXT)')\n        conn.commit()\n        conn.close()\n\n    def save_problem(self, problem_id, date_created, problem_text, solution_text, problem_type):\n        conn = sqlite3.connect(self.db_name)\n        c = conn.cursor()\n        c.execute('INSERT INTO problems VALUES (?,?,?,?,?)', (problem_id, date_created, problem_text, solution_text, problem_type))\n        conn.commit()\n        conn.close()"
    },
    {
      "path": "your_project\\統計検定1級\\graph.py",
      "overview": "Pythonコード。\nクラス: ProbabilityDistributionVisualizer。\n関数: __init__, _safe_savefig, plot_probability, plot_t_test, plot_regression, plot_time_series, plot_econometrics, plot_multivariate_normal, plot_distribution_properties, plot_high_moment, plot_variance_analysis, plot_nonparametric_test, plot_linear_combination, confidence_ellipse, mvn_pdf, ecdf。\n",
      "content": "import matplotlib\nmatplotlib.use('Agg')\nimport matplotlib.pyplot as plt\nimport math\nimport numpy as np\nimport logging\nimport os\nfrom math import factorial, exp\nfrom scipy.stats import norm, t as t_dist, f as f_dist, poisson, expon\nfrom statsmodels.graphics.tsaplots import plot_acf\nimport traceback\n\nclass ProbabilityDistributionVisualizer:\n\n    def __init__(self):\n        pass\n\n    def _safe_savefig(self, output_path):\n        \"\"\"\n        画像を確実にPNGで出力し、ファイルサイズなどをログするヘルパー関数。\n        \"\"\"\n        try:\n            (_, ext) = os.path.splitext(output_path)\n            if ext.lower() == '.png':\n                plt.savefig(output_path, format='png')\n            else:\n                plt.savefig(output_path, format='png')\n            plt.close()\n            if os.path.exists(output_path):\n                fsize = os.path.getsize(output_path)\n                logging.info(f'Saved figure: {output_path} (size: {fsize} bytes)')\n            else:\n                logging.warning(f'File not found after saving: {output_path}')\n        except Exception as e:\n            logging.error(f'Failed to save figure to {output_path}, error={e}')\n            plt.close()\n\n    def plot_probability(self, params, output_path):\n        ptype = params.get('problem_type')\n        try:\n            if ptype == 'binomial':\n                p = params['p']\n                n = params['n']\n                k = params['k']\n                x = range(n + 1)\n                from math import comb\n                pmf = [comb(n, i) * p ** i * (1 - p) ** (n - i) for i in x]\n                cdf = np.cumsum(pmf)\n                plt.figure(figsize=(10, 5))\n                plt.subplot(1, 2, 1)\n                plt.bar(x, pmf, color='skyblue')\n                plt.title(f'Binomial PMF n={n}, p={p}')\n                plt.xlabel('k')\n                plt.ylabel('P(X=k)')\n                plt.axvline(k, color='red', linestyle='--', label='Target k')\n                plt.legend()\n                plt.subplot(1, 2, 2)\n                plt.step(x, cdf, where='post', color='green')\n                plt.title('CDF')\n                plt.xlabel('k')\n                plt.ylabel('P(X<=k)')\n                plt.axvline(k, color='red', linestyle='--')\n                plt.tight_layout()\n                self._safe_savefig(output_path)\n                return True\n            elif ptype == 'poisson':\n                lam = params['lambda']\n                k = params['k']\n                x = range(k + 10 + 1)\n                pmf = [lam ** i * exp(-lam) / factorial(i) for i in x]\n                cdf = np.cumsum(pmf)\n                plt.figure(figsize=(10, 5))\n                plt.subplot(1, 2, 1)\n                plt.bar(x, pmf, color='orange')\n                plt.title(f'Poisson(lambda={lam}) PMF')\n                plt.xlabel('k')\n                plt.ylabel('P(X=k)')\n                plt.axvline(k, color='red', linestyle='--', label='Target k')\n                plt.legend()\n                plt.subplot(1, 2, 2)\n                plt.step(x, cdf, where='post', color='green')\n                plt.title('CDF')\n                plt.xlabel('k')\n                plt.ylabel('P(X<=k)')\n                plt.axvline(k, color='red', linestyle='--')\n                plt.tight_layout()\n                self._safe_savefig(output_path)\n                return True\n            elif ptype == 'conditional_probability':\n                P_A = params['P_A']\n                P_BA = params['P_B_given_A']\n                P_AB = params['P_A_and_B']\n                plt.figure()\n                vals = [P_A, P_BA, P_AB]\n                labels = ['P(A)', 'P(B|A)', 'P(A∩B)']\n                plt.bar(labels, vals, color=['blue', 'green', 'red'])\n                plt.title('Conditional Probability Visualization')\n                plt.ylabel('Probability')\n                plt.tight_layout()\n                self._safe_savefig(output_path)\n                return True\n            else:\n                return False\n        except Exception as e:\n            logging.error(f'Error in plot_probability: {e}')\n            plt.close()\n            return False\n\n    def plot_t_test(self, params, output_path):\n        try:\n            alpha = params['alpha']\n            df = params['df']\n            t_stat = params['t_stat']\n            critical_value = params['critical_value']\n            x = np.linspace(t_dist.ppf(0.001, df), t_dist.ppf(0.999, df), 1000)\n            y = t_dist.pdf(x, df)\n            plt.figure(figsize=(10, 5))\n            plt.plot(x, y, 'b-', label=f't-dist(df={df})')\n            plt.axvline(x=critical_value, color='r', linestyle='--', label='critical +')\n            plt.axvline(x=-critical_value, color='r', linestyle='--', label='critical -')\n            plt.axvline(x=t_stat, color='g', label='t-stat')\n            p_area_x = x[x > critical_value]\n            plt.fill_between(p_area_x, t_dist.pdf(p_area_x, df), color='red', alpha=0.3)\n            p_area_x2 = x[x < -critical_value]\n            plt.fill_between(p_area_x2, t_dist.pdf(p_area_x2, df), color='red', alpha=0.3)\n            plt.title('t-test visualization')\n            plt.xlabel('t')\n            plt.ylabel('pdf')\n            plt.legend()\n            plt.tight_layout()\n            self._safe_savefig(output_path)\n            return True\n        except Exception as e:\n            logging.error(f't検定グラフ生成エラー: {e}')\n            plt.close()\n            return False\n\n    def plot_regression(self, x_values, y_values, beta_0_hat, beta_1_hat, output_path):\n        try:\n            import statsmodels.api as sm\n            X = sm.add_constant(x_values)\n            model = sm.OLS(y_values, X).fit()\n            residuals = model.resid\n            (fig, axes) = plt.subplots(2, 2, figsize=(10, 10))\n            ax = axes[0, 0]\n            ax.scatter(x_values, y_values, color='blue', label='data')\n            x_line = np.linspace(min(x_values), max(x_values), 100)\n            y_line = beta_0_hat + beta_1_hat * x_line\n            ax.plot(x_line, y_line, color='red', label='reg line')\n            ax.set_title('Data & Regression Line')\n            ax.set_xlabel('X')\n            ax.set_ylabel('Y')\n            ax.legend()\n            ax = axes[0, 1]\n            ax.hist(residuals, bins=20, color='green', alpha=0.7)\n            ax.set_title('Residual Histogram')\n            ax.set_xlabel('Residual')\n            ax.set_ylabel('Frequency')\n            sm.qqplot(residuals, line='45', ax=axes[1, 0], color='purple')\n            axes[1, 0].set_title('Q-Q plot of Residuals')\n            fitted = model.fittedvalues\n            ax = axes[1, 1]\n            ax.scatter(fitted, residuals, color='orange')\n            ax.axhline(y=0, color='red', linestyle='--')\n            ax.set_title('Residuals vs Fitted')\n            ax.set_xlabel('Fitted')\n            ax.set_ylabel('Residuals')\n            fig.tight_layout()\n            self._safe_savefig(output_path)\n            return True\n        except Exception as e:\n            logging.error(f'回帰分析グラフ生成中にエラー: {e}')\n            plt.close()\n            return False\n\n    def plot_time_series(self, params, output_path):\n        try:\n            ts = params['time_series']\n            (fig, axes) = plt.subplots(2, 1, figsize=(10, 8))\n            axes[0].plot(ts, color='blue')\n            axes[0].set_title('Time Series Data')\n            axes[0].set_xlabel('Time')\n            axes[0].set_ylabel('Value')\n            plot_acf(ts, ax=axes[1])\n            axes[1].set_title('Autocorrelation Function')\n            fig.tight_layout()\n            self._safe_savefig(output_path)\n            return True\n        except Exception as e:\n            logging.error(f'時系列分析グラフエラー: {e}')\n            plt.close()\n            return False\n\n    def plot_econometrics(self, params, output_path):\n        try:\n            import statsmodels.api as sm\n            X = np.column_stack((params['x1_values'], params['x2_values']))\n            Y = np.array(params['y_values'])\n            Xc = sm.add_constant(X)\n            model = sm.OLS(Y, Xc).fit()\n            residuals = model.resid\n            fitted = model.fittedvalues\n            (fig, axes) = plt.subplots(2, 2, figsize=(10, 10))\n            ax = axes[0, 0]\n            ax.scatter(params['x1_values'], Y, color='blue', alpha=0.7, label='X1-Y')\n            ax.set_title('X1 vs Y')\n            ax.set_xlabel('X1')\n            ax.set_ylabel('Y')\n            ax = axes[0, 1]\n            ax.scatter(params['x2_values'], Y, color='green', alpha=0.7, label='X2-Y')\n            ax.set_title('X2 vs Y')\n            ax.set_xlabel('X2')\n            ax.set_ylabel('Y')\n            ax = axes[1, 0]\n            ax.hist(residuals, bins=20, color='gray', alpha=0.7)\n            ax.set_title('Residuals Histogram')\n            ax.set_xlabel('Residual')\n            ax = axes[1, 1]\n            ax.scatter(fitted, residuals, color='red', alpha=0.7)\n            ax.axhline(y=0, color='black', linestyle='--')\n            ax.set_title('Residuals vs Fitted')\n            ax.set_xlabel('Fitted')\n            ax.set_ylabel('Residuals')\n            fig.tight_layout()\n            self._safe_savefig(output_path)\n            return True\n        except Exception as e:\n            logging.error(f'計量経済学グラフエラー: {e}')\n            plt.close()\n            return False\n\n    def plot_multivariate_normal(self, params, output_path):\n        try:\n            mu = params['mu']\n            sigma = np.array(params['sigma'])\n            fig = plt.figure(figsize=(10, 10))\n            from matplotlib.patches import Ellipse\n            import matplotlib.transforms as transforms\n\n            def confidence_ellipse(mu, cov, ax, n_std=1.96, facecolor='none', **kwargs):\n                pearson = cov[0, 1] / np.sqrt(cov[0, 0] * cov[1, 1])\n                ell_radius_x = np.sqrt(1 + pearson)\n                ell_radius_y = np.sqrt(1 - pearson)\n                ellipse = Ellipse((0, 0), width=ell_radius_x * 2, height=ell_radius_y * 2, facecolor=facecolor, **kwargs)\n                scale_x = np.sqrt(cov[0, 0]) * n_std\n                scale_y = np.sqrt(cov[1, 1]) * n_std\n                transf = transforms.Affine2D().rotate_deg(45).scale(scale_x, scale_y).translate(mu[0], mu[1])\n                ellipse.set_transform(transf + ax.transData)\n                return ax.add_patch(ellipse)\n            ax = fig.add_subplot(2, 2, 1)\n            ax.set_title('Confidence Ellipse')\n            confidence_ellipse(mu, sigma, ax, edgecolor='red')\n            ax.scatter(mu[0], mu[1], c='blue', marker='x', label='mean')\n            ax.legend()\n            ax.set_xlabel('X1')\n            ax.set_ylabel('X2')\n            ax2 = fig.add_subplot(2, 2, 2)\n            x = np.linspace(mu[0] - 4 * np.sqrt(sigma[0, 0]), mu[0] + 4 * np.sqrt(sigma[0, 0]), 100)\n            y = np.linspace(mu[1] - 4 * np.sqrt(sigma[1, 1]), mu[1] + 4 * np.sqrt(sigma[1, 1]), 100)\n            (X, Y) = np.meshgrid(x, y)\n            pos = np.dstack((X, Y))\n\n            def mvn_pdf(xarr, muarr, cov):\n                det = np.linalg.det(cov)\n                inv = np.linalg.inv(cov)\n                diff = xarr - muarr\n                return 1.0 / (2 * np.pi * np.sqrt(det)) * np.exp(-0.5 * (diff @ inv @ diff.T))\n            Z = np.empty(X.shape)\n            for i in range(X.shape[0]):\n                for j in range(X.shape[1]):\n                    Z[i, j] = mvn_pdf(np.array([X[i, j], Y[i, j]]), np.array(mu), sigma)\n            ax2.contour(X, Y, Z, levels=5, cmap='Blues')\n            ax2.set_title('Contour')\n            ax3 = fig.add_subplot(2, 2, 3)\n            X_marg = norm(loc=mu[0], scale=np.sqrt(sigma[0, 0]))\n            x_line = np.linspace(mu[0] - 4 * np.sqrt(sigma[0, 0]), mu[0] + 4 * np.sqrt(sigma[0, 0]), 100)\n            y_line = X_marg.pdf(x_line)\n            ax3.plot(x_line, y_line, 'r-')\n            ax3.set_title('Marginal X1 distribution')\n            ax3.set_xlabel('X1')\n            ax3.set_ylabel('pdf')\n            ax4 = fig.add_subplot(2, 2, 4)\n            Y_marg = norm(loc=mu[1], scale=np.sqrt(sigma[1, 1]))\n            y_line = Y_marg.pdf(x_line)\n            ax4.plot(x_line, y_line, 'g-')\n            ax4.set_title('Marginal X2 distribution')\n            ax4.set_xlabel('X2')\n            ax4.set_ylabel('pdf')\n            fig.tight_layout()\n            self._safe_savefig(output_path)\n            return True\n        except Exception as e:\n            logging.error(f'多変量正規分布グラフエラー: {e}')\n            plt.close()\n            return False\n\n    def plot_distribution_properties(self, params, output_path):\n        try:\n            dist = params['distribution']\n            plt.figure(figsize=(10, 5))\n            if dist == '正規分布':\n                mu = 0\n                sigma = 1\n                x = np.linspace(mu - 4 * sigma, mu + 4 * sigma, 200)\n                y = norm.pdf(x, mu, sigma)\n                plt.plot(x, y, 'b-')\n                plt.axvline(mu, color='r', linestyle='--', label='mean')\n                plt.axvline(mu + sigma, color='g', linestyle=':', label='mean+sigma')\n                plt.axvline(mu - sigma, color='g', linestyle=':')\n                plt.title('Normal Distribution (mu=0, sigma=1)')\n                plt.legend()\n            elif dist == 'ポアソン分布':\n                lam = 3\n                x = np.arange(0, 15)\n                y = poisson.pmf(x, lam)\n                plt.bar(x, y, color='skyblue')\n                plt.axvline(lam, color='r', linestyle='--', label='mean=lambda=3')\n                plt.title('Poisson(lambda=3)')\n                plt.legend()\n            elif dist == '指数分布':\n                lam = 1\n                x = np.linspace(0, 5, 200)\n                y = expon.pdf(x, scale=1 / lam)\n                plt.plot(x, y, 'b-')\n                plt.axvline(1 / lam, color='r', linestyle='--', label='mean=1/lambda')\n                plt.title('Exponential(lambda=1)')\n                plt.legend()\n            plt.tight_layout()\n            self._safe_savefig(output_path)\n            return True\n        except Exception as e:\n            logging.error(f'分布性質グラフエラー: {e}')\n            plt.close()\n            return False\n\n    def plot_high_moment(self, params, output_path):\n        try:\n            mu = params['mu']\n            sigma = params['sigma']\n            n = params['n']\n            moment = params['moment']\n            x = np.linspace(mu - 4 * sigma, mu + 4 * sigma, 200)\n            y = norm.pdf(x, mu, sigma)\n            plt.figure(figsize=(10, 5))\n            plt.plot(x, y, 'b-', label='Normal pdf')\n            plt.axvline(mu, color='r', linestyle='--', label='mean')\n            plt.axvline(mu + sigma, color='g', linestyle=':', label='mu±sigma')\n            plt.axvline(mu - sigma, color='g', linestyle=':')\n            plt.title(f'Normal distribution (mu={mu}, sigma={sigma}). n={n}-th moment={moment}')\n            plt.legend()\n            plt.tight_layout()\n            self._safe_savefig(output_path)\n            return True\n        except Exception as e:\n            logging.error(f'高次モーメントグラフエラー: {e}')\n            plt.close()\n            return False\n\n    def plot_variance_analysis(self, params, output_path):\n        try:\n            group_count = params['group_count']\n            sample_sizes = params['sample_sizes']\n            means = params['means']\n            variances = params['variances']\n            data = []\n            for i in range(group_count):\n                np.random.seed(i)\n                samples = np.random.normal(means[i], np.sqrt(variances[i]), sample_sizes[i])\n                data.append(samples)\n            plt.figure(figsize=(8, 6))\n            plt.boxplot(data, labels=[f'Group{i + 1}' for i in range(group_count)])\n            plt.title('ANOVA: Boxplots of groups')\n            plt.ylabel('Value')\n            plt.tight_layout()\n            self._safe_savefig(output_path)\n            return True\n        except Exception as e:\n            logging.error(f'分散分析グラフエラー: {e}')\n            plt.close()\n            return False\n\n    def plot_nonparametric_test(self, params, output_path):\n        try:\n            s1 = params['sample1']\n            s2 = params['sample2']\n\n            def ecdf(data):\n                d_sorted = np.sort(data)\n                y = np.arange(1, len(d_sorted) + 1) / len(d_sorted)\n                return (d_sorted, y)\n            (x1, y1) = ecdf(s1)\n            (x2, y2) = ecdf(s2)\n            plt.figure(figsize=(8, 6))\n            plt.step(x1, y1, where='post', label='Sample1 ECDF', color='blue')\n            plt.step(x2, y2, where='post', label='Sample2 ECDF', color='red')\n            plt.title('Nonparametric Test Visualization')\n            plt.xlabel('Value')\n            plt.ylabel('ECDF')\n            plt.legend()\n            plt.tight_layout()\n            self._safe_savefig(output_path)\n            return True\n        except Exception as e:\n            logging.error(f'ノンパラ検定グラフエラー: {e}')\n            plt.close()\n            return False\n\n    def plot_linear_combination(self, params, output_path):\n        try:\n            a = params['a']\n            b = params['b']\n            mu1 = params['mu1']\n            mu2 = params['mu2']\n            sig1 = params['sigma1_squared']\n            sig2 = params['sigma2_squared']\n            np.random.seed(123)\n            x = np.random.normal(mu1, np.sqrt(sig1), 1000)\n            y = np.random.normal(mu2, np.sqrt(sig2), 1000)\n            Z = a * x + b * y\n            plt.figure(figsize=(8, 6))\n            plt.hist(Z, bins=30, density=True, alpha=0.7, color='purple', label='Simulated Z')\n            E_Z = a * mu1 + b * mu2\n            Var_Z = a ** 2 * sig1 + b ** 2 * sig2\n            X_line = np.linspace(E_Z - 4 * np.sqrt(Var_Z), E_Z + 4 * np.sqrt(Var_Z), 200)\n            Y_line = norm.pdf(X_line, E_Z, np.sqrt(Var_Z))\n            plt.plot(X_line, Y_line, 'r-', label='Theoretical PDF')\n            plt.title('Linear Combination Distribution')\n            plt.xlabel('Z')\n            plt.ylabel('Density')\n            plt.legend()\n            plt.tight_layout()\n            self._safe_savefig(output_path)\n            return True\n        except Exception as e:\n            logging.error(f'線形結合グラフエラー: {e}')\n            plt.close()\n            return False"
    },
    {
      "path": "your_project\\統計検定1級\\gui.py",
      "overview": "Pythonコード。\nクラス: InteractiveSolverGUI。\n関数: __init__, run。\n",
      "content": "import tkinter as tk\nimport tkinter.font as tkfont\nfrom config import config\n\nclass InteractiveSolverGUI:\n\n    def __init__(self):\n        self.root = tk.Tk()\n        self.root.title(config.get('gui_settings', 'window_title', default='統計検定1級 インタラクティブ問題解答システム'))\n        font_size = config.get('gui_settings', 'font_size', default=14)\n        tkfont.nametofont('TkDefaultFont').configure(size=font_size)\n\n    def run(self):\n        self.root.mainloop()"
    },
    {
      "path": "your_project\\統計検定1級\\main.py",
      "overview": "Pythonコード。\n関数: main。\n",
      "content": "import sys\nimport os\nif __name__ == '__main__' and __package__ is None:\n    parent_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n    sys.path.insert(0, parent_dir)\n    __package__ = 'your_project'\nfrom main_app import MainApp\nfrom gui import InteractiveSolverGUI\nfrom config import config\nimport logging\nlog_level = config.get('log_level', default='INFO')\nlogging.basicConfig(level=getattr(logging, log_level.upper(), logging.INFO))\n\ndef main():\n    pdf_gen = config.get('pdf_generation', default={})\n    problem_count = pdf_gen.get('problem_count', 9)\n    if len(sys.argv) > 1 and sys.argv[1] == '--generate-pdf':\n        app = MainApp()\n        app.generate_and_compile(problem_count)\n    else:\n        gui = InteractiveSolverGUI()\n        gui.run()\nif __name__ == '__main__':\n    app = MainApp()\n    app.generate_and_compile(1)"
    },
    {
      "path": "your_project\\統計検定1級\\main_app.py",
      "overview": "Pythonコード。\nクラス: MainApp。\n関数: __init__, generate_latex_header, generate_and_compile, compile_latex。\n",
      "content": "import os, subprocess\nfrom config import config\nfrom problem_generator import ProblemGenerator\nimport logging\nimport traceback\n\nclass MainApp:\n\n    def __init__(self):\n        self.output_tex_file = config.get('output_tex_file', default='practice_problems.tex')\n        if self.output_tex_file is None:\n            self.output_tex_file = 'practice_problems.tex'\n        self.enable_visualization = config.get('enable_visualization', default=True)\n        self.problem_templates_dir = config.get('problem_templates_directory', default='templates')\n        self.output_directory = config.get('output_directory', default='outputs')\n        self.latex_path = config.get('latex_path', default='xelatex')\n        self.generator = ProblemGenerator()\n\n    def generate_latex_header(self):\n        cjk_font = config.get('cjk_main_font', default='Yu Gothic')\n        if cjk_font is None:\n            cjk_font = 'Yu Gothic'\n        header = ['\\\\documentclass{article}', '\\\\usepackage{amsmath}', '\\\\usepackage{amssymb}', '\\\\usepackage{graphicx}', '\\\\usepackage{float}', '\\\\usepackage{geometry}', '\\\\usepackage{xeCJK}', '\\\\usepackage{fontspec}', '\\\\setmainfont{Times New Roman}', f'\\\\setCJKmainfont{{{cjk_font}}}', '\\\\geometry{a4paper, margin=1in}', '\\\\begin{document}']\n        return header\n\n    def generate_and_compile(self, problem_count):\n        script_dir = os.path.dirname(os.path.abspath(__file__))\n        latex_content = self.generate_latex_header()\n        out_dir = self.output_directory\n        if out_dir is None:\n            out_dir = 'outputs'\n        output_dir = os.path.join(script_dir, out_dir)\n        os.makedirs(output_dir, exist_ok=True)\n        tex_file_path = os.path.join(output_dir, self.output_tex_file)\n        for idx in range(1, problem_count + 1):\n            try:\n                result = self.generator.generate_problem()\n                if result:\n                    (problem_id, date_created, problem_type, problem_text, solution_text, graph_filename) = result\n                    latex_content.append(f'\\\\section*{{問題 {idx}}}')\n                    latex_content.append(problem_text)\n                    if self.enable_visualization and graph_filename:\n                        latex_content.append('\\\\begin{figure}[H]')\n                        latex_content.append('\\\\centering')\n                        graph_relative_path = os.path.join('graphs', graph_filename).replace('\\\\', '/')\n                        latex_content.append(f'\\\\includegraphics[width=0.8\\\\textwidth]{{{graph_relative_path}}}')\n                        latex_content.append('\\\\end{figure}')\n                    latex_content.append('\\\\subsection*{解答}')\n                    latex_content.append(solution_text)\n                    latex_content.append('\\\\newpage')\n                else:\n                    logging.warning(f'問題 {idx} の生成に失敗しました。')\n                    print(f'問題 {idx} の生成に失敗しました。')\n            except Exception as e:\n                logging.error(f'問題 {idx} の生成中にエラー: {e}')\n                logging.error(traceback.format_exc())\n                print(f'問題 {idx} 生成エラー。ログを確認')\n        templates_dir = self.problem_templates_dir\n        if templates_dir is None:\n            templates_dir = 'templates'\n        latex_content.append('\\\\clearpage')\n        latex_content.append('\\\\input{../' + templates_dir + '/distribution_relations.tex}')\n        latex_content.append('\\\\end{document}')\n        with open(tex_file_path, 'w', encoding='utf-8') as tex_file:\n            tex_file.write('\\n'.join(latex_content))\n        self.compile_latex(tex_file_path)\n\n    def compile_latex(self, tex_file_path):\n        tex_file_name = os.path.basename(tex_file_path)\n        latex_path = self.latex_path\n        if not latex_path or not os.path.exists(latex_path):\n            print(f'LaTeXコンパイラが見つかりません: {latex_path}')\n            return\n        process = subprocess.run([latex_path, '-interaction=nonstopmode', tex_file_name], cwd=os.path.dirname(tex_file_path), stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n        log_file_path = os.path.join(os.path.dirname(tex_file_path), 'latex_compile.log')\n        with open(log_file_path, 'w', encoding='utf-8') as f:\n            f.write(process.stdout or '')\n            f.write(process.stderr or '')\n        if process.returncode != 0:\n            print('LaTeXコンパイルでエラー')\n        else:\n            pdf_file = tex_file_path.replace('.tex', '.pdf')\n            if os.path.exists(pdf_file):\n                print(f'PDF生成成功: {pdf_file}')\n            else:\n                print('PDFファイル未発見')"
    },
    {
      "path": "your_project\\統計検定1級\\problem_generator.py",
      "overview": "Pythonコード。\nクラス: ProblemGenerator。\n関数: __init__, load_topics, get_problem_types_by_topic, generate_problem。\n",
      "content": "import random, os, logging, uuid\nfrom datetime import datetime\nfrom config import config\nfrom database import DatabaseManager\nfrom problem_types.problem_factory import ProblemFactory\nimport json\nimport traceback\n\nclass ProblemGenerator:\n\n    def __init__(self):\n        self.db_path = config.get('database_path', default='data/data.db')\n        self.output_directory = config.get('output_directory', default='outputs')\n        self.enable_visualization = config.get('enable_visualization', default=True)\n        self.problem_types_weights = config.get('problem_types', default={})\n        self.factory = ProblemFactory()\n        self.db_manager = DatabaseManager()\n        self.db_manager.setup_database()\n        script_dir = os.path.dirname(os.path.abspath(__file__))\n        out_dir = self.output_directory\n        if out_dir is None:\n            out_dir = 'outputs'\n        self.output_dir = os.path.join(script_dir, out_dir, 'graphs')\n        os.makedirs(self.output_dir, exist_ok=True)\n        self.topics_data = self.load_topics()\n\n    def load_topics(self):\n        script_dir = os.path.dirname(os.path.abspath(__file__))\n        topics_file = os.path.join(script_dir, 'topics.json')\n        if not os.path.exists(topics_file):\n            raise FileNotFoundError(f\"'{topics_file}'がない\")\n        with open(topics_file, 'r', encoding='utf-8') as f:\n            return json.load(f)\n\n    def get_problem_types_by_topic(self, topic):\n        if 'topics' not in self.topics_data or topic not in self.topics_data['topics']:\n            return []\n        return self.topics_data['topics'][topic]\n\n    def generate_problem(self):\n        try:\n            if not self.problem_types_weights:\n                raise ValueError('問題タイプの重みが設定されていません。')\n            types = list(self.problem_types_weights.keys())\n            weights = list(self.problem_types_weights.values())\n            problem_type = random.choices(types, weights=weights, k=1)[0]\n            problem = self.factory.create_problem(problem_type)\n            problem.generate_parameters()\n            problem_text = problem.generate_problem_text()\n            solution_text = problem.generate_solution_text()\n            enable_vis = self.enable_visualization and hasattr(problem, 'generate_graph')\n            graph_filename = None\n            if enable_vis:\n                graph_id = str(uuid.uuid4()) + '.png'\n                graph_path = os.path.join(self.output_dir, graph_id)\n                success = problem.generate_graph(graph_path)\n                if success:\n                    graph_filename = graph_id\n            problem_id = str(uuid.uuid4())\n            date_created = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n            self.db_manager.save_problem(problem_id, date_created, problem_text, solution_text, problem_type)\n            return (problem_id, date_created, problem_type, problem_text, solution_text, graph_filename)\n        except Exception as e:\n            logging.error(f'問題生成中にエラー: {e}')\n            logging.error(traceback.format_exc())\n            return None"
    },
    {
      "path": "your_project\\統計検定1級\\sympy_solver.py",
      "overview": "Pythonコード。\nクラス: SympySolver。\n関数: __init__, check_equivalence。\n",
      "content": "class SympySolver:\n\n    def __init__(self):\n        pass\n\n    def check_equivalence(self, user_input, correct_answer):\n        return (False, '')"
    },
    {
      "path": "your_project\\統計検定1級\\topics.json",
      "overview": "JSONファイル (辞書)。キー: topics\n",
      "content": "{\n  \"topics\": {\n    \"確率論\": [\n      \"probability_definition\",\n      \"conditional_probability\",\n      \"distribution_functions\",\n      \"joint_distribution\",\n      \"probability\"\n    ],\n    \"統計的推定\": [\n      \"statistical_inference\",\n      \"t_test\"\n    ],\n    \"統計的検定\": [\n      \"statistical_inference\",\n      \"t_test\"\n    ],\n    \"回帰分析\": [\n      \"regression_analysis\"\n    ],\n    \"分散分析\": [\n      \"variance_analysis\"\n    ],\n    \"ノンパラメトリック検定\": [\n      \"nonparametric_test\"\n    ]\n  }\n}\n"
    },
    {
      "path": "your_project\\統計検定1級\\.vscode\\launch.json",
      "overview": "JSONファイル (辞書)。キー: version, configurations\n",
      "content": "{\n    \"version\": \"0.2.0\",\n    \"configurations\": [\n        {\n            \"name\": \"Python: 現在のファイル\",\n            \"type\": \"python\",\n            \"request\": \"launch\",\n            \"program\": \"${file}\",\n            \"console\": \"integratedTerminal\",\n            \"justMyCode\": true,\n            \"args\": [\"--generate-pdf\"]\n\n        }\n    ]\n}"
    },
    {
      "path": "your_project\\統計検定1級\\problem_types\\conjugate_problems.py",
      "overview": "Pythonコード。\nクラス: BetaBinomialConjugateProblem, GammaPoissonConjugateProblem, DirichletMultinomialConjugateProblem, BinomialPoissonApproxProblem, PoissonNormalApproxProblem。\n関数: __init__, generate_parameters, generate_explanation, generate_graph, __init__, generate_parameters, generate_explanation, generate_graph, __init__, generate_parameters, generate_explanation, generate_graph, __init__, generate_parameters, generate_explanation, generate_graph, __init__, generate_parameters, generate_explanation, generate_graph, B, B。\n",
      "content": "import math, random\nfrom problem_types.problem import Problem\nfrom math import comb, factorial, exp, gamma\nimport numpy as np\n\nclass BetaBinomialConjugateProblem(Problem):\n\n    def __init__(self):\n        super().__init__('beta_binomial_conjugate_problem.tex')\n\n    def generate_parameters(self):\n        self.params['alpha'] = random.randint(1, 5)\n        self.params['beta'] = random.randint(1, 5)\n        self.params['n'] = random.randint(5, 20)\n        self.params['k'] = random.randint(0, self.params['n'])\n\n        def B(x, y):\n            return gamma(x) * gamma(y) / gamma(x + y)\n        p_x = comb(self.params['n'], self.params['k']) * B(self.params['k'] + self.params['alpha'], self.params['n'] - self.params['k'] + self.params['beta']) / B(self.params['alpha'], self.params['beta'])\n        self.params['probability'] = round(p_x, 4)\n\n    def generate_explanation(self):\n        return 'Beta+Binomial->Beta-Binomial'\n\n    def generate_graph(self, o):\n        return False\n\nclass GammaPoissonConjugateProblem(Problem):\n\n    def __init__(self):\n        super().__init__('gamma_poisson_conjugate_problem.tex')\n\n    def generate_parameters(self):\n        self.params['alpha'] = random.randint(1, 5)\n        self.params['beta'] = random.uniform(0.5, 2.0)\n        self.params['k'] = random.randint(0, 20)\n        p = self.params['beta'] / (self.params['beta'] + 1)\n        q = 1 - p\n        negbin_p = comb(self.params['k'] + self.params['alpha'] - 1, self.params['k']) * q ** self.params['k'] * p ** self.params['alpha']\n        self.params['probability'] = round(negbin_p, 4)\n\n    def generate_explanation(self):\n        return 'Gamma+Poisson->Negative Binomial'\n\n    def generate_graph(self, o):\n        return False\n\nclass DirichletMultinomialConjugateProblem(Problem):\n\n    def __init__(self):\n        super().__init__('dirichlet_multinomial_conjugate_problem.tex')\n\n    def generate_parameters(self):\n        m = random.randint(2, 4)\n        self.params['m'] = m\n        self.params['n'] = random.randint(5, 20)\n        self.params['alpha_vec'] = [random.uniform(1, 3) for _ in range(m)]\n        counts = [0] * m\n        remain = self.params['n']\n        for i in range(m - 1):\n            c = random.randint(0, remain)\n            counts[i] = c\n            remain -= c\n        counts[-1] = remain\n        self.params['counts'] = counts\n\n        def B(alpha):\n            import numpy as np\n            return np.prod([gamma(a) for a in alpha]) / gamma(sum(alpha))\n        alpha_x = [self.params['alpha_vec'][i] + counts[i] for i in range(m)]\n        num = B(alpha_x)\n        den = B(self.params['alpha_vec'])\n        multinomial_coef = math.factorial(self.params['n'])\n        for c in counts:\n            multinomial_coef /= math.factorial(c)\n        p_x = multinomial_coef * (num / den)\n        self.params['probability'] = round(p_x, 4)\n\n    def generate_explanation(self):\n        return 'Dirichlet+Multinomial->Dirichlet-Multinomial'\n\n    def generate_graph(self, o):\n        return False\n\nclass BinomialPoissonApproxProblem(Problem):\n\n    def __init__(self):\n        super().__init__('binomial_poisson_approx_problem.tex')\n\n    def generate_parameters(self):\n        lam = random.uniform(2, 5)\n        n = random.randint(50, 200)\n        p = lam / n\n        k = random.randint(0, int(lam * 3))\n        binom_p = comb(n, k) * p ** k * (1 - p) ** (n - k)\n        poisson_p = lam ** k * exp(-lam) / math.factorial(k)\n        self.params['n'] = n\n        self.params['p'] = round(p, 6)\n        self.params['k'] = k\n        self.params['lambda'] = round(lam, 3)\n        self.params['binom_p'] = round(binom_p, 6)\n        self.params['poisson_p'] = round(poisson_p, 6)\n\n    def generate_explanation(self):\n        return 'Binomial->Poisson近似条件'\n\n    def generate_graph(self, o):\n        return False\n\nclass PoissonNormalApproxProblem(Problem):\n\n    def __init__(self):\n        super().__init__('poisson_normal_approx_problem.tex')\n\n    def generate_parameters(self):\n        lam = random.randint(30, 100)\n        low = max(0, int(lam - 3 * math.sqrt(lam)))\n        high = int(lam + 3 * math.sqrt(lam))\n        k = random.randint(low, high)\n        poisson_p = lam ** k * exp(-lam) / math.factorial(k)\n        self.params['lambda'] = lam\n        self.params['k'] = k\n        self.params['poisson_p'] = round(poisson_p, 6)\n        self.params['mean'] = lam\n        self.params['variance'] = lam\n\n    def generate_explanation(self):\n        return 'Poisson->Normal近似(λ大)'\n\n    def generate_graph(self, o):\n        return False"
    },
    {
      "path": "your_project\\統計検定1級\\problem_types\\problem.py",
      "overview": "Pythonコード。\nクラス: Problem, ProbabilityProblem, StatisticalInferenceProblem, RegressionAnalysisProblem, TimeSeriesAnalysisProblem, EconometricsProblem, LinearCombinationProblem, DistributionPropertiesProblem, HighMomentProblem, MultivariateNormalProblem, VarianceAnalysisProblem, NonParametricTestProblem, ProbabilityDefinitionProblem, ConditionalProbabilityProblem, DistributionFunctionsProblem, JointDistributionProblem。\n関数: __init__, generate_parameters, generate_problem_text, generate_solution_text, generate_explanation, generate_graph, __init__, generate_parameters, _variant_binomial, _variant_poisson, _variant_conditional_probability, generate_explanation, generate_graph, __init__, generate_parameters, generate_explanation, generate_graph, __init__, generate_parameters, generate_explanation, generate_graph, __init__, generate_parameters, generate_explanation, generate_graph, __init__, generate_parameters, generate_explanation, generate_graph, __init__, generate_parameters, generate_explanation, generate_graph, __init__, generate_parameters, generate_explanation, generate_graph, __init__, generate_parameters, generate_explanation, generate_graph, __init__, generate_parameters, generate_explanation, generate_graph, __init__, generate_parameters, generate_explanation, generate_graph, __init__, generate_parameters, generate_explanation, generate_graph, __init__, __init__, __init__, __init__。\n",
      "content": "from abc import ABC, abstractmethod\nimport os\nimport random\nimport logging\nfrom config import config\nfrom jinja2 import Environment, FileSystemLoader\nfrom graph import ProbabilityDistributionVisualizer\nimport traceback\nfrom math import comb, exp, factorial\nfrom scipy.stats import norm, stats\n\nclass Problem(ABC):\n\n    def __init__(self, template_name):\n        self.params = {}\n        self.template_name = template_name\n        templates_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), '..', config.get('problem_templates_directory', default='templates'))\n        self.env = Environment(loader=FileSystemLoader(templates_dir), autoescape=False)\n        self.visualizer = ProbabilityDistributionVisualizer()\n\n    @abstractmethod\n    def generate_parameters(self):\n        pass\n\n    def generate_problem_text(self):\n        template = self.env.get_template(self.template_name)\n        return template.render(**self.params, show_solution=False)\n\n    def generate_solution_text(self):\n        template = self.env.get_template(self.template_name)\n        self.params['explanation'] = self.generate_explanation()\n        return template.render(**self.params, show_solution=True)\n\n    def generate_explanation(self):\n        return ''\n\n    @abstractmethod\n    def generate_graph(self, output_path):\n        pass\n\nclass ProbabilityProblem(Problem):\n\n    def __init__(self):\n        super().__init__('probability_problem.tex')\n\n    def generate_parameters(self):\n        variants = [self._variant_binomial, self._variant_poisson, self._variant_conditional_probability]\n        v = random.choice(variants)\n        v()\n\n    def _variant_binomial(self):\n        self.params['problem_type'] = 'binomial'\n        self.params['n'] = random.randint(5, 20)\n        self.params['p'] = round(random.uniform(0.1, 0.9), 2)\n        self.params['k'] = random.randint(0, self.params['n'])\n        from math import comb\n        prob = comb(self.params['n'], self.params['k']) * self.params['p'] ** self.params['k'] * (1 - self.params['p']) ** (self.params['n'] - self.params['k'])\n        self.params['probability'] = round(prob, 6)\n\n    def _variant_poisson(self):\n        self.params['problem_type'] = 'poisson'\n        self.params['lambda'] = round(random.uniform(0.5, 5.0), 2)\n        self.params['k'] = random.randint(0, 10)\n        lam = self.params['lambda']\n        k = self.params['k']\n        prob = lam ** k * exp(-lam) / factorial(k)\n        self.params['probability'] = round(prob, 6)\n\n    def _variant_conditional_probability(self):\n        self.params['problem_type'] = 'conditional_probability'\n        self.params['P_A'] = round(random.uniform(0.1, 0.9), 2)\n        self.params['P_B_given_A'] = round(random.uniform(0.1, 0.9), 2)\n        self.params['P_A_and_B'] = round(self.params['P_A'] * self.params['P_B_given_A'], 6)\n\n    def generate_explanation(self):\n        t = self.params['problem_type']\n        if t == 'binomial':\n            return '二項分布の公式を使用'\n        elif t == 'poisson':\n            return 'ポアソン分布の公式を使用'\n        elif t == 'conditional_probability':\n            return '条件付き確率P(A∩B)=P(A)*P(B|A)'\n\n    def generate_graph(self, output_path):\n        return self.visualizer.plot_probability(self.params, output_path)\n\nclass StatisticalInferenceProblem(Problem):\n\n    def __init__(self):\n        super().__init__('statistical_inference_problem.tex')\n\n    def generate_parameters(self):\n        self.params['sample_mean'] = round(random.uniform(50, 100), 2)\n        self.params['sample_std'] = round(random.uniform(5, 15), 2)\n        self.params['n'] = random.randint(30, 50)\n        self.params['alpha'] = 0.05\n        df = self.params['n'] - 1\n        self.params['critical_value'] = round(abs(stats.t.ppf(self.params['alpha'], df)), 4)\n        self.params['t_stat'] = round(random.uniform(-5, 5), 4)\n\n    def generate_explanation(self):\n        return f\"有意水準{int(self.params['alpha'] * 100)}%での検定。\"\n\n    def generate_graph(self, output_path):\n        return self.visualizer.plot_t_test(self.params, output_path)\n\nclass RegressionAnalysisProblem(Problem):\n\n    def __init__(self):\n        super().__init__('regression_analysis_problem.tex')\n\n    def generate_parameters(self):\n        self.params['beta_0'] = round(random.uniform(0, 10), 2)\n        self.params['beta_1'] = round(random.uniform(0, 5), 2)\n        self.params['n'] = random.randint(10, 30)\n        x_values = [random.uniform(0, 10) for _ in range(self.params['n'])]\n        self.params['data_points'] = [(x, round(self.params['beta_0'] + self.params['beta_1'] * x + random.gauss(0, 1), 2)) for x in x_values]\n\n    def generate_explanation(self):\n        return '回帰直線の推定値に基づく予測。'\n\n    def generate_graph(self, output_path):\n        x_vals = [x for (x, y) in self.params['data_points']]\n        y_vals = [y for (x, y) in self.params['data_points']]\n        try:\n            import matplotlib.pyplot as plt\n            plt.figure()\n            plt.scatter(x_vals, y_vals, color='blue', label='Data points')\n            fitted_vals = [self.params['beta_0'] + self.params['beta_1'] * x for x in x_vals]\n            plt.plot(x_vals, fitted_vals, color='red', label='Fitted line')\n            plt.title('Regression Line')\n            plt.legend()\n            plt.savefig(output_path)\n            plt.close()\n            return True\n        except:\n            return False\n\nclass TimeSeriesAnalysisProblem(Problem):\n\n    def __init__(self):\n        super().__init__('time_series_analysis_problem.tex')\n\n    def generate_parameters(self):\n        self.params['phi'] = round(random.uniform(0.1, 0.9), 2)\n        self.params['sigma'] = round(random.uniform(0.5, 2.0), 2)\n        self.params['n'] = random.randint(50, 100)\n        data = [0]\n        for i in range(1, self.params['n']):\n            data.append(self.params['phi'] * data[i - 1] + random.gauss(0, self.params['sigma']))\n        self.params['time_series_data'] = [round(x, 2) for x in data]\n\n    def generate_explanation(self):\n        return '自己回帰モデル AR(1) のプロセスから生成。'\n\n    def generate_graph(self, output_path):\n        try:\n            import matplotlib.pyplot as plt\n            plt.figure()\n            plt.plot(self.params['time_series_data'], label='Time Series')\n            plt.title('AR(1) Time Series')\n            plt.xlabel('Time')\n            plt.ylabel('Value')\n            plt.legend()\n            plt.savefig(output_path)\n            plt.close()\n            return True\n        except:\n            return False\n\nclass EconometricsProblem(Problem):\n\n    def __init__(self):\n        super().__init__('econometrics_problem.tex')\n\n    def generate_parameters(self):\n        self.params['beta_0'] = round(random.uniform(0, 5), 2)\n        self.params['beta_1'] = round(random.uniform(0, 5), 2)\n        self.params['gdp_growth'] = round(random.uniform(-5, 10), 2)\n        self.params['inflation_rate'] = round(random.uniform(0, 10), 2)\n\n    def generate_explanation(self):\n        return '単回帰モデルのパラメータに基づく指標。'\n\n    def generate_graph(self, output_path):\n        try:\n            import matplotlib.pyplot as plt\n            plt.figure()\n            plt.scatter(self.params['inflation_rate'], self.params['gdp_growth'], color='green')\n            plt.title('Econometrics Scatter')\n            plt.xlabel('Inflation Rate (%)')\n            plt.ylabel('GDP Growth (%)')\n            plt.savefig(output_path)\n            plt.close()\n            return True\n        except:\n            return False\n\nclass LinearCombinationProblem(Problem):\n\n    def __init__(self):\n        super().__init__('linear_combination_problem.tex')\n\n    def generate_parameters(self):\n        self.params['a'] = random.randint(1, 5)\n        self.params['b'] = random.randint(1, 5)\n        self.params['x'] = random.randint(10, 50)\n        self.params['y'] = random.randint(10, 50)\n\n    def generate_explanation(self):\n        return '線形結合における係数と変数の組み合わせ。'\n\n    def generate_graph(self, output_path):\n        try:\n            import matplotlib.pyplot as plt\n            import numpy as np\n            X = np.linspace(0, self.params['x'], 100)\n            Y = self.params['a'] * X / self.params['b']\n            plt.figure()\n            plt.plot(X, Y, label=f\"{self.params['a']}*x + {self.params['b']}*y = C\")\n            plt.title('Linear Combination')\n            plt.xlabel('x')\n            plt.ylabel('y')\n            plt.legend()\n            plt.savefig(output_path)\n            plt.close()\n            return True\n        except:\n            return False\n\nclass DistributionPropertiesProblem(Problem):\n\n    def __init__(self):\n        super().__init__('distribution_properties_problem.tex')\n\n    def generate_parameters(self):\n        self.params['mean'] = round(random.uniform(0, 10), 2)\n        self.params['variance'] = round(random.uniform(1, 5), 2)\n        self.params['skewness'] = round(random.uniform(-1, 1), 2)\n        self.params['kurtosis'] = round(random.uniform(2, 5), 2)\n\n    def generate_explanation(self):\n        return '正規分布の性質に関する指標。'\n\n    def generate_graph(self, output_path):\n        try:\n            import matplotlib.pyplot as plt\n            import numpy as np\n            from scipy.stats import norm\n            mu = self.params['mean']\n            sigma = self.params['variance'] ** 0.5\n            x = np.linspace(mu - 4 * sigma, mu + 4 * sigma, 200)\n            y = norm.pdf(x, mu, sigma)\n            plt.figure()\n            plt.plot(x, y, label=f'N({mu}, {sigma ** 2}) PDF')\n            plt.title('Normal Distribution PDF')\n            plt.xlabel('x')\n            plt.ylabel('Density')\n            plt.legend()\n            plt.savefig(output_path)\n            plt.close()\n            return True\n        except:\n            return False\n\nclass HighMomentProblem(Problem):\n\n    def __init__(self):\n        super().__init__('high_moment_problem.tex')\n\n    def generate_parameters(self):\n        self.params['values'] = [round(random.uniform(-5, 5), 2) for _ in range(100)]\n        n = len(self.params['values'])\n        mean_val = sum(self.params['values']) / n\n        m3 = sum(((x - mean_val) ** 3 for x in self.params['values'])) / n\n        m4 = sum(((x - mean_val) ** 4 for x in self.params['values'])) / n\n        self.params['skewness'] = round(m3 / (self.params['values'][0] if self.params['values'][0] != 0 else 1), 4)\n        self.params['kurtosis'] = round(m4 / (self.params['values'][0] if self.params['values'][0] != 0 else 1) ** 2, 4)\n\n    def generate_explanation(self):\n        return '高次モーメントの計算結果。'\n\n    def generate_graph(self, output_path):\n        try:\n            import matplotlib.pyplot as plt\n            plt.figure()\n            plt.hist(self.params['values'], bins=20, color='skyblue', edgecolor='black')\n            plt.title('Value Distribution')\n            plt.xlabel('Value')\n            plt.ylabel('Frequency')\n            plt.savefig(output_path)\n            plt.close()\n            return True\n        except:\n            return False\n\nclass MultivariateNormalProblem(Problem):\n\n    def __init__(self):\n        super().__init__('multivariate_normal_problem.tex')\n\n    def generate_parameters(self):\n        self.params['mean_x'] = round(random.uniform(0, 5), 2)\n        self.params['mean_y'] = round(random.uniform(0, 5), 2)\n        self.params['var_x'] = round(random.uniform(1, 3), 2)\n        self.params['var_y'] = round(random.uniform(1, 3), 2)\n        self.params['corr_xy'] = round(random.uniform(0, 0.9), 2)\n\n    def generate_explanation(self):\n        return '多変量正規分布のパラメータに基づく性質。'\n\n    def generate_graph(self, output_path):\n        try:\n            import numpy as np\n            import matplotlib.pyplot as plt\n            from matplotlib.patches import Ellipse\n            mean = [self.params['mean_x'], self.params['mean_y']]\n            cov = [[self.params['var_x'], self.params['corr_xy'] * self.params['var_x'] ** 0.5 * self.params['var_y'] ** 0.5], [self.params['corr_xy'] * self.params['var_x'] ** 0.5 * self.params['var_y'] ** 0.5, self.params['var_y']]]\n            (vals, vecs) = np.linalg.eigh(cov)\n            order = vals.argsort()[::-1]\n            (vals, vecs) = (vals[order], vecs[:, order])\n            theta = np.degrees(np.arctan2(*vecs[:, 0][::-1]))\n            (w, h) = 2 * np.sqrt(vals)\n            ell = Ellipse(xy=mean, width=w, height=h, angle=theta, edgecolor='red', facecolor='none', label='Confidence Ellipse')\n            (fig, ax) = plt.subplots()\n            ax.add_patch(ell)\n            ax.scatter(mean[0], mean[1], c='red', label='Mean')\n            ax.set_title('Multivariate Normal Confidence Ellipse')\n            ax.set_xlabel('X')\n            ax.set_ylabel('Y')\n            ax.legend()\n            plt.savefig(output_path)\n            plt.close()\n            return True\n        except:\n            return False\n\nclass VarianceAnalysisProblem(Problem):\n\n    def __init__(self):\n        super().__init__('variance_analysis_problem.tex')\n\n    def generate_parameters(self):\n        group1 = [round(random.uniform(0, 10), 2) for _ in range(10)]\n        group2 = [round(random.uniform(0, 10), 2) for _ in range(10)]\n        self.params['group1_mean'] = round(sum(group1) / len(group1), 2)\n        self.params['group2_mean'] = round(sum(group2) / len(group2), 2)\n        self.params['ss_between'] = round((self.params['group1_mean'] - self.params['group2_mean']) ** 2 * len(group1), 2)\n        self.params['ss_within'] = round(sum(((x - self.params['group1_mean']) ** 2 for x in group1)) + sum(((y - self.params['group2_mean']) ** 2 for y in group2)), 2)\n\n    def generate_explanation(self):\n        return '2群間の平均の差の分析。'\n\n    def generate_graph(self, output_path):\n        try:\n            import matplotlib.pyplot as plt\n            data = [self.params['group1_mean'], self.params['group2_mean']]\n            plt.figure()\n            plt.bar(['Group1', 'Group2'], data, color=['blue', 'orange'])\n            plt.title('Group Means Comparison')\n            plt.ylabel('Mean Value')\n            plt.savefig(output_path)\n            plt.close()\n            return True\n        except:\n            return False\n\nclass NonParametricTestProblem(Problem):\n\n    def __init__(self):\n        super().__init__('nonparametric_test_problem.tex')\n\n    def generate_parameters(self):\n        self.params['sample1'] = [round(random.uniform(0, 50), 2) for _ in range(15)]\n        self.params['sample2'] = [round(random.uniform(0, 50), 2) for _ in range(15)]\n\n    def generate_explanation(self):\n        return 'ノンパラメトリック検定のための順位データ。'\n\n    def generate_graph(self, output_path):\n        try:\n            import matplotlib.pyplot as plt\n            sample1 = sorted(self.params['sample1'])\n            sample2 = sorted(self.params['sample2'])\n            n1 = len(sample1)\n            n2 = len(sample2)\n            plt.figure()\n            plt.step(sample1, [i / n1 for i in range(1, n1 + 1)], label='Sample1 ECDF')\n            plt.step(sample2, [i / n2 for i in range(1, n2 + 1)], label='Sample2 ECDF')\n            plt.title('ECDF Comparison')\n            plt.xlabel('Value')\n            plt.ylabel('ECDF')\n            plt.legend()\n            plt.savefig(output_path)\n            plt.close()\n            return True\n        except:\n            return False\n\nclass ProbabilityDefinitionProblem(ProbabilityProblem):\n\n    def __init__(self):\n        super().__init__()\n\nclass ConditionalProbabilityProblem(ProbabilityProblem):\n\n    def __init__(self):\n        super().__init__()\n\nclass DistributionFunctionsProblem(ProbabilityProblem):\n\n    def __init__(self):\n        super().__init__()\n\nclass JointDistributionProblem(ProbabilityProblem):\n\n    def __init__(self):\n        super().__init__()"
    },
    {
      "path": "your_project\\統計検定1級\\problem_types\\problem_factory.py",
      "overview": "Pythonコード。\nクラス: ProblemFactory。\n関数: __init__, create_problem。\n",
      "content": "import logging, traceback\nfrom problem_types.problem import ProbabilityProblem, StatisticalInferenceProblem, RegressionAnalysisProblem, TimeSeriesAnalysisProblem, EconometricsProblem, LinearCombinationProblem, DistributionPropertiesProblem, HighMomentProblem, MultivariateNormalProblem, VarianceAnalysisProblem, NonParametricTestProblem, ProbabilityDefinitionProblem, ConditionalProbabilityProblem, DistributionFunctionsProblem, JointDistributionProblem\nfrom problem_types.conjugate_problems import BetaBinomialConjugateProblem, GammaPoissonConjugateProblem, DirichletMultinomialConjugateProblem, BinomialPoissonApproxProblem, PoissonNormalApproxProblem\n\nclass ProblemFactory:\n\n    def __init__(self):\n        self.problem_classes = {'probability': ProbabilityProblem, 'statistical_inference': StatisticalInferenceProblem, 'regression_analysis': RegressionAnalysisProblem, 'time_series_analysis': TimeSeriesAnalysisProblem, 'econometrics': EconometricsProblem, 'linear_combination': LinearCombinationProblem, 'distribution_properties': DistributionPropertiesProblem, 'high_moment': HighMomentProblem, 'multivariate_normal': MultivariateNormalProblem, 'probability_definition': ProbabilityDefinitionProblem, 'conditional_probability': ConditionalProbabilityProblem, 'distribution_functions': DistributionFunctionsProblem, 'joint_distribution': JointDistributionProblem, 't_test': StatisticalInferenceProblem, 'variance_analysis': VarianceAnalysisProblem, 'nonparametric_test': NonParametricTestProblem, 'beta_binomial_conjugate': BetaBinomialConjugateProblem, 'gamma_poisson_conjugate': GammaPoissonConjugateProblem, 'dirichlet_multinomial_conjugate': DirichletMultinomialConjugateProblem, 'binomial_poisson_approx': BinomialPoissonApproxProblem, 'poisson_normal_approx': PoissonNormalApproxProblem}\n\n    def create_problem(self, problem_type):\n        pc = self.problem_classes.get(problem_type)\n        if pc:\n            try:\n                return pc()\n            except Exception as e:\n                logging.error(f'{problem_type} problem generation error:{e}')\n                logging.error(traceback.format_exc())\n                raise\n        else:\n            raise ValueError(f'Unknown problem type:{problem_type}')"
    },
    {
      "path": "your_project\\統計検定1級\\templates\\distribution_properties_problem.tex",
      "overview": "TeXファイル。行数: 7\n",
      "content": "% distribution_properties_problem.tex\n{% if not show_solution %}\n{{ distribution }} の平均と分散を求めよ。\n{% else %}\n平均: ${{ properties.mean }}$, 分散: ${{ properties.variance }}$\n{{ explanation }}\n{% endif %}\n"
    },
    {
      "path": "your_project\\統計検定1級\\templates\\distribution_relations.tex",
      "overview": "TeXファイル。行数: 7\n",
      "content": "% distribution_relations.tex\n\\section*{Distribution Relations}\n- Beta+Binomial -> Beta-Binomial\n- Gamma+Poisson -> Negative Binomial\n- Dirichlet+Multinomial -> Dirichlet-Multinomial\n- Binomial->Poisson approximation\n- Poisson->Normal approximation\n"
    },
    {
      "path": "your_project\\統計検定1級\\templates\\econometrics_problem.tex",
      "overview": "TeXファイル。行数: 6\n",
      "content": "% econometrics_problem.tex\n{% if not show_solution %}\n計量経済学モデルに関する問題\n{% else %}\n解答と推定量\n{{ explanation }}\n{% endif %}"
    },
    {
      "path": "your_project\\統計検定1級\\templates\\high_moment_problem.tex",
      "overview": "TeXファイル。行数: 6\n",
      "content": "% high_moment_problem.tex\n{% if not show_solution %}\n正規分布の{{ n }}次モーメントは？(例: $E[X^{n}]$ )\n{% else %}\n$E[X^{n}]={{ moment }}$\n{{ explanation }}\n{% endif %}"
    },
    {
      "path": "your_project\\統計検定1級\\templates\\linear_combination_problem.tex",
      "overview": "TeXファイル。行数: 6\n",
      "content": "% linear_combination_problem.tex\n{% if not show_solution %}\nZ=aX+bY のE[Z],Var[Z]\n{% else %}\nE[Z],Var[Z]\n{{ explanation }}\n{% endif %}"
    },
    {
      "path": "your_project\\統計検定1級\\templates\\multivariate_normal_problem.tex",
      "overview": "TeXファイル。行数: 6\n",
      "content": "% multivariate_normal_problem.tex\n{% if not show_solution %}\n多変量正規に関する問題\n{% else %}\n解答\n{{ explanation }}\n{% endif %}"
    },
    {
      "path": "your_project\\統計検定1級\\templates\\nonparametric_test_problem.tex",
      "overview": "TeXファイル。行数: 6\n",
      "content": "% nonparametric_test_problem.tex\n{% if not show_solution %}\nノンパラ検定問題\n{% else %}\n検定結果\n{{ explanation }}\n{% endif %}"
    },
    {
      "path": "your_project\\統計検定1級\\templates\\poisson_normal_approx_problem.tex",
      "overview": "TeXファイル。行数: 6\n",
      "content": "% poisson_normal_approx_problem.tex\n{% if not show_solution %}\nPoisson→Normal近似\n{% else %}\nPoisson P=${{ poisson_p }}$, mean=${{mean}}$ var=${{variance}}$\n{{ explanation }}\n{% endif %}"
    },
    {
      "path": "your_project\\統計検定1級\\templates\\probability_definition_problem.tex",
      "overview": "TeXファイル。行数: 6\n",
      "content": "% probability_definition_problem.tex\n{% if not show_solution %}\nP(A∩B)求めよ\n{% else %}\n結果\n{{ explanation }}\n{% endif %}"
    },
    {
      "path": "your_project\\統計検定1級\\templates\\probability_problem.tex",
      "overview": "TeXファイル。行数: 7\n",
      "content": "% probability_problem.tex\n{% if not show_solution %}\n確率計算問題（例）\n問題タイプ: {{ problem_type }}\n{% else %}\n解答と説明: {{ explanation }}\n計算結果: P = {{ probability }}\n{% endif %}"
    },
    {
      "path": "your_project\\統計検定1級\\templates\\regression_analysis_problem.tex",
      "overview": "TeXファイル。行数: 6\n",
      "content": "% regression_analysis_problem.tex\n{% if not show_solution %}\n回帰分析問題\n{% else %}\n回帰係数結果\n{{ explanation }}\n{% endif %}"
    },
    {
      "path": "your_project\\統計検定1級\\templates\\statistical_inference_problem.tex",
      "overview": "TeXファイル。行数: 6\n",
      "content": "% statistical_inference_problem.tex\n{% if not show_solution %}\n統計的推定/検定問題\n{% else %}\n検定結果\n{{ explanation }}\n{% endif %}"
    },
    {
      "path": "your_project\\統計検定1級\\templates\\time_series_analysis_problem.tex",
      "overview": "TeXファイル。行数: 6\n",
      "content": "% time_series_analysis_problem.tex\n{% if not show_solution %}\n時系列分析問題\n{% else %}\n解答と説明\n{{ explanation }}\n{% endif %}"
    },
    {
      "path": "your_project\\統計検定1級\\templates\\variance_analysis_problem.tex",
      "overview": "TeXファイル。行数: 6\n",
      "content": "% variance_analysis_problem.tex\n{% if not show_solution %}\n分散分析問題\n{% else %}\nANOVA結果\n{{ explanation }}\n{% endif %}"
    },
    {
      "path": "統計検定1級\\config.py",
      "overview": "Pythonコード。\nクラス: ConfigManager。\n関数: __new__, load_config, get。\n",
      "content": "import json, os, logging\nfrom typing import Any\n\nclass ConfigManager:\n    _instance = None\n\n    def __new__(cls):\n        if cls._instance is None:\n            cls._instance = super(ConfigManager, cls).__new__(cls)\n            cls._instance._config = {}\n        return cls._instance\n\n    def load_config(self, config_path: str) -> None:\n        if not os.path.exists(config_path):\n            logging.warning(f'設定ファイルが見つかりません: {config_path}')\n            self._config = {}\n        else:\n            try:\n                with open(config_path, 'r', encoding='utf-8') as f:\n                    content = f.read().strip()\n                    if not content:\n                        self._config = {}\n                    else:\n                        self._config = json.loads(content)\n                logging.info(f'設定ファイル読込完了: {config_path}')\n            except Exception as e:\n                logging.warning(f'設定ファイル読込中にエラー: {str(e)}', exc_info=True)\n                self._config = {}\n\n    def get(self, *keys: str, default: Any=None) -> Any:\n        data = self._config\n        for k in keys:\n            if isinstance(data, dict) and k in data:\n                data = data[k]\n            else:\n                return default\n        if isinstance(data, dict) and 'value' in data:\n            return data['value']\n        return data\n\nconfig = ConfigManager()\n"
    },
    {
      "path": "統計検定1級\\database.py",
      "overview": "Pythonコード。\nクラス: DatabaseManager。\n関数: __init__, setup_database, save_problem。\n",
      "content": "import sqlite3, os\nfrom config import config\n\nclass DatabaseManager:\n\n    def __init__(self):\n        script_dir = os.path.dirname(os.path.abspath(__file__))\n        db_path = config.get('database_path', default='data/data.db')\n        if db_path is None:\n            db_path = 'data/data.db'\n        self.db_name = os.path.join(script_dir, db_path)\n        os.makedirs(os.path.dirname(self.db_name), exist_ok=True)\n\n    def setup_database(self):\n        conn = sqlite3.connect(self.db_name)\n        c = conn.cursor()\n        c.execute('CREATE TABLE IF NOT EXISTS problems (id TEXT PRIMARY KEY,date_created TEXT,problem_text TEXT,solution_text TEXT,problem_type TEXT)')\n        conn.commit()\n        conn.close()\n\n    def save_problem(self, problem_id, date_created, problem_text, solution_text, problem_type):\n        conn = sqlite3.connect(self.db_name)\n        c = conn.cursor()\n        c.execute('INSERT INTO problems VALUES (?,?,?,?,?)', (problem_id, date_created, problem_text, solution_text, problem_type))\n        conn.commit()\n        conn.close()\n"
    },
    {
      "path": "統計検定1級\\graph.py",
      "overview": "Pythonコード。\nクラス: ProbabilityDistributionVisualizer。\n関数: __init__, _safe_savefig, plot_probability, plot_t_test, plot_regression, plot_time_series, plot_econometrics, plot_multivariate_normal, plot_distribution_properties, plot_high_moment, plot_variance_analysis, plot_nonparametric_test, plot_linear_combination, confidence_ellipse, mvn_pdf, ecdf。\n",
      "content": "import matplotlib\nmatplotlib.use('Agg')\nimport matplotlib.pyplot as plt\nimport math\nimport numpy as np\nimport logging\nimport os\nfrom math import factorial, exp\nfrom scipy.stats import norm, t as t_dist, f as f_dist, poisson, expon\nfrom statsmodels.graphics.tsaplots import plot_acf\nimport traceback\n\nclass ProbabilityDistributionVisualizer:\n\n    def __init__(self):\n        pass\n\n    def _safe_savefig(self, output_path):\n        \"\"\"\n        画像を確実にPNGで出力し、ファイルサイズなどをログするヘルパー関数。\n        \"\"\"\n        try:\n            (_, ext) = os.path.splitext(output_path)\n            if ext.lower() == '.png':\n                plt.savefig(output_path, format='png')\n            else:\n                plt.savefig(output_path, format='png')\n            plt.close()\n            if os.path.exists(output_path):\n                fsize = os.path.getsize(output_path)\n                logging.info(f'Saved figure: {output_path} (size: {fsize} bytes)')\n            else:\n                logging.warning(f'File not found after saving: {output_path}')\n        except Exception as e:\n            logging.error(f'Failed to save figure to {output_path}, error={e}')\n            plt.close()\n\n    def plot_probability(self, params, output_path):\n        ptype = params.get('problem_type')\n        try:\n            if ptype == 'binomial':\n                p = params['p']\n                n = params['n']\n                k = params['k']\n                x = range(n + 1)\n                from math import comb\n                pmf = [comb(n, i) * p ** i * (1 - p) ** (n - i) for i in x]\n                cdf = np.cumsum(pmf)\n                plt.figure(figsize=(10, 5))\n                plt.subplot(1, 2, 1)\n                plt.bar(x, pmf, color='skyblue')\n                plt.title(f'Binomial PMF n={n}, p={p}')\n                plt.xlabel('k')\n                plt.ylabel('P(X=k)')\n                plt.axvline(k, color='red', linestyle='--', label='Target k')\n                plt.legend()\n                plt.subplot(1, 2, 2)\n                plt.step(x, cdf, where='post', color='green')\n                plt.title('CDF')\n                plt.xlabel('k')\n                plt.ylabel('P(X<=k)')\n                plt.axvline(k, color='red', linestyle='--')\n                plt.tight_layout()\n                self._safe_savefig(output_path)\n                return True\n            elif ptype == 'poisson':\n                lam = params['lambda']\n                k = params['k']\n                x = range(k + 10 + 1)\n                pmf = [lam ** i * exp(-lam) / factorial(i) for i in x]\n                cdf = np.cumsum(pmf)\n                plt.figure(figsize=(10, 5))\n                plt.subplot(1, 2, 1)\n                plt.bar(x, pmf, color='orange')\n                plt.title(f'Poisson(lambda={lam}) PMF')\n                plt.xlabel('k')\n                plt.ylabel('P(X=k)')\n                plt.axvline(k, color='red', linestyle='--', label='Target k')\n                plt.legend()\n                plt.subplot(1, 2, 2)\n                plt.step(x, cdf, where='post', color='green')\n                plt.title('CDF')\n                plt.xlabel('k')\n                plt.ylabel('P(X<=k)')\n                plt.axvline(k, color='red', linestyle='--')\n                plt.tight_layout()\n                self._safe_savefig(output_path)\n                return True\n            elif ptype == 'conditional_probability':\n                P_A = params['P_A']\n                P_BA = params['P_B_given_A']\n                P_AB = params['P_A_and_B']\n                plt.figure()\n                vals = [P_A, P_BA, P_AB]\n                labels = ['P(A)', 'P(B|A)', 'P(A∩B)']\n                plt.bar(labels, vals, color=['blue', 'green', 'red'])\n                plt.title('Conditional Probability Visualization')\n                plt.ylabel('Probability')\n                plt.tight_layout()\n                self._safe_savefig(output_path)\n                return True\n            else:\n                return False\n        except Exception as e:\n            logging.error(f'Error in plot_probability: {e}')\n            plt.close()\n            return False\n\n    def plot_t_test(self, params, output_path):\n        try:\n            alpha = params['alpha']\n            df = params['df']\n            t_stat = params['t_stat']\n            critical_value = params['critical_value']\n            x = np.linspace(t_dist.ppf(0.001, df), t_dist.ppf(0.999, df), 1000)\n            y = t_dist.pdf(x, df)\n            plt.figure(figsize=(10, 5))\n            plt.plot(x, y, 'b-', label=f't-dist(df={df})')\n            plt.axvline(x=critical_value, color='r', linestyle='--', label='critical +')\n            plt.axvline(x=-critical_value, color='r', linestyle='--', label='critical -')\n            plt.axvline(x=t_stat, color='g', label='t-stat')\n            p_area_x = x[x > critical_value]\n            plt.fill_between(p_area_x, t_dist.pdf(p_area_x, df), color='red', alpha=0.3)\n            p_area_x2 = x[x < -critical_value]\n            plt.fill_between(p_area_x2, t_dist.pdf(p_area_x2, df), color='red', alpha=0.3)\n            plt.title('t-test visualization')\n            plt.xlabel('t')\n            plt.ylabel('pdf')\n            plt.legend()\n            plt.tight_layout()\n            self._safe_savefig(output_path)\n            return True\n        except Exception as e:\n            logging.error(f't検定グラフ生成エラー: {e}')\n            plt.close()\n            return False\n\n    def plot_regression(self, x_values, y_values, beta_0_hat, beta_1_hat, output_path):\n        try:\n            import statsmodels.api as sm\n            X = sm.add_constant(x_values)\n            model = sm.OLS(y_values, X).fit()\n            residuals = model.resid\n            (fig, axes) = plt.subplots(2, 2, figsize=(10, 10))\n            ax = axes[0, 0]\n            ax.scatter(x_values, y_values, color='blue', label='data')\n            x_line = np.linspace(min(x_values), max(x_values), 100)\n            y_line = beta_0_hat + beta_1_hat * x_line\n            ax.plot(x_line, y_line, color='red', label='reg line')\n            ax.set_title('Data & Regression Line')\n            ax.set_xlabel('X')\n            ax.set_ylabel('Y')\n            ax.legend()\n            ax = axes[0, 1]\n            ax.hist(residuals, bins=20, color='green', alpha=0.7)\n            ax.set_title('Residual Histogram')\n            ax.set_xlabel('Residual')\n            ax.set_ylabel('Frequency')\n            sm.qqplot(residuals, line='45', ax=axes[1, 0], color='purple')\n            axes[1, 0].set_title('Q-Q plot of Residuals')\n            fitted = model.fittedvalues\n            ax = axes[1, 1]\n            ax.scatter(fitted, residuals, color='orange')\n            ax.axhline(y=0, color='red', linestyle='--')\n            ax.set_title('Residuals vs Fitted')\n            ax.set_xlabel('Fitted')\n            ax.set_ylabel('Residuals')\n            fig.tight_layout()\n            self._safe_savefig(output_path)\n            return True\n        except Exception as e:\n            logging.error(f'回帰分析グラフ生成中にエラー: {e}')\n            plt.close()\n            return False\n\n    def plot_time_series(self, params, output_path):\n        try:\n            ts = params['time_series']\n            (fig, axes) = plt.subplots(2, 1, figsize=(10, 8))\n            axes[0].plot(ts, color='blue')\n            axes[0].set_title('Time Series Data')\n            axes[0].set_xlabel('Time')\n            axes[0].set_ylabel('Value')\n            plot_acf(ts, ax=axes[1])\n            axes[1].set_title('Autocorrelation Function')\n            fig.tight_layout()\n            self._safe_savefig(output_path)\n            return True\n        except Exception as e:\n            logging.error(f'時系列分析グラフエラー: {e}')\n            plt.close()\n            return False\n\n    def plot_econometrics(self, params, output_path):\n        try:\n            import statsmodels.api as sm\n            X = np.column_stack((params['x1_values'], params['x2_values']))\n            Y = np.array(params['y_values'])\n            Xc = sm.add_constant(X)\n            model = sm.OLS(Y, Xc).fit()\n            residuals = model.resid\n            fitted = model.fittedvalues\n            (fig, axes) = plt.subplots(2, 2, figsize=(10, 10))\n            ax = axes[0, 0]\n            ax.scatter(params['x1_values'], Y, color='blue', alpha=0.7, label='X1-Y')\n            ax.set_title('X1 vs Y')\n            ax.set_xlabel('X1')\n            ax.set_ylabel('Y')\n            ax = axes[0, 1]\n            ax.scatter(params['x2_values'], Y, color='green', alpha=0.7, label='X2-Y')\n            ax.set_title('X2 vs Y')\n            ax.set_xlabel('X2')\n            ax.set_ylabel('Y')\n            ax = axes[1, 0]\n            ax.hist(residuals, bins=20, color='gray', alpha=0.7)\n            ax.set_title('Residuals Histogram')\n            ax.set_xlabel('Residual')\n            ax = axes[1, 1]\n            ax.scatter(fitted, residuals, color='red', alpha=0.7)\n            ax.axhline(y=0, color='black', linestyle='--')\n            ax.set_title('Residuals vs Fitted')\n            ax.set_xlabel('Fitted')\n            ax.set_ylabel('Residuals')\n            fig.tight_layout()\n            self._safe_savefig(output_path)\n            return True\n        except Exception as e:\n            logging.error(f'計量経済学グラフエラー: {e}')\n            plt.close()\n            return False\n\n    def plot_multivariate_normal(self, params, output_path):\n        try:\n            mu = params['mu']\n            sigma = np.array(params['sigma'])\n            fig = plt.figure(figsize=(10, 10))\n            from matplotlib.patches import Ellipse\n            import matplotlib.transforms as transforms\n\n            def confidence_ellipse(mu, cov, ax, n_std=1.96, facecolor='none', **kwargs):\n                pearson = cov[0, 1] / np.sqrt(cov[0, 0] * cov[1, 1])\n                ell_radius_x = np.sqrt(1 + pearson)\n                ell_radius_y = np.sqrt(1 - pearson)\n                ellipse = Ellipse((0, 0), width=ell_radius_x * 2, height=ell_radius_y * 2, facecolor=facecolor, **kwargs)\n                scale_x = np.sqrt(cov[0, 0]) * n_std\n                scale_y = np.sqrt(cov[1, 1]) * n_std\n                transf = transforms.Affine2D().rotate_deg(45).scale(scale_x, scale_y).translate(mu[0], mu[1])\n                ellipse.set_transform(transf + ax.transData)\n                return ax.add_patch(ellipse)\n            ax = fig.add_subplot(2, 2, 1)\n            ax.set_title('Confidence Ellipse')\n            confidence_ellipse(mu, sigma, ax, edgecolor='red')\n            ax.scatter(mu[0], mu[1], c='blue', marker='x', label='mean')\n            ax.legend()\n            ax.set_xlabel('X1')\n            ax.set_ylabel('X2')\n            ax2 = fig.add_subplot(2, 2, 2)\n            x = np.linspace(mu[0] - 4 * np.sqrt(sigma[0, 0]), mu[0] + 4 * np.sqrt(sigma[0, 0]), 100)\n            y = np.linspace(mu[1] - 4 * np.sqrt(sigma[1, 1]), mu[1] + 4 * np.sqrt(sigma[1, 1]), 100)\n            (X, Y) = np.meshgrid(x, y)\n            pos = np.dstack((X, Y))\n\n            def mvn_pdf(xarr, muarr, cov):\n                det = np.linalg.det(cov)\n                inv = np.linalg.inv(cov)\n                diff = xarr - muarr\n                return 1.0 / (2 * np.pi * np.sqrt(det)) * np.exp(-0.5 * (diff @ inv @ diff.T))\n            Z = np.empty(X.shape)\n            for i in range(X.shape[0]):\n                for j in range(X.shape[1]):\n                    Z[i, j] = mvn_pdf(np.array([X[i, j], Y[i, j]]), np.array(mu), sigma)\n            ax2.contour(X, Y, Z, levels=5, cmap='Blues')\n            ax2.set_title('Contour')\n            ax3 = fig.add_subplot(2, 2, 3)\n            X_marg = norm(loc=mu[0], scale=np.sqrt(sigma[0, 0]))\n            x_line = np.linspace(mu[0] - 4 * np.sqrt(sigma[0, 0]), mu[0] + 4 * np.sqrt(sigma[0, 0]), 100)\n            y_line = X_marg.pdf(x_line)\n            ax3.plot(x_line, y_line, 'r-')\n            ax3.set_title('Marginal X1 distribution')\n            ax3.set_xlabel('X1')\n            ax3.set_ylabel('pdf')\n            ax4 = fig.add_subplot(2, 2, 4)\n            Y_marg = norm(loc=mu[1], scale=np.sqrt(sigma[1, 1]))\n            y_line = Y_marg.pdf(x_line)\n            ax4.plot(x_line, y_line, 'g-')\n            ax4.set_title('Marginal X2 distribution')\n            ax4.set_xlabel('X2')\n            ax4.set_ylabel('pdf')\n            fig.tight_layout()\n            self._safe_savefig(output_path)\n            return True\n        except Exception as e:\n            logging.error(f'多変量正規分布グラフエラー: {e}')\n            plt.close()\n            return False\n\n    def plot_distribution_properties(self, params, output_path):\n        try:\n            dist = params['distribution']\n            plt.figure(figsize=(10, 5))\n            if dist == '正規分布':\n                mu = 0\n                sigma = 1\n                x = np.linspace(mu - 4 * sigma, mu + 4 * sigma, 200)\n                y = norm.pdf(x, mu, sigma)\n                plt.plot(x, y, 'b-')\n                plt.axvline(mu, color='r', linestyle='--', label='mean')\n                plt.axvline(mu + sigma, color='g', linestyle=':', label='mean+sigma')\n                plt.axvline(mu - sigma, color='g', linestyle=':')\n                plt.title('Normal Distribution (mu=0, sigma=1)')\n                plt.legend()\n            elif dist == 'ポアソン分布':\n                lam = 3\n                x = np.arange(0, 15)\n                y = poisson.pmf(x, lam)\n                plt.bar(x, y, color='skyblue')\n                plt.axvline(lam, color='r', linestyle='--', label='mean=lambda=3')\n                plt.title('Poisson(lambda=3)')\n                plt.legend()\n            elif dist == '指数分布':\n                lam = 1\n                x = np.linspace(0, 5, 200)\n                y = expon.pdf(x, scale=1 / lam)\n                plt.plot(x, y, 'b-')\n                plt.axvline(1 / lam, color='r', linestyle='--', label='mean=1/lambda')\n                plt.title('Exponential(lambda=1)')\n                plt.legend()\n            plt.tight_layout()\n            self._safe_savefig(output_path)\n            return True\n        except Exception as e:\n            logging.error(f'分布性質グラフエラー: {e}')\n            plt.close()\n            return False\n\n    def plot_high_moment(self, params, output_path):\n        try:\n            mu = params['mu']\n            sigma = params['sigma']\n            n = params['n']\n            moment = params['moment']\n            x = np.linspace(mu - 4 * sigma, mu + 4 * sigma, 200)\n            y = norm.pdf(x, mu, sigma)\n            plt.figure(figsize=(10, 5))\n            plt.plot(x, y, 'b-', label='Normal pdf')\n            plt.axvline(mu, color='r', linestyle='--', label='mean')\n            plt.axvline(mu + sigma, color='g', linestyle=':', label='mu±sigma')\n            plt.axvline(mu - sigma, color='g', linestyle=':')\n            plt.title(f'Normal distribution (mu={mu}, sigma={sigma}). n={n}-th moment={moment}')\n            plt.legend()\n            plt.tight_layout()\n            self._safe_savefig(output_path)\n            return True\n        except Exception as e:\n            logging.error(f'高次モーメントグラフエラー: {e}')\n            plt.close()\n            return False\n\n    def plot_variance_analysis(self, params, output_path):\n        try:\n            group_count = params['group_count']\n            sample_sizes = params['sample_sizes']\n            means = params['means']\n            variances = params['variances']\n            data = []\n            for i in range(group_count):\n                np.random.seed(i)\n                samples = np.random.normal(means[i], np.sqrt(variances[i]), sample_sizes[i])\n                data.append(samples)\n            plt.figure(figsize=(8, 6))\n            plt.boxplot(data, labels=[f'Group{i + 1}' for i in range(group_count)])\n            plt.title('ANOVA: Boxplots of groups')\n            plt.ylabel('Value')\n            plt.tight_layout()\n            self._safe_savefig(output_path)\n            return True\n        except Exception as e:\n            logging.error(f'分散分析グラフエラー: {e}')\n            plt.close()\n            return False\n\n    def plot_nonparametric_test(self, params, output_path):\n        try:\n            s1 = params['sample1']\n            s2 = params['sample2']\n\n            def ecdf(data):\n                d_sorted = np.sort(data)\n                y = np.arange(1, len(d_sorted) + 1) / len(d_sorted)\n                return (d_sorted, y)\n            (x1, y1) = ecdf(s1)\n            (x2, y2) = ecdf(s2)\n            plt.figure(figsize=(8, 6))\n            plt.step(x1, y1, where='post', label='Sample1 ECDF', color='blue')\n            plt.step(x2, y2, where='post', label='Sample2 ECDF', color='red')\n            plt.title('Nonparametric Test Visualization')\n            plt.xlabel('Value')\n            plt.ylabel('ECDF')\n            plt.legend()\n            plt.tight_layout()\n            self._safe_savefig(output_path)\n            return True\n        except Exception as e:\n            logging.error(f'ノンパラ検定グラフエラー: {e}')\n            plt.close()\n            return False\n\n    def plot_linear_combination(self, params, output_path):\n        try:\n            a = params['a']\n            b = params['b']\n            mu1 = params['mu1']\n            mu2 = params['mu2']\n            sig1 = params['sigma1_squared']\n            sig2 = params['sigma2_squared']\n            np.random.seed(123)\n            x = np.random.normal(mu1, np.sqrt(sig1), 1000)\n            y = np.random.normal(mu2, np.sqrt(sig2), 1000)\n            Z = a * x + b * y\n            plt.figure(figsize=(8, 6))\n            plt.hist(Z, bins=30, density=True, alpha=0.7, color='purple', label='Simulated Z')\n            E_Z = a * mu1 + b * mu2\n            Var_Z = a ** 2 * sig1 + b ** 2 * sig2\n            X_line = np.linspace(E_Z - 4 * np.sqrt(Var_Z), E_Z + 4 * np.sqrt(Var_Z), 200)\n            Y_line = norm.pdf(X_line, E_Z, np.sqrt(Var_Z))\n            plt.plot(X_line, Y_line, 'r-', label='Theoretical PDF')\n            plt.title('Linear Combination Distribution')\n            plt.xlabel('Z')\n            plt.ylabel('Density')\n            plt.legend()\n            plt.tight_layout()\n            self._safe_savefig(output_path)\n            return True\n        except Exception as e:\n            logging.error(f'線形結合グラフエラー: {e}')\n            plt.close()\n            return False"
    },
    {
      "path": "統計検定1級\\gui.py",
      "overview": "Pythonコード。\nクラス: InteractiveSolverGUI。\n関数: __init__, run。\n",
      "content": "import tkinter as tk\nimport tkinter.font as tkfont\nfrom config import config\n\nclass InteractiveSolverGUI:\n\n    def __init__(self):\n        self.root = tk.Tk()\n        # Set window title from config\n        self.root.title(config.get('gui_settings', 'window_title', default='統計検定1級 インタラクティブ問題解答システム'))\n        # Set default font size\n        font_size = config.get('gui_settings', 'font_size', default=14)\n        tkfont.nametofont('TkDefaultFont').configure(size=font_size)\n\n    def run(self):\n        self.root.mainloop()\n"
    },
    {
      "path": "統計検定1級\\main.py",
      "overview": "Pythonコード。\n関数: main。\n",
      "content": "# your_project/main.py\nimport sys\nimport os\n\n# --- このブロックは main.py を直接実行した場合の対策 ---\nif __name__ == '__main__' and __package__ is None:\n    parent_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n    sys.path.insert(0, parent_dir)\n    __package__ = \"your_project\"\n# ---------------------------------------------------------------\n\nfrom main_app import MainApp\nfrom gui import InteractiveSolverGUI\nfrom config import config\nimport logging\nlog_level = config.get('log_level', default='INFO')\nlogging.basicConfig(level=getattr(logging, log_level.upper(), logging.INFO))\n\ndef main():\n    pdf_gen = config.get('pdf_generation', default={})\n    problem_count = pdf_gen.get('problem_count', 9)\n    if len(sys.argv) > 1 and sys.argv[1] == '--generate-pdf':\n        app = MainApp()\n        app.generate_and_compile(problem_count)\n    else:\n        gui = InteractiveSolverGUI()\n        gui.run()\n\nif __name__ == '__main__':\n#    main()\n    app = MainApp()\n    app.generate_and_compile(1)"
    },
    {
      "path": "統計検定1級\\main_app.py",
      "overview": "Pythonコード。\nクラス: MainApp。\n関数: __init__, generate_latex_header, generate_and_compile, compile_latex。\n",
      "content": "import os, subprocess\nfrom config import config\nfrom problem_generator import ProblemGenerator\nimport logging\nimport traceback\n\nclass MainApp:\n\n    def __init__(self):\n        self.output_tex_file = config.get('output_tex_file', default='practice_problems.tex')\n        if self.output_tex_file is None:\n            self.output_tex_file = 'practice_problems.tex'\n        self.enable_visualization = config.get('enable_visualization', default=True)\n        self.problem_templates_dir = config.get('problem_templates_directory', default='templates')\n        self.output_directory = config.get('output_directory', default='outputs')\n        self.latex_path = config.get('latex_path', default='xelatex')\n        self.generator = ProblemGenerator()\n\n    def generate_latex_header(self):\n        cjk_font = config.get('cjk_main_font', default='Yu Gothic')\n        if cjk_font is None:\n            cjk_font = 'Yu Gothic'\n        header = [\n            '\\\\documentclass{article}', '\\\\usepackage{amsmath}', '\\\\usepackage{amssymb}',\n            '\\\\usepackage{graphicx}', '\\\\usepackage{float}', '\\\\usepackage{geometry}',\n            '\\\\usepackage{xeCJK}', '\\\\usepackage{fontspec}', '\\\\setmainfont{Times New Roman}',\n            f'\\\\setCJKmainfont{{{cjk_font}}}', '\\\\geometry{a4paper, margin=1in}', '\\\\begin{document}'\n        ]\n        return header\n\n    def generate_and_compile(self, problem_count):\n        script_dir = os.path.dirname(os.path.abspath(__file__))\n        latex_content = self.generate_latex_header()\n        out_dir = self.output_directory\n        if out_dir is None:\n            out_dir = 'outputs'\n        output_dir = os.path.join(script_dir, out_dir)\n        os.makedirs(output_dir, exist_ok=True)\n        tex_file_path = os.path.join(output_dir, self.output_tex_file)\n        for idx in range(1, problem_count + 1):\n            try:\n                result = self.generator.generate_problem()\n                if result:\n                    (problem_id, date_created, problem_type, problem_text, solution_text, graph_filename) = result\n                    latex_content.append(f'\\\\section*{{問題 {idx}}}')\n                    latex_content.append(problem_text)\n                    if self.enable_visualization and graph_filename:\n                        latex_content.append('\\\\begin{figure}[H]')\n                        latex_content.append('\\\\centering')\n                        graph_relative_path = os.path.join('graphs', graph_filename).replace('\\\\', '/')\n                        latex_content.append(f'\\\\includegraphics[width=0.8\\\\textwidth]{{{graph_relative_path}}}')\n                        latex_content.append('\\\\end{figure}')\n                    latex_content.append('\\\\subsection*{解答}')\n                    latex_content.append(solution_text)\n                    latex_content.append('\\\\newpage')\n                else:\n                    logging.warning(f'問題 {idx} の生成に失敗しました。')\n                    print(f'問題 {idx} の生成に失敗しました。')\n            except Exception as e:\n                logging.error(f'問題 {idx} の生成中にエラー: {e}')\n                logging.error(traceback.format_exc())\n                print(f'問題 {idx} 生成エラー。ログを確認')\n        templates_dir = self.problem_templates_dir\n        if templates_dir is None:\n            templates_dir = 'templates'\n        latex_content.append('\\\\clearpage')\n        latex_content.append('\\\\input{../' + templates_dir + '/distribution_relations.tex}')\n        latex_content.append('\\\\end{document}')\n        with open(tex_file_path, 'w', encoding='utf-8') as tex_file:\n            tex_file.write('\\n'.join(latex_content))\n        self.compile_latex(tex_file_path)\n\n    def compile_latex(self, tex_file_path):\n        tex_file_name = os.path.basename(tex_file_path)\n        latex_path = self.latex_path\n        if not latex_path or not os.path.exists(latex_path):\n            print(f'LaTeXコンパイラが見つかりません: {latex_path}')\n            return\n        process = subprocess.run([latex_path, '-interaction=nonstopmode', tex_file_name],\n                                 cwd=os.path.dirname(tex_file_path),\n                                 stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n        log_file_path = os.path.join(os.path.dirname(tex_file_path), 'latex_compile.log')\n        with open(log_file_path, 'w', encoding='utf-8') as f:\n            f.write(process.stdout or '')\n            f.write(process.stderr or '')\n        if process.returncode != 0:\n            print('LaTeXコンパイルでエラー')\n        else:\n            pdf_file = tex_file_path.replace('.tex', '.pdf')\n            if os.path.exists(pdf_file):\n                print(f'PDF生成成功: {pdf_file}')\n            else:\n                print('PDFファイル未発見')\n"
    },
    {
      "path": "統計検定1級\\problem_generator.py",
      "overview": "Pythonコード。\nクラス: ProblemGenerator。\n関数: __init__, load_topics, get_problem_types_by_topic, generate_problem。\n",
      "content": "import random, os, logging, uuid\nfrom datetime import datetime\nfrom config import config\nfrom database import DatabaseManager\nfrom problem_types.problem_factory import ProblemFactory\nimport json\nimport traceback\n\nclass ProblemGenerator:\n\n    def __init__(self):\n        self.db_path = config.get('database_path', default='data/data.db')\n        self.output_directory = config.get('output_directory', default='outputs')\n        self.enable_visualization = config.get('enable_visualization', default=True)\n        self.problem_types_weights = config.get('problem_types', default={})\n        self.factory = ProblemFactory()\n        self.db_manager = DatabaseManager()\n        self.db_manager.setup_database()\n        script_dir = os.path.dirname(os.path.abspath(__file__))\n        out_dir = self.output_directory\n        if out_dir is None:\n            out_dir = 'outputs'\n        self.output_dir = os.path.join(script_dir, out_dir, 'graphs')\n        os.makedirs(self.output_dir, exist_ok=True)\n        self.topics_data = self.load_topics()\n\n    def load_topics(self):\n        script_dir = os.path.dirname(os.path.abspath(__file__))\n        topics_file = os.path.join(script_dir, 'topics.json')\n        if not os.path.exists(topics_file):\n            raise FileNotFoundError(f\"'{topics_file}'がない\")\n        with open(topics_file, 'r', encoding='utf-8') as f:\n            return json.load(f)\n\n    def get_problem_types_by_topic(self, topic):\n        if 'topics' not in self.topics_data or topic not in self.topics_data['topics']:\n            return []\n        return self.topics_data['topics'][topic]\n\n    def generate_problem(self):\n        try:\n            # 重み付きランダムに問題タイプを選択\n            if not self.problem_types_weights:\n                raise ValueError(\"問題タイプの重みが設定されていません。\")\n            types = list(self.problem_types_weights.keys())\n            weights = list(self.problem_types_weights.values())\n            problem_type = random.choices(types, weights=weights, k=1)[0]\n            # 問題クラスのインスタンスを生成\n            problem = self.factory.create_problem(problem_type)\n            problem.generate_parameters()\n            problem_text = problem.generate_problem_text()\n            solution_text = problem.generate_solution_text()\n            enable_vis = self.enable_visualization and hasattr(problem, 'generate_graph')\n            graph_filename = None\n            if enable_vis:\n                # 可視化が有効ならグラフを生成\n                graph_id = str(uuid.uuid4()) + '.png'\n                graph_path = os.path.join(self.output_dir, graph_id)\n                success = problem.generate_graph(graph_path)\n                if success:\n                    graph_filename = graph_id\n            # データベースに保存\n            problem_id = str(uuid.uuid4())\n            date_created = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n            self.db_manager.save_problem(problem_id, date_created, problem_text, solution_text, problem_type)\n            return (problem_id, date_created, problem_type, problem_text, solution_text, graph_filename)\n        except Exception as e:\n            logging.error(f'問題生成中にエラー: {e}')\n            logging.error(traceback.format_exc())\n            return None\n"
    },
    {
      "path": "統計検定1級\\sympy_solver.py",
      "overview": "Pythonコード。\nクラス: SympySolver。\n関数: __init__, check_equivalence。\n",
      "content": "class SympySolver:\n\n    def __init__(self):\n        pass\n\n    def check_equivalence(self, user_input, correct_answer):\n        return (False, '')"
    },
    {
      "path": "統計検定1級\\topics.json",
      "overview": "JSONファイル (辞書)。キー: topics\n",
      "content": "{\n  \"topics\": {\n    \"確率論\": [\n      \"probability_definition\",\n      \"conditional_probability\",\n      \"distribution_functions\",\n      \"joint_distribution\",\n      \"probability\"\n    ],\n    \"統計的推定\": [\n      \"statistical_inference\",\n      \"t_test\"\n    ],\n    \"統計的検定\": [\n      \"statistical_inference\",\n      \"t_test\"\n    ],\n    \"回帰分析\": [\n      \"regression_analysis\"\n    ],\n    \"分散分析\": [\n      \"variance_analysis\"\n    ],\n    \"ノンパラメトリック検定\": [\n      \"nonparametric_test\"\n    ]\n  }\n}\n"
    },
    {
      "path": "統計検定1級\\.vscode\\launch.json",
      "overview": "JSONファイル (辞書)。キー: version, configurations\n",
      "content": "{\n    \"version\": \"0.2.0\",\n    \"configurations\": [\n        {\n            \"name\": \"Python: 現在のファイル\",\n            \"type\": \"python\",\n            \"request\": \"launch\",\n            \"program\": \"${file}\",\n            \"console\": \"integratedTerminal\",\n            \"justMyCode\": true,\n            \"args\": [\"--generate-pdf\"]\n\n        }\n    ]\n}"
    },
    {
      "path": "統計検定1級\\problem_types\\conjugate_problems.py",
      "overview": "Pythonコード。\nクラス: BetaBinomialConjugateProblem, GammaPoissonConjugateProblem, DirichletMultinomialConjugateProblem, BinomialPoissonApproxProblem, PoissonNormalApproxProblem。\n関数: __init__, generate_parameters, generate_explanation, generate_graph, __init__, generate_parameters, generate_explanation, generate_graph, __init__, generate_parameters, generate_explanation, generate_graph, __init__, generate_parameters, generate_explanation, generate_graph, __init__, generate_parameters, generate_explanation, generate_graph, B, B。\n",
      "content": "import math, random\nfrom problem_types.problem import Problem\nfrom math import comb, factorial, exp, gamma\nimport numpy as np\n\nclass BetaBinomialConjugateProblem(Problem):\n\n    def __init__(self):\n        super().__init__('beta_binomial_conjugate_problem.tex')\n\n    def generate_parameters(self):\n        self.params['alpha'] = random.randint(1, 5)\n        self.params['beta'] = random.randint(1, 5)\n        self.params['n'] = random.randint(5, 20)\n        self.params['k'] = random.randint(0, self.params['n'])\n\n        def B(x, y):\n            return gamma(x) * gamma(y) / gamma(x + y)\n        p_x = comb(self.params['n'], self.params['k']) * B(self.params['k'] + self.params['alpha'], self.params['n'] - self.params['k'] + self.params['beta']) / B(self.params['alpha'], self.params['beta'])\n        self.params['probability'] = round(p_x, 4)\n\n    def generate_explanation(self):\n        return 'Beta+Binomial->Beta-Binomial'\n\n    def generate_graph(self, o):\n        return False\n\nclass GammaPoissonConjugateProblem(Problem):\n\n    def __init__(self):\n        super().__init__('gamma_poisson_conjugate_problem.tex')\n\n    def generate_parameters(self):\n        self.params['alpha'] = random.randint(1, 5)\n        self.params['beta'] = random.uniform(0.5, 2.0)\n        self.params['k'] = random.randint(0, 20)\n        p = self.params['beta'] / (self.params['beta'] + 1)\n        q = 1 - p\n        negbin_p = comb(self.params['k'] + self.params['alpha'] - 1, self.params['k']) * q ** self.params['k'] * p ** self.params['alpha']\n        self.params['probability'] = round(negbin_p, 4)\n\n    def generate_explanation(self):\n        return 'Gamma+Poisson->Negative Binomial'\n\n    def generate_graph(self, o):\n        return False\n\nclass DirichletMultinomialConjugateProblem(Problem):\n\n    def __init__(self):\n        super().__init__('dirichlet_multinomial_conjugate_problem.tex')\n\n    def generate_parameters(self):\n        m = random.randint(2, 4)\n        self.params['m'] = m\n        self.params['n'] = random.randint(5, 20)\n        self.params['alpha_vec'] = [random.uniform(1, 3) for _ in range(m)]\n        counts = [0] * m\n        remain = self.params['n']\n        for i in range(m - 1):\n            c = random.randint(0, remain)\n            counts[i] = c\n            remain -= c\n        counts[-1] = remain\n        self.params['counts'] = counts\n\n        def B(alpha):\n            import numpy as np\n            return np.prod([gamma(a) for a in alpha]) / gamma(sum(alpha))\n        alpha_x = [self.params['alpha_vec'][i] + counts[i] for i in range(m)]\n        num = B(alpha_x)\n        den = B(self.params['alpha_vec'])\n        multinomial_coef = math.factorial(self.params['n'])\n        for c in counts:\n            multinomial_coef /= math.factorial(c)\n        p_x = multinomial_coef * (num / den)\n        self.params['probability'] = round(p_x, 4)\n\n    def generate_explanation(self):\n        return 'Dirichlet+Multinomial->Dirichlet-Multinomial'\n\n    def generate_graph(self, o):\n        return False\n\nclass BinomialPoissonApproxProblem(Problem):\n\n    def __init__(self):\n        super().__init__('binomial_poisson_approx_problem.tex')\n\n    def generate_parameters(self):\n        lam = random.uniform(2, 5)\n        n = random.randint(50, 200)\n        p = lam / n\n        k = random.randint(0, int(lam * 3))\n        binom_p = comb(n, k) * p ** k * (1 - p) ** (n - k)\n        poisson_p = lam ** k * exp(-lam) / math.factorial(k)\n        self.params['n'] = n\n        self.params['p'] = round(p, 6)\n        self.params['k'] = k\n        self.params['lambda'] = round(lam, 3)\n        self.params['binom_p'] = round(binom_p, 6)\n        self.params['poisson_p'] = round(poisson_p, 6)\n\n    def generate_explanation(self):\n        return 'Binomial->Poisson近似条件'\n\n    def generate_graph(self, o):\n        return False\n\nclass PoissonNormalApproxProblem(Problem):\n\n    def __init__(self):\n        super().__init__('poisson_normal_approx_problem.tex')\n\n    def generate_parameters(self):\n        lam = random.randint(30, 100)\n        low = max(0, int(lam - 3 * math.sqrt(lam)))\n        high = int(lam + 3 * math.sqrt(lam))\n        k = random.randint(low, high)\n        poisson_p = lam ** k * exp(-lam) / math.factorial(k)\n        self.params['lambda'] = lam\n        self.params['k'] = k\n        self.params['poisson_p'] = round(poisson_p, 6)\n        self.params['mean'] = lam\n        self.params['variance'] = lam\n\n    def generate_explanation(self):\n        return 'Poisson->Normal近似(λ大)'\n\n    def generate_graph(self, o):\n        return False"
    },
    {
      "path": "統計検定1級\\problem_types\\problem.py",
      "overview": "Pythonコード。\nクラス: Problem, ProbabilityProblem, StatisticalInferenceProblem, RegressionAnalysisProblem, TimeSeriesAnalysisProblem, EconometricsProblem, LinearCombinationProblem, DistributionPropertiesProblem, HighMomentProblem, MultivariateNormalProblem, VarianceAnalysisProblem, NonParametricTestProblem, ProbabilityDefinitionProblem, ConditionalProbabilityProblem, DistributionFunctionsProblem, JointDistributionProblem。\n関数: __init__, generate_parameters, generate_problem_text, generate_solution_text, generate_explanation, generate_graph, __init__, generate_parameters, _variant_binomial, _variant_poisson, _variant_conditional_probability, generate_explanation, generate_graph, __init__, generate_parameters, generate_explanation, generate_graph, __init__, generate_parameters, generate_explanation, generate_graph, __init__, generate_parameters, generate_explanation, generate_graph, __init__, generate_parameters, generate_explanation, generate_graph, __init__, generate_parameters, generate_explanation, generate_graph, __init__, generate_parameters, generate_explanation, generate_graph, __init__, generate_parameters, generate_explanation, generate_graph, __init__, generate_parameters, generate_explanation, generate_graph, __init__, generate_parameters, generate_explanation, generate_graph, __init__, generate_parameters, generate_explanation, generate_graph, __init__, __init__, __init__, __init__。\n",
      "content": "from abc import ABC, abstractmethod\nimport os\nimport random\nimport logging\nfrom config import config\nfrom jinja2 import Environment, FileSystemLoader\nfrom graph import ProbabilityDistributionVisualizer\nimport traceback\nfrom math import comb, exp, factorial\nfrom scipy.stats import norm, stats\n\nclass Problem(ABC):\n\n    def __init__(self, template_name):\n        self.params = {}\n        self.template_name = template_name\n        templates_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), '..', config.get('problem_templates_directory', default='templates'))\n        self.env = Environment(loader=FileSystemLoader(templates_dir), autoescape=False)\n        self.visualizer = ProbabilityDistributionVisualizer()\n\n    @abstractmethod\n    def generate_parameters(self):\n        pass\n\n    def generate_problem_text(self):\n        template = self.env.get_template(self.template_name)\n        return template.render(**self.params, show_solution=False)\n\n    def generate_solution_text(self):\n        template = self.env.get_template(self.template_name)\n        self.params['explanation'] = self.generate_explanation()\n        return template.render(**self.params, show_solution=True)\n\n    def generate_explanation(self):\n        return ''\n\n    @abstractmethod\n    def generate_graph(self, output_path):\n        pass\n\nclass ProbabilityProblem(Problem):\n\n    def __init__(self):\n        super().__init__('probability_problem.tex')\n\n    def generate_parameters(self):\n        variants = [self._variant_binomial, self._variant_poisson, self._variant_conditional_probability]\n        v = random.choice(variants)\n        v()\n\n    def _variant_binomial(self):\n        self.params['problem_type'] = 'binomial'\n        self.params['n'] = random.randint(5, 20)\n        self.params['p'] = round(random.uniform(0.1, 0.9), 2)\n        self.params['k'] = random.randint(0, self.params['n'])\n        from math import comb\n        prob = comb(self.params['n'], self.params['k']) * self.params['p'] ** self.params['k'] * (1 - self.params['p']) ** (self.params['n'] - self.params['k'])\n        self.params['probability'] = round(prob, 6)\n\n    def _variant_poisson(self):\n        self.params['problem_type'] = 'poisson'\n        self.params['lambda'] = round(random.uniform(0.5, 5.0), 2)\n        self.params['k'] = random.randint(0, 10)\n        lam = self.params['lambda']\n        k = self.params['k']\n        prob = lam ** k * exp(-lam) / factorial(k)\n        self.params['probability'] = round(prob, 6)\n\n    def _variant_conditional_probability(self):\n        self.params['problem_type'] = 'conditional_probability'\n        self.params['P_A'] = round(random.uniform(0.1, 0.9), 2)\n        self.params['P_B_given_A'] = round(random.uniform(0.1, 0.9), 2)\n        self.params['P_A_and_B'] = round(self.params['P_A'] * self.params['P_B_given_A'], 6)\n\n    def generate_explanation(self):\n        t = self.params['problem_type']\n        if t == 'binomial':\n            return '二項分布の公式を使用'\n        elif t == 'poisson':\n            return 'ポアソン分布の公式を使用'\n        elif t == 'conditional_probability':\n            return '条件付き確率P(A∩B)=P(A)*P(B|A)'\n\n    def generate_graph(self, output_path):\n        return self.visualizer.plot_probability(self.params, output_path)\n\nclass StatisticalInferenceProblem(Problem):\n\n    def __init__(self):\n        super().__init__('statistical_inference_problem.tex')\n\n    def generate_parameters(self):\n        self.params['sample_mean'] = round(random.uniform(50, 100), 2)\n        self.params['sample_std'] = round(random.uniform(5, 15), 2)\n        self.params['n'] = random.randint(30, 50)\n        self.params['alpha'] = 0.05\n        df = self.params['n'] - 1\n        # 臨界値と検定統計量を計算（片側検定の場合）\n        self.params['critical_value'] = round(abs(stats.t.ppf(self.params['alpha'], df)), 4)\n        self.params['t_stat'] = round(random.uniform(-5, 5), 4)\n\n    def generate_explanation(self):\n        return f\"有意水準{int(self.params['alpha'] * 100)}%での検定。\"\n\n    def generate_graph(self, output_path):\n        return self.visualizer.plot_t_test(self.params, output_path)\n\nclass RegressionAnalysisProblem(Problem):\n\n    def __init__(self):\n        super().__init__('regression_analysis_problem.tex')\n\n    def generate_parameters(self):\n        # 回帰直線: y = beta_0 + beta_1 * x + e\n        self.params['beta_0'] = round(random.uniform(0, 10), 2)\n        self.params['beta_1'] = round(random.uniform(0, 5), 2)\n        self.params['n'] = random.randint(10, 30)\n        # 生成するデータ点\n        x_values = [random.uniform(0, 10) for _ in range(self.params['n'])]\n        # 回帰モデルに基づいてyを生成\n        self.params['data_points'] = [(x, round(self.params['beta_0'] + self.params['beta_1'] * x + random.gauss(0, 1), 2)) for x in x_values]\n\n    def generate_explanation(self):\n        return \"回帰直線の推定値に基づく予測。\"\n\n    def generate_graph(self, output_path):\n        # 回帰分析の散布図と回帰直線\n        x_vals = [x for x, y in self.params['data_points']]\n        y_vals = [y for x, y in self.params['data_points']]\n        try:\n            import matplotlib.pyplot as plt\n            plt.figure()\n            plt.scatter(x_vals, y_vals, color='blue', label='Data points')\n            # プロットするために再計算\n            fitted_vals = [self.params['beta_0'] + self.params['beta_1'] * x for x in x_vals]\n            plt.plot(x_vals, fitted_vals, color='red', label='Fitted line')\n            plt.title('Regression Line')\n            plt.legend()\n            plt.savefig(output_path)\n            plt.close()\n            return True\n        except:\n            return False\n\nclass TimeSeriesAnalysisProblem(Problem):\n\n    def __init__(self):\n        super().__init__('time_series_analysis_problem.tex')\n\n    def generate_parameters(self):\n        # 単純なAR(1)プロセスのデータを生成\n        self.params['phi'] = round(random.uniform(0.1, 0.9), 2)\n        self.params['sigma'] = round(random.uniform(0.5, 2.0), 2)\n        self.params['n'] = random.randint(50, 100)\n        # AR(1)データ生成\n        data = [0]\n        for i in range(1, self.params['n']):\n            data.append(self.params['phi'] * data[i-1] + random.gauss(0, self.params['sigma']))\n        self.params['time_series_data'] = [round(x, 2) for x in data]\n\n    def generate_explanation(self):\n        return \"自己回帰モデル AR(1) のプロセスから生成。\"\n\n    def generate_graph(self, output_path):\n        try:\n            import matplotlib.pyplot as plt\n            plt.figure()\n            plt.plot(self.params['time_series_data'], label='Time Series')\n            plt.title('AR(1) Time Series')\n            plt.xlabel('Time')\n            plt.ylabel('Value')\n            plt.legend()\n            plt.savefig(output_path)\n            plt.close()\n            return True\n        except:\n            return False\n\nclass EconometricsProblem(Problem):\n\n    def __init__(self):\n        super().__init__('econometrics_problem.tex')\n\n    def generate_parameters(self):\n        # 単回帰モデルの係数を生成\n        self.params['beta_0'] = round(random.uniform(0, 5), 2)\n        self.params['beta_1'] = round(random.uniform(0, 5), 2)\n        # ダミーデータとしてGDP成長率とインフレ率を生成\n        self.params['gdp_growth'] = round(random.uniform(-5, 10), 2)\n        self.params['inflation_rate'] = round(random.uniform(0, 10), 2)\n\n    def generate_explanation(self):\n        return \"単回帰モデルのパラメータに基づく指標。\"\n\n    def generate_graph(self, output_path):\n        try:\n            import matplotlib.pyplot as plt\n            # インフレ率とGDP成長率の関係をプロット\n            plt.figure()\n            plt.scatter(self.params['inflation_rate'], self.params['gdp_growth'], color='green')\n            plt.title('Econometrics Scatter')\n            plt.xlabel('Inflation Rate (%)')\n            plt.ylabel('GDP Growth (%)')\n            plt.savefig(output_path)\n            plt.close()\n            return True\n        except:\n            return False\n\nclass LinearCombinationProblem(Problem):\n\n    def __init__(self):\n        super().__init__('linear_combination_problem.tex')\n\n    def generate_parameters(self):\n        # 2変数の線形結合に関する問題パラメータを生成\n        self.params['a'] = random.randint(1, 5)\n        self.params['b'] = random.randint(1, 5)\n        self.params['x'] = random.randint(10, 50)\n        self.params['y'] = random.randint(10, 50)\n\n    def generate_explanation(self):\n        return \"線形結合における係数と変数の組み合わせ。\"\n\n    def generate_graph(self, output_path):\n        try:\n            import matplotlib.pyplot as plt\n            import numpy as np\n            # 線形結合: a * x + b * y の断面図\n            X = np.linspace(0, self.params['x'], 100)\n            Y = (self.params['a'] * X) / self.params['b']\n            plt.figure()\n            plt.plot(X, Y, label=f\"{self.params['a']}*x + {self.params['b']}*y = C\")\n            plt.title('Linear Combination')\n            plt.xlabel('x')\n            plt.ylabel('y')\n            plt.legend()\n            plt.savefig(output_path)\n            plt.close()\n            return True\n        except:\n            return False\n\nclass DistributionPropertiesProblem(Problem):\n\n    def __init__(self):\n        super().__init__('distribution_properties_problem.tex')\n\n    def generate_parameters(self):\n        # 正規分布の平均と分散に関する問題\n        self.params['mean'] = round(random.uniform(0, 10), 2)\n        self.params['variance'] = round(random.uniform(1, 5), 2)\n        # 分布の性質として歪度と尖度を計算\n        self.params['skewness'] = round(random.uniform(-1, 1), 2)\n        self.params['kurtosis'] = round(random.uniform(2, 5), 2)\n\n    def generate_explanation(self):\n        return \"正規分布の性質に関する指標。\"\n\n    def generate_graph(self, output_path):\n        try:\n            import matplotlib.pyplot as plt\n            import numpy as np\n            from scipy.stats import norm\n            # 指定された平均と分散の正規分布のPDFをプロット\n            mu = self.params['mean']\n            sigma = self.params['variance'] ** 0.5\n            x = np.linspace(mu - 4*sigma, mu + 4*sigma, 200)\n            y = norm.pdf(x, mu, sigma)\n            plt.figure()\n            plt.plot(x, y, label=f'N({mu}, {sigma**2}) PDF')\n            plt.title('Normal Distribution PDF')\n            plt.xlabel('x')\n            plt.ylabel('Density')\n            plt.legend()\n            plt.savefig(output_path)\n            plt.close()\n            return True\n        except:\n            return False\n\nclass HighMomentProblem(Problem):\n\n    def __init__(self):\n        super().__init__('high_moment_problem.tex')\n\n    def generate_parameters(self):\n        # 高次モーメントに関する問題パラメータを生成\n        self.params['values'] = [round(random.uniform(-5, 5), 2) for _ in range(100)]\n        # 3次モーメント（歪度）と4次モーメント（尖度）を計算\n        n = len(self.params['values'])\n        mean_val = sum(self.params['values']) / n\n        m3 = sum((x - mean_val) ** 3 for x in self.params['values']) / n\n        m4 = sum((x - mean_val) ** 4 for x in self.params['values']) / n\n        self.params['skewness'] = round(m3 / (self.params['values'][0] if self.params['values'][0] != 0 else 1), 4)  # 簡易計算\n        self.params['kurtosis'] = round(m4 / ((self.params['values'][0] if self.params['values'][0] != 0 else 1) ** 2), 4)  # 簡易計算\n\n    def generate_explanation(self):\n        return \"高次モーメントの計算結果。\"\n\n    def generate_graph(self, output_path):\n        try:\n            import matplotlib.pyplot as plt\n            # データのヒストグラムをプロット\n            plt.figure()\n            plt.hist(self.params['values'], bins=20, color='skyblue', edgecolor='black')\n            plt.title('Value Distribution')\n            plt.xlabel('Value')\n            plt.ylabel('Frequency')\n            plt.savefig(output_path)\n            plt.close()\n            return True\n        except:\n            return False\n\nclass MultivariateNormalProblem(Problem):\n\n    def __init__(self):\n        super().__init__('multivariate_normal_problem.tex')\n\n    def generate_parameters(self):\n        # 2次元正規分布の共分散行列を生成\n        self.params['mean_x'] = round(random.uniform(0, 5), 2)\n        self.params['mean_y'] = round(random.uniform(0, 5), 2)\n        self.params['var_x'] = round(random.uniform(1, 3), 2)\n        self.params['var_y'] = round(random.uniform(1, 3), 2)\n        # 負にならないように相関係数を制限\n        self.params['corr_xy'] = round(random.uniform(0, 0.9), 2)\n\n    def generate_explanation(self):\n        return \"多変量正規分布のパラメータに基づく性質。\"\n\n    def generate_graph(self, output_path):\n        try:\n            import numpy as np\n            import matplotlib.pyplot as plt\n            from matplotlib.patches import Ellipse\n            # 平均と分散に基づく共分散行列を構成\n            mean = [self.params['mean_x'], self.params['mean_y']]\n            cov = [[self.params['var_x'], self.params['corr_xy'] * (self.params['var_x'] ** 0.5) * (self.params['var_y'] ** 0.5)],\n                   [self.params['corr_xy'] * (self.params['var_x'] ** 0.5) * (self.params['var_y'] ** 0.5), self.params['var_y']]]\n            # 等高線（信頼楕円）をプロット\n            vals, vecs = np.linalg.eigh(cov)\n            order = vals.argsort()[::-1]\n            vals, vecs = vals[order], vecs[:, order]\n            theta = np.degrees(np.arctan2(*vecs[:, 0][::-1]))\n            w, h = 2 * np.sqrt(vals)\n            ell = Ellipse(xy=mean, width=w, height=h, angle=theta, edgecolor='red', facecolor='none', label='Confidence Ellipse')\n            fig, ax = plt.subplots()\n            ax.add_patch(ell)\n            ax.scatter(mean[0], mean[1], c='red', label='Mean')\n            ax.set_title('Multivariate Normal Confidence Ellipse')\n            ax.set_xlabel('X')\n            ax.set_ylabel('Y')\n            ax.legend()\n            plt.savefig(output_path)\n            plt.close()\n            return True\n        except:\n            return False\n\nclass VarianceAnalysisProblem(Problem):\n\n    def __init__(self):\n        super().__init__('variance_analysis_problem.tex')\n\n    def generate_parameters(self):\n        # 2群のデータを生成し、分散分析用のパラメータを計算\n        group1 = [round(random.uniform(0, 10), 2) for _ in range(10)]\n        group2 = [round(random.uniform(0, 10), 2) for _ in range(10)]\n        self.params['group1_mean'] = round(sum(group1) / len(group1), 2)\n        self.params['group2_mean'] = round(sum(group2) / len(group2), 2)\n        # 平方和を計算（単純化のためANOVAではなく2標本t検定に準拠）\n        self.params['ss_between'] = round(((self.params['group1_mean'] - self.params['group2_mean']) ** 2) * len(group1), 2)\n        self.params['ss_within'] = round(sum((x - self.params['group1_mean']) ** 2 for x in group1) + sum((y - self.params['group2_mean']) ** 2 for y in group2), 2)\n\n    def generate_explanation(self):\n        return \"2群間の平均の差の分析。\"\n\n    def generate_graph(self, output_path):\n        try:\n            import matplotlib.pyplot as plt\n            # 2群のデータ分布を箱ひげ図でプロット\n            data = [self.params['group1_mean'], self.params['group2_mean']]\n            plt.figure()\n            plt.bar(['Group1', 'Group2'], data, color=['blue', 'orange'])\n            plt.title('Group Means Comparison')\n            plt.ylabel('Mean Value')\n            plt.savefig(output_path)\n            plt.close()\n            return True\n        except:\n            return False\n\nclass NonParametricTestProblem(Problem):\n\n    def __init__(self):\n        super().__init__('nonparametric_test_problem.tex')\n\n    def generate_parameters(self):\n        # ノンパラメトリック検定用のデータ生成\n        self.params['sample1'] = [round(random.uniform(0, 50), 2) for _ in range(15)]\n        self.params['sample2'] = [round(random.uniform(0, 50), 2) for _ in range(15)]\n\n    def generate_explanation(self):\n        return \"ノンパラメトリック検定のための順位データ。\"\n\n    def generate_graph(self, output_path):\n        try:\n            import matplotlib.pyplot as plt\n            # 2サンプルの累積分布関数（ECDF）をプロット\n            sample1 = sorted(self.params['sample1'])\n            sample2 = sorted(self.params['sample2'])\n            n1 = len(sample1)\n            n2 = len(sample2)\n            plt.figure()\n            plt.step(sample1, [i/n1 for i in range(1, n1+1)], label='Sample1 ECDF')\n            plt.step(sample2, [i/n2 for i in range(1, n2+1)], label='Sample2 ECDF')\n            plt.title('ECDF Comparison')\n            plt.xlabel('Value')\n            plt.ylabel('ECDF')\n            plt.legend()\n            plt.savefig(output_path)\n            plt.close()\n            return True\n        except:\n            return False\n\nclass ProbabilityDefinitionProblem(ProbabilityProblem):\n\n    def __init__(self):\n        super().__init__()\n        # このクラスでは ProbabilityProblem の変種を利用（実質同じ生成）\n\nclass ConditionalProbabilityProblem(ProbabilityProblem):\n\n    def __init__(self):\n        super().__init__()\n        # このクラスでは ProbabilityProblem の変種を利用\n\nclass DistributionFunctionsProblem(ProbabilityProblem):\n\n    def __init__(self):\n        super().__init__()\n        # このクラスでは ProbabilityProblem の変種を利用\n\nclass JointDistributionProblem(ProbabilityProblem):\n\n    def __init__(self):\n        super().__init__()\n        # このクラスでは ProbabilityProblem の変種を利用\n"
    },
    {
      "path": "統計検定1級\\problem_types\\problem_factory.py",
      "overview": "Pythonコード。\nクラス: ProblemFactory。\n関数: __init__, create_problem。\n",
      "content": "import logging, traceback\nfrom problem_types.problem import ProbabilityProblem, StatisticalInferenceProblem, RegressionAnalysisProblem, TimeSeriesAnalysisProblem, EconometricsProblem, LinearCombinationProblem, DistributionPropertiesProblem, HighMomentProblem, MultivariateNormalProblem, VarianceAnalysisProblem, NonParametricTestProblem, ProbabilityDefinitionProblem, ConditionalProbabilityProblem, DistributionFunctionsProblem, JointDistributionProblem\nfrom problem_types.conjugate_problems import BetaBinomialConjugateProblem, GammaPoissonConjugateProblem, DirichletMultinomialConjugateProblem, BinomialPoissonApproxProblem, PoissonNormalApproxProblem\n\nclass ProblemFactory:\n\n    def __init__(self):\n        self.problem_classes = {\n            'probability': ProbabilityProblem,\n            'statistical_inference': StatisticalInferenceProblem,\n            'regression_analysis': RegressionAnalysisProblem,\n            'time_series_analysis': TimeSeriesAnalysisProblem,\n            'econometrics': EconometricsProblem,\n            'linear_combination': LinearCombinationProblem,\n            'distribution_properties': DistributionPropertiesProblem,\n            'high_moment': HighMomentProblem,\n            'multivariate_normal': MultivariateNormalProblem,\n            'probability_definition': ProbabilityDefinitionProblem,\n            'conditional_probability': ConditionalProbabilityProblem,\n            'distribution_functions': DistributionFunctionsProblem,\n            'joint_distribution': JointDistributionProblem,\n            't_test': StatisticalInferenceProblem,\n            'variance_analysis': VarianceAnalysisProblem,\n            'nonparametric_test': NonParametricTestProblem,\n            'beta_binomial_conjugate': BetaBinomialConjugateProblem,\n            'gamma_poisson_conjugate': GammaPoissonConjugateProblem,\n            'dirichlet_multinomial_conjugate': DirichletMultinomialConjugateProblem,\n            'binomial_poisson_approx': BinomialPoissonApproxProblem,\n            'poisson_normal_approx': PoissonNormalApproxProblem\n        }\n\n    def create_problem(self, problem_type):\n        pc = self.problem_classes.get(problem_type)\n        if pc:\n            try:\n                return pc()\n            except Exception as e:\n                logging.error(f'{problem_type} problem generation error:{e}')\n                logging.error(traceback.format_exc())\n                raise\n        else:\n            raise ValueError(f'Unknown problem type:{problem_type}')\n"
    },
    {
      "path": "統計検定1級\\templates\\distribution_properties_problem.tex",
      "overview": "TeXファイル。行数: 7\n",
      "content": "% distribution_properties_problem.tex\n{% if not show_solution %}\n{{ distribution }} の平均と分散を求めよ。\n{% else %}\n平均: ${{ properties.mean }}$, 分散: ${{ properties.variance }}$\n{{ explanation }}\n{% endif %}\n"
    },
    {
      "path": "統計検定1級\\templates\\distribution_relations.tex",
      "overview": "TeXファイル。行数: 7\n",
      "content": "% distribution_relations.tex\n\\section*{Distribution Relations}\n- Beta+Binomial -> Beta-Binomial\n- Gamma+Poisson -> Negative Binomial\n- Dirichlet+Multinomial -> Dirichlet-Multinomial\n- Binomial->Poisson approximation\n- Poisson->Normal approximation\n"
    },
    {
      "path": "統計検定1級\\templates\\econometrics_problem.tex",
      "overview": "TeXファイル。行数: 6\n",
      "content": "% econometrics_problem.tex\n{% if not show_solution %}\n計量経済学モデルに関する問題\n{% else %}\n解答と推定量\n{{ explanation }}\n{% endif %}"
    },
    {
      "path": "統計検定1級\\templates\\high_moment_problem.tex",
      "overview": "TeXファイル。行数: 6\n",
      "content": "% high_moment_problem.tex\n{% if not show_solution %}\n正規分布の{{ n }}次モーメントは？(例: $E[X^{n}]$ )\n{% else %}\n$E[X^{n}]={{ moment }}$\n{{ explanation }}\n{% endif %}"
    },
    {
      "path": "統計検定1級\\templates\\linear_combination_problem.tex",
      "overview": "TeXファイル。行数: 6\n",
      "content": "% linear_combination_problem.tex\n{% if not show_solution %}\nZ=aX+bY のE[Z],Var[Z]\n{% else %}\nE[Z],Var[Z]\n{{ explanation }}\n{% endif %}"
    },
    {
      "path": "統計検定1級\\templates\\multivariate_normal_problem.tex",
      "overview": "TeXファイル。行数: 6\n",
      "content": "% multivariate_normal_problem.tex\n{% if not show_solution %}\n多変量正規に関する問題\n{% else %}\n解答\n{{ explanation }}\n{% endif %}"
    },
    {
      "path": "統計検定1級\\templates\\nonparametric_test_problem.tex",
      "overview": "TeXファイル。行数: 6\n",
      "content": "% nonparametric_test_problem.tex\n{% if not show_solution %}\nノンパラ検定問題\n{% else %}\n検定結果\n{{ explanation }}\n{% endif %}"
    },
    {
      "path": "統計検定1級\\templates\\poisson_normal_approx_problem.tex",
      "overview": "TeXファイル。行数: 6\n",
      "content": "% poisson_normal_approx_problem.tex\n{% if not show_solution %}\nPoisson→Normal近似\n{% else %}\nPoisson P=${{ poisson_p }}$, mean=${{mean}}$ var=${{variance}}$\n{{ explanation }}\n{% endif %}"
    },
    {
      "path": "統計検定1級\\templates\\probability_definition_problem.tex",
      "overview": "TeXファイル。行数: 6\n",
      "content": "% probability_definition_problem.tex\n{% if not show_solution %}\nP(A∩B)求めよ\n{% else %}\n結果\n{{ explanation }}\n{% endif %}"
    },
    {
      "path": "統計検定1級\\templates\\probability_problem.tex",
      "overview": "TeXファイル。行数: 7\n",
      "content": "% probability_problem.tex\n{% if not show_solution %}\n確率計算問題（例）\n問題タイプ: {{ problem_type }}\n{% else %}\n解答と説明: {{ explanation }}\n計算結果: P = {{ probability }}\n{% endif %}"
    },
    {
      "path": "統計検定1級\\templates\\regression_analysis_problem.tex",
      "overview": "TeXファイル。行数: 6\n",
      "content": "% regression_analysis_problem.tex\n{% if not show_solution %}\n回帰分析問題\n{% else %}\n回帰係数結果\n{{ explanation }}\n{% endif %}"
    },
    {
      "path": "統計検定1級\\templates\\statistical_inference_problem.tex",
      "overview": "TeXファイル。行数: 6\n",
      "content": "% statistical_inference_problem.tex\n{% if not show_solution %}\n統計的推定/検定問題\n{% else %}\n検定結果\n{{ explanation }}\n{% endif %}"
    },
    {
      "path": "統計検定1級\\templates\\time_series_analysis_problem.tex",
      "overview": "TeXファイル。行数: 6\n",
      "content": "% time_series_analysis_problem.tex\n{% if not show_solution %}\n時系列分析問題\n{% else %}\n解答と説明\n{{ explanation }}\n{% endif %}"
    },
    {
      "path": "統計検定1級\\templates\\variance_analysis_problem.tex",
      "overview": "TeXファイル。行数: 6\n",
      "content": "% variance_analysis_problem.tex\n{% if not show_solution %}\n分散分析問題\n{% else %}\nANOVA結果\n{{ explanation }}\n{% endif %}"
    },
    {
      "path": "your_project\\collected_scripts (2).json",
      "overview": "JSONファイル (辞書)。キー: system_overview, settings, scripts\n",
      "content": "{\n  \"system_overview\": \"確率統計の網羅的理解\",\n  \"settings\": {\n    \"data_directory\": {\n      \"value\": \"data\",\n      \"description\": \"データディレクトリのパス。\"\n    },\n    \"log_level\": {\n      \"value\": \"INFO\",\n      \"description\": \"ログレベルの設定（例: INFO、DEBUG、ERROR）。\"\n    },\n    \"output_directory\": {\n      \"value\": \"outputs\",\n      \"description\": \"出力ディレクトリのパス。\"\n    },\n    \"database_path\": {\n      \"value\": \"data/data.db\",\n      \"description\": \"データベースのパス。\"\n    },\n    \"problem_templates_directory\": {\n      \"value\": \"templates\",\n      \"description\": \"問題テンプレートディレクトリのパス。\"\n    },\n    \"output_tex_file\": {\n      \"value\": \"practice_problems.tex\",\n      \"description\": \"生成されるLaTeXファイルの名前。\"\n    },\n    \"latex_path\": {\n      \"value\": \"C:/Users/KEN/Desktop/TAROML/texlive/2024/bin/windows/xelatex.exe\",\n      \"description\": \"LaTeXコンパイラのパス。\"\n    },\n    \"cjk_main_font\": {\n      \"value\": \"Yu Gothic\",\n      \"description\": \"使用するCJKフォント。\"\n    },\n    \"problem_types\": {\n      \"value\": {\n        \"probability_definition\": 0.05,\n        \"conditional_probability\": 0.05,\n        \"distribution_functions\": 0.05,\n        \"joint_distribution\": 0.05,\n        \"statistical_inference\": 0.1,\n        \"regression_analysis\": 0.1,\n        \"linear_combination\": 0.1,\n        \"distribution_properties\": 0.1,\n        \"high_moment\": 0.1,\n        \"variance_analysis\": 0.1,\n        \"nonparametric_test\": 0.1,\n        \"beta_binomial_conjugate\": 0.05,\n        \"gamma_poisson_conjugate\": 0.05,\n        \"dirichlet_multinomial_conjugate\": 0.05,\n        \"binomial_poisson_approx\": 0.05,\n        \"poisson_normal_approx\": 0.05\n      },\n      \"description\": \"問題タイプとその割合。\"\n    },\n    \"pdf_generation\": {\n      \"value\": {\n        \"problem_count\": 9\n      },\n      \"description\": \"PDF生成の設定。\"\n    },\n    \"gui_settings\": {\n      \"value\": {\n        \"window_title\": \"統計検定1級 インタラクティブ問題解答システム\",\n        \"font_size\": 14\n      },\n      \"description\": \"GUIの設定。\"\n    },\n    \"enable_visualization\": {\n      \"value\": true,\n      \"description\": \"可視化機能の有効化。\"\n    }\n  },\n  \"scripts\": [\n    {\n      \"path\": \".vscode\\\\launch.json\",\n      \"overview\": \"このファイルはテキストファイルまたはその他の形式です。\\n行数: 17\\n\",\n      \"content\": \"{\\n    // IntelliSense を使用して利用可能な属性を学べます。\\n    // 既存の属性の説明をホバーして表示します。\\n    // 詳細情報は次を確認してください: https://go.microsoft.com/fwlink/?linkid=830387\\n    \\\"version\\\": \\\"0.2.0\\\",\\n    \\\"configurations\\\": [\\n        {\\n            \\\"name\\\": \\\"Python: 現在のファイル\\\",\\n            \\\"type\\\": \\\"python\\\",\\n            \\\"request\\\": \\\"launch\\\",\\n            \\\"program\\\": \\\"${file}\\\",\\n            \\\"console\\\": \\\"integratedTerminal\\\",\\n            \\\"justMyCode\\\": true,\\n            \\\"args\\\": [\\\"--generate-pdf\\\"]\\n\\n        }\\n    ]\\n}\"\n    },\n    {\n      \"path\": \"統計検定1級\\\\config.json\",\n      \"overview\": \"このファイルはテキストファイルまたはその他の形式です。\\n行数: 37\\n\",\n      \"content\": \"\"\n    },\n    {\n      \"path\": \"統計検定1級\\\\config.py\",\n      \"overview\": \"このスクリプトはPythonコードです。\\n定義されているクラス: Config。\\n定義されている関数: __init__, load_config, get。\\n\",\n      \"content\": \"# config.py\\nimport json, os\\nclass Config:\\n    def __init__(self, config_file='config.json'):\\n        script_dir=os.path.dirname(os.path.abspath(__file__))\\n        self.config_file=os.path.join(script_dir,config_file)\\n        self.settings=self.load_config()\\n    def load_config(self):\\n        if not os.path.exists(self.config_file):\\n            # ファイルが無ければ空dict\\n            return {}\\n        try:\\n            with open(self.config_file,'r',encoding='utf-8')as f:\\n                content=f.read().strip()\\n                if not content:\\n                    # 空ファイルなら{}扱い\\n                    return {}\\n                return json.loads(content)\\n        except Exception:\\n            # JSONパース失敗時も{}\\n            return {}\\n    def get(self,*keys,default=None):\\n        data=self.settings\\n        for k in keys:\\n            if isinstance(data, dict) and k in data:\\n                data=data[k]\\n            else:\\n                return default\\n        return data\\n\\nconfig=Config()\"\n    },\n    {\n      \"path\": \"統計検定1級\\\\database.py\",\n      \"overview\": \"このスクリプトはPythonコードです。\\n定義されているクラス: DatabaseManager。\\n定義されている関数: __init__, setup_database, save_problem。\\n\",\n      \"content\": \"# database.py\\nimport sqlite3,os\\nfrom config import config\\nclass DatabaseManager:\\n    def __init__(self):\\n        script_dir=os.path.dirname(os.path.abspath(__file__))\\n        db_path=config.get('settings','database_path',default='data/data.db')\\n        if db_path is None:\\n            db_path='data/data.db'\\n        self.db_name=os.path.join(script_dir,db_path)\\n        os.makedirs(os.path.dirname(self.db_name), exist_ok=True)\\n    def setup_database(self):\\n        conn=sqlite3.connect(self.db_name)\\n        c=conn.cursor()\\n        c.execute('CREATE TABLE IF NOT EXISTS problems (id TEXT PRIMARY KEY,date_created TEXT,problem_text TEXT,solution_text TEXT,problem_type TEXT)')\\n        conn.commit()\\n        conn.close()\\n    def save_problem(self,problem_id,date_created,problem_text,solution_text,problem_type):\\n        conn=sqlite3.connect(self.db_name)\\n        c=conn.cursor()\\n        c.execute('INSERT INTO problems VALUES (?,?,?,?,?)',(problem_id,date_created,problem_text,solution_text,problem_type))\\n        conn.commit()\\n        conn.close()\"\n    },\n    {\n      \"path\": \"統計検定1級\\\\graph.py\",\n      \"overview\": \"このスクリプトはPythonコードです。\\n定義されているクラス: ProbabilityDistributionVisualizer。\\n定義されている関数: __init__, plot_probability, plot_t_test, plot_regression, plot_time_series, plot_econometrics, plot_multivariate_normal, plot_distribution_properties, plot_high_moment, plot_variance_analysis, plot_nonparametric_test, plot_linear_combination, confidence_ellipse, mvn_pdf, ecdf。\\n\",\n      \"content\": \"# graph.py\\nimport matplotlib\\nmatplotlib.use('Agg')\\nimport matplotlib.pyplot as plt\\nimport math\\nimport numpy as np\\nimport logging\\nimport os\\nfrom math import factorial, exp\\nfrom scipy.stats import norm, t as t_dist, f as f_dist, poisson, expon\\nfrom statsmodels.graphics.tsaplots import plot_acf\\nimport traceback\\n\\nclass ProbabilityDistributionVisualizer:\\n    def __init__(self):\\n        pass\\n\\n    def plot_probability(self, params, output_path):\\n        # PMF + CDF、kを強調\\n        ptype = params.get('problem_type')\\n        try:\\n            if ptype == 'binomial':\\n                p=params['p']\\n                n=params['n']\\n                k=params['k']\\n                x=range(n+1)\\n                from math import comb\\n                pmf=[comb(n,i)*(p**i)*((1-p)**(n-i)) for i in x]\\n                cdf=np.cumsum(pmf)\\n\\n                plt.figure(figsize=(10,5))\\n                plt.subplot(1,2,1)\\n                plt.bar(x,pmf,color='skyblue')\\n                plt.title(f'Binomial PMF n={n},p={p}')\\n                plt.xlabel('k')\\n                plt.ylabel('P(X=k)')\\n                plt.axvline(k,color='red',linestyle='--',label='Target k')\\n                plt.legend()\\n\\n                plt.subplot(1,2,2)\\n                plt.step(x,cdf,where='post',color='green')\\n                plt.title('CDF')\\n                plt.xlabel('k')\\n                plt.ylabel('P(X<=k)')\\n                plt.axvline(k,color='red',linestyle='--')\\n                plt.tight_layout()\\n                plt.savefig(output_path)\\n                plt.close()\\n                return True\\n            elif ptype == 'poisson':\\n                lam=params['lambda']\\n                k=params['k']\\n                x=range(k+10+1)\\n                pmf=[(lam**i)*exp(-lam)/factorial(i) for i in x]\\n                cdf=np.cumsum(pmf)\\n\\n                plt.figure(figsize=(10,5))\\n                plt.subplot(1,2,1)\\n                plt.bar(x,pmf,color='orange')\\n                plt.title(f'Poisson(lambda={lam}) PMF')\\n                plt.xlabel('k')\\n                plt.ylabel('P(X=k)')\\n                plt.axvline(k,color='red',linestyle='--',label='Target k')\\n                plt.legend()\\n\\n                plt.subplot(1,2,2)\\n                plt.step(x,cdf,where='post',color='green')\\n                plt.title('CDF')\\n                plt.xlabel('k')\\n                plt.ylabel('P(X<=k)')\\n                plt.axvline(k,color='red',linestyle='--')\\n                plt.tight_layout()\\n                plt.savefig(output_path)\\n                plt.close()\\n                return True\\n            elif ptype == 'conditional_probability':\\n                # 条件付き確率の場合は特別なグラフ(分割表)を可視化\\n                P_A=params['P_A']\\n                P_BA=params['P_B_given_A']\\n                P_AB=params['P_A_and_B']\\n                # 簡単な棒グラフでP(A),P(B|A),P(A∩B)の関係を表示\\n                plt.figure()\\n                vals=[P_A,P_BA,P_AB]\\n                labels=['P(A)','P(B|A)','P(A∩B)']\\n                plt.bar(labels,vals,color=['blue','green','red'])\\n                plt.title('Conditional Probability Visualization')\\n                plt.ylabel('Probability')\\n                plt.tight_layout()\\n                plt.savefig(output_path)\\n                plt.close()\\n                return True\\n            else:\\n                # 未知タイプには何もしない\\n                return False\\n        except Exception as e:\\n            logging.error(f\\\"Error in plot_probability: {e}\\\")\\n            return False\\n\\n    def plot_t_test(self, params, output_path):\\n        # t分布+棄却域+標本平均から計算されたt値など\\n        # 追加で、標準正規近似や、p値部分の色塗りなど\\n        try:\\n            alpha = params['alpha']\\n            df = params['df']\\n            t_stat = params['t_stat']\\n            critical_value = params['critical_value']\\n\\n            x = np.linspace(t_dist.ppf(0.001, df), t_dist.ppf(0.999, df), 1000)\\n            y = t_dist.pdf(x, df)\\n\\n            plt.figure(figsize=(10,5))\\n            plt.plot(x, y, 'b-', label=f't-dist(df={df})')\\n            # 臨界値に線\\n            plt.axvline(x=critical_value,color='r',linestyle='--',label='critical +')\\n            plt.axvline(x=-critical_value,color='r',linestyle='--',label='critical -')\\n            # t統計量\\n            plt.axvline(x=t_stat,color='g',label='t-stat')\\n\\n            # p値領域を色付け\\n            # 両側検定として|t|>crit\\n            p_area_x = x[x>critical_value]\\n            plt.fill_between(p_area_x,t_dist.pdf(p_area_x,df),color='red',alpha=0.3)\\n            p_area_x2 = x[x<-critical_value]\\n            plt.fill_between(p_area_x2,t_dist.pdf(p_area_x2,df),color='red',alpha=0.3)\\n\\n            plt.title('t-test visualization')\\n            plt.xlabel('t')\\n            plt.ylabel('pdf')\\n            plt.legend()\\n            plt.tight_layout()\\n\\n            # 保存\\n            plt.savefig(output_path)\\n            plt.close()\\n            return True\\n        except Exception as e:\\n            logging.error(f\\\"t検定グラフ生成エラー: {e}\\\")\\n            return False\\n\\n    def plot_regression(self, x_values, y_values, beta_0_hat, beta_1_hat, output_path):\\n        # 回帰直線+データ点 + 残差ヒスト + Q-Qプロットなど複数図\\n        try:\\n            import statsmodels.api as sm\\n            X = sm.add_constant(x_values)\\n            model = sm.OLS(y_values, X).fit()\\n            residuals = model.resid\\n\\n            fig,axes = plt.subplots(2,2,figsize=(10,10))\\n\\n            # Scatter + regression line\\n            ax=axes[0,0]\\n            ax.scatter(x_values,y_values,color='blue',label='data')\\n            x_line=np.linspace(min(x_values),max(x_values),100)\\n            y_line=beta_0_hat+beta_1_hat*x_line\\n            ax.plot(x_line,y_line,color='red',label='reg line')\\n            ax.set_title('Data & Regression Line')\\n            ax.set_xlabel('X')\\n            ax.set_ylabel('Y')\\n            ax.legend()\\n\\n            # Residual histogram\\n            ax=axes[0,1]\\n            ax.hist(residuals,bins=20,color='green',alpha=0.7)\\n            ax.set_title('Residual Histogram')\\n            ax.set_xlabel('Residual')\\n            ax.set_ylabel('Frequency')\\n\\n            # Q-Q plot of residuals\\n            sm.qqplot(residuals, line='45', ax=axes[1,0],color='purple')\\n            axes[1,0].set_title('Q-Q plot of Residuals')\\n\\n            # Residuals vs fitted\\n            fitted=model.fittedvalues\\n            ax=axes[1,1]\\n            ax.scatter(fitted,residuals,color='orange')\\n            ax.axhline(y=0,color='red',linestyle='--')\\n            ax.set_title('Residuals vs Fitted')\\n            ax.set_xlabel('Fitted')\\n            ax.set_ylabel('Residuals')\\n\\n            fig.tight_layout()\\n            fig.savefig(output_path)\\n            plt.close(fig)\\n            return True\\n        except Exception as e:\\n            logging.error(f\\\"回帰分析グラフ生成中にエラー: {e}\\\")\\n            return False\\n\\n    def plot_time_series(self, params, output_path):\\n        # 時系列データ + ACFプロット\\n        try:\\n            ts = params['time_series']\\n\\n            fig,axes=plt.subplots(2,1,figsize=(10,8))\\n            axes[0].plot(ts, color='blue')\\n            axes[0].set_title('Time Series Data')\\n            axes[0].set_xlabel('Time')\\n            axes[0].set_ylabel('Value')\\n\\n            plot_acf(ts,ax=axes[1])\\n            axes[1].set_title('Autocorrelation Function')\\n\\n            fig.tight_layout()\\n            fig.savefig(output_path)\\n            plt.close(fig)\\n            return True\\n        except Exception as e:\\n            logging.error(f\\\"時系列分析グラフエラー: {e}\\\")\\n            return False\\n\\n    def plot_econometrics(self, params, output_path):\\n        # X1,YやX2,Y 散布図 + 残差分析\\n        try:\\n            import statsmodels.api as sm\\n            X = np.column_stack((params['x1_values'], params['x2_values']))\\n            Y = np.array(params['y_values'])\\n            Xc = sm.add_constant(X)\\n            model = sm.OLS(Y,Xc).fit()\\n            residuals = model.resid\\n            fitted = model.fittedvalues\\n\\n            fig,axes=plt.subplots(2,2,figsize=(10,10))\\n\\n            # X1 vs Y\\n            ax=axes[0,0]\\n            ax.scatter(params['x1_values'],Y,color='blue',alpha=0.7,label='X1-Y')\\n            ax.set_title('X1 vs Y')\\n            ax.set_xlabel('X1')\\n            ax.set_ylabel('Y')\\n\\n            # X2 vs Y\\n            ax=axes[0,1]\\n            ax.scatter(params['x2_values'],Y,color='green',alpha=0.7,label='X2-Y')\\n            ax.set_title('X2 vs Y')\\n            ax.set_xlabel('X2')\\n            ax.set_ylabel('Y')\\n\\n            # Residuals histogram\\n            ax=axes[1,0]\\n            ax.hist(residuals,bins=20,color='gray',alpha=0.7)\\n            ax.set_title('Residuals Histogram')\\n            ax.set_xlabel('Residual')\\n\\n            # Residuals vs Fitted\\n            ax=axes[1,1]\\n            ax.scatter(fitted,residuals,color='red',alpha=0.7)\\n            ax.axhline(y=0,color='black',linestyle='--')\\n            ax.set_title('Residuals vs Fitted')\\n            ax.set_xlabel('Fitted')\\n            ax.set_ylabel('Residuals')\\n\\n            fig.tight_layout()\\n            fig.savefig(output_path)\\n            plt.close(fig)\\n            return True\\n        except Exception as e:\\n            logging.error(f\\\"計量経済学グラフエラー: {e}\\\")\\n            return False\\n\\n    def plot_multivariate_normal(self, params, output_path):\\n        # 信頼楕円 + 等高線 + 周辺分布\\n        try:\\n            mu = params['mu']\\n            sigma = np.array(params['sigma'])\\n\\n            fig = plt.figure(figsize=(10,10))\\n            from matplotlib.patches import Ellipse\\n            import matplotlib.transforms as transforms\\n\\n            def confidence_ellipse(mu,cov,ax,n_std=1.96,facecolor='none',**kwargs):\\n                pearson = cov[0,1]/np.sqrt(cov[0,0]*cov[1,1])\\n                ell_radius_x = np.sqrt(1+pearson)\\n                ell_radius_y = np.sqrt(1-pearson)\\n                ellipse=Ellipse((0,0),width=ell_radius_x*2,height=ell_radius_y*2,facecolor=facecolor,**kwargs)\\n                scale_x = np.sqrt(cov[0,0])*n_std\\n                scale_y = np.sqrt(cov[1,1])*n_std\\n                transf = transforms.Affine2D().rotate_deg(45).scale(scale_x,scale_y).translate(mu[0],mu[1])\\n                ellipse.set_transform(transf+ax.transData)\\n                return ax.add_patch(ellipse)\\n\\n            ax=fig.add_subplot(2,2,1)\\n            ax.set_title('Confidence Ellipse')\\n            confidence_ellipse(mu,sigma,ax,edgecolor='red')\\n            ax.scatter(mu[0],mu[1],c='blue',marker='x',label='mean')\\n            ax.legend()\\n            ax.set_xlabel('X1')\\n            ax.set_ylabel('X2')\\n\\n            # 等高線\\n            ax2=fig.add_subplot(2,2,2)\\n            x = np.linspace(mu[0]-4*np.sqrt(sigma[0,0]),mu[0]+4*np.sqrt(sigma[0,0]),100)\\n            y = np.linspace(mu[1]-4*np.sqrt(sigma[1,1]),mu[1]+4*np.sqrt(sigma[1,1]),100)\\n            X,Y = np.meshgrid(x,y)\\n            pos = np.dstack((X,Y))\\n            def mvn_pdf(x, mu, cov):\\n                n=2\\n                det=np.linalg.det(cov)\\n                inv=np.linalg.inv(cov)\\n                diff=(x - mu)\\n                return (1./(2*np.pi*np.sqrt(det)))*np.exp(-0.5*(diff@inv@diff.T))\\n            Z=np.empty(X.shape)\\n            for i in range(X.shape[0]):\\n                for j in range(X.shape[1]):\\n                    Z[i,j]=mvn_pdf(np.array([X[i,j],Y[i,j]]),np.array(mu),sigma)\\n            ax2.contour(X,Y,Z,levels=5,cmap='Blues')\\n            ax2.set_title('Contour')\\n\\n            # 周辺分布\\n            ax3=fig.add_subplot(2,2,3)\\n            # marginal X1\\n            X_marg = norm(loc=mu[0],scale=np.sqrt(sigma[0,0]))\\n            x_line = np.linspace(mu[0]-4*np.sqrt(sigma[0,0]),mu[0]+4*np.sqrt(sigma[0,0]),100)\\n            y_line = X_marg.pdf(x_line)\\n            ax3.plot(x_line,y_line,'r-')\\n            ax3.set_title('Marginal X1 distribution')\\n            ax3.set_xlabel('X1')\\n            ax3.set_ylabel('pdf')\\n\\n            ax4=fig.add_subplot(2,2,4)\\n            Y_marg=norm(loc=mu[1],scale=np.sqrt(sigma[1,1]))\\n            y_line=Y_marg.pdf(x_line)\\n            ax4.plot(x_line,y_line,'g-')\\n            ax4.set_title('Marginal X2 distribution')\\n            ax4.set_xlabel('X2')\\n            ax4.set_ylabel('pdf')\\n\\n            fig.tight_layout()\\n            fig.savefig(output_path)\\n            plt.close(fig)\\n            return True\\n        except Exception as e:\\n            logging.error(f\\\"多変量正規分布グラフエラー: {e}\\\")\\n            return False\\n\\n    def plot_distribution_properties(self, params, output_path):\\n        # 選択された分布に対し、PDF+平均分散表示ライン\\n        try:\\n            dist = params['distribution']\\n            plt.figure(figsize=(10,5))\\n            if dist=='正規分布':\\n                mu=0; sigma=1\\n                x=np.linspace(mu-4*sigma,mu+4*sigma,200)\\n                y=norm.pdf(x,mu,sigma)\\n                plt.plot(x,y,'b-')\\n                plt.axvline(mu,color='r',linestyle='--',label='mean')\\n                plt.axvline(mu+sigma,color='g',linestyle=':',label='mean+sigma')\\n                plt.axvline(mu-sigma,color='g',linestyle=':')\\n                plt.title('Normal Distribution (mu=0,sigma=1)')\\n                plt.legend()\\n            elif dist=='ポアソン分布':\\n                lam=3\\n                x=np.arange(0,15)\\n                y=poisson.pmf(x,lam)\\n                plt.bar(x,y,color='skyblue')\\n                plt.axvline(lam,color='r',linestyle='--',label='mean=lambda=3')\\n                plt.title('Poisson(lambda=3)')\\n                plt.legend()\\n            elif dist=='指数分布':\\n                lam=1\\n                x=np.linspace(0,5,200)\\n                y=expon.pdf(x,scale=1/lam)\\n                plt.plot(x,y,'b-')\\n                plt.axvline(1/lam,color='r',linestyle='--',label='mean=1/lambda')\\n                plt.title('Exponential(lambda=1)')\\n                plt.legend()\\n            plt.tight_layout()\\n            plt.savefig(output_path)\\n            plt.close()\\n            return True\\n        except Exception as e:\\n            logging.error(f\\\"分布性質グラフエラー: {e}\\\")\\n            return False\\n\\n    def plot_high_moment(self, params, output_path):\\n        # 正規分布pdf上に平均、±σ、n次モーメント近傍など\\n        try:\\n            mu=params['mu']\\n            sigma=params['sigma']\\n            n=params['n']\\n            moment=params['moment']\\n            x=np.linspace(mu-4*sigma,mu+4*sigma,200)\\n            y=norm.pdf(x,mu,sigma)\\n\\n            plt.figure(figsize=(10,5))\\n            plt.plot(x,y,'b-',label='Normal pdf')\\n            plt.axvline(mu,color='r',linestyle='--',label='mean')\\n            plt.axvline(mu+sigma,color='g',linestyle=':',label='mu±sigma')\\n            plt.axvline(mu-sigma,color='g',linestyle=':')\\n            plt.title(f'Normal distribution (mu={mu}, sigma={sigma}). n={n}-th moment={moment}')\\n            plt.legend()\\n            plt.tight_layout()\\n            plt.savefig(output_path)\\n            plt.close()\\n            return True\\n        except Exception as e:\\n            logging.error(f\\\"高次モーメントグラフエラー: {e}\\\")\\n            return False\\n\\n    def plot_variance_analysis(self, params, output_path):\\n        # 箱ひげ図など\\n        try:\\n            group_count=params['group_count']\\n            sample_sizes=params['sample_sizes']\\n            means=params['means']\\n            variances=params['variances']\\n\\n            # データを仮に正規生成して可視化\\n            # 単に箱ひげ図で分布の違いを視覚化\\n            data=[]\\n            for i in range(group_count):\\n                # 各グループ：mean, varから乱数生成\\n                np.random.seed(i)\\n                samples=np.random.normal(means[i],np.sqrt(variances[i]),sample_sizes[i])\\n                data.append(samples)\\n\\n            plt.figure(figsize=(8,6))\\n            plt.boxplot(data,labels=[f'Group{i+1}' for i in range(group_count)])\\n            plt.title('ANOVA: Boxplots of groups')\\n            plt.ylabel('Value')\\n            plt.tight_layout()\\n            plt.savefig(output_path)\\n            plt.close()\\n            return True\\n        except Exception as e:\\n            logging.error(f\\\"分散分析グラフエラー: {e}\\\")\\n            return False\\n\\n    def plot_nonparametric_test(self, params, output_path):\\n        # ノンパラ検定：2サンプルECDFなど\\n        try:\\n            s1=params['sample1']\\n            s2=params['sample2']\\n\\n            def ecdf(data):\\n                d_sorted = np.sort(data)\\n                y = np.arange(1,len(d_sorted)+1)/len(d_sorted)\\n                return d_sorted,y\\n\\n            x1,y1=ecdf(s1)\\n            x2,y2=ecdf(s2)\\n\\n            plt.figure(figsize=(8,6))\\n            plt.step(x1,y1,where='post',label='Sample1 ECDF',color='blue')\\n            plt.step(x2,y2,where='post',label='Sample2 ECDF',color='red')\\n            plt.title('Nonparametric Test Visualization')\\n            plt.xlabel('Value')\\n            plt.ylabel('ECDF')\\n            plt.legend()\\n            plt.tight_layout()\\n            plt.savefig(output_path)\\n            plt.close()\\n            return True\\n        except Exception as e:\\n            logging.error(f\\\"ノンパラ検定グラフエラー: {e}\\\")\\n            return False\\n\\n    def plot_linear_combination(self, params, output_path):\\n        # 合成Z=aX+bYをシミュレーション\\n        try:\\n            a=params['a']\\n            b=params['b']\\n            mu1=params['mu1']\\n            mu2=params['mu2']\\n            sig1=params['sigma1_squared']\\n            sig2=params['sigma2_squared']\\n            # シミュレーションでZ生成\\n            np.random.seed(123)\\n            x = np.random.normal(mu1, np.sqrt(sig1),1000)\\n            y = np.random.normal(mu2, np.sqrt(sig2),1000)\\n            Z=a*x+b*y\\n            plt.figure(figsize=(8,6))\\n            plt.hist(Z,bins=30,density=True,alpha=0.7,color='purple',label='Simulated Z')\\n            # 理論分布：N(a*mu1+b*mu2, a²sigma1+b²sigma2)\\n            E_Z=a*mu1+b*mu2\\n            Var_Z=a**2*sig1+b**2*sig2\\n            X_line=np.linspace(E_Z-4*np.sqrt(Var_Z),E_Z+4*np.sqrt(Var_Z),200)\\n            Y_line=norm.pdf(X_line,E_Z,np.sqrt(Var_Z))\\n            plt.plot(X_line,Y_line,'r-',label='Theoretical PDF')\\n            plt.title('Linear Combination Distribution')\\n            plt.xlabel('Z')\\n            plt.ylabel('Density')\\n            plt.legend()\\n            plt.tight_layout()\\n            plt.savefig(output_path)\\n            plt.close()\\n            return True\\n        except Exception as e:\\n            logging.error(f\\\"線形結合グラフエラー: {e}\\\")\\n            return False\\n\"\n    },\n    {\n      \"path\": \"統計検定1級\\\\gui.py\",\n      \"overview\": \"このスクリプトはPythonコードです。\\n定義されているクラス: InteractiveSolverGUI。\\n定義されている関数: __init__, run。\\n\",\n      \"content\": \"# gui.py\\nimport tkinter as tk\\nclass InteractiveSolverGUI:\\n    def __init__(self):\\n        self.root=tk.Tk()\\n        self.root.title('統計検定1級 インタラクティブ問題解答システム')\\n    def run(self):\\n        self.root.mainloop()\"\n    },\n    {\n      \"path\": \"統計検定1級\\\\main.py\",\n      \"overview\": \"このスクリプトはPythonコードです。\\n定義されている関数: main。\\n\",\n      \"content\": \"# main.py\\nimport sys\\nfrom main_app import MainApp\\nfrom gui import InteractiveSolverGUI\\nfrom config import config\\n\\ndef main():\\n    pdf_gen=config.get('settings','pdf_generation',default={})\\n    problem_count=pdf_gen.get('problem_count',9)\\n\\n    if len(sys.argv)>1 and sys.argv[1]=='--generate-pdf':\\n        app=MainApp()\\n        app.generate_and_compile(problem_count)\\n    else:\\n        gui=InteractiveSolverGUI()\\n        gui.run()\\n\\nif __name__=='__main__':\\n    main()\\n\"\n    },\n    {\n      \"path\": \"統計検定1級\\\\main_app.py\",\n      \"overview\": \"このスクリプトはPythonコードです。\\n定義されているクラス: MainApp。\\n定義されている関数: __init__, generate_latex_header, generate_and_compile, compile_latex。\\n\",\n      \"content\": \"# main_app.py\\nimport os,subprocess\\nfrom config import config\\nfrom problem_generator import ProblemGenerator\\nimport logging\\nimport traceback\\n\\nclass MainApp:\\n    def __init__(self):\\n        # 必要設定を__init__でまとめて取得\\n        self.output_tex_file = config.get('settings','output_tex_file',default='practice_problems.tex')\\n        if self.output_tex_file is None:\\n            self.output_tex_file='practice_problems.tex'\\n\\n        self.enable_visualization = config.get('settings','enable_visualization',default=True)\\n        self.problem_templates_dir = config.get('settings','problem_templates_directory',default='templates')\\n        self.output_directory = config.get('settings','output_directory',default='outputs')\\n        self.latex_path = config.get('settings','latex_path',default='xelatex')\\n\\n        # ProblemGeneratorの生成\\n        self.generator = ProblemGenerator()\\n\\n    def generate_latex_header(self):\\n        cjk_font=config.get('settings','cjk_main_font',default='Yu Gothic')\\n        if cjk_font is None:\\n            cjk_font='Yu Gothic'\\n        header=[\\n            '\\\\\\\\documentclass{article}',\\n            '\\\\\\\\usepackage{amsmath}',\\n            '\\\\\\\\usepackage{amssymb}',\\n            '\\\\\\\\usepackage{graphicx}',\\n            '\\\\\\\\usepackage{float}',\\n            '\\\\\\\\usepackage{geometry}',\\n            '\\\\\\\\usepackage{xeCJK}',\\n            '\\\\\\\\usepackage{fontspec}',\\n            '\\\\\\\\setmainfont{Times New Roman}',\\n            f'\\\\\\\\setCJKmainfont{{{cjk_font}}}',\\n            '\\\\\\\\geometry{a4paper, margin=1in}',\\n            '\\\\\\\\begin{document}'\\n        ]\\n        return header\\n\\n    def generate_and_compile(self,problem_count):\\n        script_dir=os.path.dirname(os.path.abspath(__file__))\\n        latex_content=self.generate_latex_header()\\n\\n        out_dir = self.output_directory\\n        if out_dir is None:\\n            out_dir='outputs'\\n        output_dir=os.path.join(script_dir,out_dir)\\n        os.makedirs(output_dir,exist_ok=True)\\n        tex_file_path=os.path.join(output_dir,self.output_tex_file)\\n\\n        for idx in range(1,problem_count+1):\\n            try:\\n                result=self.generator.generate_problem()\\n                if result:\\n                    problem_id,date_created,problem_type,problem_text,solution_text,graph_filename=result\\n\\n                    latex_content.append(f'\\\\\\\\section*{{問題 {idx}}}')\\n                    latex_content.append(problem_text)\\n\\n                    if self.enable_visualization and graph_filename:\\n                        latex_content.append('\\\\\\\\begin{figure}[H]')\\n                        latex_content.append('\\\\\\\\centering')\\n                        graph_relative_path=os.path.join('graphs',graph_filename).replace('\\\\\\\\','/')\\n                        latex_content.append(f'\\\\\\\\includegraphics[width=0.8\\\\\\\\textwidth]{{{graph_relative_path}}}')\\n                        latex_content.append('\\\\\\\\end{figure}')\\n\\n                    latex_content.append('\\\\\\\\subsection*{解答}')\\n                    latex_content.append(solution_text)\\n                    latex_content.append('\\\\\\\\newpage')\\n                else:\\n                    logging.warning(f\\\"問題 {idx} の生成に失敗しました。\\\")\\n                    print(f\\\"問題 {idx} の生成に失敗しました。\\\")\\n            except Exception as e:\\n                logging.error(f\\\"問題 {idx} の生成中にエラー: {e}\\\")\\n                logging.error(traceback.format_exc())\\n                print(f\\\"問題 {idx} 生成エラー。ログを確認\\\")\\n\\n        templates_dir = self.problem_templates_dir\\n        if templates_dir is None:\\n            templates_dir='templates'\\n        latex_content.append('\\\\\\\\clearpage')\\n        latex_content.append('\\\\\\\\input{../'+templates_dir+'/distribution_relations.tex}')\\n\\n        latex_content.append('\\\\\\\\end{document}')\\n\\n        with open(tex_file_path,'w',encoding='utf-8')as tex_file:\\n            tex_file.write('\\\\n'.join(latex_content))\\n\\n        self.compile_latex(tex_file_path)\\n\\n    def compile_latex(self,tex_file_path):\\n        tex_file_name=os.path.basename(tex_file_path)\\n        latex_path=self.latex_path\\n        if not latex_path or not os.path.exists(latex_path):\\n            print(f\\\"LaTeXコンパイラが見つかりません: {latex_path}\\\")\\n            return\\n        process=subprocess.run([latex_path,'-interaction=nonstopmode',tex_file_name],cwd=os.path.dirname(tex_file_path),stdout=subprocess.PIPE,stderr=subprocess.PIPE,text=True)\\n        log_file_path=os.path.join(os.path.dirname(tex_file_path),'latex_compile.log')\\n        with open(log_file_path,'w',encoding='utf-8')as f:\\n            f.write(process.stdout or '')\\n            f.write(process.stderr or '')\\n        if process.returncode!=0:\\n            print(\\\"LaTeXコンパイルでエラー\\\")\\n        else:\\n            pdf_file=tex_file_path.replace('.tex','.pdf')\\n            if os.path.exists(pdf_file):\\n                print(f\\\"PDF生成成功: {pdf_file}\\\")\\n            else:\\n                print(\\\"PDFファイル未発見\\\")\\n\"\n    },\n    {\n      \"path\": \"統計検定1級\\\\problem_generator.py\",\n      \"overview\": \"このスクリプトはPythonコードです。\\n定義されているクラス: ProblemGenerator。\\n定義されている関数: __init__, load_topics, get_problem_types_by_topic, generate_problem。\\n\",\n      \"content\": \"# problem_generator.py\\nimport random,os,logging,uuid\\nfrom datetime import datetime\\nfrom config import config\\nfrom database import DatabaseManager\\nfrom problem_types.problem_factory import ProblemFactory\\nimport json\\nimport traceback\\n\\nclass ProblemGenerator:\\n    def __init__(self):\\n        self.db_path = config.get('settings','database_path',default='data/data.db')\\n        self.output_directory = config.get('settings','output_directory',default='outputs')\\n        self.enable_visualization = config.get('settings','enable_visualization',default=True)\\n        self.problem_types_weights = config.get('settings','problem_types',default={})\\n\\n        self.factory=ProblemFactory()\\n        self.db_manager=DatabaseManager()\\n        self.db_manager.setup_database()\\n\\n        script_dir=os.path.dirname(os.path.abspath(__file__))\\n        out_dir=self.output_directory\\n        if out_dir is None:\\n            out_dir='outputs'\\n        self.output_dir=os.path.join(script_dir,out_dir,'graphs')\\n        os.makedirs(self.output_dir,exist_ok=True)\\n        self.topics_data=self.load_topics()\\n\\n    def load_topics(self):\\n        script_dir=os.path.dirname(os.path.abspath(__file__))\\n        topics_file=os.path.join(script_dir,'topics.json')\\n        if not os.path.exists(topics_file):\\n            raise FileNotFoundError(f\\\"'{topics_file}'がない\\\")\\n        with open(topics_file,'r',encoding='utf-8')as f:\\n            return json.load(f)\\n\\n    def get_problem_types_by_topic(self,topic):\\n        return self.topics_data['topics'].get(topic,[])\\n\\n    def generate_problem(self,selected_topic=None):\\n        try:\\n            ptypes=self.problem_types_weights\\n            if selected_topic:\\n                problem_types=self.get_problem_types_by_topic(selected_topic)\\n                if not problem_types:\\n                    return None\\n                problem_type=random.choice(problem_types)\\n            else:\\n                pts=list(ptypes.keys())\\n                pwt=list(ptypes.values())\\n                if not pts:\\n                    pts=['probability']\\n                    pwt=[1.0]\\n                problem_type=random.choices(pts,weights=pwt,k=1)[0]\\n\\n            problem=self.factory.create_problem(problem_type)\\n            problem.generate_parameters()\\n            problem_text=problem.generate_problem_text()\\n            solution_text=problem.generate_solution_text()\\n\\n            enable_vis=self.enable_visualization\\n            if enable_vis is None:\\n                enable_vis=True\\n\\n            graph_filename=None\\n            if enable_vis:\\n                graph_filename=f\\\"graph_{uuid.uuid4().hex}.png\\\"\\n                graph_filepath=os.path.join(self.output_dir,graph_filename)\\n                if not problem.generate_graph(graph_filepath):\\n                    graph_filename=None\\n\\n            problem_id=uuid.uuid4().hex\\n            date_created=datetime.now().strftime('%Y-%m-%d %H:%M:%S')\\n\\n            self.db_manager.save_problem(problem_id,date_created,problem_text,solution_text,problem_type)\\n\\n            return problem_id,date_created,problem_type,problem_text,solution_text,graph_filename\\n        except Exception as e:\\n            logging.error(f\\\"問題生成エラー: {e}\\\")\\n            logging.error(traceback.format_exc())\\n            return None\\n\"\n    },\n    {\n      \"path\": \"統計検定1級\\\\sympy_solver.py\",\n      \"overview\": \"このスクリプトはPythonコードです。\\n定義されているクラス: SympySolver。\\n定義されている関数: __init__, check_equivalence。\\n\",\n      \"content\": \"# sympy_solver.py\\nclass SympySolver:\\n    def __init__(self):\\n        pass\\n    def check_equivalence(self,user_input,correct_answer):\\n        # 簡略化:常にFalseでエラーなし\\n        return False,\\\"\\\"\"\n    },\n    {\n      \"path\": \"統計検定1級\\\\topics.json\",\n      \"overview\": \"このファイルはテキストファイルまたはその他の形式です。\\n行数: 28\\n\",\n      \"content\": \"{\\n  \\\"topics\\\": {\\n    \\\"確率論\\\": [\\n      \\\"probability_definition\\\",\\n      \\\"conditional_probability\\\",\\n      \\\"distribution_functions\\\",\\n      \\\"joint_distribution\\\",\\n      \\\"probability\\\"\\n    ],\\n    \\\"統計的推定\\\": [\\n      \\\"statistical_inference\\\",\\n      \\\"t_test\\\"\\n    ],\\n    \\\"統計的検定\\\": [\\n      \\\"statistical_inference\\\",\\n      \\\"t_test\\\"\\n    ],\\n    \\\"回帰分析\\\": [\\n      \\\"regression_analysis\\\"\\n    ],\\n    \\\"分散分析\\\": [\\n      \\\"variance_analysis\\\"\\n    ],\\n    \\\"ノンパラメトリック検定\\\": [\\n      \\\"nonparametric_test\\\"\\n    ]\\n  }\\n}\\n\"\n    },\n    {\n      \"path\": \"統計検定1級\\\\__init__.py\",\n      \"overview\": \"このスクリプトはPythonコードです。\\n\",\n      \"content\": \"\"\n    },\n    {\n      \"path\": \"統計検定1級\\\\.vscode\\\\launch.json\",\n      \"overview\": \"このファイルはテキストファイルまたはその他の形式です。\\n行数: 14\\n\",\n      \"content\": \"{\\n    \\\"version\\\": \\\"0.2.0\\\",\\n    \\\"configurations\\\": [\\n        {\\n            \\\"name\\\": \\\"Python: 現在のファイル\\\",\\n            \\\"type\\\": \\\"python\\\",\\n            \\\"request\\\": \\\"launch\\\",\\n            \\\"program\\\": \\\"${file}\\\",\\n            \\\"console\\\": \\\"integratedTerminal\\\",\\n            \\\"justMyCode\\\": true,\\n            \\\"args\\\": [\\\"--generate-pdf\\\"]\\n\\n        }\\n    ]\\n}\"\n    },\n    {\n      \"path\": \"統計検定1級\\\\problem_types\\\\conjugate_problems.py\",\n      \"overview\": \"このスクリプトはPythonコードです。\\n定義されているクラス: BetaBinomialConjugateProblem, GammaPoissonConjugateProblem, DirichletMultinomialConjugateProblem, BinomialPoissonApproxProblem, PoissonNormalApproxProblem。\\n定義されている関数: __init__, generate_parameters, generate_explanation, generate_graph, __init__, generate_parameters, generate_explanation, generate_graph, __init__, generate_parameters, generate_explanation, generate_graph, __init__, generate_parameters, generate_explanation, generate_graph, __init__, generate_parameters, generate_explanation, generate_graph, B, B。\\n\",\n      \"content\": \"# conjugate_problems.py\\nimport math,random\\nfrom problem_types.problem import Problem\\nfrom math import comb,factorial,exp,gamma\\nimport numpy as np\\n\\nclass BetaBinomialConjugateProblem(Problem):\\n    def __init__(self):\\n        super().__init__('beta_binomial_conjugate_problem.tex')\\n    def generate_parameters(self):\\n        self.params['alpha']=random.randint(1,5)\\n        self.params['beta']=random.randint(1,5)\\n        self.params['n']=random.randint(5,20)\\n        self.params['k']=random.randint(0,self.params['n'])\\n        def B(x,y):\\n            return (gamma(x)*gamma(y))/gamma(x+y)\\n        p_x=comb(self.params['n'],self.params['k'])*B(self.params['k']+self.params['alpha'],self.params['n']-self.params['k']+self.params['beta'])/B(self.params['alpha'],self.params['beta'])\\n        self.params['probability']=round(p_x,4)\\n    def generate_explanation(self):\\n        return \\\"Beta+Binomial->Beta-Binomial\\\"\\n    def generate_graph(self,o):return False\\n\\nclass GammaPoissonConjugateProblem(Problem):\\n    def __init__(self):\\n        super().__init__('gamma_poisson_conjugate_problem.tex')\\n    def generate_parameters(self):\\n        self.params['alpha']=random.randint(1,5)\\n        self.params['beta']=random.uniform(0.5,2.0)\\n        self.params['k']=random.randint(0,20)\\n        p=self.params['beta']/(self.params['beta']+1)\\n        q=1-p\\n        negbin_p=comb(self.params['k']+self.params['alpha']-1,self.params['k'])*(q**self.params['k'])*(p**self.params['alpha'])\\n        self.params['probability']=round(negbin_p,4)\\n    def generate_explanation(self):\\n        return \\\"Gamma+Poisson->Negative Binomial\\\"\\n    def generate_graph(self,o):return False\\n\\nclass DirichletMultinomialConjugateProblem(Problem):\\n    def __init__(self):\\n        super().__init__('dirichlet_multinomial_conjugate_problem.tex')\\n    def generate_parameters(self):\\n        m=random.randint(2,4)\\n        self.params['m']=m\\n        self.params['n']=random.randint(5,20)\\n        self.params['alpha_vec']=[random.uniform(1,3) for _ in range(m)]\\n        counts=[0]*m\\n        remain=self.params['n']\\n        for i in range(m-1):\\n            c=random.randint(0,remain)\\n            counts[i]=c\\n            remain-=c\\n        counts[-1]=remain\\n        self.params['counts']=counts\\n        def B(alpha):\\n            import numpy as np\\n            return (np.prod([gamma(a) for a in alpha]))/gamma(sum(alpha))\\n        alpha_x=[self.params['alpha_vec'][i]+counts[i] for i in range(m)]\\n        num=B(alpha_x)\\n        den=B(self.params['alpha_vec'])\\n        multinomial_coef=math.factorial(self.params['n'])\\n        for c in counts:\\n            multinomial_coef/=math.factorial(c)\\n        p_x=multinomial_coef*(num/den)\\n        self.params['probability']=round(p_x,4)\\n    def generate_explanation(self):\\n        return \\\"Dirichlet+Multinomial->Dirichlet-Multinomial\\\"\\n    def generate_graph(self,o):return False\\n\\nclass BinomialPoissonApproxProblem(Problem):\\n    def __init__(self):\\n        super().__init__('binomial_poisson_approx_problem.tex')\\n    def generate_parameters(self):\\n        lam=random.uniform(2,5)\\n        n=random.randint(50,200)\\n        p=lam/n\\n        k=random.randint(0,int(lam*3))\\n        binom_p=comb(n,k)*(p**k)*((1-p)**(n-k))\\n        poisson_p=(lam**k)*exp(-lam)/math.factorial(k)\\n        self.params['n']=n\\n        self.params['p']=round(p,6)\\n        self.params['k']=k\\n        self.params['lambda']=round(lam,3)\\n        self.params['binom_p']=round(binom_p,6)\\n        self.params['poisson_p']=round(poisson_p,6)\\n    def generate_explanation(self):\\n        return \\\"Binomial->Poisson近似条件\\\"\\n    def generate_graph(self,o):return False\\n\\nclass PoissonNormalApproxProblem(Problem):\\n    def __init__(self):\\n        super().__init__('poisson_normal_approx_problem.tex')\\n    def generate_parameters(self):\\n        lam=random.randint(30,100)\\n        low=max(0,int(lam-3*math.sqrt(lam)))\\n        high=int(lam+3*math.sqrt(lam))\\n        k=random.randint(low,high)\\n        poisson_p=(lam**k)*exp(-lam)/math.factorial(k)\\n        self.params['lambda']=lam\\n        self.params['k']=k\\n        self.params['poisson_p']=round(poisson_p,6)\\n        self.params['mean']=lam\\n        self.params['variance']=lam\\n    def generate_explanation(self):\\n        return \\\"Poisson->Normal近似(λ大)\\\"\\n    def generate_graph(self,o):return False\"\n    },\n    {\n      \"path\": \"統計検定1級\\\\problem_types\\\\problem.py\",\n      \"overview\": \"このスクリプトはPythonコードです。\\n定義されているクラス: Problem, ProbabilityProblem, StatisticalInferenceProblem, RegressionAnalysisProblem, TimeSeriesAnalysisProblem, EconometricsProblem, LinearCombinationProblem, DistributionPropertiesProblem, HighMomentProblem, MultivariateNormalProblem, VarianceAnalysisProblem, NonParametricTestProblem, ProbabilityDefinitionProblem, ConditionalProbabilityProblem, DistributionFunctionsProblem, JointDistributionProblem。\\n定義されている関数: __init__, generate_parameters, generate_problem_text, generate_solution_text, generate_explanation, generate_graph, __init__, generate_parameters, _variant_binomial, _variant_poisson, _variant_conditional_probability, generate_explanation, generate_graph, __init__, generate_parameters, generate_graph, __init__, generate_parameters, generate_graph, __init__, generate_parameters, generate_graph, __init__, generate_parameters, generate_graph, __init__, generate_parameters, generate_explanation, generate_graph, __init__, generate_parameters, generate_graph, __init__, generate_parameters, generate_graph, __init__, generate_parameters, generate_explanation, generate_graph, __init__, generate_parameters, generate_explanation, generate_graph, __init__, generate_parameters, generate_explanation, generate_graph, __init__, generate_parameters, generate_explanation, generate_graph, __init__, generate_parameters, generate_explanation, generate_graph, __init__, generate_parameters, generate_explanation, generate_graph, __init__, generate_parameters, generate_explanation, generate_graph。\\n\",\n      \"content\": \"# problem_types/problem.py\\n\\nfrom abc import ABC, abstractmethod\\nimport os\\nimport random\\nimport logging\\nfrom config import config\\nfrom jinja2 import Environment, FileSystemLoader\\nfrom graph import ProbabilityDistributionVisualizer\\nimport traceback\\nfrom math import comb,exp,factorial\\nfrom scipy.stats import norm,stats\\n\\nclass Problem(ABC):\\n    def __init__(self, template_name):\\n        self.params = {}\\n        self.template_name = template_name\\n        templates_dir = os.path.join(\\n            os.path.dirname(os.path.abspath(__file__)),\\n            '..',\\n            config.get('problem_templates_directory', default='templates')\\n        )\\n        self.env = Environment(loader=FileSystemLoader(templates_dir), autoescape=False)\\n        self.visualizer = ProbabilityDistributionVisualizer()\\n\\n    @abstractmethod\\n    def generate_parameters(self):\\n        pass\\n\\n    def generate_problem_text(self):\\n        template = self.env.get_template(self.template_name)\\n        return template.render(**self.params, show_solution=False)\\n\\n    def generate_solution_text(self):\\n        template = self.env.get_template(self.template_name)\\n        self.params['explanation'] = self.generate_explanation()\\n        return template.render(**self.params, show_solution=True)\\n\\n    def generate_explanation(self):\\n        return \\\"\\\"\\n\\n    @abstractmethod\\n    def generate_graph(self, output_path):\\n        pass\\n\\nclass ProbabilityProblem(Problem):\\n    def __init__(self):\\n        super().__init__('probability_problem.tex')\\n    def generate_parameters(self):\\n        variants=[self._variant_binomial,self._variant_poisson,self._variant_conditional_probability]\\n        v=random.choice(variants)\\n        v()\\n    def _variant_binomial(self):\\n        self.params['problem_type']='binomial'\\n        self.params['n']=random.randint(5,20)\\n        self.params['p']=round(random.uniform(0.1,0.9),2)\\n        self.params['k']=random.randint(0,self.params['n'])\\n        from math import comb\\n        prob=comb(self.params['n'],self.params['k'])*(self.params['p']**self.params['k'])*((1-self.params['p'])**(self.params['n']-self.params['k']))\\n        self.params['probability']=round(prob,6)\\n    def _variant_poisson(self):\\n        self.params['problem_type']='poisson'\\n        self.params['lambda']=round(random.uniform(0.5,5.0),2)\\n        self.params['k']=random.randint(0,10)\\n        lam=self.params['lambda']\\n        k=self.params['k']\\n        prob=(lam**k)*exp(-lam)/factorial(k)\\n        self.params['probability']=round(prob,6)\\n    def _variant_conditional_probability(self):\\n        self.params['problem_type']='conditional_probability'\\n        self.params['P_A']=round(random.uniform(0.1,0.9),2)\\n        self.params['P_B_given_A']=round(random.uniform(0.1,0.9),2)\\n        self.params['P_A_and_B']=round(self.params['P_A']*self.params['P_B_given_A'],6)\\n    def generate_explanation(self):\\n        t=self.params['problem_type']\\n        if t=='binomial':\\n            return \\\"二項分布の公式を使用\\\"\\n        elif t=='poisson':\\n            return \\\"ポアソン分布の公式を使用\\\"\\n        elif t=='conditional_probability':\\n            return \\\"条件付き確率P(A∩B)=P(A)*P(B|A)\\\"\\n    def generate_graph(self,output_path):\\n        return self.visualizer.plot_probability(self.params,output_path)\\n\\nclass StatisticalInferenceProblem(Problem):\\n    def __init__(self):\\n        super().__init__('statistical_inference_problem.tex')\\n    def generate_parameters(self):\\n        self.params['sample_mean']=round(random.uniform(50,100),2)\\n        self.params['sample_std']=round(random.uniform(5,15),2)\\n        self.params['n']=random.randint(30,100)\\n        self.params['population_mean']=round(random.uniform(50,100),2)\\n        self.params['alpha']=round(random.uniform(0.01,0.1),2)\\n        t_stat = (self.params['sample_mean']-self.params['population_mean'])/(self.params['sample_std']/(self.params['n']**0.5))\\n        t_stat=round(t_stat,4)\\n        df=self.params['n']-1\\n        cv=round(stats.t.ppf(1-self.params['alpha']/2,df=df),4)\\n        reject='棄却' if abs(t_stat)>cv else '棄却しない'\\n        self.params['t_stat']=t_stat\\n        self.params['critical_value']=cv\\n        self.params['reject_null']=reject\\n        self.params['df']=df\\n    def generate_graph(self,output_path):\\n        return self.visualizer.plot_t_test(self.params,output_path)\\n\\nclass RegressionAnalysisProblem(Problem):\\n    def __init__(self):\\n        super().__init__('regression_analysis_problem.tex')\\n    def generate_parameters(self):\\n        self.params['beta_0']=round(random.uniform(0,10),2)\\n        self.params['beta_1']=round(random.uniform(-5,5),2)\\n        self.params['n']=random.randint(50,200)\\n        self.params['x_values']=[round(random.uniform(0,100),2) for _ in range(self.params['n'])]\\n        self.params['epsilon']=[round(random.gauss(0,10),2) for _ in range(self.params['n'])]\\n        self.params['y_values']=[self.params['beta_0']+self.params['beta_1']*x+e for x,e in zip(self.params['x_values'],self.params['epsilon'])]\\n        import numpy as np\\n        X=np.array(self.params['x_values'])\\n        Y=np.array(self.params['y_values'])\\n        beta_1_hat=np.cov(X,Y,bias=True)[0,1]/np.var(X)\\n        beta_0_hat=np.mean(Y)-beta_1_hat*np.mean(X)\\n        self.params['beta_0_hat']=round(beta_0_hat,4)\\n        self.params['beta_1_hat']=round(beta_1_hat,4)\\n    def generate_graph(self,output_path):\\n        return self.visualizer.plot_regression(self.params['x_values'],self.params['y_values'],self.params['beta_0_hat'],self.params['beta_1_hat'],output_path)\\n\\nclass TimeSeriesAnalysisProblem(Problem):\\n    def __init__(self):\\n        super().__init__('time_series_analysis_problem.tex')\\n    def generate_parameters(self):\\n        self.params['phi']=round(random.uniform(0.5,0.9),2)\\n        self.params['theta']=round(random.uniform(-0.5,0.5),2)\\n        self.params['n']=100\\n        self.params['epsilon']=[random.gauss(0,1) for _ in range(self.params['n'])]\\n        self.params['time_series']=[0]*self.params['n']\\n        for t in range(1,self.params['n']):\\n            self.params['time_series'][t]=self.params['phi']*self.params['time_series'][t-1]+self.params['epsilon'][t]+self.params['theta']*self.params['epsilon'][t-1]\\n    def generate_graph(self,output_path):\\n        return self.visualizer.plot_time_series(self.params,output_path)\\n\\nclass EconometricsProblem(Problem):\\n    def __init__(self):\\n        super().__init__('econometrics_problem.tex')\\n    def generate_parameters(self):\\n        self.params['beta_0']=round(random.uniform(0,5),2)\\n        self.params['beta_1']=round(random.uniform(0,1),2)\\n        self.params['beta_2']=round(random.uniform(-1,0),2)\\n        self.params['n']=random.randint(50,200)\\n        self.params['x1_values']=[round(random.uniform(0,100),2) for _ in range(self.params['n'])]\\n        self.params['x2_values']=[round(random.uniform(0,100),2) for _ in range(self.params['n'])]\\n        self.params['epsilon']=[round(random.gauss(0,5),2) for _ in range(self.params['n'])]\\n        self.params['y_values']=[self.params['beta_0']+self.params['beta_1']*x1+self.params['beta_2']*x2+e for x1,x2,e in zip(self.params['x1_values'],self.params['x2_values'],self.params['epsilon'])]\\n        import numpy as np\\n        X=np.column_stack((np.ones(self.params['n']),self.params['x1_values'],self.params['x2_values']))\\n        Y=np.array(self.params['y_values'])\\n        beta_hat=np.linalg.inv(X.T@X)@X.T@Y\\n        self.params['beta_0_hat']=round(beta_hat[0],4)\\n        self.params['beta_1_hat']=round(beta_hat[1],4)\\n        self.params['beta_2_hat']=round(beta_hat[2],4)\\n    def generate_graph(self,output_path):\\n        return self.visualizer.plot_econometrics(self.params,output_path)\\n\\nclass LinearCombinationProblem(Problem):\\n    def __init__(self):\\n        super().__init__('linear_combination_problem.tex')\\n    def generate_parameters(self):\\n        self.params['a']=random.randint(1,5)\\n        self.params['b']=random.randint(1,5)\\n        self.params['mu1']=round(random.uniform(0,10),2)\\n        self.params['mu2']=round(random.uniform(0,10),2)\\n        self.params['sigma1_squared']=round(random.uniform(1,5),2)\\n        self.params['sigma2_squared']=round(random.uniform(1,5),2)\\n        E_Z=self.params['a']*self.params['mu1']+self.params['b']*self.params['mu2']\\n        Var_Z=(self.params['a']**2)*self.params['sigma1_squared']+(self.params['b']**2)*self.params['sigma2_squared']\\n        self.params['E_Z']=round(E_Z,4)\\n        self.params['Var_Z']=round(Var_Z,4)\\n    def generate_explanation(self):\\n        return \\\"線形結合の期待値・分散計算\\\"\\n    def generate_graph(self,output_path):\\n        return self.visualizer.plot_linear_combination(self.params,output_path)\\n\\nclass DistributionPropertiesProblem(Problem):\\n    def __init__(self):\\n        super().__init__('distribution_properties_problem.tex')\\n    def generate_parameters(self):\\n        dist_choice=random.choice(['正規分布','ポアソン分布','指数分布'])\\n        self.params['distribution']=dist_choice\\n        if dist_choice=='正規分布':\\n            self.params['properties']={'mean':'\\\\\\\\mu','variance':'\\\\\\\\sigma^2'}\\n        elif dist_choice=='ポアソン分布':\\n            self.params['properties']={'mean':'\\\\\\\\lambda','variance':'\\\\\\\\lambda'}\\n        else:\\n            self.params['properties']={'mean':'1/\\\\\\\\lambda','variance':'1/\\\\\\\\lambda^2'}\\n    def generate_graph(self,output_path):\\n        return self.visualizer.plot_distribution_properties(self.params,output_path)\\n\\nclass HighMomentProblem(Problem):\\n    def __init__(self):\\n        super().__init__('high_moment_problem.tex')\\n    def generate_parameters(self):\\n        self.params['n']=random.randint(3,5)\\n        self.params['mu']=round(random.uniform(0,10),2)\\n        self.params['sigma']=round(random.uniform(1,5),2)\\n        from scipy.stats import norm\\n        m=norm.moment(self.params['n'],loc=self.params['mu'],scale=self.params['sigma'])\\n        self.params['moment']=round(m,4)\\n    def generate_graph(self,output_path):\\n        return self.visualizer.plot_high_moment(self.params,output_path)\\n\\nclass MultivariateNormalProblem(Problem):\\n    def __init__(self):\\n        super().__init__('multivariate_normal_problem.tex')\\n    def generate_parameters(self):\\n        self.params['mu']=[round(random.uniform(0,10),2) for _ in range(2)]\\n        self.params['sigma']=[[round(random.uniform(1,5),2),round(random.uniform(0,2),2)],[round(random.uniform(0,2),2),round(random.uniform(1,5),2)]]\\n    def generate_explanation(self):\\n        return \\\"多変量正規分布の性質を利用\\\"\\n    def generate_graph(self,output_path):\\n        return self.visualizer.plot_multivariate_normal(self.params,output_path)\\n\\nclass VarianceAnalysisProblem(Problem):\\n    def __init__(self):\\n        super().__init__('variance_analysis_problem.tex')\\n    def generate_parameters(self):\\n        self.params['group_count']=random.randint(2,4)\\n        self.params['sample_sizes']=[random.randint(5,20) for _ in range(self.params['group_count'])]\\n        self.params['means']=[round(random.uniform(10,50),2) for _ in range(self.params['group_count'])]\\n        self.params['variances']=[round(random.uniform(1,5),2) for _ in range(self.params['group_count'])]\\n        total_n=sum(self.params['sample_sizes'])\\n        group_count=self.params['group_count']\\n        means=self.params['means']\\n        variances=self.params['variances']\\n        sample_sizes=self.params['sample_sizes']\\n        grand_mean=sum([means[i]*sample_sizes[i] for i in range(group_count)])/total_n\\n        ssb=sum([sample_sizes[i]*(means[i]-grand_mean)**2 for i in range(group_count)])\\n        ssw=sum([(sample_sizes[i]-1)*variances[i] for i in range(group_count)])\\n        df_between=group_count-1\\n        df_within=total_n-group_count\\n        msb=ssb/df_between\\n        msw=ssw/df_within\\n        F=msb/msw\\n        alpha=0.05\\n        from scipy.stats import f\\n        F_critical=f.ppf(1-alpha,df_between,df_within)\\n        reject='棄却する' if F>F_critical else '棄却しない'\\n        self.params['F_value']=round(F,4)\\n        self.params['F_critical']=round(F_critical,4)\\n        self.params['reject_null']=reject\\n        self.params['df_between']=df_between\\n        self.params['df_within']=df_within\\n    def generate_explanation(self):\\n        return \\\"一元配置分散分析による検定\\\"\\n    def generate_graph(self,output_path):\\n        return self.visualizer.plot_variance_analysis(self.params,output_path)\\n\\nclass NonParametricTestProblem(Problem):\\n    def __init__(self):\\n        super().__init__('nonparametric_test_problem.tex')\\n    def generate_parameters(self):\\n        self.params['test_type']=random.choice(['Mann-Whitney U','Kruskal-Wallis','Wilcoxon Signed-Rank'])\\n        self.params['sample1']=[round(random.uniform(10,100),2) for _ in range(random.randint(5,20))]\\n        self.params['sample2']=[round(random.uniform(10,100),2) for _ in range(random.randint(5,20))]\\n        self.params['test_result']='有意差なし(例)'\\n    def generate_explanation(self):\\n        return \\\"ノンパラ検定で中央値差を検定\\\"\\n    def generate_graph(self,output_path):\\n        return self.visualizer.plot_nonparametric_test(self.params,output_path)\\n\\nclass ProbabilityDefinitionProblem(Problem):\\n    def __init__(self):\\n        super().__init__('probability_definition_problem.tex')\\n    def generate_parameters(self):\\n        self.params['P_A']=round(random.uniform(0.1,0.9),2)\\n        self.params['P_B']=round(random.uniform(0.1,0.9),2)\\n        self.params['P_A_and_B']=round(self.params['P_A']*self.params['P_B'],4)\\n    def generate_explanation(self):\\n        return \\\"独立性利用 P(A∩B)=P(A)*P(B)\\\"\\n    def generate_graph(self,output_path):\\n        # 単純な棒グラフでP(A), P(B)とP(A∩B)を表示\\n        try:\\n            PA=self.params['P_A']\\n            PB=self.params['P_B']\\n            PAB=self.params['P_A_and_B']\\n            plt.figure()\\n            plt.bar(['P(A)','P(B)','P(A∩B)'],[PA,PB,PAB],color=['blue','green','red'])\\n            plt.title('Probability Definition')\\n            plt.tight_layout()\\n            plt.savefig(output_path)\\n            plt.close()\\n            return True\\n        except:\\n            return False\\n\\nclass ConditionalProbabilityProblem(Problem):\\n    def __init__(self):\\n        super().__init__('conditional_probability_problem.tex')\\n    def generate_parameters(self):\\n        self.params['P_A']=round(random.uniform(0.2,0.8),2)\\n        self.params['P_B_given_A']=round(random.uniform(0.2,0.8),2)\\n        self.params['P_A_and_B']=round(self.params['P_A']*self.params['P_B_given_A'],4)\\n    def generate_explanation(self):\\n        return \\\"P(A∩B)=P(A)*P(B|A)\\\"\\n    def generate_graph(self,output_path):\\n        # graph.pyでconditional_probabilityで実装済み\\n        from graph import ProbabilityDistributionVisualizer\\n        vis=ProbabilityDistributionVisualizer()\\n        return vis.plot_probability(self.params,output_path)\\n\\nclass DistributionFunctionsProblem(Problem):\\n    def __init__(self):\\n        super().__init__('distribution_functions_problem.tex')\\n    def generate_parameters(self):\\n        self.params['function_type']=random.choice(['pdf','cdf'])\\n        self.params['distribution']=random.choice(['正規分布','指数分布'])\\n        if self.params['distribution']=='正規分布':\\n            self.params['mean']=round(random.uniform(-5,5),2)\\n            self.params['std']=round(random.uniform(1,3),2)\\n        else:\\n            self.params['lambda']=round(random.uniform(0.5,2.0),2)\\n    def generate_explanation(self):\\n        return \\\"pdfやcdf定義式利用\\\"\\n    def generate_graph(self,output_path):\\n        # ここは実装なし、増やしてもよいが現状テンプレ通り\\n        return False\\n\\nclass JointDistributionProblem(Problem):\\n    def __init__(self):\\n        super().__init__('joint_distribution_problem.tex')\\n    def generate_parameters(self):\\n        A_and_B=round(random.uniform(0.05,0.2),2)\\n        A_and_notB=round(random.uniform(0.05,0.2),2)\\n        notA_and_B=round(random.uniform(0.05,0.2),2)\\n        notA_and_notB=round(random.uniform(0.05,0.2),2)\\n        total=A_and_B+A_and_notB+notA_and_B+notA_and_notB\\n        A_and_B/=total\\n        A_and_notB/=total\\n        notA_and_B/=total\\n        notA_and_notB/=total\\n        self.params['joint_probabilities']={'A_and_B':round(A_and_B,4),'A_and_not_B':round(A_and_notB,4),'not_A_and_B':round(notA_and_B,4),'not_A_and_not_B':round(notA_and_notB,4)}\\n        P_A=A_and_B+A_and_notB\\n        P_B=A_and_B+notA_and_B\\n        P_BA=A_and_B/P_A if P_A>0 else 0.0\\n        self.params['P_A']=round(P_A,4)\\n        self.params['P_B']=round(P_B,4)\\n        self.params['P_B_given_A']=round(P_BA,4)\\n    def generate_explanation(self):\\n        return \\\"同時→周辺→条件付き確率\\\"\\n    def generate_graph(self,output_path):\\n        # 簡易的にjoint分布表をHeatmapで可視化\\n        try:\\n            p=self.params['joint_probabilities']\\n            matrix=np.array([[p['A_and_B'],p['A_and_not_B']],[p['not_A_and_B'],p['not_A_and_not_B']]])\\n            plt.figure()\\n            plt.imshow(matrix,cmap='Blues',interpolation='nearest')\\n            plt.colorbar(label='Probability')\\n            plt.xticks([0,1],['B','not B'])\\n            plt.yticks([0,1],['A','not A'])\\n            plt.title('Joint Distribution Heatmap')\\n            plt.tight_layout()\\n            plt.savefig(output_path)\\n            plt.close()\\n            return True\\n        except:\\n            return False\\n\"\n    },\n    {\n      \"path\": \"統計検定1級\\\\problem_types\\\\problem_factory.py\",\n      \"overview\": \"このスクリプトはPythonコードです。\\n定義されているクラス: ProblemFactory。\\n定義されている関数: __init__, create_problem。\\n\",\n      \"content\": \"# problem_types/problem_factory.py\\n\\nimport logging,traceback\\nfrom problem_types.problem import ProbabilityProblem,StatisticalInferenceProblem,RegressionAnalysisProblem,TimeSeriesAnalysisProblem,EconometricsProblem,LinearCombinationProblem,DistributionPropertiesProblem,HighMomentProblem,MultivariateNormalProblem,VarianceAnalysisProblem,NonParametricTestProblem,ProbabilityDefinitionProblem,ConditionalProbabilityProblem,DistributionFunctionsProblem,JointDistributionProblem\\nfrom problem_types.conjugate_problems import BetaBinomialConjugateProblem,GammaPoissonConjugateProblem,DirichletMultinomialConjugateProblem,BinomialPoissonApproxProblem,PoissonNormalApproxProblem\\n\\nclass ProblemFactory:\\n    def __init__(self):\\n        self.problem_classes={\\n            'probability':ProbabilityProblem,\\n            'statistical_inference':StatisticalInferenceProblem,\\n            'regression_analysis':RegressionAnalysisProblem,\\n            'time_series_analysis':TimeSeriesAnalysisProblem,\\n            'econometrics':EconometricsProblem,\\n            'linear_combination':LinearCombinationProblem,\\n            'distribution_properties':DistributionPropertiesProblem,\\n            'high_moment':HighMomentProblem,\\n            'multivariate_normal':MultivariateNormalProblem,\\n            'probability_definition':ProbabilityDefinitionProblem,\\n            'conditional_probability':ConditionalProbabilityProblem,\\n            'distribution_functions':DistributionFunctionsProblem,\\n            'joint_distribution':JointDistributionProblem,\\n            't_test':StatisticalInferenceProblem,\\n            'variance_analysis':VarianceAnalysisProblem,\\n            'nonparametric_test':NonParametricTestProblem,\\n            'beta_binomial_conjugate':BetaBinomialConjugateProblem,\\n            'gamma_poisson_conjugate':GammaPoissonConjugateProblem,\\n            'dirichlet_multinomial_conjugate':DirichletMultinomialConjugateProblem,\\n            'binomial_poisson_approx':BinomialPoissonApproxProblem,\\n            'poisson_normal_approx':PoissonNormalApproxProblem\\n        }\\n    def create_problem(self,problem_type):\\n        pc=self.problem_classes.get(problem_type)\\n        if pc:\\n            try:\\n                return pc()\\n            except Exception as e:\\n                logging.error(f\\\"{problem_type} problem generation error:{e}\\\")\\n                logging.error(traceback.format_exc())\\n                raise\\n        else:\\n            raise ValueError(f\\\"Unknown problem type:{problem_type}\\\")\"\n    },\n    {\n      \"path\": \"統計検定1級\\\\problem_types\\\\__init__.py\",\n      \"overview\": \"このスクリプトはPythonコードです。\\n\",\n      \"content\": \"\"\n    },\n    {\n      \"path\": \"統計検定1級\\\\templates\\\\distribution_properties_problem.tex\",\n      \"overview\": \"このファイルはテキストファイルまたはその他の形式です。\\n行数: 7\\n\",\n      \"content\": \"% distribution_properties_problem.tex\\n{% if not show_solution %}\\n{{ distribution }} の平均と分散を求めよ。\\n{% else %}\\n平均: ${{ properties.mean }}$, 分散: ${{ properties.variance }}$\\n{{ explanation }}\\n{% endif %}\\n\"\n    },\n    {\n      \"path\": \"統計検定1級\\\\templates\\\\distribution_relations.tex\",\n      \"overview\": \"このファイルはテキストファイルまたはその他の形式です。\\n行数: 7\\n\",\n      \"content\": \"% distribution_relations.tex\\n\\\\section*{Distribution Relations}\\n- Beta+Binomial -> Beta-Binomial\\n- Gamma+Poisson -> Negative Binomial\\n- Dirichlet+Multinomial -> Dirichlet-Multinomial\\n- Binomial->Poisson approximation\\n- Poisson->Normal approximation\\n\"\n    },\n    {\n      \"path\": \"統計検定1級\\\\templates\\\\econometrics_problem.tex\",\n      \"overview\": \"このファイルはテキストファイルまたはその他の形式です。\\n行数: 6\\n\",\n      \"content\": \"% econometrics_problem.tex\\n{% if not show_solution %}\\n計量経済学モデルに関する問題\\n{% else %}\\n解答と推定量\\n{{ explanation }}\\n{% endif %}\"\n    },\n    {\n      \"path\": \"統計検定1級\\\\templates\\\\high_moment_problem.tex\",\n      \"overview\": \"このファイルはテキストファイルまたはその他の形式です。\\n行数: 6\\n\",\n      \"content\": \"% high_moment_problem.tex\\n{% if not show_solution %}\\n正規分布の{{ n }}次モーメントは？(例: $E[X^{n}]$ )\\n{% else %}\\n$E[X^{n}]={{ moment }}$\\n{{ explanation }}\\n{% endif %}\"\n    },\n    {\n      \"path\": \"統計検定1級\\\\templates\\\\linear_combination_problem.tex\",\n      \"overview\": \"このファイルはテキストファイルまたはその他の形式です。\\n行数: 6\\n\",\n      \"content\": \"% linear_combination_problem.tex\\n{% if not show_solution %}\\nZ=aX+bY のE[Z],Var[Z]\\n{% else %}\\nE[Z],Var[Z]\\n{{ explanation }}\\n{% endif %}\"\n    },\n    {\n      \"path\": \"統計検定1級\\\\templates\\\\multivariate_normal_problem.tex\",\n      \"overview\": \"このファイルはテキストファイルまたはその他の形式です。\\n行数: 6\\n\",\n      \"content\": \"% multivariate_normal_problem.tex\\n{% if not show_solution %}\\n多変量正規に関する問題\\n{% else %}\\n解答\\n{{ explanation }}\\n{% endif %}\"\n    },\n    {\n      \"path\": \"統計検定1級\\\\templates\\\\nonparametric_test_problem.tex\",\n      \"overview\": \"このファイルはテキストファイルまたはその他の形式です。\\n行数: 6\\n\",\n      \"content\": \"% nonparametric_test_problem.tex\\n{% if not show_solution %}\\nノンパラ検定問題\\n{% else %}\\n検定結果\\n{{ explanation }}\\n{% endif %}\"\n    },\n    {\n      \"path\": \"統計検定1級\\\\templates\\\\poisson_normal_approx_problem.tex\",\n      \"overview\": \"このファイルはテキストファイルまたはその他の形式です。\\n行数: 6\\n\",\n      \"content\": \"% poisson_normal_approx_problem.tex\\n{% if not show_solution %}\\nPoisson→Normal近似\\n{% else %}\\nPoisson P=${{ poisson_p }}$, mean=${{mean}}$ var=${{variance}}$\\n{{ explanation }}\\n{% endif %}\"\n    },\n    {\n      \"path\": \"統計検定1級\\\\templates\\\\probability_definition_problem.tex\",\n      \"overview\": \"このファイルはテキストファイルまたはその他の形式です。\\n行数: 6\\n\",\n      \"content\": \"% probability_definition_problem.tex\\n{% if not show_solution %}\\nP(A∩B)求めよ\\n{% else %}\\n結果\\n{{ explanation }}\\n{% endif %}\"\n    },\n    {\n      \"path\": \"統計検定1級\\\\templates\\\\probability_problem.tex\",\n      \"overview\": \"このファイルはテキストファイルまたはその他の形式です。\\n行数: 7\\n\",\n      \"content\": \"% probability_problem.tex\\n{% if not show_solution %}\\n確率計算問題（例）\\n問題タイプ: {{ problem_type }}\\n{% else %}\\n解答と説明: {{ explanation }}\\n計算結果: P = {{ probability }}\\n{% endif %}\"\n    },\n    {\n      \"path\": \"統計検定1級\\\\templates\\\\regression_analysis_problem.tex\",\n      \"overview\": \"このファイルはテキストファイルまたはその他の形式です。\\n行数: 6\\n\",\n      \"content\": \"% regression_analysis_problem.tex\\n{% if not show_solution %}\\n回帰分析問題\\n{% else %}\\n回帰係数結果\\n{{ explanation }}\\n{% endif %}\"\n    },\n    {\n      \"path\": \"統計検定1級\\\\templates\\\\statistical_inference_problem.tex\",\n      \"overview\": \"このファイルはテキストファイルまたはその他の形式です。\\n行数: 6\\n\",\n      \"content\": \"% statistical_inference_problem.tex\\n{% if not show_solution %}\\n統計的推定/検定問題\\n{% else %}\\n検定結果\\n{{ explanation }}\\n{% endif %}\"\n    },\n    {\n      \"path\": \"統計検定1級\\\\templates\\\\time_series_analysis_problem.tex\",\n      \"overview\": \"このファイルはテキストファイルまたはその他の形式です。\\n行数: 6\\n\",\n      \"content\": \"% time_series_analysis_problem.tex\\n{% if not show_solution %}\\n時系列分析問題\\n{% else %}\\n解答と説明\\n{{ explanation }}\\n{% endif %}\"\n    },\n    {\n      \"path\": \"統計検定1級\\\\templates\\\\variance_analysis_problem.tex\",\n      \"overview\": \"このファイルはテキストファイルまたはその他の形式です。\\n行数: 6\\n\",\n      \"content\": \"% variance_analysis_problem.tex\\n{% if not show_solution %}\\n分散分析問題\\n{% else %}\\nANOVA結果\\n{{ explanation }}\\n{% endif %}\"\n    }\n  ]\n}"
    },
    {
      "path": "your_project\\collected_scripts - コピー (2).json",
      "overview": "JSONファイル (辞書)。キー: system_overview, settings, scripts\n",
      "content": "{\n  \"system_overview\": \"確率統計の網羅的理解\",\n  \"settings\": {\n    \"data_directory\": {\n      \"value\": \"data\",\n      \"description\": \"データディレクトリのパス。\"\n    },\n    \"log_level\": {\n      \"value\": \"INFO\",\n      \"description\": \"ログレベルの設定（例: INFO、DEBUG、ERROR）。\"\n    },\n    \"output_directory\": {\n      \"value\": \"outputs\",\n      \"description\": \"出力ディレクトリのパス。\"\n    },\n    \"database_path\": {\n      \"value\": \"data/data.db\",\n      \"description\": \"データベースのパス。\"\n    },\n    \"problem_templates_directory\": {\n      \"value\": \"templates\",\n      \"description\": \"問題テンプレートディレクトリのパス。\"\n    },\n    \"output_tex_file\": {\n      \"value\": \"practice_problems.tex\",\n      \"description\": \"生成されるLaTeXファイルの名前。\"\n    },\n    \"latex_path\": {\n      \"value\": \"C:/Users/KEN/Desktop/TAROML/texlive/2024/bin/windows/xelatex.exe\",\n      \"description\": \"LaTeXコンパイラのパス。\"\n    },\n    \"cjk_main_font\": {\n      \"value\": \"Yu Gothic\",\n      \"description\": \"使用するCJKフォント。\"\n    },\n    \"problem_types\": {\n      \"value\": {\n        \"probability_definition\": 0.05,\n        \"conditional_probability\": 0.05,\n        \"distribution_functions\": 0.05,\n        \"joint_distribution\": 0.05,\n        \"statistical_inference\": 0.1,\n        \"regression_analysis\": 0.1,\n        \"linear_combination\": 0.1,\n        \"distribution_properties\": 0.1,\n        \"high_moment\": 0.1,\n        \"variance_analysis\": 0.1,\n        \"nonparametric_test\": 0.1,\n        \"beta_binomial_conjugate\": 0.05,\n        \"gamma_poisson_conjugate\": 0.05,\n        \"dirichlet_multinomial_conjugate\": 0.05,\n        \"binomial_poisson_approx\": 0.05,\n        \"poisson_normal_approx\": 0.05\n      },\n      \"description\": \"問題タイプとその割合。\"\n    },\n    \"pdf_generation\": {\n      \"value\": {\n        \"problem_count\": 9\n      },\n      \"description\": \"PDF生成の設定。\"\n    },\n    \"gui_settings\": {\n      \"value\": {\n        \"window_title\": \"統計検定1級 インタラクティブ問題解答システム\",\n        \"font_size\": 14\n      },\n      \"description\": \"GUIの設定。\"\n    },\n    \"enable_visualization\": {\n      \"value\": true,\n      \"description\": \"可視化機能の有効化。\"\n    }\n  },\n  \"scripts\": [\n    {\n      \"path\": \".vscode\\\\launch.json\",\n      \"overview\": \"このファイルはテキストファイルまたはその他の形式です。\\n行数: 17\\n\",\n      \"content\": \"{\\n    // IntelliSense を使用して利用可能な属性を学べます。\\n    // 既存の属性の説明をホバーして表示します。\\n    // 詳細情報は次を確認してください: https://go.microsoft.com/fwlink/?linkid=830387\\n    \\\"version\\\": \\\"0.2.0\\\",\\n    \\\"configurations\\\": [\\n        {\\n            \\\"name\\\": \\\"Python: 現在のファイル\\\",\\n            \\\"type\\\": \\\"python\\\",\\n            \\\"request\\\": \\\"launch\\\",\\n            \\\"program\\\": \\\"${file}\\\",\\n            \\\"console\\\": \\\"integratedTerminal\\\",\\n            \\\"justMyCode\\\": true,\\n            \\\"args\\\": [\\\"--generate-pdf\\\"]\\n\\n        }\\n    ]\\n}\"\n    },\n    {\n      \"path\": \"統計検定1級\\\\config.json\",\n      \"overview\": \"このファイルはテキストファイルまたはその他の形式です。\\n行数: 37\\n\",\n      \"content\": \"\"\n    },\n    {\n      \"path\": \"統計検定1級\\\\config.py\",\n      \"overview\": \"このスクリプトはPythonコードです。\\n定義されているクラス: Config。\\n定義されている関数: __init__, load_config, get。\\n\",\n      \"content\": \"# config.py\\nimport json, os\\nclass Config:\\n    def __init__(self, config_file='config.json'):\\n        script_dir=os.path.dirname(os.path.abspath(__file__))\\n        self.config_file=os.path.join(script_dir,config_file)\\n        self.settings=self.load_config()\\n    def load_config(self):\\n        if not os.path.exists(self.config_file):\\n            # ファイルが無ければ空dict\\n            return {}\\n        try:\\n            with open(self.config_file,'r',encoding='utf-8')as f:\\n                content=f.read().strip()\\n                if not content:\\n                    # 空ファイルなら{}扱い\\n                    return {}\\n                return json.loads(content)\\n        except Exception:\\n            # JSONパース失敗時も{}\\n            return {}\\n    def get(self,*keys,default=None):\\n        data=self.settings\\n        for k in keys:\\n            if isinstance(data, dict) and k in data:\\n                data=data[k]\\n            else:\\n                return default\\n        return data\\n\\nconfig=Config()\"\n    },\n    {\n      \"path\": \"統計検定1級\\\\database.py\",\n      \"overview\": \"このスクリプトはPythonコードです。\\n定義されているクラス: DatabaseManager。\\n定義されている関数: __init__, setup_database, save_problem。\\n\",\n      \"content\": \"# database.py\\nimport sqlite3,os\\nfrom config import config\\nclass DatabaseManager:\\n    def __init__(self):\\n        script_dir=os.path.dirname(os.path.abspath(__file__))\\n        db_path=config.get('settings','database_path',default='data/data.db')\\n        if db_path is None:\\n            db_path='data/data.db'\\n        self.db_name=os.path.join(script_dir,db_path)\\n        os.makedirs(os.path.dirname(self.db_name), exist_ok=True)\\n    def setup_database(self):\\n        conn=sqlite3.connect(self.db_name)\\n        c=conn.cursor()\\n        c.execute('CREATE TABLE IF NOT EXISTS problems (id TEXT PRIMARY KEY,date_created TEXT,problem_text TEXT,solution_text TEXT,problem_type TEXT)')\\n        conn.commit()\\n        conn.close()\\n    def save_problem(self,problem_id,date_created,problem_text,solution_text,problem_type):\\n        conn=sqlite3.connect(self.db_name)\\n        c=conn.cursor()\\n        c.execute('INSERT INTO problems VALUES (?,?,?,?,?)',(problem_id,date_created,problem_text,solution_text,problem_type))\\n        conn.commit()\\n        conn.close()\"\n    },\n    {\n      \"path\": \"統計検定1級\\\\graph.py\",\n      \"overview\": \"このスクリプトはPythonコードです。\\n定義されているクラス: ProbabilityDistributionVisualizer。\\n定義されている関数: __init__, plot_probability, plot_t_test, plot_regression, plot_time_series, plot_econometrics, plot_multivariate_normal, plot_distribution_properties, plot_high_moment, plot_variance_analysis, plot_nonparametric_test, plot_linear_combination, confidence_ellipse, mvn_pdf, ecdf。\\n\",\n      \"content\": \"# graph.py\\nimport matplotlib\\nmatplotlib.use('Agg')\\nimport matplotlib.pyplot as plt\\nimport math\\nimport numpy as np\\nimport logging\\nimport os\\nfrom math import factorial, exp\\nfrom scipy.stats import norm, t as t_dist, f as f_dist, poisson, expon\\nfrom statsmodels.graphics.tsaplots import plot_acf\\nimport traceback\\n\\nclass ProbabilityDistributionVisualizer:\\n    def __init__(self):\\n        pass\\n\\n    def plot_probability(self, params, output_path):\\n        # PMF + CDF、kを強調\\n        ptype = params.get('problem_type')\\n        try:\\n            if ptype == 'binomial':\\n                p=params['p']\\n                n=params['n']\\n                k=params['k']\\n                x=range(n+1)\\n                from math import comb\\n                pmf=[comb(n,i)*(p**i)*((1-p)**(n-i)) for i in x]\\n                cdf=np.cumsum(pmf)\\n\\n                plt.figure(figsize=(10,5))\\n                plt.subplot(1,2,1)\\n                plt.bar(x,pmf,color='skyblue')\\n                plt.title(f'Binomial PMF n={n},p={p}')\\n                plt.xlabel('k')\\n                plt.ylabel('P(X=k)')\\n                plt.axvline(k,color='red',linestyle='--',label='Target k')\\n                plt.legend()\\n\\n                plt.subplot(1,2,2)\\n                plt.step(x,cdf,where='post',color='green')\\n                plt.title('CDF')\\n                plt.xlabel('k')\\n                plt.ylabel('P(X<=k)')\\n                plt.axvline(k,color='red',linestyle='--')\\n                plt.tight_layout()\\n                plt.savefig(output_path)\\n                plt.close()\\n                return True\\n            elif ptype == 'poisson':\\n                lam=params['lambda']\\n                k=params['k']\\n                x=range(k+10+1)\\n                pmf=[(lam**i)*exp(-lam)/factorial(i) for i in x]\\n                cdf=np.cumsum(pmf)\\n\\n                plt.figure(figsize=(10,5))\\n                plt.subplot(1,2,1)\\n                plt.bar(x,pmf,color='orange')\\n                plt.title(f'Poisson(lambda={lam}) PMF')\\n                plt.xlabel('k')\\n                plt.ylabel('P(X=k)')\\n                plt.axvline(k,color='red',linestyle='--',label='Target k')\\n                plt.legend()\\n\\n                plt.subplot(1,2,2)\\n                plt.step(x,cdf,where='post',color='green')\\n                plt.title('CDF')\\n                plt.xlabel('k')\\n                plt.ylabel('P(X<=k)')\\n                plt.axvline(k,color='red',linestyle='--')\\n                plt.tight_layout()\\n                plt.savefig(output_path)\\n                plt.close()\\n                return True\\n            elif ptype == 'conditional_probability':\\n                # 条件付き確率の場合は特別なグラフ(分割表)を可視化\\n                P_A=params['P_A']\\n                P_BA=params['P_B_given_A']\\n                P_AB=params['P_A_and_B']\\n                # 簡単な棒グラフでP(A),P(B|A),P(A∩B)の関係を表示\\n                plt.figure()\\n                vals=[P_A,P_BA,P_AB]\\n                labels=['P(A)','P(B|A)','P(A∩B)']\\n                plt.bar(labels,vals,color=['blue','green','red'])\\n                plt.title('Conditional Probability Visualization')\\n                plt.ylabel('Probability')\\n                plt.tight_layout()\\n                plt.savefig(output_path)\\n                plt.close()\\n                return True\\n            else:\\n                # 未知タイプには何もしない\\n                return False\\n        except Exception as e:\\n            logging.error(f\\\"Error in plot_probability: {e}\\\")\\n            return False\\n\\n    def plot_t_test(self, params, output_path):\\n        # t分布+棄却域+標本平均から計算されたt値など\\n        # 追加で、標準正規近似や、p値部分の色塗りなど\\n        try:\\n            alpha = params['alpha']\\n            df = params['df']\\n            t_stat = params['t_stat']\\n            critical_value = params['critical_value']\\n\\n            x = np.linspace(t_dist.ppf(0.001, df), t_dist.ppf(0.999, df), 1000)\\n            y = t_dist.pdf(x, df)\\n\\n            plt.figure(figsize=(10,5))\\n            plt.plot(x, y, 'b-', label=f't-dist(df={df})')\\n            # 臨界値に線\\n            plt.axvline(x=critical_value,color='r',linestyle='--',label='critical +')\\n            plt.axvline(x=-critical_value,color='r',linestyle='--',label='critical -')\\n            # t統計量\\n            plt.axvline(x=t_stat,color='g',label='t-stat')\\n\\n            # p値領域を色付け\\n            # 両側検定として|t|>crit\\n            p_area_x = x[x>critical_value]\\n            plt.fill_between(p_area_x,t_dist.pdf(p_area_x,df),color='red',alpha=0.3)\\n            p_area_x2 = x[x<-critical_value]\\n            plt.fill_between(p_area_x2,t_dist.pdf(p_area_x2,df),color='red',alpha=0.3)\\n\\n            plt.title('t-test visualization')\\n            plt.xlabel('t')\\n            plt.ylabel('pdf')\\n            plt.legend()\\n            plt.tight_layout()\\n\\n            # 保存\\n            plt.savefig(output_path)\\n            plt.close()\\n            return True\\n        except Exception as e:\\n            logging.error(f\\\"t検定グラフ生成エラー: {e}\\\")\\n            return False\\n\\n    def plot_regression(self, x_values, y_values, beta_0_hat, beta_1_hat, output_path):\\n        # 回帰直線+データ点 + 残差ヒスト + Q-Qプロットなど複数図\\n        try:\\n            import statsmodels.api as sm\\n            X = sm.add_constant(x_values)\\n            model = sm.OLS(y_values, X).fit()\\n            residuals = model.resid\\n\\n            fig,axes = plt.subplots(2,2,figsize=(10,10))\\n\\n            # Scatter + regression line\\n            ax=axes[0,0]\\n            ax.scatter(x_values,y_values,color='blue',label='data')\\n            x_line=np.linspace(min(x_values),max(x_values),100)\\n            y_line=beta_0_hat+beta_1_hat*x_line\\n            ax.plot(x_line,y_line,color='red',label='reg line')\\n            ax.set_title('Data & Regression Line')\\n            ax.set_xlabel('X')\\n            ax.set_ylabel('Y')\\n            ax.legend()\\n\\n            # Residual histogram\\n            ax=axes[0,1]\\n            ax.hist(residuals,bins=20,color='green',alpha=0.7)\\n            ax.set_title('Residual Histogram')\\n            ax.set_xlabel('Residual')\\n            ax.set_ylabel('Frequency')\\n\\n            # Q-Q plot of residuals\\n            sm.qqplot(residuals, line='45', ax=axes[1,0],color='purple')\\n            axes[1,0].set_title('Q-Q plot of Residuals')\\n\\n            # Residuals vs fitted\\n            fitted=model.fittedvalues\\n            ax=axes[1,1]\\n            ax.scatter(fitted,residuals,color='orange')\\n            ax.axhline(y=0,color='red',linestyle='--')\\n            ax.set_title('Residuals vs Fitted')\\n            ax.set_xlabel('Fitted')\\n            ax.set_ylabel('Residuals')\\n\\n            fig.tight_layout()\\n            fig.savefig(output_path)\\n            plt.close(fig)\\n            return True\\n        except Exception as e:\\n            logging.error(f\\\"回帰分析グラフ生成中にエラー: {e}\\\")\\n            return False\\n\\n    def plot_time_series(self, params, output_path):\\n        # 時系列データ + ACFプロット\\n        try:\\n            ts = params['time_series']\\n\\n            fig,axes=plt.subplots(2,1,figsize=(10,8))\\n            axes[0].plot(ts, color='blue')\\n            axes[0].set_title('Time Series Data')\\n            axes[0].set_xlabel('Time')\\n            axes[0].set_ylabel('Value')\\n\\n            plot_acf(ts,ax=axes[1])\\n            axes[1].set_title('Autocorrelation Function')\\n\\n            fig.tight_layout()\\n            fig.savefig(output_path)\\n            plt.close(fig)\\n            return True\\n        except Exception as e:\\n            logging.error(f\\\"時系列分析グラフエラー: {e}\\\")\\n            return False\\n\\n    def plot_econometrics(self, params, output_path):\\n        # X1,YやX2,Y 散布図 + 残差分析\\n        try:\\n            import statsmodels.api as sm\\n            X = np.column_stack((params['x1_values'], params['x2_values']))\\n            Y = np.array(params['y_values'])\\n            Xc = sm.add_constant(X)\\n            model = sm.OLS(Y,Xc).fit()\\n            residuals = model.resid\\n            fitted = model.fittedvalues\\n\\n            fig,axes=plt.subplots(2,2,figsize=(10,10))\\n\\n            # X1 vs Y\\n            ax=axes[0,0]\\n            ax.scatter(params['x1_values'],Y,color='blue',alpha=0.7,label='X1-Y')\\n            ax.set_title('X1 vs Y')\\n            ax.set_xlabel('X1')\\n            ax.set_ylabel('Y')\\n\\n            # X2 vs Y\\n            ax=axes[0,1]\\n            ax.scatter(params['x2_values'],Y,color='green',alpha=0.7,label='X2-Y')\\n            ax.set_title('X2 vs Y')\\n            ax.set_xlabel('X2')\\n            ax.set_ylabel('Y')\\n\\n            # Residuals histogram\\n            ax=axes[1,0]\\n            ax.hist(residuals,bins=20,color='gray',alpha=0.7)\\n            ax.set_title('Residuals Histogram')\\n            ax.set_xlabel('Residual')\\n\\n            # Residuals vs Fitted\\n            ax=axes[1,1]\\n            ax.scatter(fitted,residuals,color='red',alpha=0.7)\\n            ax.axhline(y=0,color='black',linestyle='--')\\n            ax.set_title('Residuals vs Fitted')\\n            ax.set_xlabel('Fitted')\\n            ax.set_ylabel('Residuals')\\n\\n            fig.tight_layout()\\n            fig.savefig(output_path)\\n            plt.close(fig)\\n            return True\\n        except Exception as e:\\n            logging.error(f\\\"計量経済学グラフエラー: {e}\\\")\\n            return False\\n\\n    def plot_multivariate_normal(self, params, output_path):\\n        # 信頼楕円 + 等高線 + 周辺分布\\n        try:\\n            mu = params['mu']\\n            sigma = np.array(params['sigma'])\\n\\n            fig = plt.figure(figsize=(10,10))\\n            from matplotlib.patches import Ellipse\\n            import matplotlib.transforms as transforms\\n\\n            def confidence_ellipse(mu,cov,ax,n_std=1.96,facecolor='none',**kwargs):\\n                pearson = cov[0,1]/np.sqrt(cov[0,0]*cov[1,1])\\n                ell_radius_x = np.sqrt(1+pearson)\\n                ell_radius_y = np.sqrt(1-pearson)\\n                ellipse=Ellipse((0,0),width=ell_radius_x*2,height=ell_radius_y*2,facecolor=facecolor,**kwargs)\\n                scale_x = np.sqrt(cov[0,0])*n_std\\n                scale_y = np.sqrt(cov[1,1])*n_std\\n                transf = transforms.Affine2D().rotate_deg(45).scale(scale_x,scale_y).translate(mu[0],mu[1])\\n                ellipse.set_transform(transf+ax.transData)\\n                return ax.add_patch(ellipse)\\n\\n            ax=fig.add_subplot(2,2,1)\\n            ax.set_title('Confidence Ellipse')\\n            confidence_ellipse(mu,sigma,ax,edgecolor='red')\\n            ax.scatter(mu[0],mu[1],c='blue',marker='x',label='mean')\\n            ax.legend()\\n            ax.set_xlabel('X1')\\n            ax.set_ylabel('X2')\\n\\n            # 等高線\\n            ax2=fig.add_subplot(2,2,2)\\n            x = np.linspace(mu[0]-4*np.sqrt(sigma[0,0]),mu[0]+4*np.sqrt(sigma[0,0]),100)\\n            y = np.linspace(mu[1]-4*np.sqrt(sigma[1,1]),mu[1]+4*np.sqrt(sigma[1,1]),100)\\n            X,Y = np.meshgrid(x,y)\\n            pos = np.dstack((X,Y))\\n            def mvn_pdf(x, mu, cov):\\n                n=2\\n                det=np.linalg.det(cov)\\n                inv=np.linalg.inv(cov)\\n                diff=(x - mu)\\n                return (1./(2*np.pi*np.sqrt(det)))*np.exp(-0.5*(diff@inv@diff.T))\\n            Z=np.empty(X.shape)\\n            for i in range(X.shape[0]):\\n                for j in range(X.shape[1]):\\n                    Z[i,j]=mvn_pdf(np.array([X[i,j],Y[i,j]]),np.array(mu),sigma)\\n            ax2.contour(X,Y,Z,levels=5,cmap='Blues')\\n            ax2.set_title('Contour')\\n\\n            # 周辺分布\\n            ax3=fig.add_subplot(2,2,3)\\n            # marginal X1\\n            X_marg = norm(loc=mu[0],scale=np.sqrt(sigma[0,0]))\\n            x_line = np.linspace(mu[0]-4*np.sqrt(sigma[0,0]),mu[0]+4*np.sqrt(sigma[0,0]),100)\\n            y_line = X_marg.pdf(x_line)\\n            ax3.plot(x_line,y_line,'r-')\\n            ax3.set_title('Marginal X1 distribution')\\n            ax3.set_xlabel('X1')\\n            ax3.set_ylabel('pdf')\\n\\n            ax4=fig.add_subplot(2,2,4)\\n            Y_marg=norm(loc=mu[1],scale=np.sqrt(sigma[1,1]))\\n            y_line=Y_marg.pdf(x_line)\\n            ax4.plot(x_line,y_line,'g-')\\n            ax4.set_title('Marginal X2 distribution')\\n            ax4.set_xlabel('X2')\\n            ax4.set_ylabel('pdf')\\n\\n            fig.tight_layout()\\n            fig.savefig(output_path)\\n            plt.close(fig)\\n            return True\\n        except Exception as e:\\n            logging.error(f\\\"多変量正規分布グラフエラー: {e}\\\")\\n            return False\\n\\n    def plot_distribution_properties(self, params, output_path):\\n        # 選択された分布に対し、PDF+平均分散表示ライン\\n        try:\\n            dist = params['distribution']\\n            plt.figure(figsize=(10,5))\\n            if dist=='正規分布':\\n                mu=0; sigma=1\\n                x=np.linspace(mu-4*sigma,mu+4*sigma,200)\\n                y=norm.pdf(x,mu,sigma)\\n                plt.plot(x,y,'b-')\\n                plt.axvline(mu,color='r',linestyle='--',label='mean')\\n                plt.axvline(mu+sigma,color='g',linestyle=':',label='mean+sigma')\\n                plt.axvline(mu-sigma,color='g',linestyle=':')\\n                plt.title('Normal Distribution (mu=0,sigma=1)')\\n                plt.legend()\\n            elif dist=='ポアソン分布':\\n                lam=3\\n                x=np.arange(0,15)\\n                y=poisson.pmf(x,lam)\\n                plt.bar(x,y,color='skyblue')\\n                plt.axvline(lam,color='r',linestyle='--',label='mean=lambda=3')\\n                plt.title('Poisson(lambda=3)')\\n                plt.legend()\\n            elif dist=='指数分布':\\n                lam=1\\n                x=np.linspace(0,5,200)\\n                y=expon.pdf(x,scale=1/lam)\\n                plt.plot(x,y,'b-')\\n                plt.axvline(1/lam,color='r',linestyle='--',label='mean=1/lambda')\\n                plt.title('Exponential(lambda=1)')\\n                plt.legend()\\n            plt.tight_layout()\\n            plt.savefig(output_path)\\n            plt.close()\\n            return True\\n        except Exception as e:\\n            logging.error(f\\\"分布性質グラフエラー: {e}\\\")\\n            return False\\n\\n    def plot_high_moment(self, params, output_path):\\n        # 正規分布pdf上に平均、±σ、n次モーメント近傍など\\n        try:\\n            mu=params['mu']\\n            sigma=params['sigma']\\n            n=params['n']\\n            moment=params['moment']\\n            x=np.linspace(mu-4*sigma,mu+4*sigma,200)\\n            y=norm.pdf(x,mu,sigma)\\n\\n            plt.figure(figsize=(10,5))\\n            plt.plot(x,y,'b-',label='Normal pdf')\\n            plt.axvline(mu,color='r',linestyle='--',label='mean')\\n            plt.axvline(mu+sigma,color='g',linestyle=':',label='mu±sigma')\\n            plt.axvline(mu-sigma,color='g',linestyle=':')\\n            plt.title(f'Normal distribution (mu={mu}, sigma={sigma}). n={n}-th moment={moment}')\\n            plt.legend()\\n            plt.tight_layout()\\n            plt.savefig(output_path)\\n            plt.close()\\n            return True\\n        except Exception as e:\\n            logging.error(f\\\"高次モーメントグラフエラー: {e}\\\")\\n            return False\\n\\n    def plot_variance_analysis(self, params, output_path):\\n        # 箱ひげ図など\\n        try:\\n            group_count=params['group_count']\\n            sample_sizes=params['sample_sizes']\\n            means=params['means']\\n            variances=params['variances']\\n\\n            # データを仮に正規生成して可視化\\n            # 単に箱ひげ図で分布の違いを視覚化\\n            data=[]\\n            for i in range(group_count):\\n                # 各グループ：mean, varから乱数生成\\n                np.random.seed(i)\\n                samples=np.random.normal(means[i],np.sqrt(variances[i]),sample_sizes[i])\\n                data.append(samples)\\n\\n            plt.figure(figsize=(8,6))\\n            plt.boxplot(data,labels=[f'Group{i+1}' for i in range(group_count)])\\n            plt.title('ANOVA: Boxplots of groups')\\n            plt.ylabel('Value')\\n            plt.tight_layout()\\n            plt.savefig(output_path)\\n            plt.close()\\n            return True\\n        except Exception as e:\\n            logging.error(f\\\"分散分析グラフエラー: {e}\\\")\\n            return False\\n\\n    def plot_nonparametric_test(self, params, output_path):\\n        # ノンパラ検定：2サンプルECDFなど\\n        try:\\n            s1=params['sample1']\\n            s2=params['sample2']\\n\\n            def ecdf(data):\\n                d_sorted = np.sort(data)\\n                y = np.arange(1,len(d_sorted)+1)/len(d_sorted)\\n                return d_sorted,y\\n\\n            x1,y1=ecdf(s1)\\n            x2,y2=ecdf(s2)\\n\\n            plt.figure(figsize=(8,6))\\n            plt.step(x1,y1,where='post',label='Sample1 ECDF',color='blue')\\n            plt.step(x2,y2,where='post',label='Sample2 ECDF',color='red')\\n            plt.title('Nonparametric Test Visualization')\\n            plt.xlabel('Value')\\n            plt.ylabel('ECDF')\\n            plt.legend()\\n            plt.tight_layout()\\n            plt.savefig(output_path)\\n            plt.close()\\n            return True\\n        except Exception as e:\\n            logging.error(f\\\"ノンパラ検定グラフエラー: {e}\\\")\\n            return False\\n\\n    def plot_linear_combination(self, params, output_path):\\n        # 合成Z=aX+bYをシミュレーション\\n        try:\\n            a=params['a']\\n            b=params['b']\\n            mu1=params['mu1']\\n            mu2=params['mu2']\\n            sig1=params['sigma1_squared']\\n            sig2=params['sigma2_squared']\\n            # シミュレーションでZ生成\\n            np.random.seed(123)\\n            x = np.random.normal(mu1, np.sqrt(sig1),1000)\\n            y = np.random.normal(mu2, np.sqrt(sig2),1000)\\n            Z=a*x+b*y\\n            plt.figure(figsize=(8,6))\\n            plt.hist(Z,bins=30,density=True,alpha=0.7,color='purple',label='Simulated Z')\\n            # 理論分布：N(a*mu1+b*mu2, a²sigma1+b²sigma2)\\n            E_Z=a*mu1+b*mu2\\n            Var_Z=a**2*sig1+b**2*sig2\\n            X_line=np.linspace(E_Z-4*np.sqrt(Var_Z),E_Z+4*np.sqrt(Var_Z),200)\\n            Y_line=norm.pdf(X_line,E_Z,np.sqrt(Var_Z))\\n            plt.plot(X_line,Y_line,'r-',label='Theoretical PDF')\\n            plt.title('Linear Combination Distribution')\\n            plt.xlabel('Z')\\n            plt.ylabel('Density')\\n            plt.legend()\\n            plt.tight_layout()\\n            plt.savefig(output_path)\\n            plt.close()\\n            return True\\n        except Exception as e:\\n            logging.error(f\\\"線形結合グラフエラー: {e}\\\")\\n            return False\\n\"\n    },\n    {\n      \"path\": \"統計検定1級\\\\gui.py\",\n      \"overview\": \"このスクリプトはPythonコードです。\\n定義されているクラス: InteractiveSolverGUI。\\n定義されている関数: __init__, run。\\n\",\n      \"content\": \"# gui.py\\nimport tkinter as tk\\nclass InteractiveSolverGUI:\\n    def __init__(self):\\n        self.root=tk.Tk()\\n        self.root.title('統計検定1級 インタラクティブ問題解答システム')\\n    def run(self):\\n        self.root.mainloop()\"\n    },\n    {\n      \"path\": \"統計検定1級\\\\main.py\",\n      \"overview\": \"このスクリプトはPythonコードです。\\n定義されている関数: main。\\n\",\n      \"content\": \"# main.py\\nimport sys\\nfrom main_app import MainApp\\nfrom gui import InteractiveSolverGUI\\nfrom config import config\\n\\ndef main():\\n    pdf_gen=config.get('settings','pdf_generation',default={})\\n    problem_count=pdf_gen.get('problem_count',9)\\n\\n    if len(sys.argv)>1 and sys.argv[1]=='--generate-pdf':\\n        app=MainApp()\\n        app.generate_and_compile(problem_count)\\n    else:\\n        gui=InteractiveSolverGUI()\\n        gui.run()\\n\\nif __name__=='__main__':\\n    main()\\n\"\n    },\n    {\n      \"path\": \"統計検定1級\\\\main_app.py\",\n      \"overview\": \"このスクリプトはPythonコードです。\\n定義されているクラス: MainApp。\\n定義されている関数: __init__, generate_latex_header, generate_and_compile, compile_latex。\\n\",\n      \"content\": \"# main_app.py\\nimport os,subprocess\\nfrom config import config\\nfrom problem_generator import ProblemGenerator\\nimport logging\\nimport traceback\\n\\nclass MainApp:\\n    def __init__(self):\\n        # 必要設定を__init__でまとめて取得\\n        self.output_tex_file = config.get('settings','output_tex_file',default='practice_problems.tex')\\n        if self.output_tex_file is None:\\n            self.output_tex_file='practice_problems.tex'\\n\\n        self.enable_visualization = config.get('settings','enable_visualization',default=True)\\n        self.problem_templates_dir = config.get('settings','problem_templates_directory',default='templates')\\n        self.output_directory = config.get('settings','output_directory',default='outputs')\\n        self.latex_path = config.get('settings','latex_path',default='xelatex')\\n\\n        # ProblemGeneratorの生成\\n        self.generator = ProblemGenerator()\\n\\n    def generate_latex_header(self):\\n        cjk_font=config.get('settings','cjk_main_font',default='Yu Gothic')\\n        if cjk_font is None:\\n            cjk_font='Yu Gothic'\\n        header=[\\n            '\\\\\\\\documentclass{article}',\\n            '\\\\\\\\usepackage{amsmath}',\\n            '\\\\\\\\usepackage{amssymb}',\\n            '\\\\\\\\usepackage{graphicx}',\\n            '\\\\\\\\usepackage{float}',\\n            '\\\\\\\\usepackage{geometry}',\\n            '\\\\\\\\usepackage{xeCJK}',\\n            '\\\\\\\\usepackage{fontspec}',\\n            '\\\\\\\\setmainfont{Times New Roman}',\\n            f'\\\\\\\\setCJKmainfont{{{cjk_font}}}',\\n            '\\\\\\\\geometry{a4paper, margin=1in}',\\n            '\\\\\\\\begin{document}'\\n        ]\\n        return header\\n\\n    def generate_and_compile(self,problem_count):\\n        script_dir=os.path.dirname(os.path.abspath(__file__))\\n        latex_content=self.generate_latex_header()\\n\\n        out_dir = self.output_directory\\n        if out_dir is None:\\n            out_dir='outputs'\\n        output_dir=os.path.join(script_dir,out_dir)\\n        os.makedirs(output_dir,exist_ok=True)\\n        tex_file_path=os.path.join(output_dir,self.output_tex_file)\\n\\n        for idx in range(1,problem_count+1):\\n            try:\\n                result=self.generator.generate_problem()\\n                if result:\\n                    problem_id,date_created,problem_type,problem_text,solution_text,graph_filename=result\\n\\n                    latex_content.append(f'\\\\\\\\section*{{問題 {idx}}}')\\n                    latex_content.append(problem_text)\\n\\n                    if self.enable_visualization and graph_filename:\\n                        latex_content.append('\\\\\\\\begin{figure}[H]')\\n                        latex_content.append('\\\\\\\\centering')\\n                        graph_relative_path=os.path.join('graphs',graph_filename).replace('\\\\\\\\','/')\\n                        latex_content.append(f'\\\\\\\\includegraphics[width=0.8\\\\\\\\textwidth]{{{graph_relative_path}}}')\\n                        latex_content.append('\\\\\\\\end{figure}')\\n\\n                    latex_content.append('\\\\\\\\subsection*{解答}')\\n                    latex_content.append(solution_text)\\n                    latex_content.append('\\\\\\\\newpage')\\n                else:\\n                    logging.warning(f\\\"問題 {idx} の生成に失敗しました。\\\")\\n                    print(f\\\"問題 {idx} の生成に失敗しました。\\\")\\n            except Exception as e:\\n                logging.error(f\\\"問題 {idx} の生成中にエラー: {e}\\\")\\n                logging.error(traceback.format_exc())\\n                print(f\\\"問題 {idx} 生成エラー。ログを確認\\\")\\n\\n        templates_dir = self.problem_templates_dir\\n        if templates_dir is None:\\n            templates_dir='templates'\\n        latex_content.append('\\\\\\\\clearpage')\\n        latex_content.append('\\\\\\\\input{../'+templates_dir+'/distribution_relations.tex}')\\n\\n        latex_content.append('\\\\\\\\end{document}')\\n\\n        with open(tex_file_path,'w',encoding='utf-8')as tex_file:\\n            tex_file.write('\\\\n'.join(latex_content))\\n\\n        self.compile_latex(tex_file_path)\\n\\n    def compile_latex(self,tex_file_path):\\n        tex_file_name=os.path.basename(tex_file_path)\\n        latex_path=self.latex_path\\n        if not latex_path or not os.path.exists(latex_path):\\n            print(f\\\"LaTeXコンパイラが見つかりません: {latex_path}\\\")\\n            return\\n        process=subprocess.run([latex_path,'-interaction=nonstopmode',tex_file_name],cwd=os.path.dirname(tex_file_path),stdout=subprocess.PIPE,stderr=subprocess.PIPE,text=True)\\n        log_file_path=os.path.join(os.path.dirname(tex_file_path),'latex_compile.log')\\n        with open(log_file_path,'w',encoding='utf-8')as f:\\n            f.write(process.stdout or '')\\n            f.write(process.stderr or '')\\n        if process.returncode!=0:\\n            print(\\\"LaTeXコンパイルでエラー\\\")\\n        else:\\n            pdf_file=tex_file_path.replace('.tex','.pdf')\\n            if os.path.exists(pdf_file):\\n                print(f\\\"PDF生成成功: {pdf_file}\\\")\\n            else:\\n                print(\\\"PDFファイル未発見\\\")\\n\"\n    },\n    {\n      \"path\": \"統計検定1級\\\\problem_generator.py\",\n      \"overview\": \"このスクリプトはPythonコードです。\\n定義されているクラス: ProblemGenerator。\\n定義されている関数: __init__, load_topics, get_problem_types_by_topic, generate_problem。\\n\",\n      \"content\": \"# problem_generator.py\\nimport random,os,logging,uuid\\nfrom datetime import datetime\\nfrom config import config\\nfrom database import DatabaseManager\\nfrom problem_types.problem_factory import ProblemFactory\\nimport json\\nimport traceback\\n\\nclass ProblemGenerator:\\n    def __init__(self):\\n        self.db_path = config.get('settings','database_path',default='data/data.db')\\n        self.output_directory = config.get('settings','output_directory',default='outputs')\\n        self.enable_visualization = config.get('settings','enable_visualization',default=True)\\n        self.problem_types_weights = config.get('settings','problem_types',default={})\\n\\n        self.factory=ProblemFactory()\\n        self.db_manager=DatabaseManager()\\n        self.db_manager.setup_database()\\n\\n        script_dir=os.path.dirname(os.path.abspath(__file__))\\n        out_dir=self.output_directory\\n        if out_dir is None:\\n            out_dir='outputs'\\n        self.output_dir=os.path.join(script_dir,out_dir,'graphs')\\n        os.makedirs(self.output_dir,exist_ok=True)\\n        self.topics_data=self.load_topics()\\n\\n    def load_topics(self):\\n        script_dir=os.path.dirname(os.path.abspath(__file__))\\n        topics_file=os.path.join(script_dir,'topics.json')\\n        if not os.path.exists(topics_file):\\n            raise FileNotFoundError(f\\\"'{topics_file}'がない\\\")\\n        with open(topics_file,'r',encoding='utf-8')as f:\\n            return json.load(f)\\n\\n    def get_problem_types_by_topic(self,topic):\\n        return self.topics_data['topics'].get(topic,[])\\n\\n    def generate_problem(self,selected_topic=None):\\n        try:\\n            ptypes=self.problem_types_weights\\n            if selected_topic:\\n                problem_types=self.get_problem_types_by_topic(selected_topic)\\n                if not problem_types:\\n                    return None\\n                problem_type=random.choice(problem_types)\\n            else:\\n                pts=list(ptypes.keys())\\n                pwt=list(ptypes.values())\\n                if not pts:\\n                    pts=['probability']\\n                    pwt=[1.0]\\n                problem_type=random.choices(pts,weights=pwt,k=1)[0]\\n\\n            problem=self.factory.create_problem(problem_type)\\n            problem.generate_parameters()\\n            problem_text=problem.generate_problem_text()\\n            solution_text=problem.generate_solution_text()\\n\\n            enable_vis=self.enable_visualization\\n            if enable_vis is None:\\n                enable_vis=True\\n\\n            graph_filename=None\\n            if enable_vis:\\n                graph_filename=f\\\"graph_{uuid.uuid4().hex}.png\\\"\\n                graph_filepath=os.path.join(self.output_dir,graph_filename)\\n                if not problem.generate_graph(graph_filepath):\\n                    graph_filename=None\\n\\n            problem_id=uuid.uuid4().hex\\n            date_created=datetime.now().strftime('%Y-%m-%d %H:%M:%S')\\n\\n            self.db_manager.save_problem(problem_id,date_created,problem_text,solution_text,problem_type)\\n\\n            return problem_id,date_created,problem_type,problem_text,solution_text,graph_filename\\n        except Exception as e:\\n            logging.error(f\\\"問題生成エラー: {e}\\\")\\n            logging.error(traceback.format_exc())\\n            return None\\n\"\n    },\n    {\n      \"path\": \"統計検定1級\\\\sympy_solver.py\",\n      \"overview\": \"このスクリプトはPythonコードです。\\n定義されているクラス: SympySolver。\\n定義されている関数: __init__, check_equivalence。\\n\",\n      \"content\": \"# sympy_solver.py\\nclass SympySolver:\\n    def __init__(self):\\n        pass\\n    def check_equivalence(self,user_input,correct_answer):\\n        # 簡略化:常にFalseでエラーなし\\n        return False,\\\"\\\"\"\n    },\n    {\n      \"path\": \"統計検定1級\\\\topics.json\",\n      \"overview\": \"このファイルはテキストファイルまたはその他の形式です。\\n行数: 28\\n\",\n      \"content\": \"{\\n  \\\"topics\\\": {\\n    \\\"確率論\\\": [\\n      \\\"probability_definition\\\",\\n      \\\"conditional_probability\\\",\\n      \\\"distribution_functions\\\",\\n      \\\"joint_distribution\\\",\\n      \\\"probability\\\"\\n    ],\\n    \\\"統計的推定\\\": [\\n      \\\"statistical_inference\\\",\\n      \\\"t_test\\\"\\n    ],\\n    \\\"統計的検定\\\": [\\n      \\\"statistical_inference\\\",\\n      \\\"t_test\\\"\\n    ],\\n    \\\"回帰分析\\\": [\\n      \\\"regression_analysis\\\"\\n    ],\\n    \\\"分散分析\\\": [\\n      \\\"variance_analysis\\\"\\n    ],\\n    \\\"ノンパラメトリック検定\\\": [\\n      \\\"nonparametric_test\\\"\\n    ]\\n  }\\n}\\n\"\n    },\n    {\n      \"path\": \"統計検定1級\\\\__init__.py\",\n      \"overview\": \"このスクリプトはPythonコードです。\\n\",\n      \"content\": \"\"\n    },\n    {\n      \"path\": \"統計検定1級\\\\.vscode\\\\launch.json\",\n      \"overview\": \"このファイルはテキストファイルまたはその他の形式です。\\n行数: 14\\n\",\n      \"content\": \"{\\n    \\\"version\\\": \\\"0.2.0\\\",\\n    \\\"configurations\\\": [\\n        {\\n            \\\"name\\\": \\\"Python: 現在のファイル\\\",\\n            \\\"type\\\": \\\"python\\\",\\n            \\\"request\\\": \\\"launch\\\",\\n            \\\"program\\\": \\\"${file}\\\",\\n            \\\"console\\\": \\\"integratedTerminal\\\",\\n            \\\"justMyCode\\\": true,\\n            \\\"args\\\": [\\\"--generate-pdf\\\"]\\n\\n        }\\n    ]\\n}\"\n    },\n    {\n      \"path\": \"統計検定1級\\\\problem_types\\\\conjugate_problems.py\",\n      \"overview\": \"このスクリプトはPythonコードです。\\n定義されているクラス: BetaBinomialConjugateProblem, GammaPoissonConjugateProblem, DirichletMultinomialConjugateProblem, BinomialPoissonApproxProblem, PoissonNormalApproxProblem。\\n定義されている関数: __init__, generate_parameters, generate_explanation, generate_graph, __init__, generate_parameters, generate_explanation, generate_graph, __init__, generate_parameters, generate_explanation, generate_graph, __init__, generate_parameters, generate_explanation, generate_graph, __init__, generate_parameters, generate_explanation, generate_graph, B, B。\\n\",\n      \"content\": \"# conjugate_problems.py\\nimport math,random\\nfrom problem_types.problem import Problem\\nfrom math import comb,factorial,exp,gamma\\nimport numpy as np\\n\\nclass BetaBinomialConjugateProblem(Problem):\\n    def __init__(self):\\n        super().__init__('beta_binomial_conjugate_problem.tex')\\n    def generate_parameters(self):\\n        self.params['alpha']=random.randint(1,5)\\n        self.params['beta']=random.randint(1,5)\\n        self.params['n']=random.randint(5,20)\\n        self.params['k']=random.randint(0,self.params['n'])\\n        def B(x,y):\\n            return (gamma(x)*gamma(y))/gamma(x+y)\\n        p_x=comb(self.params['n'],self.params['k'])*B(self.params['k']+self.params['alpha'],self.params['n']-self.params['k']+self.params['beta'])/B(self.params['alpha'],self.params['beta'])\\n        self.params['probability']=round(p_x,4)\\n    def generate_explanation(self):\\n        return \\\"Beta+Binomial->Beta-Binomial\\\"\\n    def generate_graph(self,o):return False\\n\\nclass GammaPoissonConjugateProblem(Problem):\\n    def __init__(self):\\n        super().__init__('gamma_poisson_conjugate_problem.tex')\\n    def generate_parameters(self):\\n        self.params['alpha']=random.randint(1,5)\\n        self.params['beta']=random.uniform(0.5,2.0)\\n        self.params['k']=random.randint(0,20)\\n        p=self.params['beta']/(self.params['beta']+1)\\n        q=1-p\\n        negbin_p=comb(self.params['k']+self.params['alpha']-1,self.params['k'])*(q**self.params['k'])*(p**self.params['alpha'])\\n        self.params['probability']=round(negbin_p,4)\\n    def generate_explanation(self):\\n        return \\\"Gamma+Poisson->Negative Binomial\\\"\\n    def generate_graph(self,o):return False\\n\\nclass DirichletMultinomialConjugateProblem(Problem):\\n    def __init__(self):\\n        super().__init__('dirichlet_multinomial_conjugate_problem.tex')\\n    def generate_parameters(self):\\n        m=random.randint(2,4)\\n        self.params['m']=m\\n        self.params['n']=random.randint(5,20)\\n        self.params['alpha_vec']=[random.uniform(1,3) for _ in range(m)]\\n        counts=[0]*m\\n        remain=self.params['n']\\n        for i in range(m-1):\\n            c=random.randint(0,remain)\\n            counts[i]=c\\n            remain-=c\\n        counts[-1]=remain\\n        self.params['counts']=counts\\n        def B(alpha):\\n            import numpy as np\\n            return (np.prod([gamma(a) for a in alpha]))/gamma(sum(alpha))\\n        alpha_x=[self.params['alpha_vec'][i]+counts[i] for i in range(m)]\\n        num=B(alpha_x)\\n        den=B(self.params['alpha_vec'])\\n        multinomial_coef=math.factorial(self.params['n'])\\n        for c in counts:\\n            multinomial_coef/=math.factorial(c)\\n        p_x=multinomial_coef*(num/den)\\n        self.params['probability']=round(p_x,4)\\n    def generate_explanation(self):\\n        return \\\"Dirichlet+Multinomial->Dirichlet-Multinomial\\\"\\n    def generate_graph(self,o):return False\\n\\nclass BinomialPoissonApproxProblem(Problem):\\n    def __init__(self):\\n        super().__init__('binomial_poisson_approx_problem.tex')\\n    def generate_parameters(self):\\n        lam=random.uniform(2,5)\\n        n=random.randint(50,200)\\n        p=lam/n\\n        k=random.randint(0,int(lam*3))\\n        binom_p=comb(n,k)*(p**k)*((1-p)**(n-k))\\n        poisson_p=(lam**k)*exp(-lam)/math.factorial(k)\\n        self.params['n']=n\\n        self.params['p']=round(p,6)\\n        self.params['k']=k\\n        self.params['lambda']=round(lam,3)\\n        self.params['binom_p']=round(binom_p,6)\\n        self.params['poisson_p']=round(poisson_p,6)\\n    def generate_explanation(self):\\n        return \\\"Binomial->Poisson近似条件\\\"\\n    def generate_graph(self,o):return False\\n\\nclass PoissonNormalApproxProblem(Problem):\\n    def __init__(self):\\n        super().__init__('poisson_normal_approx_problem.tex')\\n    def generate_parameters(self):\\n        lam=random.randint(30,100)\\n        low=max(0,int(lam-3*math.sqrt(lam)))\\n        high=int(lam+3*math.sqrt(lam))\\n        k=random.randint(low,high)\\n        poisson_p=(lam**k)*exp(-lam)/math.factorial(k)\\n        self.params['lambda']=lam\\n        self.params['k']=k\\n        self.params['poisson_p']=round(poisson_p,6)\\n        self.params['mean']=lam\\n        self.params['variance']=lam\\n    def generate_explanation(self):\\n        return \\\"Poisson->Normal近似(λ大)\\\"\\n    def generate_graph(self,o):return False\"\n    },\n    {\n      \"path\": \"統計検定1級\\\\problem_types\\\\problem.py\",\n      \"overview\": \"このスクリプトはPythonコードです。\\n定義されているクラス: Problem, ProbabilityProblem, StatisticalInferenceProblem, RegressionAnalysisProblem, TimeSeriesAnalysisProblem, EconometricsProblem, LinearCombinationProblem, DistributionPropertiesProblem, HighMomentProblem, MultivariateNormalProblem, VarianceAnalysisProblem, NonParametricTestProblem, ProbabilityDefinitionProblem, ConditionalProbabilityProblem, DistributionFunctionsProblem, JointDistributionProblem。\\n定義されている関数: __init__, generate_parameters, generate_problem_text, generate_solution_text, generate_explanation, generate_graph, __init__, generate_parameters, _variant_binomial, _variant_poisson, _variant_conditional_probability, generate_explanation, generate_graph, __init__, generate_parameters, generate_graph, __init__, generate_parameters, generate_graph, __init__, generate_parameters, generate_graph, __init__, generate_parameters, generate_graph, __init__, generate_parameters, generate_explanation, generate_graph, __init__, generate_parameters, generate_graph, __init__, generate_parameters, generate_graph, __init__, generate_parameters, generate_explanation, generate_graph, __init__, generate_parameters, generate_explanation, generate_graph, __init__, generate_parameters, generate_explanation, generate_graph, __init__, generate_parameters, generate_explanation, generate_graph, __init__, generate_parameters, generate_explanation, generate_graph, __init__, generate_parameters, generate_explanation, generate_graph, __init__, generate_parameters, generate_explanation, generate_graph。\\n\",\n      \"content\": \"# problem_types/problem.py\\n\\nfrom abc import ABC, abstractmethod\\nimport os\\nimport random\\nimport logging\\nfrom config import config\\nfrom jinja2 import Environment, FileSystemLoader\\nfrom graph import ProbabilityDistributionVisualizer\\nimport traceback\\nfrom math import comb,exp,factorial\\nfrom scipy.stats import norm,stats\\n\\nclass Problem(ABC):\\n    def __init__(self, template_name):\\n        self.params = {}\\n        self.template_name = template_name\\n        templates_dir = os.path.join(\\n            os.path.dirname(os.path.abspath(__file__)),\\n            '..',\\n            config.get('problem_templates_directory', default='templates')\\n        )\\n        self.env = Environment(loader=FileSystemLoader(templates_dir), autoescape=False)\\n        self.visualizer = ProbabilityDistributionVisualizer()\\n\\n    @abstractmethod\\n    def generate_parameters(self):\\n        pass\\n\\n    def generate_problem_text(self):\\n        template = self.env.get_template(self.template_name)\\n        return template.render(**self.params, show_solution=False)\\n\\n    def generate_solution_text(self):\\n        template = self.env.get_template(self.template_name)\\n        self.params['explanation'] = self.generate_explanation()\\n        return template.render(**self.params, show_solution=True)\\n\\n    def generate_explanation(self):\\n        return \\\"\\\"\\n\\n    @abstractmethod\\n    def generate_graph(self, output_path):\\n        pass\\n\\nclass ProbabilityProblem(Problem):\\n    def __init__(self):\\n        super().__init__('probability_problem.tex')\\n    def generate_parameters(self):\\n        variants=[self._variant_binomial,self._variant_poisson,self._variant_conditional_probability]\\n        v=random.choice(variants)\\n        v()\\n    def _variant_binomial(self):\\n        self.params['problem_type']='binomial'\\n        self.params['n']=random.randint(5,20)\\n        self.params['p']=round(random.uniform(0.1,0.9),2)\\n        self.params['k']=random.randint(0,self.params['n'])\\n        from math import comb\\n        prob=comb(self.params['n'],self.params['k'])*(self.params['p']**self.params['k'])*((1-self.params['p'])**(self.params['n']-self.params['k']))\\n        self.params['probability']=round(prob,6)\\n    def _variant_poisson(self):\\n        self.params['problem_type']='poisson'\\n        self.params['lambda']=round(random.uniform(0.5,5.0),2)\\n        self.params['k']=random.randint(0,10)\\n        lam=self.params['lambda']\\n        k=self.params['k']\\n        prob=(lam**k)*exp(-lam)/factorial(k)\\n        self.params['probability']=round(prob,6)\\n    def _variant_conditional_probability(self):\\n        self.params['problem_type']='conditional_probability'\\n        self.params['P_A']=round(random.uniform(0.1,0.9),2)\\n        self.params['P_B_given_A']=round(random.uniform(0.1,0.9),2)\\n        self.params['P_A_and_B']=round(self.params['P_A']*self.params['P_B_given_A'],6)\\n    def generate_explanation(self):\\n        t=self.params['problem_type']\\n        if t=='binomial':\\n            return \\\"二項分布の公式を使用\\\"\\n        elif t=='poisson':\\n            return \\\"ポアソン分布の公式を使用\\\"\\n        elif t=='conditional_probability':\\n            return \\\"条件付き確率P(A∩B)=P(A)*P(B|A)\\\"\\n    def generate_graph(self,output_path):\\n        return self.visualizer.plot_probability(self.params,output_path)\\n\\nclass StatisticalInferenceProblem(Problem):\\n    def __init__(self):\\n        super().__init__('statistical_inference_problem.tex')\\n    def generate_parameters(self):\\n        self.params['sample_mean']=round(random.uniform(50,100),2)\\n        self.params['sample_std']=round(random.uniform(5,15),2)\\n        self.params['n']=random.randint(30,100)\\n        self.params['population_mean']=round(random.uniform(50,100),2)\\n        self.params['alpha']=round(random.uniform(0.01,0.1),2)\\n        t_stat = (self.params['sample_mean']-self.params['population_mean'])/(self.params['sample_std']/(self.params['n']**0.5))\\n        t_stat=round(t_stat,4)\\n        df=self.params['n']-1\\n        cv=round(stats.t.ppf(1-self.params['alpha']/2,df=df),4)\\n        reject='棄却' if abs(t_stat)>cv else '棄却しない'\\n        self.params['t_stat']=t_stat\\n        self.params['critical_value']=cv\\n        self.params['reject_null']=reject\\n        self.params['df']=df\\n    def generate_graph(self,output_path):\\n        return self.visualizer.plot_t_test(self.params,output_path)\\n\\nclass RegressionAnalysisProblem(Problem):\\n    def __init__(self):\\n        super().__init__('regression_analysis_problem.tex')\\n    def generate_parameters(self):\\n        self.params['beta_0']=round(random.uniform(0,10),2)\\n        self.params['beta_1']=round(random.uniform(-5,5),2)\\n        self.params['n']=random.randint(50,200)\\n        self.params['x_values']=[round(random.uniform(0,100),2) for _ in range(self.params['n'])]\\n        self.params['epsilon']=[round(random.gauss(0,10),2) for _ in range(self.params['n'])]\\n        self.params['y_values']=[self.params['beta_0']+self.params['beta_1']*x+e for x,e in zip(self.params['x_values'],self.params['epsilon'])]\\n        import numpy as np\\n        X=np.array(self.params['x_values'])\\n        Y=np.array(self.params['y_values'])\\n        beta_1_hat=np.cov(X,Y,bias=True)[0,1]/np.var(X)\\n        beta_0_hat=np.mean(Y)-beta_1_hat*np.mean(X)\\n        self.params['beta_0_hat']=round(beta_0_hat,4)\\n        self.params['beta_1_hat']=round(beta_1_hat,4)\\n    def generate_graph(self,output_path):\\n        return self.visualizer.plot_regression(self.params['x_values'],self.params['y_values'],self.params['beta_0_hat'],self.params['beta_1_hat'],output_path)\\n\\nclass TimeSeriesAnalysisProblem(Problem):\\n    def __init__(self):\\n        super().__init__('time_series_analysis_problem.tex')\\n    def generate_parameters(self):\\n        self.params['phi']=round(random.uniform(0.5,0.9),2)\\n        self.params['theta']=round(random.uniform(-0.5,0.5),2)\\n        self.params['n']=100\\n        self.params['epsilon']=[random.gauss(0,1) for _ in range(self.params['n'])]\\n        self.params['time_series']=[0]*self.params['n']\\n        for t in range(1,self.params['n']):\\n            self.params['time_series'][t]=self.params['phi']*self.params['time_series'][t-1]+self.params['epsilon'][t]+self.params['theta']*self.params['epsilon'][t-1]\\n    def generate_graph(self,output_path):\\n        return self.visualizer.plot_time_series(self.params,output_path)\\n\\nclass EconometricsProblem(Problem):\\n    def __init__(self):\\n        super().__init__('econometrics_problem.tex')\\n    def generate_parameters(self):\\n        self.params['beta_0']=round(random.uniform(0,5),2)\\n        self.params['beta_1']=round(random.uniform(0,1),2)\\n        self.params['beta_2']=round(random.uniform(-1,0),2)\\n        self.params['n']=random.randint(50,200)\\n        self.params['x1_values']=[round(random.uniform(0,100),2) for _ in range(self.params['n'])]\\n        self.params['x2_values']=[round(random.uniform(0,100),2) for _ in range(self.params['n'])]\\n        self.params['epsilon']=[round(random.gauss(0,5),2) for _ in range(self.params['n'])]\\n        self.params['y_values']=[self.params['beta_0']+self.params['beta_1']*x1+self.params['beta_2']*x2+e for x1,x2,e in zip(self.params['x1_values'],self.params['x2_values'],self.params['epsilon'])]\\n        import numpy as np\\n        X=np.column_stack((np.ones(self.params['n']),self.params['x1_values'],self.params['x2_values']))\\n        Y=np.array(self.params['y_values'])\\n        beta_hat=np.linalg.inv(X.T@X)@X.T@Y\\n        self.params['beta_0_hat']=round(beta_hat[0],4)\\n        self.params['beta_1_hat']=round(beta_hat[1],4)\\n        self.params['beta_2_hat']=round(beta_hat[2],4)\\n    def generate_graph(self,output_path):\\n        return self.visualizer.plot_econometrics(self.params,output_path)\\n\\nclass LinearCombinationProblem(Problem):\\n    def __init__(self):\\n        super().__init__('linear_combination_problem.tex')\\n    def generate_parameters(self):\\n        self.params['a']=random.randint(1,5)\\n        self.params['b']=random.randint(1,5)\\n        self.params['mu1']=round(random.uniform(0,10),2)\\n        self.params['mu2']=round(random.uniform(0,10),2)\\n        self.params['sigma1_squared']=round(random.uniform(1,5),2)\\n        self.params['sigma2_squared']=round(random.uniform(1,5),2)\\n        E_Z=self.params['a']*self.params['mu1']+self.params['b']*self.params['mu2']\\n        Var_Z=(self.params['a']**2)*self.params['sigma1_squared']+(self.params['b']**2)*self.params['sigma2_squared']\\n        self.params['E_Z']=round(E_Z,4)\\n        self.params['Var_Z']=round(Var_Z,4)\\n    def generate_explanation(self):\\n        return \\\"線形結合の期待値・分散計算\\\"\\n    def generate_graph(self,output_path):\\n        return self.visualizer.plot_linear_combination(self.params,output_path)\\n\\nclass DistributionPropertiesProblem(Problem):\\n    def __init__(self):\\n        super().__init__('distribution_properties_problem.tex')\\n    def generate_parameters(self):\\n        dist_choice=random.choice(['正規分布','ポアソン分布','指数分布'])\\n        self.params['distribution']=dist_choice\\n        if dist_choice=='正規分布':\\n            self.params['properties']={'mean':'\\\\\\\\mu','variance':'\\\\\\\\sigma^2'}\\n        elif dist_choice=='ポアソン分布':\\n            self.params['properties']={'mean':'\\\\\\\\lambda','variance':'\\\\\\\\lambda'}\\n        else:\\n            self.params['properties']={'mean':'1/\\\\\\\\lambda','variance':'1/\\\\\\\\lambda^2'}\\n    def generate_graph(self,output_path):\\n        return self.visualizer.plot_distribution_properties(self.params,output_path)\\n\\nclass HighMomentProblem(Problem):\\n    def __init__(self):\\n        super().__init__('high_moment_problem.tex')\\n    def generate_parameters(self):\\n        self.params['n']=random.randint(3,5)\\n        self.params['mu']=round(random.uniform(0,10),2)\\n        self.params['sigma']=round(random.uniform(1,5),2)\\n        from scipy.stats import norm\\n        m=norm.moment(self.params['n'],loc=self.params['mu'],scale=self.params['sigma'])\\n        self.params['moment']=round(m,4)\\n    def generate_graph(self,output_path):\\n        return self.visualizer.plot_high_moment(self.params,output_path)\\n\\nclass MultivariateNormalProblem(Problem):\\n    def __init__(self):\\n        super().__init__('multivariate_normal_problem.tex')\\n    def generate_parameters(self):\\n        self.params['mu']=[round(random.uniform(0,10),2) for _ in range(2)]\\n        self.params['sigma']=[[round(random.uniform(1,5),2),round(random.uniform(0,2),2)],[round(random.uniform(0,2),2),round(random.uniform(1,5),2)]]\\n    def generate_explanation(self):\\n        return \\\"多変量正規分布の性質を利用\\\"\\n    def generate_graph(self,output_path):\\n        return self.visualizer.plot_multivariate_normal(self.params,output_path)\\n\\nclass VarianceAnalysisProblem(Problem):\\n    def __init__(self):\\n        super().__init__('variance_analysis_problem.tex')\\n    def generate_parameters(self):\\n        self.params['group_count']=random.randint(2,4)\\n        self.params['sample_sizes']=[random.randint(5,20) for _ in range(self.params['group_count'])]\\n        self.params['means']=[round(random.uniform(10,50),2) for _ in range(self.params['group_count'])]\\n        self.params['variances']=[round(random.uniform(1,5),2) for _ in range(self.params['group_count'])]\\n        total_n=sum(self.params['sample_sizes'])\\n        group_count=self.params['group_count']\\n        means=self.params['means']\\n        variances=self.params['variances']\\n        sample_sizes=self.params['sample_sizes']\\n        grand_mean=sum([means[i]*sample_sizes[i] for i in range(group_count)])/total_n\\n        ssb=sum([sample_sizes[i]*(means[i]-grand_mean)**2 for i in range(group_count)])\\n        ssw=sum([(sample_sizes[i]-1)*variances[i] for i in range(group_count)])\\n        df_between=group_count-1\\n        df_within=total_n-group_count\\n        msb=ssb/df_between\\n        msw=ssw/df_within\\n        F=msb/msw\\n        alpha=0.05\\n        from scipy.stats import f\\n        F_critical=f.ppf(1-alpha,df_between,df_within)\\n        reject='棄却する' if F>F_critical else '棄却しない'\\n        self.params['F_value']=round(F,4)\\n        self.params['F_critical']=round(F_critical,4)\\n        self.params['reject_null']=reject\\n        self.params['df_between']=df_between\\n        self.params['df_within']=df_within\\n    def generate_explanation(self):\\n        return \\\"一元配置分散分析による検定\\\"\\n    def generate_graph(self,output_path):\\n        return self.visualizer.plot_variance_analysis(self.params,output_path)\\n\\nclass NonParametricTestProblem(Problem):\\n    def __init__(self):\\n        super().__init__('nonparametric_test_problem.tex')\\n    def generate_parameters(self):\\n        self.params['test_type']=random.choice(['Mann-Whitney U','Kruskal-Wallis','Wilcoxon Signed-Rank'])\\n        self.params['sample1']=[round(random.uniform(10,100),2) for _ in range(random.randint(5,20))]\\n        self.params['sample2']=[round(random.uniform(10,100),2) for _ in range(random.randint(5,20))]\\n        self.params['test_result']='有意差なし(例)'\\n    def generate_explanation(self):\\n        return \\\"ノンパラ検定で中央値差を検定\\\"\\n    def generate_graph(self,output_path):\\n        return self.visualizer.plot_nonparametric_test(self.params,output_path)\\n\\nclass ProbabilityDefinitionProblem(Problem):\\n    def __init__(self):\\n        super().__init__('probability_definition_problem.tex')\\n    def generate_parameters(self):\\n        self.params['P_A']=round(random.uniform(0.1,0.9),2)\\n        self.params['P_B']=round(random.uniform(0.1,0.9),2)\\n        self.params['P_A_and_B']=round(self.params['P_A']*self.params['P_B'],4)\\n    def generate_explanation(self):\\n        return \\\"独立性利用 P(A∩B)=P(A)*P(B)\\\"\\n    def generate_graph(self,output_path):\\n        # 単純な棒グラフでP(A), P(B)とP(A∩B)を表示\\n        try:\\n            PA=self.params['P_A']\\n            PB=self.params['P_B']\\n            PAB=self.params['P_A_and_B']\\n            plt.figure()\\n            plt.bar(['P(A)','P(B)','P(A∩B)'],[PA,PB,PAB],color=['blue','green','red'])\\n            plt.title('Probability Definition')\\n            plt.tight_layout()\\n            plt.savefig(output_path)\\n            plt.close()\\n            return True\\n        except:\\n            return False\\n\\nclass ConditionalProbabilityProblem(Problem):\\n    def __init__(self):\\n        super().__init__('conditional_probability_problem.tex')\\n    def generate_parameters(self):\\n        self.params['P_A']=round(random.uniform(0.2,0.8),2)\\n        self.params['P_B_given_A']=round(random.uniform(0.2,0.8),2)\\n        self.params['P_A_and_B']=round(self.params['P_A']*self.params['P_B_given_A'],4)\\n    def generate_explanation(self):\\n        return \\\"P(A∩B)=P(A)*P(B|A)\\\"\\n    def generate_graph(self,output_path):\\n        # graph.pyでconditional_probabilityで実装済み\\n        from graph import ProbabilityDistributionVisualizer\\n        vis=ProbabilityDistributionVisualizer()\\n        return vis.plot_probability(self.params,output_path)\\n\\nclass DistributionFunctionsProblem(Problem):\\n    def __init__(self):\\n        super().__init__('distribution_functions_problem.tex')\\n    def generate_parameters(self):\\n        self.params['function_type']=random.choice(['pdf','cdf'])\\n        self.params['distribution']=random.choice(['正規分布','指数分布'])\\n        if self.params['distribution']=='正規分布':\\n            self.params['mean']=round(random.uniform(-5,5),2)\\n            self.params['std']=round(random.uniform(1,3),2)\\n        else:\\n            self.params['lambda']=round(random.uniform(0.5,2.0),2)\\n    def generate_explanation(self):\\n        return \\\"pdfやcdf定義式利用\\\"\\n    def generate_graph(self,output_path):\\n        # ここは実装なし、増やしてもよいが現状テンプレ通り\\n        return False\\n\\nclass JointDistributionProblem(Problem):\\n    def __init__(self):\\n        super().__init__('joint_distribution_problem.tex')\\n    def generate_parameters(self):\\n        A_and_B=round(random.uniform(0.05,0.2),2)\\n        A_and_notB=round(random.uniform(0.05,0.2),2)\\n        notA_and_B=round(random.uniform(0.05,0.2),2)\\n        notA_and_notB=round(random.uniform(0.05,0.2),2)\\n        total=A_and_B+A_and_notB+notA_and_B+notA_and_notB\\n        A_and_B/=total\\n        A_and_notB/=total\\n        notA_and_B/=total\\n        notA_and_notB/=total\\n        self.params['joint_probabilities']={'A_and_B':round(A_and_B,4),'A_and_not_B':round(A_and_notB,4),'not_A_and_B':round(notA_and_B,4),'not_A_and_not_B':round(notA_and_notB,4)}\\n        P_A=A_and_B+A_and_notB\\n        P_B=A_and_B+notA_and_B\\n        P_BA=A_and_B/P_A if P_A>0 else 0.0\\n        self.params['P_A']=round(P_A,4)\\n        self.params['P_B']=round(P_B,4)\\n        self.params['P_B_given_A']=round(P_BA,4)\\n    def generate_explanation(self):\\n        return \\\"同時→周辺→条件付き確率\\\"\\n    def generate_graph(self,output_path):\\n        # 簡易的にjoint分布表をHeatmapで可視化\\n        try:\\n            p=self.params['joint_probabilities']\\n            matrix=np.array([[p['A_and_B'],p['A_and_not_B']],[p['not_A_and_B'],p['not_A_and_not_B']]])\\n            plt.figure()\\n            plt.imshow(matrix,cmap='Blues',interpolation='nearest')\\n            plt.colorbar(label='Probability')\\n            plt.xticks([0,1],['B','not B'])\\n            plt.yticks([0,1],['A','not A'])\\n            plt.title('Joint Distribution Heatmap')\\n            plt.tight_layout()\\n            plt.savefig(output_path)\\n            plt.close()\\n            return True\\n        except:\\n            return False\\n\"\n    },\n    {\n      \"path\": \"統計検定1級\\\\problem_types\\\\problem_factory.py\",\n      \"overview\": \"このスクリプトはPythonコードです。\\n定義されているクラス: ProblemFactory。\\n定義されている関数: __init__, create_problem。\\n\",\n      \"content\": \"# problem_types/problem_factory.py\\n\\nimport logging,traceback\\nfrom problem_types.problem import ProbabilityProblem,StatisticalInferenceProblem,RegressionAnalysisProblem,TimeSeriesAnalysisProblem,EconometricsProblem,LinearCombinationProblem,DistributionPropertiesProblem,HighMomentProblem,MultivariateNormalProblem,VarianceAnalysisProblem,NonParametricTestProblem,ProbabilityDefinitionProblem,ConditionalProbabilityProblem,DistributionFunctionsProblem,JointDistributionProblem\\nfrom problem_types.conjugate_problems import BetaBinomialConjugateProblem,GammaPoissonConjugateProblem,DirichletMultinomialConjugateProblem,BinomialPoissonApproxProblem,PoissonNormalApproxProblem\\n\\nclass ProblemFactory:\\n    def __init__(self):\\n        self.problem_classes={\\n            'probability':ProbabilityProblem,\\n            'statistical_inference':StatisticalInferenceProblem,\\n            'regression_analysis':RegressionAnalysisProblem,\\n            'time_series_analysis':TimeSeriesAnalysisProblem,\\n            'econometrics':EconometricsProblem,\\n            'linear_combination':LinearCombinationProblem,\\n            'distribution_properties':DistributionPropertiesProblem,\\n            'high_moment':HighMomentProblem,\\n            'multivariate_normal':MultivariateNormalProblem,\\n            'probability_definition':ProbabilityDefinitionProblem,\\n            'conditional_probability':ConditionalProbabilityProblem,\\n            'distribution_functions':DistributionFunctionsProblem,\\n            'joint_distribution':JointDistributionProblem,\\n            't_test':StatisticalInferenceProblem,\\n            'variance_analysis':VarianceAnalysisProblem,\\n            'nonparametric_test':NonParametricTestProblem,\\n            'beta_binomial_conjugate':BetaBinomialConjugateProblem,\\n            'gamma_poisson_conjugate':GammaPoissonConjugateProblem,\\n            'dirichlet_multinomial_conjugate':DirichletMultinomialConjugateProblem,\\n            'binomial_poisson_approx':BinomialPoissonApproxProblem,\\n            'poisson_normal_approx':PoissonNormalApproxProblem\\n        }\\n    def create_problem(self,problem_type):\\n        pc=self.problem_classes.get(problem_type)\\n        if pc:\\n            try:\\n                return pc()\\n            except Exception as e:\\n                logging.error(f\\\"{problem_type} problem generation error:{e}\\\")\\n                logging.error(traceback.format_exc())\\n                raise\\n        else:\\n            raise ValueError(f\\\"Unknown problem type:{problem_type}\\\")\"\n    },\n    {\n      \"path\": \"統計検定1級\\\\problem_types\\\\__init__.py\",\n      \"overview\": \"このスクリプトはPythonコードです。\\n\",\n      \"content\": \"\"\n    },\n    {\n      \"path\": \"統計検定1級\\\\templates\\\\distribution_properties_problem.tex\",\n      \"overview\": \"このファイルはテキストファイルまたはその他の形式です。\\n行数: 7\\n\",\n      \"content\": \"% distribution_properties_problem.tex\\n{% if not show_solution %}\\n{{ distribution }} の平均と分散を求めよ。\\n{% else %}\\n平均: ${{ properties.mean }}$, 分散: ${{ properties.variance }}$\\n{{ explanation }}\\n{% endif %}\\n\"\n    },\n    {\n      \"path\": \"統計検定1級\\\\templates\\\\distribution_relations.tex\",\n      \"overview\": \"このファイルはテキストファイルまたはその他の形式です。\\n行数: 7\\n\",\n      \"content\": \"% distribution_relations.tex\\n\\\\section*{Distribution Relations}\\n- Beta+Binomial -> Beta-Binomial\\n- Gamma+Poisson -> Negative Binomial\\n- Dirichlet+Multinomial -> Dirichlet-Multinomial\\n- Binomial->Poisson approximation\\n- Poisson->Normal approximation\\n\"\n    },\n    {\n      \"path\": \"統計検定1級\\\\templates\\\\econometrics_problem.tex\",\n      \"overview\": \"このファイルはテキストファイルまたはその他の形式です。\\n行数: 6\\n\",\n      \"content\": \"% econometrics_problem.tex\\n{% if not show_solution %}\\n計量経済学モデルに関する問題\\n{% else %}\\n解答と推定量\\n{{ explanation }}\\n{% endif %}\"\n    },\n    {\n      \"path\": \"統計検定1級\\\\templates\\\\high_moment_problem.tex\",\n      \"overview\": \"このファイルはテキストファイルまたはその他の形式です。\\n行数: 6\\n\",\n      \"content\": \"% high_moment_problem.tex\\n{% if not show_solution %}\\n正規分布の{{ n }}次モーメントは？(例: $E[X^{n}]$ )\\n{% else %}\\n$E[X^{n}]={{ moment }}$\\n{{ explanation }}\\n{% endif %}\"\n    },\n    {\n      \"path\": \"統計検定1級\\\\templates\\\\linear_combination_problem.tex\",\n      \"overview\": \"このファイルはテキストファイルまたはその他の形式です。\\n行数: 6\\n\",\n      \"content\": \"% linear_combination_problem.tex\\n{% if not show_solution %}\\nZ=aX+bY のE[Z],Var[Z]\\n{% else %}\\nE[Z],Var[Z]\\n{{ explanation }}\\n{% endif %}\"\n    },\n    {\n      \"path\": \"統計検定1級\\\\templates\\\\multivariate_normal_problem.tex\",\n      \"overview\": \"このファイルはテキストファイルまたはその他の形式です。\\n行数: 6\\n\",\n      \"content\": \"% multivariate_normal_problem.tex\\n{% if not show_solution %}\\n多変量正規に関する問題\\n{% else %}\\n解答\\n{{ explanation }}\\n{% endif %}\"\n    },\n    {\n      \"path\": \"統計検定1級\\\\templates\\\\nonparametric_test_problem.tex\",\n      \"overview\": \"このファイルはテキストファイルまたはその他の形式です。\\n行数: 6\\n\",\n      \"content\": \"% nonparametric_test_problem.tex\\n{% if not show_solution %}\\nノンパラ検定問題\\n{% else %}\\n検定結果\\n{{ explanation }}\\n{% endif %}\"\n    },\n    {\n      \"path\": \"統計検定1級\\\\templates\\\\poisson_normal_approx_problem.tex\",\n      \"overview\": \"このファイルはテキストファイルまたはその他の形式です。\\n行数: 6\\n\",\n      \"content\": \"% poisson_normal_approx_problem.tex\\n{% if not show_solution %}\\nPoisson→Normal近似\\n{% else %}\\nPoisson P=${{ poisson_p }}$, mean=${{mean}}$ var=${{variance}}$\\n{{ explanation }}\\n{% endif %}\"\n    },\n    {\n      \"path\": \"統計検定1級\\\\templates\\\\probability_definition_problem.tex\",\n      \"overview\": \"このファイルはテキストファイルまたはその他の形式です。\\n行数: 6\\n\",\n      \"content\": \"% probability_definition_problem.tex\\n{% if not show_solution %}\\nP(A∩B)求めよ\\n{% else %}\\n結果\\n{{ explanation }}\\n{% endif %}\"\n    },\n    {\n      \"path\": \"統計検定1級\\\\templates\\\\probability_problem.tex\",\n      \"overview\": \"このファイルはテキストファイルまたはその他の形式です。\\n行数: 7\\n\",\n      \"content\": \"% probability_problem.tex\\n{% if not show_solution %}\\n確率計算問題（例）\\n問題タイプ: {{ problem_type }}\\n{% else %}\\n解答と説明: {{ explanation }}\\n計算結果: P = {{ probability }}\\n{% endif %}\"\n    },\n    {\n      \"path\": \"統計検定1級\\\\templates\\\\regression_analysis_problem.tex\",\n      \"overview\": \"このファイルはテキストファイルまたはその他の形式です。\\n行数: 6\\n\",\n      \"content\": \"% regression_analysis_problem.tex\\n{% if not show_solution %}\\n回帰分析問題\\n{% else %}\\n回帰係数結果\\n{{ explanation }}\\n{% endif %}\"\n    },\n    {\n      \"path\": \"統計検定1級\\\\templates\\\\statistical_inference_problem.tex\",\n      \"overview\": \"このファイルはテキストファイルまたはその他の形式です。\\n行数: 6\\n\",\n      \"content\": \"% statistical_inference_problem.tex\\n{% if not show_solution %}\\n統計的推定/検定問題\\n{% else %}\\n検定結果\\n{{ explanation }}\\n{% endif %}\"\n    },\n    {\n      \"path\": \"統計検定1級\\\\templates\\\\time_series_analysis_problem.tex\",\n      \"overview\": \"このファイルはテキストファイルまたはその他の形式です。\\n行数: 6\\n\",\n      \"content\": \"% time_series_analysis_problem.tex\\n{% if not show_solution %}\\n時系列分析問題\\n{% else %}\\n解答と説明\\n{{ explanation }}\\n{% endif %}\"\n    },\n    {\n      \"path\": \"統計検定1級\\\\templates\\\\variance_analysis_problem.tex\",\n      \"overview\": \"このファイルはテキストファイルまたはその他の形式です。\\n行数: 6\\n\",\n      \"content\": \"% variance_analysis_problem.tex\\n{% if not show_solution %}\\n分散分析問題\\n{% else %}\\nANOVA結果\\n{{ explanation }}\\n{% endif %}\"\n    }\n  ]\n}"
    },
    {
      "path": "your_project\\collected_scripts - コピー.json",
      "overview": "JSONファイル (辞書)。キー: system_overview, settings, scripts\n",
      "content": "{\n  \"system_overview\": \"\",\n  \"settings\": {},\n  \"scripts\": [\n    {\n      \"path\": \"feedback.json\",\n      \"overview\": \"JSONファイル (辞書)。キー: timestamp, status, remarks, possible_improvements\\n\",\n      \"content\": \"{\\n  \\\"timestamp\\\": \\\"20250109_142942\\\",\\n  \\\"status\\\": \\\"Completed\\\",\\n  \\\"remarks\\\": \\\"\\\",\\n  \\\"possible_improvements\\\": [\\n    \\\"ログレベルをDEBUGにして詳細確認\\\",\\n    \\\"追加パラメータを考慮した解析を実装する\\\",\\n    \\\"AIプロンプトの自動生成を検討\\\"\\n  ]\\n}\"\n    },\n    {\n      \"path\": \".vscode\\\\launch.json\",\n      \"overview\": \"JSON解析エラー: Expecting property name enclosed in double quotes: line 2 column 5 (char 6)\\n\",\n      \"content\": \"{\\n    // IntelliSense を使用して利用可能な属性を学べます。\\n    // 既存の属性の説明をホバーして表示します。\\n    // 詳細情報は次を確認してください: https://go.microsoft.com/fwlink/?linkid=830387\\n    \\\"version\\\": \\\"0.2.0\\\",\\n    \\\"configurations\\\": [\\n        {\\n            \\\"name\\\": \\\"Python: 現在のファイル\\\",\\n            \\\"type\\\": \\\"python\\\",\\n            \\\"request\\\": \\\"launch\\\",\\n            \\\"program\\\": \\\"${file}\\\",\\n            \\\"console\\\": \\\"integratedTerminal\\\",\\n            \\\"justMyCode\\\": true,\\n        }\\n    ]\\n}\"\n    },\n    {\n      \"path\": \"your_project\\\\credential_manager.py\",\n      \"overview\": \"Pythonコード。\\nクラス: CredentialManager。\\n関数: decrypt_credentials。\\n\",\n      \"content\": \"import logging\\nfrom typing import Dict\\n\\nclass CredentialManager:\\n\\n    @staticmethod\\n    def decrypt_credentials(encrypted_credentials: Dict) -> Dict:\\n        \\\"\\\"\\\"\\n        入力:\\n          encrypted_credentials (dict): {\\\"user\\\": \\\"encrypted_user\\\", \\\"password\\\": \\\"encrypted_password\\\"}\\n        出力:\\n          dict – 復号化された認証情報、例: {\\\"user\\\": \\\"decrypted_user\\\", \\\"password\\\": \\\"decrypted_password\\\"}\\n        ※ 本実装はダミーで、実際には復号化処理を実装すべき箇所です。\\n        \\\"\\\"\\\"\\n        decrypted = {}\\n        for (key, value) in encrypted_credentials.items():\\n            decrypted[key] = 'decrypted_' + str(value)\\n        logging.info('CredentialManager: Credentials decrypted (dummy implementation).')\\n        return decrypted\"\n    },\n    {\n      \"path\": \"your_project\\\\data_pipeline.py\",\n      \"overview\": \"Pythonコード。\\nクラス: DataPipeline。\\n関数: run_pipeline。\\n\",\n      \"content\": \"import asyncio\\nimport logging\\nfrom typing import Union, List, Dict\\nimport pandas as pd\\nfrom data_preparation.data_loader import DataLoader\\nfrom data_preparation.data_preprocessor import DataPreprocessor\\n\\nclass DataPipeline:\\n\\n    def run_pipeline(self, input_source: Union[str, dict, list], config: Dict, data_types: List[str]) -> Dict:\\n        \\\"\\\"\\\"\\n        入力:\\n          input_source (Union[str, dict, list]): DataLoader に渡す入力ソース\\n          config (dict): 全体の設定情報\\n          data_types (List[str]): 例: ['text', 'html', 'csv', 'json', 'xls']\\n        出力:\\n          dict – {\\\"files\\\": <List[str]>, \\\"processed_data\\\": <DataFrame/その他>}\\n        \\\"\\\"\\\"\\n        loader = DataLoader(input_source, data_types)\\n        files = loader.collect_files()\\n        preprocessor = DataPreprocessor()\\n        raw_data = asyncio.run(loader.file_processor.process_files_in_parallel(files))\\n        processed_data = preprocessor.preprocess_data(raw_data)\\n        logging.info('DataPipeline: Pipeline run completed.')\\n        return {'files': files, 'processed_data': processed_data}\"\n    },\n    {\n      \"path\": \"your_project\\\\feedback.json\",\n      \"overview\": \"JSONファイル (辞書)。キー: timestamp, status, remarks, possible_improvements\\n\",\n      \"content\": \"{\\n  \\\"timestamp\\\": \\\"20250109_142942\\\",\\n  \\\"status\\\": \\\"Completed\\\",\\n  \\\"remarks\\\": \\\"\\\",\\n  \\\"possible_improvements\\\": [\\n    \\\"ログレベルをDEBUGにして詳細確認\\\",\\n    \\\"追加パラメータを考慮した解析を実装する\\\",\\n    \\\"AIプロンプトの自動生成を検討\\\"\\n  ]\\n}\"\n    },\n    {\n      \"path\": \"your_project\\\\input_parser.py\",\n      \"overview\": \"Pythonコード。\\nクラス: InputParser。\\n関数: parse_input_source。\\n\",\n      \"content\": \"import json\\nimport logging\\nfrom typing import Union\\n\\nclass InputParser:\\n\\n    @staticmethod\\n    def parse_input_source(raw_input: str) -> Union[str, dict, list]:\\n        \\\"\\\"\\\"\\n        入力:\\n          raw_input (str): コマンドラインから渡される入力文字列。JSON形式なら解析する。\\n        出力:\\n          Union[str, dict, list] – JSON形式の場合は dict または list、そうでなければそのままの文字列。\\n        \\\"\\\"\\\"\\n        try:\\n            parsed = json.loads(raw_input)\\n            logging.info('InputParser: Input parsed as JSON.')\\n            return parsed\\n        except json.JSONDecodeError:\\n            logging.info('InputParser: Input is a plain string.')\\n            return raw_input\"\n    },\n    {\n      \"path\": \"your_project\\\\main.py\",\n      \"overview\": \"Pythonコード。\\n関数: debug_show_sibling_folders, parse_arguments, validate_input_path, load_and_preprocess_data, apply_analysis_methods, save_performance_metrics, load_config, main。\\n\",\n      \"content\": \"import sys\\nimport os\\nimport logging\\nimport argparse\\nimport asyncio\\nimport datetime\\nimport json\\nfrom typing import Any, Dict, List\\nsys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))\\nfrom data_preparation.data_loader import DataLoader\\nfrom data_preparation.data_preprocessor import DataPreprocessor\\nfrom models.model_manager import ModelManager\\nfrom utils.output_manager import OutputManager\\nfrom utils.logger import Logger\\nfrom utils.db_utils import DBUtils\\nfrom performance.performance_tracker import PerformanceTracker\\nfrom visualization.visualizer import Visualizer\\nfrom utils.config_manager import ConfigManager\\nfrom input_parser import InputParser\\nfrom core.finalize import finalize_with_feedback\\n\\ndef debug_show_sibling_folders(base_dir: str) -> None:\\n    \\\"\\\"\\\"\\n    指定ディレクトリの親フォルダにあるファイル・フォルダをログ出力\\n    \\\"\\\"\\\"\\n    parent_dir = os.path.dirname(base_dir)\\n    logging.info('=== Debug: Sibling folders/files ===')\\n    try:\\n        for item in os.listdir(parent_dir):\\n            full_path = os.path.join(parent_dir, item)\\n            if os.path.isdir(full_path):\\n                logging.info(f'Dir: {item}')\\n            else:\\n                logging.info(f'File: {item}')\\n    except Exception as e:\\n        logging.warning(f'Error listing siblings: {e}', exc_info=True)\\n    logging.info('=== End of listing ===')\\n\\ndef parse_arguments(default_input: str):\\n    parser = argparse.ArgumentParser(description='汎用データ分析基盤ツール')\\n    parser.add_argument('--input', type=str, default=default_input, help='Input data source (folder path or media descriptor in JSON)')\\n    return parser.parse_args()\\n\\ndef validate_input_path(input_path: str) -> None:\\n    if not os.path.exists(input_path):\\n        raise FileNotFoundError(f'Input path not found: {input_path}')\\n\\ndef load_and_preprocess_data(input_source: Any, data_types: List[str]) -> Any:\\n    from performance.performance_tracker import PerformanceTracker\\n    performance_local = PerformanceTracker()\\n    performance_local.start_timer('data_loading')\\n    loader = DataLoader(input_source, data_types)\\n    raw_data = loader.load_data()\\n    performance_local.end_timer('data_loading')\\n    logging.info('Data loading completed.')\\n    performance_local.start_timer('data_preprocessing')\\n    from data_preparation.data_preprocessor import DataPreprocessor\\n    preprocessor = DataPreprocessor()\\n    df_data = preprocessor.preprocess_data(raw_data)\\n    performance_local.end_timer('data_preprocessing')\\n    logging.info('Data preprocessing completed.')\\n    return df_data\\n\\ndef apply_analysis_methods(df_data: Any, base_dir: str, output_dir: str, enable_visualization: bool) -> None:\\n    from performance.performance_tracker import PerformanceTracker\\n    performance_local = PerformanceTracker()\\n    from models.model_manager import ModelManager\\n    from utils.output_manager import OutputManager\\n    from visualization.visualizer import Visualizer\\n    model_manager = ModelManager()\\n    output_manager = OutputManager()\\n    available_methods = model_manager.get_available_methods()\\n    logging.info(f'Available analysis methods: {available_methods}')\\n    for method_name in available_methods:\\n        logging.info(f\\\"Applying analysis method '{method_name}'\\\")\\n        try:\\n            performance_local.start_timer(method_name)\\n            result = model_manager.apply_method(method_name, df_data)\\n            performance_local.end_timer(method_name)\\n            if result.empty:\\n                logging.info(f\\\"No valid result for '{method_name}'.\\\")\\n                continue\\n            output_manager.save_results(result, method_name, output_dir)\\n            asyncio.run(output_manager.save_to_database(result, method_name))\\n            logging.info(f\\\"Result saved for '{method_name}'.\\\")\\n            if enable_visualization:\\n                vis_dir = os.path.join(output_dir, 'visualizations')\\n                os.makedirs(vis_dir, exist_ok=True)\\n                visualizer = Visualizer()\\n                visualizer.visualize(method_name, result, vis_dir)\\n                logging.info(f\\\"Visualization saved for '{method_name}'.\\\")\\n            report_dir = os.path.join(output_dir, 'reports')\\n            os.makedirs(report_dir, exist_ok=True)\\n            try:\\n                output_manager.save_html_report(result, method_name, report_dir)\\n                logging.info(f\\\"HTML report saved for '{method_name}'.\\\")\\n            except Exception as err:\\n                logging.error(f\\\"Error generating HTML report for '{method_name}': {err}\\\", exc_info=True)\\n        except Exception as e:\\n            logging.error(f\\\"Error executing '{method_name}': {e}\\\", exc_info=True)\\n\\ndef save_performance_metrics(performance: PerformanceTracker, base_dir: str, output_dir: str) -> None:\\n    from utils.output_manager import OutputManager\\n    perf_data = performance.get_metrics()\\n    output_manager = OutputManager()\\n    csv_path = os.path.join(output_dir, 'performance_metrics.csv')\\n    output_manager.save_performance_metrics(perf_data, csv_path)\\n    logging.info(f'Performance metrics saved at: {csv_path}')\\n\\ndef load_config() -> ConfigManager:\\n    from utils.config_manager import ConfigManager\\n    config_manager = ConfigManager()\\n    config_path = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), 'config.json')\\n    config_manager.load_config(config_path)\\n    return config_manager\\n\\ndef main() -> None:\\n    run_timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\\n    base_dir = os.path.dirname(os.path.abspath(__file__))\\n    run_output_dir = os.path.join(base_dir, f'run_{run_timestamp}')\\n    os.makedirs(run_output_dir, exist_ok=True)\\n    log_dir = os.path.join(run_output_dir, 'logs')\\n    os.makedirs(log_dir, exist_ok=True)\\n    initial_log_level = 'INFO'\\n    Logger.setup_logging(log_dir, initial_log_level)\\n    logging.info('===== Execution Started =====')\\n    debug_show_sibling_folders(base_dir)\\n    config = load_config()\\n    log_level = config.get('log_level', default='INFO')\\n    data_directory = config.get('data_directory', default='data')\\n    output_directory = config.get('output_directory', default='outputs')\\n    database_path = config.get('database_path', default='data.db')\\n    data_types = config.get('data_types', default=['text', 'html', 'csv', 'json'])\\n    enable_visualization = config.get('enable_visualization', default=True)\\n    Logger.setup_logging(log_dir, log_level)\\n    actual_output_dir = os.path.join(run_output_dir, output_directory)\\n    os.makedirs(actual_output_dir, exist_ok=True)\\n    db_full_path = os.path.join(run_output_dir, os.path.basename(database_path))\\n    logging.info(f'Configuration loaded. log_level={log_level}, data_directory={data_directory}, output_directory={output_directory}')\\n    logging.info(f'run_output_dir={run_output_dir}, db_full_path={db_full_path}')\\n    performance = PerformanceTracker()\\n    performance.start_timer('total_execution')\\n    try:\\n        asyncio.run(DBUtils.initialize_database(db_full_path))\\n    except Exception as init_db_err:\\n        logging.error(f'Error initializing DB: {init_db_err}', exc_info=True)\\n        sys.exit(1)\\n    args = parse_arguments(data_directory)\\n    parent_dir = os.path.dirname(base_dir)\\n    input_path = os.path.join(parent_dir, args.input)\\n    try:\\n        validate_input_path(input_path)\\n    except FileNotFoundError as fnf_err:\\n        logging.error(fnf_err, exc_info=True)\\n        finalize_with_feedback(run_output_dir, run_timestamp, 'DB初期化失敗', extra_remarks=str(fnf_err))\\n        sys.exit(1)\\n    except Exception as val_err:\\n        logging.error(val_err, exc_info=True)\\n        finalize_with_feedback(run_output_dir, run_timestamp, '入力パス検証失敗', extra_remarks=str(val_err))\\n        sys.exit(1)\\n    try:\\n        df_data = load_and_preprocess_data(input_path, data_types)\\n    except Exception as load_err:\\n        logging.error(f'Error in data loading/preprocessing: {load_err}', exc_info=True)\\n        finalize_with_feedback(run_output_dir, run_timestamp, 'データ読み込み or 前処理失敗', extra_remarks=str(load_err))\\n        sys.exit(1)\\n    try:\\n        apply_analysis_methods(df_data, base_dir, actual_output_dir, enable_visualization)\\n    except Exception as analyze_err:\\n        logging.error(f'Error during analysis: {analyze_err}', exc_info=True)\\n        finalize_with_feedback(run_output_dir, run_timestamp, '解析失敗', extra_remarks=str(analyze_err))\\n        sys.exit(1)\\n    performance.end_timer('total_execution')\\n    save_performance_metrics(performance, base_dir, actual_output_dir)\\n    try:\\n        asyncio.run(DBUtils.close_connection())\\n    except Exception as close_err:\\n        logging.error(f'Error closing DB: {close_err}', exc_info=True)\\n    logging.info('Analysis completed. Results saved in output folder.')\\n    finalize_with_feedback(run_output_dir, run_timestamp, 'Completed')\\n    sys.exit(0)\\nif __name__ == '__main__':\\n    main()\"\n    },\n    {\n      \"path\": \"your_project\\\\media_manager.py\",\n      \"overview\": \"Pythonコード。\\nクラス: MediaManager。\\n\",\n      \"content\": \"import asyncio\\nimport logging\\nfrom typing import List, Dict\\nfrom media_scanner import scan_media\\n\\nclass MediaManager:\\n    \\\"\\\"\\\"\\n    複数の媒体ディスクリプタを管理し、各媒体からのファイル収集結果を統合するクラス。\\n    \\\"\\\"\\\"\\n\\n    async def scan_all_media(self, media_list: List[Dict]) -> List[str]:\\n        \\\"\\\"\\\"\\n        入力:\\n          media_list (List[dict]): 各媒体の設定情報リスト\\n        出力:\\n          List[str] – 各媒体から取得したファイルパスを統合したリスト\\n        \\\"\\\"\\\"\\n        loop = asyncio.get_event_loop()\\n        tasks = [loop.run_in_executor(None, scan_media, media) for media in media_list]\\n        results = await asyncio.gather(*tasks)\\n        aggregated = [item for sublist in results for item in sublist]\\n        logging.info(f'MediaManager: Aggregated {len(aggregated)} files from {len(media_list)} media sources.')\\n        return aggregated\"\n    },\n    {\n      \"path\": \"your_project\\\\media_scanner.py\",\n      \"overview\": \"Pythonコード。\\n関数: scan_media。\\n\",\n      \"content\": \"import os\\nimport logging\\nfrom typing import List, Dict\\n\\ndef scan_media(media_descriptor: Dict) -> List[str]:\\n    \\\"\\\"\\\"\\n    入力:\\n      media_descriptor (dict): 例\\n        {\\n          \\\"type\\\": \\\"local\\\" or \\\"network\\\",             // 現在は単純にマウント済みとして扱う\\n          \\\"mount_point\\\": \\\"/mnt/data\\\",                 // マウント済みのパス\\n          \\\"credentials\\\": {                            // ダミー実装（認証は未実装）\\n              \\\"user\\\": \\\"dummy_user\\\",\\n              \\\"password\\\": \\\"dummy_password\\\"\\n          },\\n          \\\"include_subdirs\\\": True,                    // サブディレクトリも探索するか\\n          \\\"file_extensions\\\": [\\\".txt\\\", \\\".csv\\\", \\\".json\\\", \\\".xls\\\", \\\".html\\\"]\\n        }\\n    出力:\\n      List[str] – 指定媒体内から収集されたファイルパスのリスト\\n    \\\"\\\"\\\"\\n    mount_point = media_descriptor.get('mount_point')\\n    include_subdirs = media_descriptor.get('include_subdirs', True)\\n    file_extensions = media_descriptor.get('file_extensions', [])\\n    if not mount_point or not os.path.exists(mount_point):\\n        logging.error(f'Mount point does not exist: {mount_point}')\\n        return []\\n    file_list = []\\n    if include_subdirs:\\n        for (root, _, files) in os.walk(mount_point):\\n            for file in files:\\n                ext = os.path.splitext(file)[1].lower()\\n                if not file_extensions or ext in file_extensions:\\n                    file_list.append(os.path.join(root, file))\\n    else:\\n        try:\\n            for file in os.listdir(mount_point):\\n                file_path = os.path.join(mount_point, file)\\n                if os.path.isfile(file_path):\\n                    ext = os.path.splitext(file)[1].lower()\\n                    if not file_extensions or ext in file_extensions:\\n                        file_list.append(file_path)\\n        except Exception as e:\\n            logging.error(f'Error listing files in {mount_point}: {e}', exc_info=True)\\n    logging.info(f'scan_media: Found {len(file_list)} files in {mount_point}')\\n    return file_list\"\n    },\n    {\n      \"path\": \"your_project\\\\__init__.py\",\n      \"overview\": \"Pythonコード。\\nDocstring(冒頭60文字): プロジェクトのルート __init__.py\\nここでは特に処理はなく、パッケージとして認識させるためだけの空ファイル。...\\n\",\n      \"content\": \"\\\"\\\"\\\"\\nプロジェクトのルート __init__.py\\nここでは特に処理はなく、パッケージとして認識させるためだけの空ファイル。\\n\\\"\\\"\\\"\"\n    },\n    {\n      \"path\": \"your_project\\\\.vscode\\\\launch.json\",\n      \"overview\": \"JSON解析エラー: Expecting property name enclosed in double quotes: line 2 column 5 (char 6)\\n\",\n      \"content\": \"{\\n    // IntelliSense を使用して利用可能な属性を学べます。\\n    // 既存の属性の説明をホバーして表示します。\\n    // 詳細情報は次を確認してください: https://go.microsoft.com/fwlink/?linkid=830387\\n    \\\"version\\\": \\\"0.2.0\\\",\\n    \\\"configurations\\\": [\\n        {\\n            \\\"name\\\": \\\"Python: 現在のファイル\\\",\\n            \\\"type\\\": \\\"python\\\",\\n            \\\"request\\\": \\\"launch\\\",\\n            \\\"program\\\": \\\"${file}\\\",\\n            \\\"console\\\": \\\"integratedTerminal\\\",\\n            \\\"justMyCode\\\": true,\\n        }\\n    ]\\n}\"\n    },\n    {\n      \"path\": \"your_project\\\\adapters\\\\db\\\\db_utils.py\",\n      \"overview\": \"Pythonコード。\\nクラス: DBUtils。\\n\",\n      \"content\": \"import aiosqlite\\nimport logging\\nimport os\\n\\nclass DBUtils:\\n    _connection = None\\n    _db_path = None\\n\\n    @staticmethod\\n    async def initialize_database(db_path='data.db'):\\n        DBUtils._db_path = db_path\\n        if not os.path.exists(db_path):\\n            conn = await aiosqlite.connect(db_path)\\n            await conn.execute('\\\\n                CREATE TABLE IF NOT EXISTS analysis_results (\\\\n                    id INTEGER PRIMARY KEY AUTOINCREMENT,\\\\n                    method_name TEXT,\\\\n                    result_data TEXT,\\\\n                    timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP\\\\n                )\\\\n            ')\\n            await conn.commit()\\n            await conn.close()\\n            logging.info(f'DB初期化完了: {db_path}')\\n        else:\\n            logging.info(f'既存DB使用: {db_path}')\\n        await DBUtils.get_connection()\\n\\n    @staticmethod\\n    async def get_connection():\\n        if DBUtils._connection is None:\\n            if DBUtils._db_path is None:\\n                raise ValueError('DBパス未設定。先にinitialize_databaseを呼ぶ必要があります。')\\n            DBUtils._connection = await aiosqlite.connect(DBUtils._db_path)\\n        return DBUtils._connection\\n\\n    @staticmethod\\n    async def close_connection():\\n        if DBUtils._connection:\\n            await DBUtils._connection.close()\\n            DBUtils._connection = None\\n            logging.info('DB接続を閉じました。')\"\n    },\n    {\n      \"path\": \"your_project\\\\adapters\\\\parsers\\\\base_parser.py\",\n      \"overview\": \"Pythonコード。\\nクラス: BaseParser。\\n関数: parse, supported_extensions, data_type。\\n\",\n      \"content\": \"from abc import ABC, abstractmethod\\nfrom typing import Any, List, Dict\\n\\nclass BaseParser(ABC):\\n\\n    @abstractmethod\\n    def parse(self, file_path: str, encoding: str='utf-8') -> List[Dict[str, Any]]:\\n        pass\\n\\n    @abstractmethod\\n    def supported_extensions(self) -> List[str]:\\n        pass\\n\\n    @abstractmethod\\n    def data_type(self) -> str:\\n        pass\"\n    },\n    {\n      \"path\": \"your_project\\\\adapters\\\\parsers\\\\csv_parser.py\",\n      \"overview\": \"Pythonコード。\\nクラス: CsvParser。\\n関数: parse, supported_extensions, data_type。\\n\",\n      \"content\": \"import pandas as pd\\nimport logging\\nfrom typing import Any, List, Dict\\nfrom .base_parser import BaseParser\\n\\nclass CsvParser(BaseParser):\\n\\n    def parse(self, file_path: str, encoding: str='utf-8') -> List[Dict[str, Any]]:\\n        try:\\n            df = pd.read_csv(file_path, encoding=encoding, errors='ignore')\\n            return df.to_dict(orient='records')\\n        except Exception as e:\\n            logging.error(f'CSVパース中エラー: {str(e)}', exc_info=True)\\n            return []\\n\\n    def supported_extensions(self) -> List[str]:\\n        return ['.csv']\\n\\n    def data_type(self) -> str:\\n        return 'csv'\"\n    },\n    {\n      \"path\": \"your_project\\\\adapters\\\\parsers\\\\html_parser.py\",\n      \"overview\": \"Pythonコード。\\nクラス: HtmlParser。\\n関数: parse, supported_extensions, data_type。\\n\",\n      \"content\": \"import logging\\nfrom typing import Any, List, Dict\\nfrom bs4 import BeautifulSoup\\nfrom .base_parser import BaseParser\\n\\nclass HtmlParser(BaseParser):\\n\\n    def parse(self, file_path: str, encoding: str='utf-8') -> List[Dict[str, Any]]:\\n        try:\\n            with open(file_path, 'r', encoding=encoding, errors='ignore') as f:\\n                soup = BeautifulSoup(f, 'html.parser')\\n            text = soup.get_text()\\n            return [{'content': text}]\\n        except Exception as e:\\n            logging.error(f'HTMLパース中エラー: {str(e)}', exc_info=True)\\n            return []\\n\\n    def supported_extensions(self) -> List[str]:\\n        return ['.html', '.htm', '.mhtml']\\n\\n    def data_type(self) -> str:\\n        return 'html'\"\n    },\n    {\n      \"path\": \"your_project\\\\adapters\\\\parsers\\\\json_parser.py\",\n      \"overview\": \"Pythonコード。\\nクラス: JsonParser。\\n関数: parse, supported_extensions, data_type。\\n\",\n      \"content\": \"import json\\nimport logging\\nfrom typing import Any, List, Dict\\nfrom .base_parser import BaseParser\\n\\nclass JsonParser(BaseParser):\\n\\n    def parse(self, file_path: str, encoding: str='utf-8') -> List[Dict[str, Any]]:\\n        try:\\n            with open(file_path, 'r', encoding=encoding, errors='ignore') as f:\\n                data = json.load(f)\\n            return [{'content': json.dumps(data, ensure_ascii=False)}]\\n        except Exception as e:\\n            logging.error(f'JSONパース中エラー: {str(e)}', exc_info=True)\\n            return []\\n\\n    def supported_extensions(self) -> List[str]:\\n        return ['.json']\\n\\n    def data_type(self) -> str:\\n        return 'json'\"\n    },\n    {\n      \"path\": \"your_project\\\\adapters\\\\parsers\\\\parser_manager.py\",\n      \"overview\": \"Pythonコード。\\nクラス: ParserManager。\\n関数: __init__, load_parsers, get_parser。\\n\",\n      \"content\": \"import importlib\\nimport pkgutil\\nimport logging\\nfrom typing import Dict\\nfrom adapters.parsers.base_parser import BaseParser\\nfrom adapters.parsers import csv_parser, json_parser, xls_parser, text_parser, html_parser\\n\\nclass ParserManager:\\n\\n    def __init__(self):\\n        self.parsers = self.load_parsers()\\n\\n    def load_parsers(self) -> Dict[str, BaseParser]:\\n        parsers_dict = {}\\n        csvp = csv_parser.CsvParser()\\n        parsers_dict[csvp.data_type()] = csvp\\n        jsonp = json_parser.JsonParser()\\n        parsers_dict[jsonp.data_type()] = jsonp\\n        xlsp = xls_parser.XlsParser()\\n        parsers_dict[xlsp.data_type()] = xlsp\\n        txtp = text_parser.TextParser()\\n        parsers_dict[txtp.data_type()] = txtp\\n        htm = html_parser.HtmlParser()\\n        parsers_dict[htm.data_type()] = htm\\n        logging.info(f'ParserManager: {len(parsers_dict)}個のパーサを登録済')\\n        return parsers_dict\\n\\n    def get_parser(self, data_type: str) -> BaseParser:\\n        return self.parsers.get(data_type)\"\n    },\n    {\n      \"path\": \"your_project\\\\adapters\\\\parsers\\\\text_parser.py\",\n      \"overview\": \"Pythonコード。\\nクラス: TextParser。\\n関数: parse, supported_extensions, data_type。\\n\",\n      \"content\": \"import logging\\nfrom typing import Any, List, Dict\\nfrom .base_parser import BaseParser\\n\\nclass TextParser(BaseParser):\\n\\n    def parse(self, file_path: str, encoding: str='utf-8') -> List[Dict[str, Any]]:\\n        try:\\n            with open(file_path, 'r', encoding=encoding, errors='ignore') as f:\\n                content = f.read()\\n            return [{'content': content}]\\n        except Exception as e:\\n            logging.error(f'テキストパース中エラー: {str(e)}', exc_info=True)\\n            return []\\n\\n    def supported_extensions(self) -> List[str]:\\n        return ['.txt', '.md']\\n\\n    def data_type(self) -> str:\\n        return 'text'\"\n    },\n    {\n      \"path\": \"your_project\\\\adapters\\\\parsers\\\\xls_parser.py\",\n      \"overview\": \"Pythonコード。\\nクラス: XlsParser。\\n関数: parse, supported_extensions, data_type。\\n\",\n      \"content\": \"import pandas as pd\\nimport logging\\nfrom typing import Any, List, Dict\\nfrom .base_parser import BaseParser\\n\\nclass XlsParser(BaseParser):\\n\\n    def parse(self, file_path: str, encoding: str='utf-8') -> List[Dict[str, Any]]:\\n        try:\\n            df = pd.read_excel(file_path, engine='xlrd')\\n            return df.to_dict(orient='records')\\n        except Exception as e:\\n            logging.error(f'XLSデータパース中エラー: {str(e)}', exc_info=True)\\n            return []\\n\\n    def supported_extensions(self) -> List[str]:\\n        return ['.xls']\\n\\n    def data_type(self) -> str:\\n        return 'xls'\"\n    },\n    {\n      \"path\": \"your_project\\\\adapters\\\\parsers\\\\__init__.py\",\n      \"overview\": \"Pythonコード。\\nDocstring(冒頭60文字): パーサーアダプタのパッケージ認識用...\\n\",\n      \"content\": \"\\\"\\\"\\\"\\nパーサーアダプタのパッケージ認識用\\n\\\"\\\"\\\"\"\n    },\n    {\n      \"path\": \"your_project\\\\application\\\\api\\\\main_api.py\",\n      \"overview\": \"Pythonコード。\\n\",\n      \"content\": \"import uvicorn\\nfrom fastapi import FastAPI\\nfrom application.services.data_service import DataService\\nfrom application.services.analysis_service import AnalysisService\\napp = FastAPI()\\n\\n@app.get('/')\\nasync def root():\\n    return {'message': 'Hello from API'}\\n\\n@app.post('/analyze')\\nasync def analyze_endpoint(input_path: str):\\n    data_service = DataService()\\n    analysis_service = AnalysisService()\\n    df_data = data_service.load_and_preprocess(input_path)\\n    analysis_service.run_analysis(df_data, 'outputs')\\n    return {'status': 'OK'}\\nif __name__ == '__main__':\\n    uvicorn.run(app, host='0.0.0.0', port=8000)\"\n    },\n    {\n      \"path\": \"your_project\\\\application\\\\api\\\\__init__.py\",\n      \"overview\": \"Pythonコード。\\nDocstring(冒頭60文字): API関連のパッケージ...\\n\",\n      \"content\": \"\\\"\\\"\\\"\\nAPI関連のパッケージ\\n\\\"\\\"\\\"\"\n    },\n    {\n      \"path\": \"your_project\\\\application\\\\cli\\\\main_cli.py\",\n      \"overview\": \"Pythonコード。\\n関数: parse_args, main。\\n\",\n      \"content\": \"import sys\\nimport logging\\nimport argparse\\nfrom application.services.data_service import DataService\\nfrom application.services.analysis_service import AnalysisService\\nfrom core.environment import prepare_environment\\nfrom core.finalize import finalize_with_feedback\\n\\ndef parse_args():\\n    parser = argparse.ArgumentParser(description='CLIツール サンプル')\\n    parser.add_argument('--input', type=str, required=True, help='入力データのパス（ファイルまたはディレクトリ）')\\n    return parser.parse_args()\\n\\ndef main():\\n    (run_timestamp, run_output_dir) = prepare_environment()\\n    args = parse_args()\\n    data_service = DataService()\\n    analysis_service = AnalysisService()\\n    try:\\n        df_data = data_service.load_and_preprocess(args.input)\\n        analysis_service.run_analysis(df_data, run_output_dir)\\n    except Exception as e:\\n        logging.error(f'CLIエラー: {e}', exc_info=True)\\n        finalize_with_feedback(run_output_dir, run_timestamp, 'Failed', str(e))\\n        sys.exit(1)\\n    finalize_with_feedback(run_output_dir, run_timestamp, 'Completed')\\n    sys.exit(0)\\nif __name__ == '__main__':\\n    main()\"\n    },\n    {\n      \"path\": \"your_project\\\\application\\\\cli\\\\__init__.py\",\n      \"overview\": \"Pythonコード。\\nDocstring(冒頭60文字): CLI関連のパッケージ...\\n\",\n      \"content\": \"\\\"\\\"\\\"\\nCLI関連のパッケージ\\n\\\"\\\"\\\"\"\n    },\n    {\n      \"path\": \"your_project\\\\application\\\\services\\\\analysis_service.py\",\n      \"overview\": \"Pythonコード。\\nクラス: AnalysisService。\\n関数: __init__, run_analysis。\\n\",\n      \"content\": \"import os\\nimport logging\\nimport asyncio\\nfrom domain.models.model_manager import ModelManager\\nfrom utils.output_manager import OutputManager\\nfrom domain.visualization.visualizer import Visualizer\\n\\nclass AnalysisService:\\n\\n    def __init__(self):\\n        self.model_manager = ModelManager()\\n        self.output_manager = OutputManager()\\n        self.visualizer = Visualizer()\\n\\n    def run_analysis(self, df_data, output_dir):\\n        methods = self.model_manager.get_available_methods()\\n        logging.info(f'利用可能な解析手法: {methods}')\\n        for m in methods:\\n            logging.info(f'解析手法 {m} 実行')\\n            result = self.model_manager.apply_method(m, df_data)\\n            if not result.empty:\\n                self.output_manager.save_results(result, m, output_dir)\\n                asyncio.run(self.output_manager.save_to_database(result, m))\\n                vis_dir = os.path.join(output_dir, 'visualizations')\\n                os.makedirs(vis_dir, exist_ok=True)\\n                self.visualizer.visualize(m, result, vis_dir)\\n            else:\\n                logging.info(f'{m} の結果は空')\"\n    },\n    {\n      \"path\": \"your_project\\\\application\\\\services\\\\data_service.py\",\n      \"overview\": \"Pythonコード。\\nクラス: DataService。\\n関数: load_and_preprocess。\\n\",\n      \"content\": \"import logging\\nimport os\\nfrom domain.data_preparation.data_loader import DataLoader\\nfrom domain.data_preparation.data_preprocessor import DataPreprocessor\\n\\nclass DataService:\\n\\n    def load_and_preprocess(self, input_path: str):\\n        loader = DataLoader(input_path)\\n        raw_data = loader.load_data()\\n        processor = DataPreprocessor()\\n        df_data = processor.preprocess_data(raw_data)\\n        if df_data.empty:\\n            logging.warning('DataService: 前処理結果が空')\\n        return df_data\"\n    },\n    {\n      \"path\": \"your_project\\\\application\\\\services\\\\__init__.py\",\n      \"overview\": \"Pythonコード。\\nDocstring(冒頭60文字): ユースケースやビジネスロジックを実装するサービス群...\\n\",\n      \"content\": \"\\\"\\\"\\\"\\nユースケースやビジネスロジックを実装するサービス群\\n\\\"\\\"\\\"\"\n    },\n    {\n      \"path\": \"your_project\\\\core\\\\environment.py\",\n      \"overview\": \"Pythonコード。\\n関数: prepare_environment。\\n\",\n      \"content\": \"import os\\nimport datetime\\nimport logging\\nfrom utils.logger import Logger\\n\\ndef prepare_environment():\\n    run_timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\\n    base_dir = os.path.dirname(os.path.abspath(__file__))\\n    project_dir = os.path.abspath(os.path.join(base_dir, '..'))\\n    run_output_dir = os.path.join(project_dir, f'run_{run_timestamp}')\\n    os.makedirs(run_output_dir, exist_ok=True)\\n    log_dir = os.path.join(run_output_dir, 'logs')\\n    os.makedirs(log_dir, exist_ok=True)\\n    Logger.setup_logging(log_dir, 'INFO')\\n    logging.info('===== 環境準備完了 =====')\\n    return (run_timestamp, run_output_dir)\"\n    },\n    {\n      \"path\": \"your_project\\\\core\\\\finalize.py\",\n      \"overview\": \"Pythonコード。\\n関数: finalize_with_feedback。\\n\",\n      \"content\": \"import os\\nimport json\\nimport logging\\n\\ndef finalize_with_feedback(run_output_dir: str, run_timestamp: str, status_msg: str, extra_remarks: str='') -> None:\\n    feedback_data = {'timestamp': run_timestamp, 'status': status_msg, 'remarks': extra_remarks, 'possible_improvements': ['ログレベルをDEBUGにして詳細確認', '追加パラメータを考慮した解析を実装する', 'AIプロンプトの自動生成を検討']}\\n    feedback_path = os.path.join(run_output_dir, 'feedback.json')\\n    try:\\n        with open(feedback_path, 'w', encoding='utf-8') as fw:\\n            json.dump(feedback_data, fw, ensure_ascii=False, indent=2)\\n        logging.info(f'フィードバック情報を {feedback_path} に保存しました。(status={status_msg})')\\n    except Exception as e:\\n        logging.error(f'フィードバック情報の保存に失敗: {str(e)}', exc_info=True)\\n    logging.info('===== 終了処理完了 =====')\"\n    },\n    {\n      \"path\": \"your_project\\\\core\\\\__init__.py\",\n      \"overview\": \"Pythonコード。\\nDocstring(冒頭60文字): コア機能(環境準備や終了処理など横断的役割)...\\n\",\n      \"content\": \"\\\"\\\"\\\"\\nコア機能(環境準備や終了処理など横断的役割)\\n\\\"\\\"\\\"\"\n    },\n    {\n      \"path\": \"your_project\\\\data_preparation\\\\data_loader.py\",\n      \"overview\": \"Pythonコード。\\nクラス: DataLoader。\\n関数: __init__, load_data, collect_files, extract_zip, is_supported_type, filter_by_data_types。\\n\",\n      \"content\": \"import os\\nimport logging\\nimport asyncio\\nimport zipfile\\nfrom typing import Any, Dict, List, Union\\nfrom utils.file_processor import FileProcessor\\nfrom utils.file_identifier import FileIdentifier\\nfrom media_scanner import scan_media\\nfrom media_manager import MediaManager\\n\\nclass DataLoader:\\n    \\\"\\\"\\\"\\n    入力ソース（文字列: 従来のフォルダパス、または dict / list: 媒体ディスクリプタ）から対象ファイルを収集し、\\n    FileProcessor を用いて並列パースを実施する。\\n    \\\"\\\"\\\"\\n\\n    def __init__(self, input_source: Union[str, dict, list], data_types: List[str]):\\n        self.input_source = input_source\\n        self.data_types = data_types if data_types is not None else ['text', 'html', 'csv', 'json', 'xls']\\n        self.file_processor = FileProcessor(self.data_types)\\n        self.file_identifier = FileIdentifier(self.data_types)\\n\\n    def load_data(self) -> Dict[str, Any]:\\n        files = self.collect_files()\\n        if not files:\\n            logging.warning('入力パスにファイルがありません。')\\n            return {}\\n        contents = asyncio.run(self.file_processor.process_files_in_parallel(files))\\n        return contents\\n\\n    def collect_files(self) -> List[str]:\\n        file_list = []\\n        if isinstance(self.input_source, str):\\n            if not os.path.exists(self.input_source):\\n                logging.warning(f'Input directory does not exist: {self.input_source}')\\n                return []\\n            for (root, _, files) in os.walk(self.input_source):\\n                for file in files:\\n                    file_path = os.path.join(root, file)\\n                    if zipfile.is_zipfile(file_path):\\n                        extracted = self.extract_zip(file_path)\\n                        file_list.extend(self.filter_by_data_types(extracted))\\n                    elif self.is_supported_type(file_path):\\n                        file_list.append(file_path)\\n        elif isinstance(self.input_source, dict):\\n            file_list = scan_media(self.input_source)\\n        elif isinstance(self.input_source, list):\\n            media_manager = MediaManager()\\n            file_list = asyncio.run(media_manager.scan_all_media(self.input_source))\\n        else:\\n            logging.error('Invalid input_source type. Must be str, dict, or list.')\\n        logging.info(f'DataLoader.collect_files: Collected {len(file_list)} files.')\\n        return file_list\\n\\n    def extract_zip(self, zip_path: str) -> List[str]:\\n        extracted_files = []\\n        try:\\n            with zipfile.ZipFile(zip_path, 'r') as zip_ref:\\n                extract_path = os.path.join(os.path.dirname(zip_path), os.path.splitext(os.path.basename(zip_path))[0])\\n                zip_ref.extractall(extract_path)\\n                for (root, _, files) in os.walk(extract_path):\\n                    for file in files:\\n                        extracted_files.append(os.path.join(root, file))\\n            logging.info(f'ZIPファイル展開完了: {zip_path}')\\n        except Exception as e:\\n            logging.error(f'ZIP展開中にエラー: {str(e)}', exc_info=True)\\n        return extracted_files\\n\\n    def is_supported_type(self, file_path: str) -> bool:\\n        ext = os.path.splitext(file_path)[1].lower()\\n        return ext in ['.txt', '.md', '.html', '.htm', '.mhtml', '.csv', '.json', '.xls']\\n\\n    def filter_by_data_types(self, file_paths: List[str]) -> List[str]:\\n        return [fp for fp in file_paths if self.is_supported_type(fp)]\"\n    },\n    {\n      \"path\": \"your_project\\\\data_preparation\\\\data_preprocessor.py\",\n      \"overview\": \"Pythonコード。\\nクラス: DataPreprocessor。\\n関数: __init__, load_stopwords, preprocess_data, normalize_text, filter_token。\\n\",\n      \"content\": \"import os\\nimport pandas as pd\\nimport logging\\nimport re\\nfrom typing import Any, Dict, Optional\\nfrom janome.tokenizer import Tokenizer\\n\\nclass DataPreprocessor:\\n    \\\"\\\"\\\"\\n    前処理クラス\\n      - テキストの小文字化、記号除去\\n      - ストップワード除去\\n      - トークナイズと基本形変換\\n    \\\"\\\"\\\"\\n\\n    def __init__(self, stopwords_path: Optional[str]=None):\\n        self.tokenizer = Tokenizer()\\n        self.stopwords = self.load_stopwords(stopwords_path)\\n\\n    def load_stopwords(self, path: Optional[str]) -> set:\\n        stopwords = set()\\n        if path and os.path.exists(path):\\n            try:\\n                with open(path, 'r', encoding='utf-8') as f:\\n                    for line in f:\\n                        w = line.strip().lower()\\n                        if w:\\n                            stopwords.add(w)\\n                logging.info(f'ストップワードファイル読み込み完了: {path} (単語数={len(stopwords)})')\\n            except Exception as e:\\n                logging.warning(f'ストップワード読込中のエラー: {str(e)}', exc_info=True)\\n        else:\\n            logging.info('ストップワードファイル未指定または見つからず。フィルタなしで進行。')\\n        return stopwords\\n\\n    def preprocess_data(self, contents: Dict[str, Any]) -> pd.DataFrame:\\n        data_frames = []\\n        for (dtype, data_list) in contents.items():\\n            if data_list:\\n                df = pd.DataFrame(data_list)\\n                df['type'] = dtype\\n                if 'content' in df.columns:\\n                    df['normalized_content'] = df['content'].apply(self.normalize_text)\\n                    tokens_list = df['normalized_content'].apply(lambda text: list(self.tokenizer.tokenize(str(text))))\\n                    df['tokens'] = tokens_list.apply(lambda tokens: [t.surface for t in tokens if self.filter_token(t.surface)])\\n                    df['lemmas'] = tokens_list.apply(lambda tokens: [t.base_form if t.base_form != '*' else t.surface for t in tokens if self.filter_token(t.base_form if t.base_form != '*' else t.surface)])\\n                    df['pos_tags'] = tokens_list.apply(lambda tokens: [t.part_of_speech for t in tokens])\\n                data_frames.append(df)\\n        if data_frames:\\n            combined = pd.concat(data_frames, ignore_index=True)\\n            logging.info(f'前処理後データ件数: {len(combined)}')\\n            return combined\\n        else:\\n            logging.warning('前処理後データが空です。')\\n            return pd.DataFrame()\\n\\n    def normalize_text(self, text: str) -> str:\\n        text = text.lower()\\n        text = re.sub('[^a-z0-9\\\\\\\\s]+', '', text)\\n        text = re.sub('\\\\\\\\s+', ' ', text).strip()\\n        return text\\n\\n    def filter_token(self, token: str) -> bool:\\n        return token not in self.stopwords and token != ''\"\n    },\n    {\n      \"path\": \"your_project\\\\data_preparation\\\\__init__.py\",\n      \"overview\": \"Pythonコード。\\n\",\n      \"content\": \"from .data_loader import DataLoader\\nfrom .data_preprocessor import DataPreprocessor\"\n    },\n    {\n      \"path\": \"your_project\\\\domain\\\\__init__.py\",\n      \"overview\": \"Pythonコード。\\nDocstring(冒頭60文字): ビジネスルール・ドメインモデル・ロジック集約層...\\n\",\n      \"content\": \"\\\"\\\"\\\"\\nビジネスルール・ドメインモデル・ロジック集約層\\n\\\"\\\"\\\"\"\n    },\n    {\n      \"path\": \"your_project\\\\domain\\\\data_preparation\\\\data_loader.py\",\n      \"overview\": \"Pythonコード。\\nクラス: DataLoader。\\n関数: __init__, load_data, collect_files, extract_zip, is_supported_type, filter_by_data_types。\\n\",\n      \"content\": \"import os\\nimport logging\\nimport asyncio\\nimport zipfile\\nfrom typing import Any, Dict, List\\nfrom utils.file_processor import FileProcessor\\n\\nclass DataLoader:\\n\\n    def __init__(self, input_path: str):\\n        self.input_path = input_path\\n        self.data_types = ['text', 'html', 'csv', 'json', 'xls']\\n        self.file_processor = FileProcessor(self.data_types)\\n\\n    def load_data(self) -> Dict[str, Any]:\\n        files = self.collect_files()\\n        if not files:\\n            logging.warning('入力パスにファイルがありません。')\\n            return {}\\n        contents = asyncio.run(self.file_processor.process_files_in_parallel(files))\\n        return contents\\n\\n    def collect_files(self) -> List[str]:\\n        if not os.path.exists(self.input_path):\\n            logging.warning(f'ディレクトリ/ファイルが存在しません: {self.input_path}')\\n            return []\\n        file_list = []\\n        if os.path.isdir(self.input_path):\\n            for (root, _, files) in os.walk(self.input_path):\\n                for file in files:\\n                    file_path = os.path.join(root, file)\\n                    if zipfile.is_zipfile(file_path):\\n                        extracted = self.extract_zip(file_path)\\n                        file_list.extend(self.filter_by_data_types(extracted))\\n                    elif self.is_supported_type(file_path):\\n                        file_list.append(file_path)\\n        elif zipfile.is_zipfile(self.input_path):\\n            extracted = self.extract_zip(self.input_path)\\n            file_list.extend(self.filter_by_data_types(extracted))\\n        elif self.is_supported_type(self.input_path):\\n            file_list.append(self.input_path)\\n        return file_list\\n\\n    def extract_zip(self, zip_path: str) -> List[str]:\\n        extracted_files = []\\n        try:\\n            with zipfile.ZipFile(zip_path, 'r') as zip_ref:\\n                extract_path = os.path.join(os.path.dirname(zip_path), os.path.splitext(os.path.basename(zip_path))[0])\\n                zip_ref.extractall(extract_path)\\n                for (root, _, files) in os.walk(extract_path):\\n                    for file in files:\\n                        extracted_files.append(os.path.join(root, file))\\n            logging.info(f'ZIPファイル展開完了: {zip_path}')\\n        except Exception as e:\\n            logging.error(f'ZIP展開中にエラー: {str(e)}', exc_info=True)\\n        return extracted_files\\n\\n    def is_supported_type(self, file_path: str) -> bool:\\n        ext = os.path.splitext(file_path)[1].lower()\\n        return ext in ['.txt', '.md', '.html', '.htm', '.mhtml', '.csv', '.json', '.xls']\\n\\n    def filter_by_data_types(self, file_paths: List[str]) -> List[str]:\\n        filtered = []\\n        for fp in file_paths:\\n            if self.is_supported_type(fp):\\n                filtered.append(fp)\\n        return filtered\"\n    },\n    {\n      \"path\": \"your_project\\\\domain\\\\data_preparation\\\\data_preprocessor.py\",\n      \"overview\": \"Pythonコード。\\nクラス: DataPreprocessor。\\n関数: __init__, load_stopwords, preprocess_data, normalize_text, filter_token。\\n\",\n      \"content\": \"import os\\nimport pandas as pd\\nimport logging\\nimport re\\nfrom typing import Any, Dict, Optional\\nfrom janome.tokenizer import Tokenizer\\n\\nclass DataPreprocessor:\\n\\n    def __init__(self, stopwords_path: Optional[str]=None):\\n        \\\"\\\"\\\"\\n        stopwords_path: 英単語のストップワードを格納したファイルのパス (例: \\\"english_stopwords.txt\\\")\\n        \\\"\\\"\\\"\\n        self.tokenizer = Tokenizer()\\n        self.stopwords = self.load_stopwords(stopwords_path)\\n\\n    def load_stopwords(self, path: Optional[str]) -> set:\\n        stopwords = set()\\n        if path and os.path.exists(path):\\n            try:\\n                with open(path, 'r', encoding='utf-8') as f:\\n                    for line in f:\\n                        w = line.strip().lower()\\n                        if w:\\n                            stopwords.add(w)\\n                logging.info(f'ストップワードファイル読み込み完了: {path} (単語数={len(stopwords)})')\\n            except Exception as e:\\n                logging.warning(f'ストップワードファイル読込中にエラー: {str(e)}', exc_info=True)\\n        else:\\n            logging.info('ストップワードファイル未指定 or 見つからず。フィルタなしで進行。')\\n        return stopwords\\n\\n    def preprocess_data(self, contents: Dict[str, Any]) -> pd.DataFrame:\\n        data_frames = []\\n        for (dtype, data_list) in contents.items():\\n            if data_list:\\n                df = pd.DataFrame(data_list)\\n                df['type'] = dtype\\n                if 'content' in df.columns:\\n                    df['normalized_content'] = df['content'].apply(self.normalize_text)\\n                    tokens_list = df['normalized_content'].apply(lambda text: list(self.tokenizer.tokenize(str(text))))\\n                    df['tokens'] = tokens_list.apply(lambda tokens: [t.surface for t in tokens if self.filter_token(t.surface)])\\n                    df['lemmas'] = tokens_list.apply(lambda tokens: [t.base_form if t.base_form != '*' else t.surface for t in tokens if self.filter_token(t.base_form if t.base_form != '*' else t.surface)])\\n                    df['pos_tags'] = tokens_list.apply(lambda tokens: [t.part_of_speech for t in tokens])\\n                data_frames.append(df)\\n        if data_frames:\\n            combined = pd.concat(data_frames, ignore_index=True)\\n            logging.info(f'前処理後データ件数: {len(combined)}')\\n            return combined\\n        else:\\n            logging.warning('前処理後データが空です。')\\n            return pd.DataFrame()\\n\\n    def normalize_text(self, text: str) -> str:\\n        text = text.lower()\\n        text = re.sub('[^a-z0-9\\\\\\\\s]+', '', text)\\n        text = re.sub('\\\\\\\\s+', ' ', text).strip()\\n        return text\\n\\n    def filter_token(self, token: str) -> bool:\\n        return token not in self.stopwords and token != ''\"\n    },\n    {\n      \"path\": \"your_project\\\\domain\\\\models\\\\model_manager.py\",\n      \"overview\": \"Pythonコード。\\nクラス: ModelManager。\\n関数: __init__, get_available_methods, apply_method。\\n\",\n      \"content\": \"import logging\\nfrom typing import Any\\nfrom .analysis_methods.bigram_analysis import BigramAnalysis\\nfrom .analysis_methods.word_frequency_analysis import WordFrequencyAnalysis\\n\\nclass ModelManager:\\n\\n    def __init__(self):\\n        self.methods = {'BigramAnalysis': BigramAnalysis(), 'WordFrequencyAnalysis': WordFrequencyAnalysis()}\\n\\n    def get_available_methods(self):\\n        return list(self.methods.keys())\\n\\n    def apply_method(self, method_name: str, data: Any, *args, **kwargs):\\n        if method_name not in self.methods:\\n            raise ValueError(f\\\"解析手法 '{method_name}' は未登録です。\\\")\\n        method_instance = self.methods[method_name]\\n        return method_instance.analyze(data, *args, **kwargs)\"\n    },\n    {\n      \"path\": \"your_project\\\\domain\\\\models\\\\__init__.py\",\n      \"overview\": \"Pythonコード。\\nDocstring(冒頭60文字): モデルやエンティティなど...\\n\",\n      \"content\": \"\\\"\\\"\\\"\\nモデルやエンティティなど\\n\\\"\\\"\\\"\"\n    },\n    {\n      \"path\": \"your_project\\\\domain\\\\models\\\\analysis_methods\\\\analysis_method_base.py\",\n      \"overview\": \"Pythonコード。\\nクラス: AnalysisMethodBase。\\n関数: analyze, get_name。\\n\",\n      \"content\": \"from abc import ABC, abstractmethod\\nimport pandas as pd\\n\\nclass AnalysisMethodBase(ABC):\\n\\n    @abstractmethod\\n    def analyze(self, data: pd.DataFrame, **kwargs) -> pd.DataFrame:\\n        pass\\n\\n    @abstractmethod\\n    def get_name(self) -> str:\\n        pass\"\n    },\n    {\n      \"path\": \"your_project\\\\domain\\\\models\\\\analysis_methods\\\\bigram_analysis.py\",\n      \"overview\": \"Pythonコード。\\nクラス: BigramAnalysis。\\n関数: __init__, analyze, get_name。\\n\",\n      \"content\": \"import logging\\nimport pandas as pd\\nfrom collections import Counter\\nfrom janome.tokenizer import Tokenizer\\nfrom itertools import islice\\nfrom .analysis_method_base import AnalysisMethodBase\\n\\nclass BigramAnalysis(AnalysisMethodBase):\\n\\n    def __init__(self):\\n        self.tokenizer = Tokenizer()\\n\\n    def analyze(self, data: pd.DataFrame, **kwargs) -> pd.DataFrame:\\n        text_data = data['normalized_content'] if 'normalized_content' in data.columns else data['content'] if 'content' in data.columns else pd.Series([])\\n        if text_data.empty:\\n            logging.info('BigramAnalysis: テキストデータなし。')\\n            return pd.DataFrame()\\n        try:\\n            all_bigrams = []\\n            if 'tokens' in data.columns:\\n                for words in data['tokens']:\\n                    bigrams = zip(words, islice(words, 1, None))\\n                    all_bigrams.extend(bigrams)\\n            else:\\n                for txt in text_data:\\n                    tokens = [t.surface for t in self.tokenizer.tokenize(txt)]\\n                    bigrams = zip(tokens, islice(tokens, 1, None))\\n                    all_bigrams.extend(bigrams)\\n            freq = Counter(all_bigrams)\\n            df_result = pd.DataFrame(freq.items(), columns=['bigram', 'frequency'])\\n            df_result['bigram'] = df_result['bigram'].apply(' '.join)\\n            total = sum(df_result['frequency'])\\n            df_result['probability'] = df_result['frequency'] / total if total > 0 else 0\\n            return df_result.sort_values(by='frequency', ascending=False)\\n        except Exception as e:\\n            logging.error(f'BigramAnalysisエラー: {str(e)}', exc_info=True)\\n            return pd.DataFrame()\\n\\n    def get_name(self) -> str:\\n        return 'BigramAnalysis'\"\n    },\n    {\n      \"path\": \"your_project\\\\domain\\\\models\\\\analysis_methods\\\\word_frequency_analysis.py\",\n      \"overview\": \"Pythonコード。\\nクラス: WordFrequencyAnalysis。\\n関数: __init__, analyze, get_name。\\n\",\n      \"content\": \"import logging\\nimport pandas as pd\\nfrom collections import Counter\\nfrom janome.tokenizer import Tokenizer\\nfrom .analysis_method_base import AnalysisMethodBase\\n\\nclass WordFrequencyAnalysis(AnalysisMethodBase):\\n\\n    def __init__(self):\\n        self.tokenizer = Tokenizer()\\n\\n    def analyze(self, data: pd.DataFrame, **kwargs) -> pd.DataFrame:\\n        text_data = data['normalized_content'] if 'normalized_content' in data.columns else data['content'] if 'content' in data.columns else pd.Series([])\\n        if text_data.empty:\\n            logging.info('WordFrequencyAnalysis: テキストデータなし。')\\n            return pd.DataFrame()\\n        try:\\n            if 'tokens' in data.columns:\\n                all_words = [w for words in data['tokens'] for w in words]\\n            else:\\n                all_words = []\\n                for txt in text_data:\\n                    tokens = [t.surface for t in self.tokenizer.tokenize(txt)]\\n                    all_words.extend(tokens)\\n            freq = Counter(all_words)\\n            df_result = pd.DataFrame(freq.items(), columns=['word', 'frequency'])\\n            total = sum(df_result['frequency'])\\n            df_result['probability'] = df_result['frequency'] / total if total > 0 else 0\\n            return df_result.sort_values(by='frequency', ascending=False)\\n        except Exception as e:\\n            logging.error(f'WordFrequencyAnalysisエラー: {str(e)}', exc_info=True)\\n            return pd.DataFrame()\\n\\n    def get_name(self) -> str:\\n        return 'WordFrequencyAnalysis'\"\n    },\n    {\n      \"path\": \"your_project\\\\domain\\\\performance\\\\performance_tracker.py\",\n      \"overview\": \"Pythonコード。\\nクラス: PerformanceTracker。\\n関数: __init__, start_timer, end_timer, get_metrics。\\n\",\n      \"content\": \"import time\\nfrom typing import Dict, List\\n\\nclass PerformanceTracker:\\n\\n    def __init__(self):\\n        self.start_times: Dict[str, float] = {}\\n        self.metrics: List[Dict[str, float]] = []\\n\\n    def start_timer(self, name: str) -> None:\\n        self.start_times[name] = time.time()\\n\\n    def end_timer(self, name: str) -> None:\\n        if name in self.start_times:\\n            elapsed = time.time() - self.start_times.pop(name)\\n            self.metrics.append({'process': name, 'elapsed_time': elapsed})\\n\\n    def get_metrics(self) -> List[Dict[str, float]]:\\n        return self.metrics\"\n    },\n    {\n      \"path\": \"your_project\\\\domain\\\\visualization\\\\visualizer.py\",\n      \"overview\": \"Pythonコード。\\nクラス: Visualizer。\\n関数: __init__, visualize, _visualize_word_frequency, _visualize_bigram_frequency。\\n\",\n      \"content\": \"import os\\nimport logging\\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\n\\nclass Visualizer:\\n\\n    def __init__(self):\\n        sns.set(style='whitegrid')\\n\\n    def visualize(self, method_name: str, data: pd.DataFrame, output_path: str) -> None:\\n        try:\\n            if method_name == 'WordFrequencyAnalysis':\\n                self._visualize_word_frequency(data, output_path)\\n            elif method_name == 'BigramAnalysis':\\n                self._visualize_bigram_frequency(data, output_path)\\n            else:\\n                logging.warning(f\\\"'{method_name}'は可視化未サポート\\\")\\n        except Exception as e:\\n            logging.error(f'可視化エラー: {str(e)}', exc_info=True)\\n\\n    def _visualize_word_frequency(self, df_data: pd.DataFrame, output_path: str):\\n        top_words = df_data.head(20)\\n        if top_words.empty:\\n            logging.info('単語頻度可視化: データ無し')\\n            return\\n        (fig, ax) = plt.subplots(figsize=(10, 6))\\n        sns.barplot(x='frequency', y='word', data=top_words, ax=ax)\\n        ax.set_title('Top 20 Words by Frequency')\\n        plt.tight_layout()\\n        os.makedirs(output_path, exist_ok=True)\\n        out_file = os.path.join(output_path, 'word_frequency.png')\\n        fig.savefig(out_file)\\n        plt.close(fig)\\n        logging.info(f'単語頻度可視化: {out_file}')\\n\\n    def _visualize_bigram_frequency(self, df_data: pd.DataFrame, output_path: str):\\n        top_bigrams = df_data.head(20)\\n        if top_bigrams.empty:\\n            logging.info('バイグラム可視化: データ無し')\\n            return\\n        (fig, ax) = plt.subplots(figsize=(10, 6))\\n        sns.barplot(x='frequency', y='bigram', data=top_bigrams, ax=ax)\\n        ax.set_title('Top 20 Bigrams by Frequency')\\n        plt.tight_layout()\\n        os.makedirs(output_path, exist_ok=True)\\n        out_file = os.path.join(output_path, 'bigram_frequency.png')\\n        fig.savefig(out_file)\\n        plt.close(fig)\\n        logging.info(f'バイグラム可視化: {out_file}')\"\n    },\n    {\n      \"path\": \"your_project\\\\models\\\\model_manager.py\",\n      \"overview\": \"Pythonコード。\\nクラス: ModelManager。\\n関数: __init__, load_methods, get_available_methods, apply_method。\\n\",\n      \"content\": \"import importlib\\nimport pkgutil\\nimport logging\\nfrom typing import List, Any, Dict\\nfrom models.analysis_methods.analysis_method_base import AnalysisMethodBase\\nimport models.analysis_methods\\n\\nclass ModelManager:\\n\\n    def __init__(self):\\n        self.methods = self.load_methods()\\n\\n    def load_methods(self) -> Dict[str, AnalysisMethodBase]:\\n        methods = {}\\n        for (_, module_name, _) in pkgutil.iter_modules(models.analysis_methods.__path__):\\n            if module_name != '__init__':\\n                module = importlib.import_module(f'models.analysis_methods.{module_name}')\\n                for attr_name in dir(module):\\n                    attr = getattr(module, attr_name)\\n                    if isinstance(attr, type) and issubclass(attr, AnalysisMethodBase) and (attr != AnalysisMethodBase):\\n                        instance = attr()\\n                        methods[instance.get_name()] = instance\\n        if not methods:\\n            logging.warning('解析手法がロードされませんでした。')\\n        return methods\\n\\n    def get_available_methods(self) -> List[str]:\\n        return list(self.methods.keys())\\n\\n    def apply_method(self, method_name: str, data: Any, *args, **kwargs) -> Any:\\n        if method_name in self.methods:\\n            return self.methods[method_name].analyze(data, *args, **kwargs)\\n        else:\\n            raise ValueError(f\\\"解析手法 '{method_name}'が見つかりません。\\\")\"\n    },\n    {\n      \"path\": \"your_project\\\\models\\\\__init__.py\",\n      \"overview\": \"Pythonコード。\\n\",\n      \"content\": \"from .model_manager import ModelManager\"\n    },\n    {\n      \"path\": \"your_project\\\\models\\\\analysis_methods\\\\analysis_method_base.py\",\n      \"overview\": \"Pythonコード。\\nクラス: AnalysisMethodBase。\\n関数: analyze, get_name。\\n\",\n      \"content\": \"from abc import ABC, abstractmethod\\nimport pandas as pd\\n\\nclass AnalysisMethodBase(ABC):\\n\\n    @abstractmethod\\n    def analyze(self, data: pd.DataFrame, **kwargs) -> pd.DataFrame:\\n        pass\\n\\n    @abstractmethod\\n    def get_name(self) -> str:\\n        pass\"\n    },\n    {\n      \"path\": \"your_project\\\\models\\\\analysis_methods\\\\bigram_analysis.py\",\n      \"overview\": \"Pythonコード。\\nクラス: BigramAnalysis。\\n関数: __init__, analyze, get_name。\\n\",\n      \"content\": \"import logging\\nimport pandas as pd\\nfrom collections import Counter\\nfrom janome.tokenizer import Tokenizer\\nfrom itertools import islice\\nfrom models.analysis_methods.analysis_method_base import AnalysisMethodBase\\n\\nclass BigramAnalysis(AnalysisMethodBase):\\n\\n    def __init__(self):\\n        self.tokenizer = Tokenizer()\\n\\n    def analyze(self, data: pd.DataFrame, **kwargs) -> pd.DataFrame:\\n        text_data = data['normalized_content'] if 'normalized_content' in data.columns else data['content'] if 'content' in data.columns else pd.Series([])\\n        if text_data.empty:\\n            logging.info('BigramAnalysis: テキストデータなし。')\\n            return pd.DataFrame()\\n        try:\\n            all_bigrams = []\\n            if 'tokens' in data.columns:\\n                for words in data['tokens']:\\n                    bigrams = zip(words, islice(words, 1, None))\\n                    all_bigrams.extend(bigrams)\\n            else:\\n                for txt in text_data:\\n                    tokens = [t.surface for t in self.tokenizer.tokenize(txt)]\\n                    bigrams = zip(tokens, islice(tokens, 1, None))\\n                    all_bigrams.extend(bigrams)\\n            freq = Counter(all_bigrams)\\n            df_result = pd.DataFrame(freq.items(), columns=['bigram', 'frequency'])\\n            df_result['bigram'] = df_result['bigram'].apply(lambda tup: ' '.join(tup))\\n            total = sum(df_result['frequency'])\\n            df_result['probability'] = df_result['frequency'] / total if total > 0 else 0\\n            return df_result.sort_values(by='frequency', ascending=False)\\n        except Exception as e:\\n            logging.error(f'BigramAnalysisエラー: {str(e)}', exc_info=True)\\n            return pd.DataFrame()\\n\\n    def get_name(self) -> str:\\n        return 'BigramAnalysis'\"\n    },\n    {\n      \"path\": \"your_project\\\\models\\\\analysis_methods\\\\word_frequency_analysis.py\",\n      \"overview\": \"Pythonコード。\\nクラス: WordFrequencyAnalysis。\\n関数: __init__, analyze, get_name。\\n\",\n      \"content\": \"import logging\\nimport pandas as pd\\nfrom collections import Counter\\nfrom janome.tokenizer import Tokenizer\\nfrom models.analysis_methods.analysis_method_base import AnalysisMethodBase\\n\\nclass WordFrequencyAnalysis(AnalysisMethodBase):\\n\\n    def __init__(self):\\n        self.tokenizer = Tokenizer()\\n\\n    def analyze(self, data: pd.DataFrame, **kwargs) -> pd.DataFrame:\\n        text_data = data['normalized_content'] if 'normalized_content' in data.columns else data['content'] if 'content' in data.columns else pd.Series([])\\n        if text_data.empty:\\n            logging.info('WordFrequencyAnalysis: テキストデータなし。')\\n            return pd.DataFrame()\\n        try:\\n            if 'tokens' in data.columns:\\n                all_words = [w for words in data['tokens'] for w in words]\\n            else:\\n                all_words = [t.surface for text in text_data for t in self.tokenizer.tokenize(text)]\\n            word_freq = Counter(all_words)\\n            df_result = pd.DataFrame(word_freq.items(), columns=['word', 'frequency'])\\n            df_result['probability'] = df_result['frequency'] / sum(df_result['frequency'])\\n            return df_result.sort_values(by='frequency', ascending=False)\\n        except Exception as e:\\n            logging.error(f'WordFrequencyAnalysisエラー: {str(e)}', exc_info=True)\\n            return pd.DataFrame()\\n\\n    def get_name(self) -> str:\\n        return 'WordFrequencyAnalysis'\"\n    },\n    {\n      \"path\": \"your_project\\\\parsers\\\\base_parser.py\",\n      \"overview\": \"Pythonコード。\\nクラス: BaseParser。\\n関数: parse, supported_extensions, data_type。\\n\",\n      \"content\": \"from abc import ABC, abstractmethod\\nfrom typing import Any, List, Dict\\n\\nclass BaseParser(ABC):\\n\\n    @abstractmethod\\n    def parse(self, file_path: str, encoding: str='utf-8') -> List[Dict[str, Any]]:\\n        pass\\n\\n    @abstractmethod\\n    def supported_extensions(self) -> List[str]:\\n        pass\\n\\n    @abstractmethod\\n    def data_type(self) -> str:\\n        pass\"\n    },\n    {\n      \"path\": \"your_project\\\\parsers\\\\csv_parser.py\",\n      \"overview\": \"Pythonコード。\\nクラス: CsvParser。\\n関数: parse, supported_extensions, data_type。\\n\",\n      \"content\": \"from parsers.base_parser import BaseParser\\nfrom typing import Any, List, Dict\\nimport pandas as pd\\nimport logging\\n\\nclass CsvParser(BaseParser):\\n\\n    def parse(self, file_path: str, encoding: str='utf-8') -> List[Dict[str, Any]]:\\n        try:\\n            df = pd.read_csv(file_path, encoding=encoding, errors='ignore')\\n            return df.to_dict(orient='records')\\n        except Exception as e:\\n            logging.error(f'CSVパース中エラー: {str(e)}', exc_info=True)\\n            return []\\n\\n    def supported_extensions(self) -> List[str]:\\n        return ['.csv']\\n\\n    def data_type(self) -> str:\\n        return 'csv'\"\n    },\n    {\n      \"path\": \"your_project\\\\parsers\\\\html_parser.py\",\n      \"overview\": \"Pythonコード。\\nクラス: HtmlParser。\\n関数: parse, supported_extensions, data_type。\\n\",\n      \"content\": \"from parsers.base_parser import BaseParser\\nfrom typing import Any, List, Dict\\nimport logging\\nfrom bs4 import BeautifulSoup\\n\\nclass HtmlParser(BaseParser):\\n\\n    def parse(self, file_path: str, encoding: str='utf-8') -> List[Dict[str, Any]]:\\n        try:\\n            with open(file_path, 'r', encoding=encoding, errors='ignore') as f:\\n                soup = BeautifulSoup(f, 'html.parser')\\n            text = soup.get_text()\\n            return [{'content': text}]\\n        except Exception as e:\\n            logging.error(f'HTMLパース中エラー: {str(e)}', exc_info=True)\\n            return []\\n\\n    def supported_extensions(self) -> List[str]:\\n        return ['.html', '.htm', '.mhtml']\\n\\n    def data_type(self) -> str:\\n        return 'html'\"\n    },\n    {\n      \"path\": \"your_project\\\\parsers\\\\json_parser.py\",\n      \"overview\": \"Pythonコード。\\nクラス: JsonParser。\\n関数: parse, supported_extensions, data_type。\\n\",\n      \"content\": \"from parsers.base_parser import BaseParser\\nfrom typing import Any, List, Dict\\nimport json\\nimport logging\\n\\nclass JsonParser(BaseParser):\\n\\n    def parse(self, file_path: str, encoding: str='utf-8') -> List[Dict[str, Any]]:\\n        try:\\n            with open(file_path, 'r', encoding=encoding, errors='ignore') as f:\\n                data = json.load(f)\\n            return [{'content': json.dumps(data, ensure_ascii=False)}]\\n        except Exception as e:\\n            logging.error(f'JSONパース中エラー: {str(e)}', exc_info=True)\\n            return []\\n\\n    def supported_extensions(self) -> List[str]:\\n        return ['.json']\\n\\n    def data_type(self) -> str:\\n        return 'json'\"\n    },\n    {\n      \"path\": \"your_project\\\\parsers\\\\parser_manager.py\",\n      \"overview\": \"Pythonコード。\\nクラス: ParserManager。\\n関数: __init__, load_parsers, get_parser。\\n\",\n      \"content\": \"import importlib\\nimport pkgutil\\nimport logging\\nfrom typing import Dict\\nfrom parsers.base_parser import BaseParser\\nimport parsers\\n\\nclass ParserManager:\\n\\n    def __init__(self):\\n        self.parsers = self.load_parsers()\\n\\n    def load_parsers(self) -> Dict[str, BaseParser]:\\n        parsers_dict = {}\\n        for (_, module_name, _) in pkgutil.iter_modules(parsers.__path__):\\n            if module_name.endswith('_parser') and module_name != 'base_parser':\\n                module = importlib.import_module(f'parsers.{module_name}')\\n                class_name = ''.join([word.capitalize() for word in module_name.split('_')])\\n                parser_class = getattr(module, class_name, None)\\n                if parser_class is None:\\n                    logging.warning(f'{module_name}に対応クラスなし')\\n                    continue\\n                parser_instance = parser_class()\\n                data_type = parser_instance.data_type()\\n                parsers_dict[data_type] = parser_instance\\n        return parsers_dict\\n\\n    def get_parser(self, data_type: str) -> BaseParser:\\n        return self.parsers.get(data_type)\"\n    },\n    {\n      \"path\": \"your_project\\\\parsers\\\\text_parser.py\",\n      \"overview\": \"Pythonコード。\\nクラス: TextParser。\\n関数: parse, supported_extensions, data_type。\\n\",\n      \"content\": \"from parsers.base_parser import BaseParser\\nfrom typing import Any, List, Dict\\nimport logging\\n\\nclass TextParser(BaseParser):\\n\\n    def parse(self, file_path: str, encoding: str='utf-8') -> List[Dict[str, Any]]:\\n        try:\\n            with open(file_path, 'r', encoding=encoding, errors='ignore') as f:\\n                content = f.read()\\n            return [{'content': content}]\\n        except Exception as e:\\n            logging.error(f'テキストパース中エラー: {str(e)}', exc_info=True)\\n            return []\\n\\n    def supported_extensions(self) -> List[str]:\\n        return ['.txt', '.md']\\n\\n    def data_type(self) -> str:\\n        return 'text'\"\n    },\n    {\n      \"path\": \"your_project\\\\parsers\\\\xls_parser.py\",\n      \"overview\": \"Pythonコード。\\nクラス: XlsParser。\\n関数: parse, supported_extensions, data_type。\\n\",\n      \"content\": \"from parsers.base_parser import BaseParser\\nfrom typing import Any, List, Dict\\nimport pandas as pd\\nimport logging\\n\\nclass XlsParser(BaseParser):\\n\\n    def parse(self, file_path: str, encoding: str='utf-8') -> List[Dict[str, Any]]:\\n        try:\\n            df = pd.read_excel(file_path, engine='xlrd')\\n            return df.to_dict(orient='records')\\n        except Exception as e:\\n            logging.error(f'XLSデータパース中エラー: {str(e)}', exc_info=True)\\n            return []\\n\\n    def supported_extensions(self) -> List[str]:\\n        return ['.xls']\\n\\n    def data_type(self) -> str:\\n        return 'xls'\"\n    },\n    {\n      \"path\": \"your_project\\\\performance\\\\performance_tracker.py\",\n      \"overview\": \"Pythonコード。\\nクラス: PerformanceTracker。\\n関数: __init__, start_timer, end_timer, get_metrics。\\n\",\n      \"content\": \"import time\\nfrom typing import Dict, List\\n\\nclass PerformanceTracker:\\n    \\\"\\\"\\\"各処理ステップの実行時間を記録するクラス\\\"\\\"\\\"\\n\\n    def __init__(self):\\n        self.start_times: Dict[str, float] = {}\\n        self.metrics: List[Dict[str, float]] = []\\n\\n    def start_timer(self, name: str) -> None:\\n        self.start_times[name] = time.time()\\n\\n    def end_timer(self, name: str) -> None:\\n        if name in self.start_times:\\n            elapsed = time.time() - self.start_times.pop(name)\\n            self.metrics.append({'process': name, 'elapsed_time': elapsed})\\n\\n    def get_metrics(self) -> List[Dict[str, float]]:\\n        return self.metrics\"\n    },\n    {\n      \"path\": \"your_project\\\\performance\\\\__init__.py\",\n      \"overview\": \"Pythonコード。\\nDocstring(冒頭60文字): プロジェクトのルート __init__.py\\nここでは特に処理はなく、パッケージとして認識させるためだけの空ファイル。...\\n\",\n      \"content\": \"\\\"\\\"\\\"\\nプロジェクトのルート __init__.py\\nここでは特に処理はなく、パッケージとして認識させるためだけの空ファイル。\\n\\\"\\\"\\\"\"\n    },\n    {\n      \"path\": \"your_project\\\\run_20250223_133141\\\\feedback.json\",\n      \"overview\": \"JSONファイル (辞書)。キー: timestamp, status, remarks, possible_improvements\\n\",\n      \"content\": \"{\\n  \\\"timestamp\\\": \\\"20250223_133141\\\",\\n  \\\"status\\\": \\\"Completed\\\",\\n  \\\"remarks\\\": \\\"\\\",\\n  \\\"possible_improvements\\\": [\\n    \\\"ログレベルをDEBUGにして詳細確認\\\",\\n    \\\"追加パラメータを考慮した解析を実装する\\\",\\n    \\\"AIプロンプトの自動生成を検討\\\"\\n  ]\\n}\"\n    },\n    {\n      \"path\": \"your_project\\\\run_20250223_151500\\\\feedback.json\",\n      \"overview\": \"JSONファイル (辞書)。キー: timestamp, status, remarks, possible_improvements\\n\",\n      \"content\": \"{\\n  \\\"timestamp\\\": \\\"20250223_151500\\\",\\n  \\\"status\\\": \\\"DB初期化失敗\\\",\\n  \\\"remarks\\\": \\\"Input path not found: c:\\\\\\\\Users\\\\\\\\KEN\\\\\\\\Desktop\\\\\\\\チェックアウト\\\\\\\\汎用分析基盤コード\\\\\\\\../../data\\\",\\n  \\\"possible_improvements\\\": [\\n    \\\"ログレベルをDEBUGにして詳細確認\\\",\\n    \\\"追加パラメータを考慮した解析を実装する\\\",\\n    \\\"AIプロンプトの自動生成を検討\\\"\\n  ]\\n}\"\n    },\n    {\n      \"path\": \"your_project\\\\run_20250223_153737\\\\feedback.json\",\n      \"overview\": \"JSONファイル (辞書)。キー: timestamp, status, remarks, possible_improvements\\n\",\n      \"content\": \"{\\n  \\\"timestamp\\\": \\\"20250223_153737\\\",\\n  \\\"status\\\": \\\"Completed\\\",\\n  \\\"remarks\\\": \\\"\\\",\\n  \\\"possible_improvements\\\": [\\n    \\\"ログレベルをDEBUGにして詳細確認\\\",\\n    \\\"追加パラメータを考慮した解析を実装する\\\",\\n    \\\"AIプロンプトの自動生成を検討\\\"\\n  ]\\n}\"\n    },\n    {\n      \"path\": \"your_project\\\\run_20250224_221720\\\\feedback.json\",\n      \"overview\": \"JSONファイル (辞書)。キー: timestamp, status, remarks, possible_improvements\\n\",\n      \"content\": \"{\\n  \\\"timestamp\\\": \\\"20250224_221720\\\",\\n  \\\"status\\\": \\\"DB初期化失敗\\\",\\n  \\\"remarks\\\": \\\"Input path not found: c:\\\\\\\\Users\\\\\\\\KEN\\\\\\\\Desktop\\\\\\\\チェックアウト\\\\\\\\汎用分析基盤コード\\\\\\\\../../data\\\",\\n  \\\"possible_improvements\\\": [\\n    \\\"ログレベルをDEBUGにして詳細確認\\\",\\n    \\\"追加パラメータを考慮した解析を実装する\\\",\\n    \\\"AIプロンプトの自動生成を検討\\\"\\n  ]\\n}\"\n    },\n    {\n      \"path\": \"your_project\\\\run_20250224_222224\\\\feedback.json\",\n      \"overview\": \"JSONファイル (辞書)。キー: timestamp, status, remarks, possible_improvements\\n\",\n      \"content\": \"{\\n  \\\"timestamp\\\": \\\"20250224_222224\\\",\\n  \\\"status\\\": \\\"Completed\\\",\\n  \\\"remarks\\\": \\\"\\\",\\n  \\\"possible_improvements\\\": [\\n    \\\"ログレベルをDEBUGにして詳細確認\\\",\\n    \\\"追加パラメータを考慮した解析を実装する\\\",\\n    \\\"AIプロンプトの自動生成を検討\\\"\\n  ]\\n}\"\n    },\n    {\n      \"path\": \"your_project\\\\run_20250224_232145\\\\feedback.json\",\n      \"overview\": \"JSONファイル (辞書)。キー: timestamp, status, remarks, possible_improvements\\n\",\n      \"content\": \"{\\n  \\\"timestamp\\\": \\\"20250224_232145\\\",\\n  \\\"status\\\": \\\"DB初期化失敗\\\",\\n  \\\"remarks\\\": \\\"Input path not found: c:\\\\\\\\Users\\\\\\\\KEN\\\\\\\\Desktop\\\\\\\\チェックアウト\\\\\\\\汎用分析基盤コード\\\\\\\\../../data\\\",\\n  \\\"possible_improvements\\\": [\\n    \\\"ログレベルをDEBUGにして詳細確認\\\",\\n    \\\"追加パラメータを考慮した解析を実装する\\\",\\n    \\\"AIプロンプトの自動生成を検討\\\"\\n  ]\\n}\"\n    },\n    {\n      \"path\": \"your_project\\\\run_20250308_025755\\\\feedback.json\",\n      \"overview\": \"JSONファイル (辞書)。キー: timestamp, status, remarks, possible_improvements\\n\",\n      \"content\": \"{\\n  \\\"timestamp\\\": \\\"20250308_025755\\\",\\n  \\\"status\\\": \\\"DB初期化失敗\\\",\\n  \\\"remarks\\\": \\\"Input path not found: C:\\\\\\\\Users\\\\\\\\KEN\\\\\\\\Desktop\\\\\\\\チェックアウト\\\\\\\\汎用分析基盤コード\\\\\\\\../../data\\\",\\n  \\\"possible_improvements\\\": [\\n    \\\"ログレベルをDEBUGにして詳細確認\\\",\\n    \\\"追加パラメータを考慮した解析を実装する\\\",\\n    \\\"AIプロンプトの自動生成を検討\\\"\\n  ]\\n}\"\n    },\n    {\n      \"path\": \"your_project\\\\run_20250308_033843\\\\feedback.json\",\n      \"overview\": \"JSONファイル (辞書)。キー: timestamp, status, remarks, possible_improvements\\n\",\n      \"content\": \"{\\n  \\\"timestamp\\\": \\\"20250308_033843\\\",\\n  \\\"status\\\": \\\"DB初期化失敗\\\",\\n  \\\"remarks\\\": \\\"Input path not found: C:\\\\\\\\Users\\\\\\\\KEN\\\\\\\\Desktop\\\\\\\\チェックアウト\\\\\\\\汎用分析基盤コード\\\\\\\\../../data\\\",\\n  \\\"possible_improvements\\\": [\\n    \\\"ログレベルをDEBUGにして詳細確認\\\",\\n    \\\"追加パラメータを考慮した解析を実装する\\\",\\n    \\\"AIプロンプトの自動生成を検討\\\"\\n  ]\\n}\"\n    },\n    {\n      \"path\": \"your_project\\\\utils\\\\config_manager.py\",\n      \"overview\": \"Pythonコード。\\nクラス: ConfigManager。\\n関数: __new__, load_config, get。\\n\",\n      \"content\": \"import json\\nimport os\\nimport logging\\nfrom typing import Any\\n\\nclass ConfigManager:\\n    _instance = None\\n\\n    def __new__(cls):\\n        if cls._instance is None:\\n            cls._instance = super(ConfigManager, cls).__new__(cls)\\n            cls._instance._config = {}\\n        return cls._instance\\n\\n    def load_config(self, config_path: str) -> None:\\n        if not os.path.exists(config_path):\\n            logging.warning(f'設定ファイルが見つかりません: {config_path}')\\n            self._config = {}\\n        else:\\n            try:\\n                with open(config_path, 'r', encoding='utf-8') as f:\\n                    content = f.read().strip()\\n                    if not content:\\n                        self._config = {}\\n                    else:\\n                        self._config = json.loads(content)\\n                logging.info(f'設定ファイル読込完了: {config_path}')\\n            except Exception as e:\\n                logging.warning(f'設定ファイル読込中にエラー: {str(e)}', exc_info=True)\\n                self._config = {}\\n\\n    def get(self, *keys: str, default: Any=None) -> Any:\\n        data = self._config\\n        for k in keys:\\n            if isinstance(data, dict) and k in data:\\n                data = data[k]\\n            else:\\n                return default\\n        if isinstance(data, dict) and 'value' in data:\\n            return data['value']\\n        return data\"\n    },\n    {\n      \"path\": \"your_project\\\\utils\\\\db_utils.py\",\n      \"overview\": \"Pythonコード。\\nクラス: DBUtils。\\n\",\n      \"content\": \"import aiosqlite\\nimport logging\\nimport os\\n\\nclass DBUtils:\\n    _connection = None\\n    _db_path = None\\n\\n    @staticmethod\\n    async def initialize_database(db_path='data.db'):\\n        DBUtils._db_path = db_path\\n        if not os.path.exists(db_path):\\n            conn = await aiosqlite.connect(db_path)\\n            await conn.execute('\\\\n                CREATE TABLE IF NOT EXISTS analysis_results (\\\\n                    id INTEGER PRIMARY KEY AUTOINCREMENT,\\\\n                    method_name TEXT,\\\\n                    result_data TEXT,\\\\n                    timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP\\\\n                )\\\\n            ')\\n            await conn.commit()\\n            await conn.close()\\n            logging.info(f'DB初期化完了: {db_path}')\\n        else:\\n            logging.info(f'既存DB使用: {db_path}')\\n        await DBUtils.get_connection()\\n\\n    @staticmethod\\n    async def get_connection():\\n        if DBUtils._connection is None:\\n            if DBUtils._db_path is None:\\n                raise ValueError('DBパス未設定。先にinitialize_databaseを呼ぶ必要があります。')\\n            DBUtils._connection = await aiosqlite.connect(DBUtils._db_path)\\n        return DBUtils._connection\\n\\n    @staticmethod\\n    async def close_connection():\\n        if DBUtils._connection:\\n            await DBUtils._connection.close()\\n            DBUtils._connection = None\\n            logging.info('DB接続を閉じました。')\"\n    },\n    {\n      \"path\": \"your_project\\\\utils\\\\file_identifier.py\",\n      \"overview\": \"Pythonコード。\\nクラス: FileIdentifier。\\n関数: __init__, identify_file_type, _identify_base_type_by_extension。\\n\",\n      \"content\": \"import os\\nimport logging\\nfrom utils.platform_identifier import PlatformIdentifier\\n\\nclass FileIdentifier:\\n\\n    def __init__(self, supported_data_types=None):\\n        self.supported_data_types = supported_data_types if supported_data_types else []\\n        self.platform_identifier = PlatformIdentifier()\\n\\n    def identify_file_type(self, file_path: str) -> str:\\n        extension = os.path.splitext(file_path)[1].lower()\\n        base_type = self._identify_base_type_by_extension(extension)\\n        if base_type in ['csv', 'json']:\\n            platform_type = self.platform_identifier.identify_platform(file_path, base_type)\\n            if platform_type in self.supported_data_types:\\n                return platform_type\\n        return base_type\\n\\n    def _identify_base_type_by_extension(self, extension: str) -> str:\\n        if extension in ['.txt', '.md']:\\n            return 'text'\\n        elif extension in ['.html', '.htm', '.mhtml']:\\n            return 'html'\\n        elif extension == '.json':\\n            return 'json'\\n        elif extension == '.csv':\\n            return 'csv'\\n        elif extension == '.xls':\\n            return 'xls'\\n        else:\\n            return 'unknown'\"\n    },\n    {\n      \"path\": \"your_project\\\\utils\\\\file_processor.py\",\n      \"overview\": \"Pythonコード。\\nクラス: FileProcessor。\\n関数: __init__, detect_encoding。\\n\",\n      \"content\": \"import os\\nimport logging\\nimport asyncio\\nfrom typing import List, Dict, Any\\nimport chardet\\nfrom utils.file_identifier import FileIdentifier\\nfrom parsers.parser_manager import ParserManager\\n\\nclass FileProcessor:\\n\\n    def __init__(self, data_types: List[str]):\\n        self.file_identifier = FileIdentifier(data_types)\\n        self.parser_manager = ParserManager()\\n\\n    async def process_files_in_parallel(self, files: List[str]) -> Dict[str, Any]:\\n        if not files:\\n            logging.info('処理対象ファイルなし。')\\n            return {}\\n        all_contents: Dict[str, List[Any]] = {}\\n        tasks = [self.process_file(file) for file in files]\\n        results = await asyncio.gather(*tasks, return_exceptions=True)\\n        for response in results:\\n            if isinstance(response, dict):\\n                for (key, value) in response.items():\\n                    all_contents.setdefault(key, []).extend(value)\\n            else:\\n                logging.error(f'ファイル処理中に例外: {response}', exc_info=True)\\n        return all_contents\\n\\n    async def process_file(self, file_path: str) -> Dict[str, List[Any]]:\\n        data_type = self.file_identifier.identify_file_type(file_path)\\n        parser = self.parser_manager.get_parser(data_type)\\n        if parser:\\n            encoding = self.detect_encoding(file_path)\\n            content = parser.parse(file_path, encoding)\\n            return {data_type: content}\\n        else:\\n            logging.warning(f\\\"未対応データタイプ '{data_type}' スキップ: {file_path}\\\")\\n            return {'unknown': []}\\n\\n    def detect_encoding(self, file_path: str) -> str:\\n        try:\\n            with open(file_path, 'rb') as f:\\n                raw_data = f.read(4096)\\n            result = chardet.detect(raw_data)\\n            encoding = result['encoding'] if result['encoding'] else 'utf-8'\\n            return encoding\\n        except Exception as e:\\n            logging.error(f'エンコーディング検出中にエラー: {str(e)}', exc_info=True)\\n            return 'utf-8'\"\n    },\n    {\n      \"path\": \"your_project\\\\utils\\\\html_report_generator.py\",\n      \"overview\": \"Pythonコード。\\nクラス: HtmlReportGenerator。\\n関数: __init__, save_html_report。\\n\",\n      \"content\": \"import os\\nimport logging\\nimport pandas as pd\\nfrom jinja2 import Environment, FileSystemLoader\\n\\nclass HtmlReportGenerator:\\n    \\\"\\\"\\\"\\n    HTMLレポート生成のみを担当し、OutputManagerとは責務を分離。\\n    \\\"\\\"\\\"\\n\\n    def __init__(self, template_dir: str='templates'):\\n        self.template_dir = template_dir\\n\\n    def save_html_report(self, data: pd.DataFrame, method_name: str, output_path: str) -> None:\\n        try:\\n            env = Environment(loader=FileSystemLoader(self.template_dir))\\n            template = env.get_template('report_template.html')\\n        except Exception as tmpl_err:\\n            logging.error(f'HTMLテンプレート読み込み失敗: {str(tmpl_err)}', exc_info=True)\\n            return\\n        try:\\n            html_content = template.render(title=method_name, data=data.to_dict(orient='records'), method_name=method_name)\\n            report_path = os.path.join(output_path, f'{method_name}.html')\\n            os.makedirs(os.path.dirname(report_path), exist_ok=True)\\n            with open(report_path, 'w', encoding='utf-8') as f:\\n                f.write(html_content)\\n            logging.info(f'HTMLレポート出力: {report_path}')\\n        except Exception as e:\\n            logging.error(f'HTMLレポート生成中エラー: {str(e)}', exc_info=True)\"\n    },\n    {\n      \"path\": \"your_project\\\\utils\\\\logger.py\",\n      \"overview\": \"Pythonコード。\\nクラス: Logger。\\n関数: setup_logging。\\n\",\n      \"content\": \"import logging\\nimport os\\nimport datetime\\n\\nclass Logger:\\n    _initialized = False\\n\\n    @staticmethod\\n    def setup_logging(log_dir: str, log_level: str) -> None:\\n        \\\"\\\"\\\"\\n        log_dir: ログファイル保存先\\n        log_level: ログレベル（例: INFO, DEBUG）\\n        \\\"\\\"\\\"\\n        if Logger._initialized:\\n            logging.getLogger(__name__).warning('Loggerは既に初期化済み。再初期化試行。')\\n        os.makedirs(log_dir, exist_ok=True)\\n        log_file = os.path.join(log_dir, 'app.log')\\n        if not hasattr(logging, log_level.upper()):\\n            logging.getLogger(__name__).warning(f\\\"無効なログレベル '{log_level}' -> 'INFO'を使用\\\")\\n            log_level = 'INFO'\\n        for handler in logging.root.handlers[:]:\\n            logging.root.removeHandler(handler)\\n        logging.basicConfig(level=getattr(logging, log_level.upper()), format='%(asctime)s - %(levelname)s - %(message)s', datefmt='%Y-%m-%d %H:%M:%S', handlers=[logging.FileHandler(log_file, encoding='utf-8'), logging.StreamHandler()])\\n        Logger._initialized = True\\n        logging.info('ロギング初期化完了。')\"\n    },\n    {\n      \"path\": \"your_project\\\\utils\\\\output_manager.py\",\n      \"overview\": \"Pythonコード。\\nクラス: OutputManager。\\n関数: save_results, save_to_csv, save_to_json, save_html_report, save_performance_metrics。\\n\",\n      \"content\": \"import os\\nimport pandas as pd\\nimport logging\\nimport json\\nfrom typing import Any, List, Dict\\nimport asyncio\\nfrom jinja2 import Environment, FileSystemLoader\\nfrom utils.db_utils import DBUtils\\n\\nclass OutputManager:\\n\\n    def save_results(self, data: pd.DataFrame, method_name: str, output_path: str) -> None:\\n        csv_output_path = os.path.join(output_path, f'{method_name}.csv')\\n        json_output_path = os.path.join(output_path, f'{method_name}.json')\\n        self.save_to_csv(data, csv_output_path)\\n        self.save_to_json(data, json_output_path)\\n\\n    def save_to_csv(self, data: pd.DataFrame, file_path: str) -> None:\\n        try:\\n            os.makedirs(os.path.dirname(file_path), exist_ok=True)\\n            data.to_csv(file_path, index=False)\\n            logging.info(f'CSV出力: {file_path}')\\n        except Exception as e:\\n            logging.error(f'CSV保存中エラー: {str(e)}', exc_info=True)\\n\\n    def save_to_json(self, data: pd.DataFrame, file_path: str) -> None:\\n        try:\\n            os.makedirs(os.path.dirname(file_path), exist_ok=True)\\n            data.to_json(file_path, orient='records', force_ascii=False)\\n            logging.info(f'JSON出力: {file_path}')\\n        except Exception as e:\\n            logging.error(f'JSON保存中エラー: {str(e)}', exc_info=True)\\n\\n    async def save_to_database(self, data: pd.DataFrame, method_name: str) -> None:\\n        try:\\n            conn = await DBUtils.get_connection()\\n            cursor = await conn.cursor()\\n            result_json = data.to_json(orient='records', force_ascii=False)\\n            await cursor.execute('INSERT INTO analysis_results (method_name, result_data) VALUES (?, ?)', (method_name, result_json))\\n            await conn.commit()\\n            logging.info(f'DB保存成功: {method_name}')\\n        except Exception as e:\\n            logging.error(f'DB保存中エラー: {str(e)}', exc_info=True)\\n\\n    def save_html_report(self, data: pd.DataFrame, method_name: str, output_path: str) -> None:\\n        try:\\n            env = Environment(loader=FileSystemLoader('templates'))\\n            template = env.get_template('report_template.html')\\n        except Exception as tmpl_err:\\n            logging.error(f'HTMLテンプレート読み込み失敗: {str(tmpl_err)}', exc_info=True)\\n            return\\n        try:\\n            html_content = template.render(title=method_name, data=data.to_dict(orient='records'), method_name=method_name)\\n            report_path = os.path.join(output_path, f'{method_name}.html')\\n            os.makedirs(os.path.dirname(report_path), exist_ok=True)\\n            with open(report_path, 'w', encoding='utf-8') as f:\\n                f.write(html_content)\\n            logging.info(f'HTMLレポート出力: {report_path}')\\n        except Exception as e:\\n            logging.error(f'HTMLレポート生成中エラー: {str(e)}', exc_info=True)\\n\\n    def save_performance_metrics(self, metrics: List[Dict[str, Any]], file_path: str) -> None:\\n        try:\\n            os.makedirs(os.path.dirname(file_path), exist_ok=True)\\n            df = pd.DataFrame(metrics)\\n            df.to_csv(file_path, index=False)\\n            logging.info(f'パフォーマンス結果CSV出力: {file_path}')\\n        except Exception as e:\\n            logging.error(f'パフォーマンス結果保存中エラー: {str(e)}', exc_info=True)\"\n    },\n    {\n      \"path\": \"your_project\\\\utils\\\\platform_feature_extractors.py\",\n      \"overview\": \"Pythonコード。\\nクラス: IPlatformFeatureExtractor, CsvPlatformFeatureExtractor, JsonPlatformFeatureExtractor。\\n関数: extract_features, extract_features, extract_features, extract_keys。\\n\",\n      \"content\": \"import pandas as pd\\nimport json\\nimport logging\\nimport os\\nfrom typing import Set, Any\\n\\nclass IPlatformFeatureExtractor:\\n    \\\"\\\"\\\"\\n    プラットフォーム特徴抽出インターフェース。\\n    ファイルからプラットフォーム判別用の特徴（カラム名やJSONキー）を抽出する。\\n    \\\"\\\"\\\"\\n\\n    def extract_features(self, file_path: str) -> Set[str]:\\n        raise NotImplementedError\\n\\nclass CsvPlatformFeatureExtractor(IPlatformFeatureExtractor):\\n\\n    def extract_features(self, file_path: str) -> Set[str]:\\n        features = set()\\n        try:\\n            df = pd.read_csv(file_path, nrows=5)\\n            features.update(df.columns.tolist())\\n        except Exception as e:\\n            logging.warning(f'CSV特徴抽出中エラー({file_path}): {str(e)}', exc_info=True)\\n        return features\\n\\nclass JsonPlatformFeatureExtractor(IPlatformFeatureExtractor):\\n\\n    def extract_features(self, file_path: str) -> Set[str]:\\n        features = set()\\n        try:\\n            with open(file_path, 'r', encoding='utf-8', errors='ignore') as jf:\\n                data = json.load(jf)\\n                features = self.extract_keys(data)\\n        except Exception as e:\\n            logging.warning(f'JSON特徴抽出中エラー({file_path}): {str(e)}', exc_info=True)\\n        return features\\n\\n    def extract_keys(self, data: Any, prefix: str='') -> Set[str]:\\n        keys = set()\\n        if isinstance(data, dict):\\n            for (k, v) in data.items():\\n                new_key = f'{prefix}.{k}' if prefix else k\\n                keys.add(new_key)\\n                keys |= self.extract_keys(v, new_key)\\n        elif isinstance(data, list):\\n            for (i, item) in enumerate(data):\\n                new_key = f'{prefix}[{i}]' if prefix else f'[{i}]'\\n                keys |= self.extract_keys(item, new_key)\\n        return keys\"\n    },\n    {\n      \"path\": \"your_project\\\\utils\\\\platform_identifier.py\",\n      \"overview\": \"Pythonコード。\\nクラス: PlatformIdentifier。\\n関数: __init__, identify_platform。\\n\",\n      \"content\": \"import logging\\nimport pandas as pd\\nimport json\\nfrom utils.platform_sample_manager import PlatformSampleManager\\n\\nclass PlatformIdentifier:\\n\\n    def __init__(self):\\n        self.sample_manager = PlatformSampleManager()\\n\\n    def identify_platform(self, file_path: str, base_type: str) -> str:\\n        try:\\n            if base_type == 'csv':\\n                df = pd.read_csv(file_path, nrows=5)\\n                return self.sample_manager.match_platform_for_csv(df)\\n            elif base_type == 'json':\\n                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\\n                    data = json.load(f)\\n                return self.sample_manager.match_platform_for_json(data)\\n        except Exception as e:\\n            logging.error(f'プラットフォーム識別中エラー: {str(e)}', exc_info=True)\\n        return base_type\"\n    },\n    {\n      \"path\": \"your_project\\\\utils\\\\platform_sample_manager.py\",\n      \"overview\": \"Pythonコード。\\nクラス: PlatformSampleManager。\\n関数: __init__, load_samples, load_platform_samples, match_platform_for_csv, match_platform_for_json。\\n\",\n      \"content\": \"import os\\nimport logging\\nimport pandas as pd\\nimport json\\nfrom utils.config_manager import ConfigManager\\nfrom utils.platform_feature_extractors import CsvPlatformFeatureExtractor, JsonPlatformFeatureExtractor\\n\\nclass PlatformSampleManager:\\n    \\\"\\\"\\\"\\n    プラットフォームごとのサンプルデータをロードし、\\n    CSV/JSON用の特徴抽出器で特徴セットを構築。\\n    解析時の類似度マッチングに利用する。\\n    \\\"\\\"\\\"\\n\\n    def __init__(self):\\n        config = ConfigManager()\\n        self.platform_samples_dir = config.get('platform_samples_directory', default='')\\n        if not self.platform_samples_dir:\\n            logging.warning('platform_samples_directory が設定されていません。サンプルロードはスキップします。')\\n            self.platform_samples_dir = ''\\n        logging.debug(f\\\"PlatformSampleManager: platform_samples_dir = '{self.platform_samples_dir}'\\\")\\n        self.platform_csv_patterns = {}\\n        self.platform_json_patterns = {}\\n        self.extractors = {'csv': CsvPlatformFeatureExtractor(), 'json': JsonPlatformFeatureExtractor()}\\n        self.load_samples()\\n\\n    def load_samples(self):\\n        if not self.platform_samples_dir:\\n            logging.warning('platform_samples_directory が空文字です。サンプルディレクトリ未設定扱い。')\\n            return\\n        if not os.path.exists(self.platform_samples_dir):\\n            logging.warning(f'プラットフォームサンプルディレクトリが存在しません: {self.platform_samples_dir}')\\n            return\\n        for platform in os.listdir(self.platform_samples_dir):\\n            p_dir = os.path.join(self.platform_samples_dir, platform)\\n            if os.path.isdir(p_dir):\\n                self.load_platform_samples(platform, p_dir)\\n\\n    def load_platform_samples(self, platform: str, p_dir: str):\\n        csv_cols = set()\\n        json_keys = set()\\n        for (root, _, files) in os.walk(p_dir):\\n            for f in files:\\n                fpath = os.path.join(root, f)\\n                ext = os.path.splitext(fpath)[1].lower()\\n                if ext == '.csv':\\n                    csv_cols |= self.extractors['csv'].extract_features(fpath)\\n                elif ext == '.json':\\n                    json_keys |= self.extractors['json'].extract_features(fpath)\\n        if csv_cols:\\n            self.platform_csv_patterns[platform] = {'columns': csv_cols}\\n        if json_keys:\\n            self.platform_json_patterns[platform] = {'keys': json_keys}\\n\\n    def match_platform_for_csv(self, df: pd.DataFrame) -> str:\\n        input_cols = set(df.columns.tolist())\\n        best_match = None\\n        best_score = 0\\n        for (platform, pattern) in self.platform_csv_patterns.items():\\n            common = len(input_cols & pattern['columns'])\\n            score = common / (len(pattern['columns']) + 1)\\n            if score > best_score:\\n                best_score = score\\n                best_match = platform\\n        return best_match if best_match else 'csv'\\n\\n    def match_platform_for_json(self, data: dict) -> str:\\n        input_keys = self.extractors['json'].extract_keys(data)\\n        best_match = None\\n        best_score = 0\\n        for (platform, pattern) in self.platform_json_patterns.items():\\n            common = len(input_keys & pattern['keys'])\\n            score = common / (len(pattern['keys']) + 1)\\n            if score > best_score:\\n                best_score = score\\n                best_match = platform\\n        return best_match if best_match else 'json'\"\n    },\n    {\n      \"path\": \"your_project\\\\utils\\\\__init__.py\",\n      \"overview\": \"Pythonコード。\\nDocstring(冒頭60文字): プロジェクトのルート __init__.py\\nここでは特に処理はなく、パッケージとして認識させるためだけの空ファイル。...\\n\",\n      \"content\": \"\\\"\\\"\\\"\\nプロジェクトのルート __init__.py\\nここでは特に処理はなく、パッケージとして認識させるためだけの空ファイル。\\n\\\"\\\"\\\"\"\n    },\n    {\n      \"path\": \"your_project\\\\visualization\\\\visualizer.py\",\n      \"overview\": \"Pythonコード。\\nクラス: Visualizer。\\n関数: __init__, visualize, _visualize_word_frequency, _visualize_bigram_frequency。\\n\",\n      \"content\": \"import os\\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\nimport logging\\n\\nclass Visualizer:\\n\\n    def __init__(self):\\n        sns.set(style='whitegrid')\\n\\n    def visualize(self, method_name: str, data: pd.DataFrame, output_path: str) -> None:\\n        try:\\n            if method_name == 'WordFrequencyAnalysis':\\n                self._visualize_word_frequency(data, output_path)\\n            elif method_name == 'BigramAnalysis':\\n                self._visualize_bigram_frequency(data, output_path)\\n            else:\\n                logging.warning(f\\\"'{method_name}'は可視化未サポート。\\\")\\n                return\\n        except Exception as e:\\n            logging.error(f'可視化中エラー: {str(e)}', exc_info=True)\\n\\n    def _visualize_word_frequency(self, data: pd.DataFrame, output_path: str) -> None:\\n        (fig, ax) = plt.subplots(figsize=(10, 6))\\n        top_words = data.head(20)\\n        sns.barplot(x='frequency', y='word', data=top_words, ax=ax)\\n        ax.set_title('Top 20 Words by Frequency')\\n        plt.tight_layout()\\n        output_file = os.path.join(output_path, 'word_frequency.png')\\n        fig.savefig(output_file)\\n        plt.close(fig)\\n        logging.info(f'単語頻度可視化保存: {output_file}')\\n\\n    def _visualize_bigram_frequency(self, data: pd.DataFrame, output_path: str) -> None:\\n        (fig, ax) = plt.subplots(figsize=(10, 6))\\n        top_bigrams = data.head(20)\\n        sns.barplot(x='frequency', y='bigram', data=top_bigrams, ax=ax)\\n        ax.set_title('Top 20 Bigrams by Frequency')\\n        plt.tight_layout()\\n        output_file = os.path.join(output_path, 'bigram_frequency.png')\\n        fig.savefig(output_file)\\n        plt.close(fig)\\n        logging.info(f'バイグラム頻度可視化保存: {output_file}')\"\n    },\n    {\n      \"path\": \"your_project\\\\visualization\\\\__init__.py\",\n      \"overview\": \"Pythonコード。\\nDocstring(冒頭60文字): プロジェクトのルート __init__.py\\nここでは特に処理はなく、パッケージとして認識させるためだけの空ファイル。...\\n\",\n      \"content\": \"\\\"\\\"\\\"\\nプロジェクトのルート __init__.py\\nここでは特に処理はなく、パッケージとして認識させるためだけの空ファイル。\\n\\\"\\\"\\\"\"\n    },\n    {\n      \"path\": \"your_project\\\\run_20250308_053814\\\\feedback.json\",\n      \"overview\": \"JSONファイル (辞書)。キー: timestamp, status, remarks, possible_improvements\\n\",\n      \"content\": \"{\\n  \\\"timestamp\\\": \\\"20250308_053814\\\",\\n  \\\"status\\\": \\\"Completed\\\",\\n  \\\"remarks\\\": \\\"\\\",\\n  \\\"possible_improvements\\\": [\\n    \\\"ログレベルをDEBUGにして詳細確認\\\",\\n    \\\"追加パラメータを考慮した解析を実装する\\\",\\n    \\\"AIプロンプトの自動生成を検討\\\"\\n  ]\\n}\"\n    },\n    {\n      \"path\": \"your_project\\\\run_20250308_054140\\\\feedback.json\",\n      \"overview\": \"JSONファイル (辞書)。キー: timestamp, status, remarks, possible_improvements\\n\",\n      \"content\": \"{\\n  \\\"timestamp\\\": \\\"20250308_054140\\\",\\n  \\\"status\\\": \\\"Completed\\\",\\n  \\\"remarks\\\": \\\"\\\",\\n  \\\"possible_improvements\\\": [\\n    \\\"ログレベルをDEBUGにして詳細確認\\\",\\n    \\\"追加パラメータを考慮した解析を実装する\\\",\\n    \\\"AIプロンプトの自動生成を検討\\\"\\n  ]\\n}\"\n    }\n  ]\n}"
    },
    {
      "path": "your_project\\理解度チェック.py",
      "overview": "Pythonコード。\nクラス: LearningAnalyzer。\n関数: __init__, get_file_hash, is_binary_file, extract_text, load_documents, analyze_topics, visualize, process_file。\n",
      "content": "import os\nimport hashlib\nimport pandas as pd\nimport nltk\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import NMF\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport concurrent.futures\nimport gzip\nfrom bs4 import BeautifulSoup\nimport sqlite3\nfrom threading import Lock\nnltk.download('punkt')\n\nclass LearningAnalyzer:\n\n    def __init__(self, folder_path):\n        self.folder_path = folder_path\n        self.documents = []\n        self.file_hashes = set()\n        self.processed_files = set()\n        self.topics = ['確率分布', '推定・検定', 'ベイズ統計', '回帰分析', '機械学習', '最適化', 'データ可視化']\n        self.conn = sqlite3.connect('processed_files.db', check_same_thread=False)\n        self.cursor = self.conn.cursor()\n        self.cursor.execute('CREATE TABLE IF NOT EXISTS processed_files (\\n                                file_path TEXT PRIMARY KEY,\\n                                file_hash TEXT\\n                               )')\n        self.conn.commit()\n        self.db_lock = Lock()\n        self.log_lock = Lock()\n\n    def get_file_hash(self, file_path):\n        \"\"\"ファイル内容のSHA-256ハッシュ値を計算\"\"\"\n        hasher = hashlib.sha256()\n        with open(file_path, 'rb') as f:\n            for chunk in iter(lambda : f.read(65536), b''):\n                hasher.update(chunk)\n        return hasher.hexdigest()\n\n    def is_binary_file(self, file_path):\n        \"\"\"先頭数百バイトを確認し、バイナリファイルか推定する\"\"\"\n        try:\n            with open(file_path, 'rb') as f:\n                chunk = f.read(512)\n        except Exception:\n            return True\n        if not chunk:\n            return False\n        if b'\\x00' in chunk:\n            return True\n        text_bytes = set(range(32, 127)) | {9, 10, 13, 8, 12}\n        nontext_count = sum((1 for b in chunk if b not in text_bytes))\n        return nontext_count / len(chunk) > 0.3\n\n    def extract_text(self, file_path):\n        \"\"\"ファイルからテキストを抽出（HTML, MHTML, GZ対応）\"\"\"\n        try:\n            if file_path.endswith('.html', '.htm', '.mhtml', '.txt', '.ipynb', '.py'):\n                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n                    soup = BeautifulSoup(f, 'html.parser')\n                    return soup.get_text()\n            elif file_path.endswith('.gz'):\n                with gzip.open(file_path, 'rb') as f:\n                    data = f.read()\n                if not data:\n                    return ''\n                sample = data[:512]\n                text_bytes = set(range(32, 127)) | {9, 10, 13, 8, 12}\n                if b'\\x00' in sample or sum((1 for b in sample if b not in text_bytes)) / len(sample) > 0.3:\n                    return None\n                return data.decode('utf-8', errors='ignore')\n            else:\n                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n                    return f.read()\n        except Exception:\n            return None\n\n    def load_documents(self):\n        \"\"\"フォルダ内の全ファイルを探索し、並列にテキスト読み込み\"\"\"\n\n        def process_file(path):\n            if path.lower().endswith(('.bin', '.exe', '.dll', '.dat')):\n                with self.log_lock:\n                    with open('skipped_files.log', 'a', encoding='utf-8') as log:\n                        log.write(f'{path}: Skipped (binary file extension)\\n')\n                return None\n            if self.is_binary_file(path):\n                with self.log_lock:\n                    with open('skipped_files.log', 'a', encoding='utf-8') as log:\n                        log.write(f'{path}: Skipped (binary content)\\n')\n                return None\n            try:\n                file_hash = self.get_file_hash(path)\n            except Exception as e:\n                with self.log_lock:\n                    with open('skipped_files.log', 'a', encoding='utf-8') as log:\n                        log.write(f'{path}: Failed to compute hash ({e})\\n')\n                return None\n            with self.db_lock:\n                self.cursor.execute('SELECT 1 FROM processed_files WHERE file_path=? OR file_hash=?', (path, file_hash))\n                found = self.cursor.fetchone()\n                if found or file_hash in self.file_hashes or path in self.processed_files:\n                    reason = 'already processed' if found else 'duplicate in current run'\n                    with open('skipped_files.log', 'a', encoding='utf-8') as log:\n                        log.write(f'{path}: Skipped ({reason})\\n')\n                    return None\n                self.file_hashes.add(file_hash)\n                self.processed_files.add(path)\n            text = self.extract_text(path)\n            if text is None or text.strip() == '':\n                with self.db_lock:\n                    self.file_hashes.discard(file_hash)\n                    self.processed_files.discard(path)\n                with self.log_lock:\n                    with open('skipped_files.log', 'a', encoding='utf-8') as log:\n                        reason = 'no text' if text == '' else 'extract error'\n                        log.write(f'{path}: Skipped ({reason})\\n')\n                return None\n            with self.db_lock:\n                try:\n                    self.cursor.execute('INSERT OR IGNORE INTO processed_files (file_path, file_hash) VALUES (?, ?)', (path, file_hash))\n                    self.conn.commit()\n                except Exception as e:\n                    with open('skipped_files.log', 'a', encoding='utf-8') as log:\n                        log.write(f'{path}: Warning - DB insert failed ({e})\\n')\n            return text\n        file_list = [os.path.join(root, f) for (root, _, files) in os.walk(self.folder_path) for f in files]\n        with concurrent.futures.ThreadPoolExecutor() as executor:\n            results = list(executor.map(process_file, file_list))\n        self.documents = [doc for doc in results if doc]\n        return len(self.documents)\n\n    def analyze_topics(self):\n        \"\"\"NMFを使用してトピック分析を行う\"\"\"\n        if not self.documents:\n            raise ValueError('文書リストが空です。先にload_documentsを実行してください。')\n        vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)\n        tfidf_matrix = vectorizer.fit_transform(self.documents)\n        nmf = NMF(n_components=len(self.topics), random_state=42)\n        topic_matrix = nmf.fit_transform(tfidf_matrix)\n        df = pd.DataFrame(topic_matrix, columns=self.topics)\n        df['Document'] = [f'Doc_{i + 1}' for i in range(len(self.documents))]\n        return df\n\n    def visualize(self, df):\n        \"\"\"ヒートマップでトピック分布を可視化\"\"\"\n        plt.figure(figsize=(10, 6))\n        sns.heatmap(df.set_index('Document'), annot=True, cmap='coolwarm')\n        plt.title('トピックごとの理解度マッピング')\n        plt.show()\nfolder_path = 'p:'\nanalyzer = LearningAnalyzer(folder_path)\nprint('文書を読み込み中...')\nanalyzer.load_documents()\nprint(f'読み込んだ文書数: {len(analyzer.documents)}')\nprint('トピック分析中...')\ndf_topics = analyzer.analyze_topics()\nprint(df_topics.head())\nanalyzer.visualize(df_topics)"
    },
    {
      "path": "your_project\\ｃ環境インストール補助.py",
      "overview": "Pythonコード。\n関数: list_envs, choose_env_hard, run_command_in_env, main。\n",
      "content": "import os\nimport sys\nimport subprocess\nfrom pathlib import Path\nCONDA_PREFIX = 'C:\\\\Users\\\\KEN\\\\anaconda3'\nCOMMAND = 'pip install pymc arviz'\nTARGET_ENV_NAME = 'python3-10-8'\n\ndef list_envs(envs_path):\n    \"\"\"指定された envs フォルダ内の仮想環境（ディレクトリ）をリストアップする\"\"\"\n    try:\n        envs = [d.name for d in Path(envs_path).iterdir() if d.is_dir()]\n        return envs\n    except Exception as e:\n        print(f'仮想環境のリスト取得に失敗: {e}')\n        sys.exit(1)\n\ndef choose_env_hard(envs):\n    \"\"\"ハードコードされた TARGET_ENV_NAME が存在するかチェックし、\n    なければ最初の環境を返す\"\"\"\n    if TARGET_ENV_NAME in envs:\n        return TARGET_ENV_NAME\n    else:\n        print(f\"指定された環境 '{TARGET_ENV_NAME}' が見つかりません。代わりに '{envs[0]}' を使用します。\")\n        return envs[0]\n\ndef run_command_in_env(env_name, command, conda_prefix):\n    \"\"\"\n    Windows の cmd.exe を利用して、指定した仮想環境をアクティベートした上で\n    コマンドを実行します。activate.bat を用いるので、システム環境変数は変更しません。\n    \"\"\"\n    activate_script = os.path.join(conda_prefix, 'Scripts', 'activate.bat')\n    if not os.path.exists(activate_script):\n        print('activate.bat が見つかりません。Anaconda のパスを確認してください。')\n        sys.exit(1)\n    full_command = f'cmd /c \"{activate_script} {env_name} && {command}\"'\n    print('実行コマンド:')\n    print(full_command)\n    try:\n        result = subprocess.run(full_command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n        print('標準出力:')\n        print(result.stdout)\n        if result.stderr:\n            print('標準エラー出力:')\n            print(result.stderr)\n    except Exception as e:\n        print(f'コマンド実行中にエラーが発生しました: {e}')\n        sys.exit(1)\n\ndef main():\n    envs_path = os.path.join(CONDA_PREFIX, 'envs')\n    if not os.path.isdir(envs_path):\n        print(f'指定された envs パスが存在しません: {envs_path}')\n        sys.exit(1)\n    available_envs = list_envs(envs_path)\n    if not available_envs:\n        print('利用可能な仮想環境が見つかりませんでした。')\n        sys.exit(1)\n    chosen_env = choose_env_hard(available_envs)\n    print(f'選択された仮想環境: {chosen_env}')\n    run_command_in_env(chosen_env, COMMAND, CONDA_PREFIX)\nif __name__ == '__main__':\n    main()"
    },
    {
      "path": "your_project\\obfuscated_latest\\統計検定1級\\config.py",
      "overview": "Pythonコード。\nクラス: create_problem。\n関数: _variant_poisson, load_topics, get。\n",
      "content": "import json, os\n\nclass create_problem:\n\n    def _variant_poisson(self, config_file='config.json'):\n        script_dir = os.path.dirname(os.path.abspath(__file__))\n        self.config_file = os.path.join(script_dir, config_file)\n        self.settings = self.load_config()\n\n    def load_topics(self):\n        if not os.path.exists(self.config_file):\n            return {}\n        try:\n            with open(self.config_file, 'r', encoding='utf-8') as f:\n                content = f.read().strip()\n                if not content:\n                    return {}\n                return json.loads(content)\n        except Exception:\n            return {}\n\n    def get(self, *keys, default=None):\n        data = self.settings\n        for k in keys:\n            if isinstance(data, dict) and k in data:\n                data = data[k]\n            else:\n                return default\n        return data\nconfig = create_problem()"
    },
    {
      "path": "your_project\\obfuscated_latest\\統計検定1級\\database.py",
      "overview": "Pythonコード。\nクラス: detect_symbol_collisions。\n関数: _variant_poisson, setup_database, setup_database。\n",
      "content": "import sqlite3, os\nfrom config import config\n\nclass detect_symbol_collisions:\n\n    def _variant_poisson(self):\n        script_dir = os.path.dirname(os.path.abspath(__file__))\n        db_path = config.get('database_path', default='data/data.db')\n        if db_path is None:\n            db_path = 'data/data.db'\n        self.db_name = os.path.join(script_dir, db_path)\n        os.makedirs(os.path.dirname(self.db_name), exist_ok=True)\n\n    def setup_database(self):\n        conn = sqlite3.connect(self.db_name)\n        c = conn.cursor()\n        c.execute('CREATE TABLE IF NOT EXISTS problems (id TEXT PRIMARY KEY,date_created TEXT,problem_text TEXT,solution_text TEXT,problem_type TEXT)')\n        conn.commit()\n        conn.close()\n\n    def setup_database(self, problem_id, date_created, problem_text, solution_text, problem_type):\n        conn = sqlite3.connect(self.db_name)\n        c = conn.cursor()\n        c.execute('INSERT INTO problems VALUES (?,?,?,?,?)', (problem_id, date_created, problem_text, solution_text, problem_type))\n        conn.commit()\n        conn.close()"
    },
    {
      "path": "your_project\\obfuscated_latest\\統計検定1級\\graph.py",
      "overview": "Pythonコード。\nクラス: process_file。\n関数: _variant_poisson, _variant_poisson, process_file, process_file, process_file, process_file, process_file, process_file, process_file, process_file, process_file, process_file, process_file, create_problem, mvn_pdf, extract_text。\n",
      "content": "import matplotlib\nmatplotlib.use('Agg')\nimport matplotlib.pyplot as plt\nimport math\nimport numpy as np\nimport logging\nimport os\nfrom math import factorial, exp\nfrom scipy.stats import norm, t as t_dist, f as f_dist, poisson, expon\nfrom statsmodels.graphics.tsaplots import plot_acf\nimport traceback\n\nclass process_file:\n\n    def _variant_poisson(self):\n        pass\n\n    def _variant_poisson(self, output_path):\n        \"\"\"\n        画像を確実にPNGで出力し、ファイルサイズなどをログするヘルパー関数。\n        \"\"\"\n        try:\n            (_, ext) = os.path.splitext(output_path)\n            if ext.lower() == '.png':\n                plt.savefig(output_path, format='png')\n            else:\n                plt.savefig(output_path, format='png')\n            plt.close()\n            if os.path.exists(output_path):\n                fsize = os.path.getsize(output_path)\n                logging.info(f'Saved figure: {output_path} (size: {fsize} bytes)')\n            else:\n                logging.warning(f'File not found after saving: {output_path}')\n        except Exception as e:\n            logging.error(f'Failed to save figure to {output_path}, error={e}')\n            plt.close()\n\n    def process_file(self, params, output_path):\n        ptype = params.get('problem_type')\n        try:\n            if ptype == 'binomial':\n                p = params['p']\n                n = params['n']\n                k = params['k']\n                x = range(n + 1)\n                from math import comb\n                pmf = [comb(n, i) * p ** i * (1 - p) ** (n - i) for i in x]\n                cdf = np.cumsum(pmf)\n                plt.figure(figsize=(10, 5))\n                plt.subplot(1, 2, 1)\n                plt.bar(x, pmf, color='skyblue')\n                plt.title(f'Binomial PMF n={n}, p={p}')\n                plt.xlabel('k')\n                plt.ylabel('P(X=k)')\n                plt.axvline(k, color='red', linestyle='--', label='Target k')\n                plt.legend()\n                plt.subplot(1, 2, 2)\n                plt.step(x, cdf, where='post', color='green')\n                plt.title('CDF')\n                plt.xlabel('k')\n                plt.ylabel('P(X<=k)')\n                plt.axvline(k, color='red', linestyle='--')\n                plt.tight_layout()\n                self._safe_savefig(output_path)\n                return True\n            elif ptype == 'poisson':\n                lam = params['lambda']\n                k = params['k']\n                x = range(k + 10 + 1)\n                pmf = [lam ** i * exp(-lam) / factorial(i) for i in x]\n                cdf = np.cumsum(pmf)\n                plt.figure(figsize=(10, 5))\n                plt.subplot(1, 2, 1)\n                plt.bar(x, pmf, color='orange')\n                plt.title(f'Poisson(lambda={lam}) PMF')\n                plt.xlabel('k')\n                plt.ylabel('P(X=k)')\n                plt.axvline(k, color='red', linestyle='--', label='Target k')\n                plt.legend()\n                plt.subplot(1, 2, 2)\n                plt.step(x, cdf, where='post', color='green')\n                plt.title('CDF')\n                plt.xlabel('k')\n                plt.ylabel('P(X<=k)')\n                plt.axvline(k, color='red', linestyle='--')\n                plt.tight_layout()\n                self._safe_savefig(output_path)\n                return True\n            elif ptype == 'conditional_probability':\n                P_A = params['P_A']\n                P_BA = params['P_B_given_A']\n                P_AB = params['P_A_and_B']\n                plt.figure()\n                vals = [P_A, P_BA, P_AB]\n                labels = ['P(A)', 'P(B|A)', 'P(A∩B)']\n                plt.bar(labels, vals, color=['blue', 'green', 'red'])\n                plt.title('Conditional Probability Visualization')\n                plt.ylabel('Probability')\n                plt.tight_layout()\n                self._safe_savefig(output_path)\n                return True\n            else:\n                return False\n        except Exception as e:\n            logging.error(f'Error in plot_probability: {e}')\n            plt.close()\n            return False\n\n    def process_file(self, params, output_path):\n        try:\n            alpha = params['alpha']\n            df = params['df']\n            t_stat = params['t_stat']\n            critical_value = params['critical_value']\n            x = np.linspace(t_dist.ppf(0.001, df), t_dist.ppf(0.999, df), 1000)\n            y = t_dist.pdf(x, df)\n            plt.figure(figsize=(10, 5))\n            plt.plot(x, y, 'b-', label=f't-dist(df={df})')\n            plt.axvline(x=critical_value, color='r', linestyle='--', label='critical +')\n            plt.axvline(x=-critical_value, color='r', linestyle='--', label='critical -')\n            plt.axvline(x=t_stat, color='g', label='t-stat')\n            p_area_x = x[x > critical_value]\n            plt.fill_between(p_area_x, t_dist.pdf(p_area_x, df), color='red', alpha=0.3)\n            p_area_x2 = x[x < -critical_value]\n            plt.fill_between(p_area_x2, t_dist.pdf(p_area_x2, df), color='red', alpha=0.3)\n            plt.title('t-test visualization')\n            plt.xlabel('t')\n            plt.ylabel('pdf')\n            plt.legend()\n            plt.tight_layout()\n            self._safe_savefig(output_path)\n            return True\n        except Exception as e:\n            logging.error(f't検定グラフ生成エラー: {e}')\n            plt.close()\n            return False\n\n    def process_file(self, x_values, y_values, beta_0_hat, beta_1_hat, output_path):\n        try:\n            import statsmodels.api as sm\n            X = sm.add_constant(x_values)\n            model = sm.OLS(y_values, X).fit()\n            residuals = model.resid\n            (fig, axes) = plt.subplots(2, 2, figsize=(10, 10))\n            ax = axes[0, 0]\n            ax.scatter(x_values, y_values, color='blue', label='data')\n            x_line = np.linspace(min(x_values), max(x_values), 100)\n            y_line = beta_0_hat + beta_1_hat * x_line\n            ax.plot(x_line, y_line, color='red', label='reg line')\n            ax.set_title('Data & Regression Line')\n            ax.set_xlabel('X')\n            ax.set_ylabel('Y')\n            ax.legend()\n            ax = axes[0, 1]\n            ax.hist(residuals, bins=20, color='green', alpha=0.7)\n            ax.set_title('Residual Histogram')\n            ax.set_xlabel('Residual')\n            ax.set_ylabel('Frequency')\n            sm.qqplot(residuals, line='45', ax=axes[1, 0], color='purple')\n            axes[1, 0].set_title('Q-Q plot of Residuals')\n            fitted = model.fittedvalues\n            ax = axes[1, 1]\n            ax.scatter(fitted, residuals, color='orange')\n            ax.axhline(y=0, color='red', linestyle='--')\n            ax.set_title('Residuals vs Fitted')\n            ax.set_xlabel('Fitted')\n            ax.set_ylabel('Residuals')\n            fig.tight_layout()\n            self._safe_savefig(output_path)\n            return True\n        except Exception as e:\n            logging.error(f'回帰分析グラフ生成中にエラー: {e}')\n            plt.close()\n            return False\n\n    def process_file(self, params, output_path):\n        try:\n            ts = params['time_series']\n            (fig, axes) = plt.subplots(2, 1, figsize=(10, 8))\n            axes[0].plot(ts, color='blue')\n            axes[0].set_title('Time Series Data')\n            axes[0].set_xlabel('Time')\n            axes[0].set_ylabel('Value')\n            plot_acf(ts, ax=axes[1])\n            axes[1].set_title('Autocorrelation Function')\n            fig.tight_layout()\n            self._safe_savefig(output_path)\n            return True\n        except Exception as e:\n            logging.error(f'時系列分析グラフエラー: {e}')\n            plt.close()\n            return False\n\n    def process_file(self, params, output_path):\n        try:\n            import statsmodels.api as sm\n            X = np.column_stack((params['x1_values'], params['x2_values']))\n            Y = np.array(params['y_values'])\n            Xc = sm.add_constant(X)\n            model = sm.OLS(Y, Xc).fit()\n            residuals = model.resid\n            fitted = model.fittedvalues\n            (fig, axes) = plt.subplots(2, 2, figsize=(10, 10))\n            ax = axes[0, 0]\n            ax.scatter(params['x1_values'], Y, color='blue', alpha=0.7, label='X1-Y')\n            ax.set_title('X1 vs Y')\n            ax.set_xlabel('X1')\n            ax.set_ylabel('Y')\n            ax = axes[0, 1]\n            ax.scatter(params['x2_values'], Y, color='green', alpha=0.7, label='X2-Y')\n            ax.set_title('X2 vs Y')\n            ax.set_xlabel('X2')\n            ax.set_ylabel('Y')\n            ax = axes[1, 0]\n            ax.hist(residuals, bins=20, color='gray', alpha=0.7)\n            ax.set_title('Residuals Histogram')\n            ax.set_xlabel('Residual')\n            ax = axes[1, 1]\n            ax.scatter(fitted, residuals, color='red', alpha=0.7)\n            ax.axhline(y=0, color='black', linestyle='--')\n            ax.set_title('Residuals vs Fitted')\n            ax.set_xlabel('Fitted')\n            ax.set_ylabel('Residuals')\n            fig.tight_layout()\n            self._safe_savefig(output_path)\n            return True\n        except Exception as e:\n            logging.error(f'計量経済学グラフエラー: {e}')\n            plt.close()\n            return False\n\n    def process_file(self, params, output_path):\n        try:\n            mu = params['mu']\n            sigma = np.array(params['sigma'])\n            fig = plt.figure(figsize=(10, 10))\n            from matplotlib.patches import Ellipse\n            import matplotlib.transforms as transforms\n\n            def create_problem(mu, cov, ax, n_std=1.96, facecolor='none', **kwargs):\n                pearson = cov[0, 1] / np.sqrt(cov[0, 0] * cov[1, 1])\n                ell_radius_x = np.sqrt(1 + pearson)\n                ell_radius_y = np.sqrt(1 - pearson)\n                ellipse = Ellipse((0, 0), width=ell_radius_x * 2, height=ell_radius_y * 2, facecolor=facecolor, **kwargs)\n                scale_x = np.sqrt(cov[0, 0]) * n_std\n                scale_y = np.sqrt(cov[1, 1]) * n_std\n                transf = transforms.Affine2D().rotate_deg(45).scale(scale_x, scale_y).translate(mu[0], mu[1])\n                ellipse.set_transform(transf + ax.transData)\n                return ax.add_patch(ellipse)\n            ax = fig.add_subplot(2, 2, 1)\n            ax.set_title('Confidence Ellipse')\n            create_problem(mu, sigma, ax, edgecolor='red')\n            ax.scatter(mu[0], mu[1], c='blue', marker='x', label='mean')\n            ax.legend()\n            ax.set_xlabel('X1')\n            ax.set_ylabel('X2')\n            ax2 = fig.add_subplot(2, 2, 2)\n            x = np.linspace(mu[0] - 4 * np.sqrt(sigma[0, 0]), mu[0] + 4 * np.sqrt(sigma[0, 0]), 100)\n            y = np.linspace(mu[1] - 4 * np.sqrt(sigma[1, 1]), mu[1] + 4 * np.sqrt(sigma[1, 1]), 100)\n            (X, Y) = np.meshgrid(x, y)\n            pos = np.dstack((X, Y))\n\n            def mvn_pdf(xarr, muarr, cov):\n                det = np.linalg.det(cov)\n                inv = np.linalg.inv(cov)\n                diff = xarr - muarr\n                return 1.0 / (2 * np.pi * np.sqrt(det)) * np.exp(-0.5 * (diff @ inv @ diff.T))\n            Z = np.empty(X.shape)\n            for i in range(X.shape[0]):\n                for j in range(X.shape[1]):\n                    Z[i, j] = mvn_pdf(np.array([X[i, j], Y[i, j]]), np.array(mu), sigma)\n            ax2.contour(X, Y, Z, levels=5, cmap='Blues')\n            ax2.set_title('Contour')\n            ax3 = fig.add_subplot(2, 2, 3)\n            X_marg = norm(loc=mu[0], scale=np.sqrt(sigma[0, 0]))\n            x_line = np.linspace(mu[0] - 4 * np.sqrt(sigma[0, 0]), mu[0] + 4 * np.sqrt(sigma[0, 0]), 100)\n            y_line = X_marg.pdf(x_line)\n            ax3.plot(x_line, y_line, 'r-')\n            ax3.set_title('Marginal X1 distribution')\n            ax3.set_xlabel('X1')\n            ax3.set_ylabel('pdf')\n            ax4 = fig.add_subplot(2, 2, 4)\n            Y_marg = norm(loc=mu[1], scale=np.sqrt(sigma[1, 1]))\n            y_line = Y_marg.pdf(x_line)\n            ax4.plot(x_line, y_line, 'g-')\n            ax4.set_title('Marginal X2 distribution')\n            ax4.set_xlabel('X2')\n            ax4.set_ylabel('pdf')\n            fig.tight_layout()\n            self._safe_savefig(output_path)\n            return True\n        except Exception as e:\n            logging.error(f'多変量正規分布グラフエラー: {e}')\n            plt.close()\n            return False\n\n    def process_file(self, params, output_path):\n        try:\n            dist = params['distribution']\n            plt.figure(figsize=(10, 5))\n            if dist == '正規分布':\n                mu = 0\n                sigma = 1\n                x = np.linspace(mu - 4 * sigma, mu + 4 * sigma, 200)\n                y = norm.pdf(x, mu, sigma)\n                plt.plot(x, y, 'b-')\n                plt.axvline(mu, color='r', linestyle='--', label='mean')\n                plt.axvline(mu + sigma, color='g', linestyle=':', label='mean+sigma')\n                plt.axvline(mu - sigma, color='g', linestyle=':')\n                plt.title('Normal Distribution (mu=0, sigma=1)')\n                plt.legend()\n            elif dist == 'ポアソン分布':\n                lam = 3\n                x = np.arange(0, 15)\n                y = poisson.pmf(x, lam)\n                plt.bar(x, y, color='skyblue')\n                plt.axvline(lam, color='r', linestyle='--', label='mean=lambda=3')\n                plt.title('Poisson(lambda=3)')\n                plt.legend()\n            elif dist == '指数分布':\n                lam = 1\n                x = np.linspace(0, 5, 200)\n                y = expon.pdf(x, scale=1 / lam)\n                plt.plot(x, y, 'b-')\n                plt.axvline(1 / lam, color='r', linestyle='--', label='mean=1/lambda')\n                plt.title('Exponential(lambda=1)')\n                plt.legend()\n            plt.tight_layout()\n            self._safe_savefig(output_path)\n            return True\n        except Exception as e:\n            logging.error(f'分布性質グラフエラー: {e}')\n            plt.close()\n            return False\n\n    def process_file(self, params, output_path):\n        try:\n            mu = params['mu']\n            sigma = params['sigma']\n            n = params['n']\n            moment = params['moment']\n            x = np.linspace(mu - 4 * sigma, mu + 4 * sigma, 200)\n            y = norm.pdf(x, mu, sigma)\n            plt.figure(figsize=(10, 5))\n            plt.plot(x, y, 'b-', label='Normal pdf')\n            plt.axvline(mu, color='r', linestyle='--', label='mean')\n            plt.axvline(mu + sigma, color='g', linestyle=':', label='mu±sigma')\n            plt.axvline(mu - sigma, color='g', linestyle=':')\n            plt.title(f'Normal distribution (mu={mu}, sigma={sigma}). n={n}-th moment={moment}')\n            plt.legend()\n            plt.tight_layout()\n            self._safe_savefig(output_path)\n            return True\n        except Exception as e:\n            logging.error(f'高次モーメントグラフエラー: {e}')\n            plt.close()\n            return False\n\n    def process_file(self, params, output_path):\n        try:\n            group_count = params['group_count']\n            sample_sizes = params['sample_sizes']\n            means = params['means']\n            variances = params['variances']\n            data = []\n            for i in range(group_count):\n                np.random.seed(i)\n                samples = np.random.normal(means[i], np.sqrt(variances[i]), sample_sizes[i])\n                data.append(samples)\n            plt.figure(figsize=(8, 6))\n            plt.boxplot(data, labels=[f'Group{i + 1}' for i in range(group_count)])\n            plt.title('ANOVA: Boxplots of groups')\n            plt.ylabel('Value')\n            plt.tight_layout()\n            self._safe_savefig(output_path)\n            return True\n        except Exception as e:\n            logging.error(f'分散分析グラフエラー: {e}')\n            plt.close()\n            return False\n\n    def process_file(self, params, output_path):\n        try:\n            s1 = params['sample1']\n            s2 = params['sample2']\n\n            def extract_text(data):\n                d_sorted = np.sort(data)\n                y = np.arange(1, len(d_sorted) + 1) / len(d_sorted)\n                return (d_sorted, y)\n            (x1, y1) = extract_text(s1)\n            (x2, y2) = extract_text(s2)\n            plt.figure(figsize=(8, 6))\n            plt.step(x1, y1, where='post', label='Sample1 ECDF', color='blue')\n            plt.step(x2, y2, where='post', label='Sample2 ECDF', color='red')\n            plt.title('Nonparametric Test Visualization')\n            plt.xlabel('Value')\n            plt.ylabel('ECDF')\n            plt.legend()\n            plt.tight_layout()\n            self._safe_savefig(output_path)\n            return True\n        except Exception as e:\n            logging.error(f'ノンパラ検定グラフエラー: {e}')\n            plt.close()\n            return False\n\n    def process_file(self, params, output_path):\n        try:\n            a = params['a']\n            b = params['b']\n            mu1 = params['mu1']\n            mu2 = params['mu2']\n            sig1 = params['sigma1_squared']\n            sig2 = params['sigma2_squared']\n            np.random.seed(123)\n            x = np.random.normal(mu1, np.sqrt(sig1), 1000)\n            y = np.random.normal(mu2, np.sqrt(sig2), 1000)\n            Z = a * x + b * y\n            plt.figure(figsize=(8, 6))\n            plt.hist(Z, bins=30, density=True, alpha=0.7, color='purple', label='Simulated Z')\n            E_Z = a * mu1 + b * mu2\n            Var_Z = a ** 2 * sig1 + b ** 2 * sig2\n            X_line = np.linspace(E_Z - 4 * np.sqrt(Var_Z), E_Z + 4 * np.sqrt(Var_Z), 200)\n            Y_line = norm.pdf(X_line, E_Z, np.sqrt(Var_Z))\n            plt.plot(X_line, Y_line, 'r-', label='Theoretical PDF')\n            plt.title('Linear Combination Distribution')\n            plt.xlabel('Z')\n            plt.ylabel('Density')\n            plt.legend()\n            plt.tight_layout()\n            self._safe_savefig(output_path)\n            return True\n        except Exception as e:\n            logging.error(f'線形結合グラフエラー: {e}')\n            plt.close()\n            return False"
    },
    {
      "path": "your_project\\obfuscated_latest\\統計検定1級\\gui.py",
      "overview": "Pythonコード。\nクラス: is_binary_file。\n関数: _variant_poisson, run_command_in_env。\n",
      "content": "import tkinter as tk\n\nclass is_binary_file:\n\n    def _variant_poisson(self):\n        self.root = tk.Tk()\n        self.root.title('統計検定1級 インタラクティブ問題解答システム')\n\n    def run_command_in_env(self):\n        self.root.mainloop()"
    },
    {
      "path": "your_project\\obfuscated_latest\\統計検定1級\\main.py",
      "overview": "Pythonコード。\n関数: mvn_pdf。\n",
      "content": "import sys\nfrom main_app import MainApp\nfrom gui import InteractiveSolverGUI\nfrom config import config\n\ndef mvn_pdf():\n    pdf_gen = config.get('pdf_generation', default={})\n    problem_count = pdf_gen.get('problem_count', 9)\n    if len(sys.argv) > 1 and sys.argv[1] == '--generate-pdf':\n        app = mvn_pdf()\n        app.generate_and_compile(problem_count)\n    else:\n        gui = is_binary_file()\n        gui.run()\nif __name__ == '__main__':\n    mvn_pdf()"
    },
    {
      "path": "your_project\\obfuscated_latest\\統計検定1級\\main_app.py",
      "overview": "Pythonコード。\nクラス: mvn_pdf。\n関数: _variant_poisson, get, get, create_problem。\n",
      "content": "import os, subprocess\nfrom config import config\nfrom problem_generator import ProblemGenerator\nimport logging\nimport traceback\n\nclass mvn_pdf:\n\n    def _variant_poisson(self):\n        self.output_tex_file = config.get('output_tex_file', default='practice_problems.tex')\n        if self.output_tex_file is None:\n            self.output_tex_file = 'practice_problems.tex'\n        self.enable_visualization = config.get('enable_visualization', default=True)\n        self.problem_templates_dir = config.get('problem_templates_directory', default='templates')\n        self.output_directory = config.get('output_directory', default='outputs')\n        self.latex_path = config.get('latex_path', default='xelatex')\n        self.generator = process_file()\n\n    def get(self):\n        cjk_font = config.get('cjk_main_font', default='Yu Gothic')\n        if cjk_font is None:\n            cjk_font = 'Yu Gothic'\n        header = ['\\\\documentclass{article}', '\\\\usepackage{amsmath}', '\\\\usepackage{amssymb}', '\\\\usepackage{graphicx}', '\\\\usepackage{float}', '\\\\usepackage{geometry}', '\\\\usepackage{xeCJK}', '\\\\usepackage{fontspec}', '\\\\setmainfont{Times New Roman}', f'\\\\setCJKmainfont{{{cjk_font}}}', '\\\\geometry{a4paper, margin=1in}', '\\\\begin{document}']\n        return header\n\n    def get(self, problem_count):\n        script_dir = os.path.dirname(os.path.abspath(__file__))\n        latex_content = self.generate_latex_header()\n        out_dir = self.output_directory\n        if out_dir is None:\n            out_dir = 'outputs'\n        output_dir = os.path.join(script_dir, out_dir)\n        os.makedirs(output_dir, exist_ok=True)\n        tex_file_path = os.path.join(output_dir, self.output_tex_file)\n        for idx in range(1, problem_count + 1):\n            try:\n                result = self.generator.generate_problem()\n                if result:\n                    (problem_id, date_created, problem_type, problem_text, solution_text, graph_filename) = result\n                    latex_content.append(f'\\\\section*{{問題 {idx}}}')\n                    latex_content.append(problem_text)\n                    if self.enable_visualization and graph_filename:\n                        latex_content.append('\\\\begin{figure}[H]')\n                        latex_content.append('\\\\centering')\n                        graph_relative_path = os.path.join('graphs', graph_filename).replace('\\\\', '/')\n                        latex_content.append(f'\\\\includegraphics[width=0.8\\\\textwidth]{{{graph_relative_path}}}')\n                        latex_content.append('\\\\end{figure}')\n                    latex_content.append('\\\\subsection*{解答}')\n                    latex_content.append(solution_text)\n                    latex_content.append('\\\\newpage')\n                else:\n                    logging.warning(f'問題 {idx} の生成に失敗しました。')\n                    print(f'問題 {idx} の生成に失敗しました。')\n            except Exception as e:\n                logging.error(f'問題 {idx} の生成中にエラー: {e}')\n                logging.error(traceback.format_exc())\n                print(f'問題 {idx} 生成エラー。ログを確認')\n        templates_dir = self.problem_templates_dir\n        if templates_dir is None:\n            templates_dir = 'templates'\n        latex_content.append('\\\\clearpage')\n        latex_content.append('\\\\input{../' + templates_dir + '/distribution_relations.tex}')\n        latex_content.append('\\\\end{document}')\n        with open(tex_file_path, 'w', encoding='utf-8') as tex_file:\n            tex_file.write('\\n'.join(latex_content))\n        self.compile_latex(tex_file_path)\n\n    def create_problem(self, tex_file_path):\n        tex_file_name = os.path.basename(tex_file_path)\n        latex_path = self.latex_path\n        if not latex_path or not os.path.exists(latex_path):\n            print(f'LaTeXコンパイラが見つかりません: {latex_path}')\n            return\n        process = subprocess.run([latex_path, '-interaction=nonstopmode', tex_file_name], cwd=os.path.dirname(tex_file_path), stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n        log_file_path = os.path.join(os.path.dirname(tex_file_path), 'latex_compile.log')\n        with open(log_file_path, 'w', encoding='utf-8') as f:\n            f.write(process.stdout or '')\n            f.write(process.stderr or '')\n        if process.returncode != 0:\n            print('LaTeXコンパイルでエラー')\n        else:\n            pdf_file = tex_file_path.replace('.tex', '.pdf')\n            if os.path.exists(pdf_file):\n                print(f'PDF生成成功: {pdf_file}')\n            else:\n                print('PDFファイル未発見')"
    },
    {
      "path": "your_project\\obfuscated_latest\\統計検定1級\\problem_generator.py",
      "overview": "Pythonコード。\nクラス: process_file。\n関数: _variant_poisson, load_topics, get, get。\n",
      "content": "import random, os, logging, uuid\nfrom datetime import datetime\nfrom config import config\nfrom database import DatabaseManager\nfrom problem_types.problem_factory import ProblemFactory\nimport json\nimport traceback\n\nclass process_file:\n\n    def _variant_poisson(self):\n        self.db_path = config.get('database_path', default='data/data.db')\n        self.output_directory = config.get('output_directory', default='outputs')\n        self.enable_visualization = config.get('enable_visualization', default=True)\n        self.problem_types_weights = config.get('problem_types', default={})\n        self.factory = process_file()\n        self.db_manager = detect_symbol_collisions()\n        self.db_manager.setup_database()\n        script_dir = os.path.dirname(os.path.abspath(__file__))\n        out_dir = self.output_directory\n        if out_dir is None:\n            out_dir = 'outputs'\n        self.output_dir = os.path.join(script_dir, out_dir, 'graphs')\n        os.makedirs(self.output_dir, exist_ok=True)\n        self.topics_data = self.load_topics()\n\n    def load_topics(self):\n        script_dir = os.path.dirname(os.path.abspath(__file__))\n        topics_file = os.path.join(script_dir, 'topics.json')\n        if not os.path.exists(topics_file):\n            raise FileNotFoundError(f\"'{topics_file}'がない\")\n        with open(topics_file, 'r', encoding='utf-8') as f:\n            return json.load(f)\n\n    def get(self, topic):\n        return self.topics_data['topics'].get(topic, [])\n\n    def get(self, selected_topic=None):\n        try:\n            ptypes = self.problem_types_weights\n            if selected_topic:\n                problem_types = self.get_problem_types_by_topic(selected_topic)\n                if not problem_types:\n                    return None\n                problem_type = random.choice(problem_types)\n            else:\n                pts = list(ptypes.keys())\n                pwt = list(ptypes.values())\n                if not pts:\n                    pts = ['probability']\n                    pwt = [1.0]\n                problem_type = random.choices(pts, weights=pwt, k=1)[0]\n            problem = self.factory.create_problem(problem_type)\n            problem.generate_parameters()\n            problem_text = problem.generate_problem_text()\n            solution_text = problem.generate_solution_text()\n            enable_vis = self.enable_visualization\n            if enable_vis is None:\n                enable_vis = True\n            graph_filename = None\n            if enable_vis:\n                graph_filename = f'graph_{uuid.uuid4().hex}.png'\n                graph_filepath = os.path.join(self.output_dir, graph_filename)\n                if not problem.generate_graph(graph_filepath):\n                    graph_filename = None\n            problem_id = uuid.uuid4().hex\n            date_created = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n            self.db_manager.save_problem(problem_id, date_created, problem_text, solution_text, problem_type)\n            return (problem_id, date_created, problem_type, problem_text, solution_text, graph_filename)\n        except Exception as e:\n            logging.error(f'問題生成エラー: {e}')\n            logging.error(traceback.format_exc())\n            return None"
    },
    {
      "path": "your_project\\obfuscated_latest\\統計検定1級\\sympy_solver.py",
      "overview": "Pythonコード。\nクラス: setup_database。\n関数: _variant_poisson, create_problem。\n",
      "content": "class setup_database:\n\n    def _variant_poisson(self):\n        pass\n\n    def create_problem(self, user_input, correct_answer):\n        return (False, '')"
    },
    {
      "path": "your_project\\obfuscated_latest\\統計検定1級\\topics.json",
      "overview": "JSONファイル (辞書)。キー: topics\n",
      "content": "{\n  \"topics\": {\n    \"確率論\": [\n      \"probability_definition\",\n      \"conditional_probability\",\n      \"distribution_functions\",\n      \"joint_distribution\",\n      \"probability\"\n    ],\n    \"統計的推定\": [\n      \"statistical_inference\",\n      \"t_test\"\n    ],\n    \"統計的検定\": [\n      \"statistical_inference\",\n      \"t_test\"\n    ],\n    \"回帰分析\": [\n      \"regression_analysis\"\n    ],\n    \"分散分析\": [\n      \"variance_analysis\"\n    ],\n    \"ノンパラメトリック検定\": [\n      \"nonparametric_test\"\n    ]\n  }\n}\n"
    },
    {
      "path": "your_project\\obfuscated_latest\\統計検定1級\\.vscode\\launch.json",
      "overview": "JSONファイル (辞書)。キー: version, configurations\n",
      "content": "{\n    \"version\": \"0.2.0\",\n    \"configurations\": [\n        {\n            \"name\": \"Python: 現在のファイル\",\n            \"type\": \"python\",\n            \"request\": \"launch\",\n            \"program\": \"${file}\",\n            \"console\": \"integratedTerminal\",\n            \"justMyCode\": true,\n            \"args\": [\"--generate-pdf\"]\n\n        }\n    ]\n}"
    },
    {
      "path": "your_project\\obfuscated_latest\\統計検定1級\\problem_types\\conjugate_problems.py",
      "overview": "Pythonコード。\nクラス: build_project, get, detect_symbol_collisions, build_project, process_file。\n関数: _variant_poisson, get, get, get, _variant_poisson, get, get, get, _variant_poisson, get, get, get, _variant_poisson, get, get, get, _variant_poisson, get, get, get, build_project, build_project。\n",
      "content": "import math, random\nfrom problem_types.problem import Problem\nfrom math import comb, factorial, exp, gamma\nimport numpy as np\n\nclass build_project(process_file):\n\n    def _variant_poisson(self):\n        super().__init__('beta_binomial_conjugate_problem.tex')\n\n    def get(self):\n        self.params['alpha'] = random.randint(1, 5)\n        self.params['beta'] = random.randint(1, 5)\n        self.params['n'] = random.randint(5, 20)\n        self.params['k'] = random.randint(0, self.params['n'])\n\n        def build_project(x, y):\n            return gamma(x) * gamma(y) / gamma(x + y)\n        p_x = comb(self.params['n'], self.params['k']) * build_project(self.params['k'] + self.params['alpha'], self.params['n'] - self.params['k'] + self.params['beta']) / build_project(self.params['alpha'], self.params['beta'])\n        self.params['probability'] = round(p_x, 4)\n\n    def get(self):\n        return 'Beta+Binomial->Beta-Binomial'\n\n    def get(self, o):\n        return False\n\nclass get(process_file):\n\n    def _variant_poisson(self):\n        super().__init__('gamma_poisson_conjugate_problem.tex')\n\n    def get(self):\n        self.params['alpha'] = random.randint(1, 5)\n        self.params['beta'] = random.uniform(0.5, 2.0)\n        self.params['k'] = random.randint(0, 20)\n        p = self.params['beta'] / (self.params['beta'] + 1)\n        q = 1 - p\n        negbin_p = comb(self.params['k'] + self.params['alpha'] - 1, self.params['k']) * q ** self.params['k'] * p ** self.params['alpha']\n        self.params['probability'] = round(negbin_p, 4)\n\n    def get(self):\n        return 'Gamma+Poisson->Negative Binomial'\n\n    def get(self, o):\n        return False\n\nclass detect_symbol_collisions(process_file):\n\n    def _variant_poisson(self):\n        super().__init__('dirichlet_multinomial_conjugate_problem.tex')\n\n    def get(self):\n        m = random.randint(2, 4)\n        self.params['m'] = m\n        self.params['n'] = random.randint(5, 20)\n        self.params['alpha_vec'] = [random.uniform(1, 3) for _ in range(m)]\n        counts = [0] * m\n        remain = self.params['n']\n        for i in range(m - 1):\n            c = random.randint(0, remain)\n            counts[i] = c\n            remain -= c\n        counts[-1] = remain\n        self.params['counts'] = counts\n\n        def build_project(alpha):\n            import numpy as np\n            return np.prod([gamma(a) for a in alpha]) / gamma(sum(alpha))\n        alpha_x = [self.params['alpha_vec'][i] + counts[i] for i in range(m)]\n        num = build_project(alpha_x)\n        den = build_project(self.params['alpha_vec'])\n        multinomial_coef = math.factorial(self.params['n'])\n        for c in counts:\n            multinomial_coef /= math.factorial(c)\n        p_x = multinomial_coef * (num / den)\n        self.params['probability'] = round(p_x, 4)\n\n    def get(self):\n        return 'Dirichlet+Multinomial->Dirichlet-Multinomial'\n\n    def get(self, o):\n        return False\n\nclass build_project(process_file):\n\n    def _variant_poisson(self):\n        super().__init__('binomial_poisson_approx_problem.tex')\n\n    def get(self):\n        lam = random.uniform(2, 5)\n        n = random.randint(50, 200)\n        p = lam / n\n        k = random.randint(0, int(lam * 3))\n        binom_p = comb(n, k) * p ** k * (1 - p) ** (n - k)\n        poisson_p = lam ** k * exp(-lam) / math.factorial(k)\n        self.params['n'] = n\n        self.params['p'] = round(p, 6)\n        self.params['k'] = k\n        self.params['lambda'] = round(lam, 3)\n        self.params['binom_p'] = round(binom_p, 6)\n        self.params['poisson_p'] = round(poisson_p, 6)\n\n    def get(self):\n        return 'Binomial->Poisson近似条件'\n\n    def get(self, o):\n        return False\n\nclass process_file(process_file):\n\n    def _variant_poisson(self):\n        super().__init__('poisson_normal_approx_problem.tex')\n\n    def get(self):\n        lam = random.randint(30, 100)\n        low = max(0, int(lam - 3 * math.sqrt(lam)))\n        high = int(lam + 3 * math.sqrt(lam))\n        k = random.randint(low, high)\n        poisson_p = lam ** k * exp(-lam) / math.factorial(k)\n        self.params['lambda'] = lam\n        self.params['k'] = k\n        self.params['poisson_p'] = round(poisson_p, 6)\n        self.params['mean'] = lam\n        self.params['variance'] = lam\n\n    def get(self):\n        return 'Poisson->Normal近似(λ大)'\n\n    def get(self, o):\n        return False"
    },
    {
      "path": "your_project\\obfuscated_latest\\統計検定1級\\problem_types\\problem.py",
      "overview": "Pythonコード。\nクラス: process_file, process_file, setup_database, run_command_in_env, topic_must_not_be_empty, extract_text, load_topics, detect_symbol_collisions, h_sym, mvn_pdf, visualize, n_sym, process_file, create_problem, detect_symbol_collisions, j_sym。\n関数: _variant_poisson, get, get, get, get, get, _variant_poisson, get, _variant_poisson, _variant_poisson, _variant_poisson, get, get, _variant_poisson, get, get, _variant_poisson, get, get, _variant_poisson, get, get, _variant_poisson, get, get, _variant_poisson, get, get, get, _variant_poisson, get, get, _variant_poisson, get, get, _variant_poisson, get, get, get, _variant_poisson, get, get, get, _variant_poisson, get, get, get, _variant_poisson, get, get, get, _variant_poisson, get, get, get, _variant_poisson, get, get, get, _variant_poisson, get, get, get。\n",
      "content": "from abc import ABC, abstractmethod\nimport os\nimport random\nimport logging\nfrom config import config\nfrom jinja2 import Environment, FileSystemLoader\nfrom graph import ProbabilityDistributionVisualizer\nimport traceback\nfrom math import comb, exp, factorial\nfrom scipy.stats import norm, stats\n\nclass process_file(ABC):\n\n    def _variant_poisson(self, template_name):\n        self.params = {}\n        self.template_name = template_name\n        templates_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), '..', config.get('problem_templates_directory', default='templates'))\n        self.env = Environment(loader=FileSystemLoader(templates_dir), autoescape=False)\n        self.visualizer = process_file()\n\n    @abstractmethod\n    def get(self):\n        pass\n\n    def get(self):\n        template = self.env.get_template(self.template_name)\n        return template.render(**self.params, show_solution=False)\n\n    def get(self):\n        template = self.env.get_template(self.template_name)\n        self.params['explanation'] = self.generate_explanation()\n        return template.render(**self.params, show_solution=True)\n\n    def get(self):\n        return ''\n\n    @abstractmethod\n    def get(self, output_path):\n        pass\n\nclass process_file(process_file):\n\n    def _variant_poisson(self):\n        super().__init__('probability_problem.tex')\n\n    def get(self):\n        variants = [self._variant_binomial, self._variant_poisson, self._variant_conditional_probability]\n        v = random.choice(variants)\n        v()\n\n    def _variant_poisson(self):\n        self.params['problem_type'] = 'binomial'\n        self.params['n'] = random.randint(5, 20)\n        self.params['p'] = round(random.uniform(0.1, 0.9), 2)\n        self.params['k'] = random.randint(0, self.params['n'])\n        from math import comb\n        prob = comb(self.params['n'], self.params['k']) * self.params['p'] ** self.params['k'] * (1 - self.params['p']) ** (self.params['n'] - self.params['k'])\n        self.params['probability'] = round(prob, 6)\n\n    def _variant_poisson(self):\n        self.params['problem_type'] = 'poisson'\n        self.params['lambda'] = round(random.uniform(0.5, 5.0), 2)\n        self.params['k'] = random.randint(0, 10)\n        lam = self.params['lambda']\n        k = self.params['k']\n        prob = lam ** k * exp(-lam) / factorial(k)\n        self.params['probability'] = round(prob, 6)\n\n    def _variant_poisson(self):\n        self.params['problem_type'] = 'conditional_probability'\n        self.params['P_A'] = round(random.uniform(0.1, 0.9), 2)\n        self.params['P_B_given_A'] = round(random.uniform(0.1, 0.9), 2)\n        self.params['P_A_and_B'] = round(self.params['P_A'] * self.params['P_B_given_A'], 6)\n\n    def get(self):\n        t = self.params['problem_type']\n        if t == 'binomial':\n            return '二項分布の公式を使用'\n        elif t == 'poisson':\n            return 'ポアソン分布の公式を使用'\n        elif t == 'conditional_probability':\n            return '条件付き確率P(A∩B)=P(A)*P(B|A)'\n\n    def get(self, output_path):\n        return self.visualizer.plot_probability(self.params, output_path)\n\nclass setup_database(process_file):\n\n    def _variant_poisson(self):\n        super().__init__('statistical_inference_problem.tex')\n\n    def get(self):\n        self.params['sample_mean'] = round(random.uniform(50, 100), 2)\n        self.params['sample_std'] = round(random.uniform(5, 15), 2)\n        self.params['n'] = random.randint(30, 100)\n        self.params['population_mean'] = round(random.uniform(50, 100), 2)\n        self.params['alpha'] = round(random.uniform(0.01, 0.1), 2)\n        t_stat = (self.params['sample_mean'] - self.params['population_mean']) / (self.params['sample_std'] / self.params['n'] ** 0.5)\n        t_stat = round(t_stat, 4)\n        df = self.params['n'] - 1\n        cv = round(stats.t.ppf(1 - self.params['alpha'] / 2, df=df), 4)\n        reject = '棄却' if abs(t_stat) > cv else '棄却しない'\n        self.params['t_stat'] = t_stat\n        self.params['critical_value'] = cv\n        self.params['reject_null'] = reject\n        self.params['df'] = df\n\n    def get(self, output_path):\n        return self.visualizer.plot_t_test(self.params, output_path)\n\nclass run_command_in_env(process_file):\n\n    def _variant_poisson(self):\n        super().__init__('regression_analysis_problem.tex')\n\n    def get(self):\n        self.params['beta_0'] = round(random.uniform(0, 10), 2)\n        self.params['beta_1'] = round(random.uniform(-5, 5), 2)\n        self.params['n'] = random.randint(50, 200)\n        self.params['x_values'] = [round(random.uniform(0, 100), 2) for _ in range(self.params['n'])]\n        self.params['epsilon'] = [round(random.gauss(0, 10), 2) for _ in range(self.params['n'])]\n        self.params['y_values'] = [self.params['beta_0'] + self.params['beta_1'] * x + e for (x, e) in zip(self.params['x_values'], self.params['epsilon'])]\n        import numpy as np\n        X = np.array(self.params['x_values'])\n        Y = np.array(self.params['y_values'])\n        beta_1_hat = np.cov(X, Y, bias=True)[0, 1] / np.var(X)\n        beta_0_hat = np.mean(Y) - beta_1_hat * np.mean(X)\n        self.params['beta_0_hat'] = round(beta_0_hat, 4)\n        self.params['beta_1_hat'] = round(beta_1_hat, 4)\n\n    def get(self, output_path):\n        return self.visualizer.plot_regression(self.params['x_values'], self.params['y_values'], self.params['beta_0_hat'], self.params['beta_1_hat'], output_path)\n\nclass topic_must_not_be_empty(process_file):\n\n    def _variant_poisson(self):\n        super().__init__('time_series_analysis_problem.tex')\n\n    def get(self):\n        self.params['phi'] = round(random.uniform(0.5, 0.9), 2)\n        self.params['theta'] = round(random.uniform(-0.5, 0.5), 2)\n        self.params['n'] = 100\n        self.params['epsilon'] = [random.gauss(0, 1) for _ in range(self.params['n'])]\n        self.params['time_series'] = [0] * self.params['n']\n        for t in range(1, self.params['n']):\n            self.params['time_series'][t] = self.params['phi'] * self.params['time_series'][t - 1] + self.params['epsilon'][t] + self.params['theta'] * self.params['epsilon'][t - 1]\n\n    def get(self, output_path):\n        return self.visualizer.plot_time_series(self.params, output_path)\n\nclass extract_text(process_file):\n\n    def _variant_poisson(self):\n        super().__init__('econometrics_problem.tex')\n\n    def get(self):\n        self.params['beta_0'] = round(random.uniform(0, 5), 2)\n        self.params['beta_1'] = round(random.uniform(0, 1), 2)\n        self.params['beta_2'] = round(random.uniform(-1, 0), 2)\n        self.params['n'] = random.randint(50, 200)\n        self.params['x1_values'] = [round(random.uniform(0, 100), 2) for _ in range(self.params['n'])]\n        self.params['x2_values'] = [round(random.uniform(0, 100), 2) for _ in range(self.params['n'])]\n        self.params['epsilon'] = [round(random.gauss(0, 5), 2) for _ in range(self.params['n'])]\n        self.params['y_values'] = [self.params['beta_0'] + self.params['beta_1'] * x1 + self.params['beta_2'] * x2 + e for (x1, x2, e) in zip(self.params['x1_values'], self.params['x2_values'], self.params['epsilon'])]\n        import numpy as np\n        X = np.column_stack((np.ones(self.params['n']), self.params['x1_values'], self.params['x2_values']))\n        Y = np.array(self.params['y_values'])\n        beta_hat = np.linalg.inv(X.T @ X) @ X.T @ Y\n        self.params['beta_0_hat'] = round(beta_hat[0], 4)\n        self.params['beta_1_hat'] = round(beta_hat[1], 4)\n        self.params['beta_2_hat'] = round(beta_hat[2], 4)\n\n    def get(self, output_path):\n        return self.visualizer.plot_econometrics(self.params, output_path)\n\nclass load_topics(process_file):\n\n    def _variant_poisson(self):\n        super().__init__('linear_combination_problem.tex')\n\n    def get(self):\n        self.params['a'] = random.randint(1, 5)\n        self.params['b'] = random.randint(1, 5)\n        self.params['mu1'] = round(random.uniform(0, 10), 2)\n        self.params['mu2'] = round(random.uniform(0, 10), 2)\n        self.params['sigma1_squared'] = round(random.uniform(1, 5), 2)\n        self.params['sigma2_squared'] = round(random.uniform(1, 5), 2)\n        E_Z = self.params['a'] * self.params['mu1'] + self.params['b'] * self.params['mu2']\n        Var_Z = self.params['a'] ** 2 * self.params['sigma1_squared'] + self.params['b'] ** 2 * self.params['sigma2_squared']\n        self.params['E_Z'] = round(E_Z, 4)\n        self.params['Var_Z'] = round(Var_Z, 4)\n\n    def get(self):\n        return '線形結合の期待値・分散計算'\n\n    def get(self, output_path):\n        return self.visualizer.plot_linear_combination(self.params, output_path)\n\nclass detect_symbol_collisions(process_file):\n\n    def _variant_poisson(self):\n        super().__init__('distribution_properties_problem.tex')\n\n    def get(self):\n        dist_choice = random.choice(['正規分布', 'ポアソン分布', '指数分布'])\n        self.params['distribution'] = dist_choice\n        if dist_choice == '正規分布':\n            self.params['properties'] = {'mean': '\\\\mu', 'variance': '\\\\sigma^2'}\n        elif dist_choice == 'ポアソン分布':\n            self.params['properties'] = {'mean': '\\\\lambda', 'variance': '\\\\lambda'}\n        else:\n            self.params['properties'] = {'mean': '1/\\\\lambda', 'variance': '1/\\\\lambda^2'}\n\n    def get(self, output_path):\n        return self.visualizer.plot_distribution_properties(self.params, output_path)\n\nclass h_sym(process_file):\n\n    def _variant_poisson(self):\n        super().__init__('high_moment_problem.tex')\n\n    def get(self):\n        self.params['n'] = random.randint(3, 5)\n        self.params['mu'] = round(random.uniform(0, 10), 2)\n        self.params['sigma'] = round(random.uniform(1, 5), 2)\n        from scipy.stats import norm\n        m = norm.moment(self.params['n'], loc=self.params['mu'], scale=self.params['sigma'])\n        self.params['moment'] = round(m, 4)\n\n    def get(self, output_path):\n        return self.visualizer.plot_high_moment(self.params, output_path)\n\nclass mvn_pdf(process_file):\n\n    def _variant_poisson(self):\n        super().__init__('multivariate_normal_problem.tex')\n\n    def get(self):\n        self.params['mu'] = [round(random.uniform(0, 10), 2) for _ in range(2)]\n        self.params['sigma'] = [[round(random.uniform(1, 5), 2), round(random.uniform(0, 2), 2)], [round(random.uniform(0, 2), 2), round(random.uniform(1, 5), 2)]]\n\n    def get(self):\n        return '多変量正規分布の性質を利用'\n\n    def get(self, output_path):\n        return self.visualizer.plot_multivariate_normal(self.params, output_path)\n\nclass visualize(process_file):\n\n    def _variant_poisson(self):\n        super().__init__('variance_analysis_problem.tex')\n\n    def get(self):\n        self.params['group_count'] = random.randint(2, 4)\n        self.params['sample_sizes'] = [random.randint(5, 20) for _ in range(self.params['group_count'])]\n        self.params['means'] = [round(random.uniform(10, 50), 2) for _ in range(self.params['group_count'])]\n        self.params['variances'] = [round(random.uniform(1, 5), 2) for _ in range(self.params['group_count'])]\n        total_n = sum(self.params['sample_sizes'])\n        group_count = self.params['group_count']\n        means = self.params['means']\n        variances = self.params['variances']\n        sample_sizes = self.params['sample_sizes']\n        grand_mean = sum([means[i] * sample_sizes[i] for i in range(group_count)]) / total_n\n        ssb = sum([sample_sizes[i] * (means[i] - grand_mean) ** 2 for i in range(group_count)])\n        ssw = sum([(sample_sizes[i] - 1) * variances[i] for i in range(group_count)])\n        df_between = group_count - 1\n        df_within = total_n - group_count\n        msb = ssb / df_between\n        msw = ssw / df_within\n        F = msb / msw\n        alpha = 0.05\n        from scipy.stats import f\n        F_critical = f.ppf(1 - alpha, df_between, df_within)\n        reject = '棄却する' if F > F_critical else '棄却しない'\n        self.params['F_value'] = round(F, 4)\n        self.params['F_critical'] = round(F_critical, 4)\n        self.params['reject_null'] = reject\n        self.params['df_between'] = df_between\n        self.params['df_within'] = df_within\n\n    def get(self):\n        return '一元配置分散分析による検定'\n\n    def get(self, output_path):\n        return self.visualizer.plot_variance_analysis(self.params, output_path)\n\nclass n_sym(process_file):\n\n    def _variant_poisson(self):\n        super().__init__('nonparametric_test_problem.tex')\n\n    def get(self):\n        self.params['test_type'] = random.choice(['Mann-Whitney U', 'Kruskal-Wallis', 'Wilcoxon Signed-Rank'])\n        self.params['sample1'] = [round(random.uniform(10, 100), 2) for _ in range(random.randint(5, 20))]\n        self.params['sample2'] = [round(random.uniform(10, 100), 2) for _ in range(random.randint(5, 20))]\n        self.params['test_result'] = '有意差なし(例)'\n\n    def get(self):\n        return 'ノンパラ検定で中央値差を検定'\n\n    def get(self, output_path):\n        return self.visualizer.plot_nonparametric_test(self.params, output_path)\n\nclass process_file(process_file):\n\n    def _variant_poisson(self):\n        super().__init__('probability_definition_problem.tex')\n\n    def get(self):\n        self.params['P_A'] = round(random.uniform(0.1, 0.9), 2)\n        self.params['P_B'] = round(random.uniform(0.1, 0.9), 2)\n        self.params['P_A_and_B'] = round(self.params['P_A'] * self.params['P_B'], 4)\n\n    def get(self):\n        return '独立性利用 P(A∩B)=P(A)*P(B)'\n\n    def get(self, output_path):\n        try:\n            PA = self.params['P_A']\n            PB = self.params['P_B']\n            PAB = self.params['P_A_and_B']\n            plt.figure()\n            plt.bar(['P(A)', 'P(B)', 'P(A∩B)'], [PA, PB, PAB], color=['blue', 'green', 'red'])\n            plt.title('Probability Definition')\n            plt.tight_layout()\n            plt.savefig(output_path)\n            plt.close()\n            return True\n        except:\n            return False\n\nclass create_problem(process_file):\n\n    def _variant_poisson(self):\n        super().__init__('conditional_probability_problem.tex')\n\n    def get(self):\n        self.params['P_A'] = round(random.uniform(0.2, 0.8), 2)\n        self.params['P_B_given_A'] = round(random.uniform(0.2, 0.8), 2)\n        self.params['P_A_and_B'] = round(self.params['P_A'] * self.params['P_B_given_A'], 4)\n\n    def get(self):\n        return 'P(A∩B)=P(A)*P(B|A)'\n\n    def get(self, output_path):\n        from graph import ProbabilityDistributionVisualizer\n        vis = process_file()\n        return vis.plot_probability(self.params, output_path)\n\nclass detect_symbol_collisions(process_file):\n\n    def _variant_poisson(self):\n        super().__init__('distribution_functions_problem.tex')\n\n    def get(self):\n        self.params['function_type'] = random.choice(['pdf', 'cdf'])\n        self.params['distribution'] = random.choice(['正規分布', '指数分布'])\n        if self.params['distribution'] == '正規分布':\n            self.params['mean'] = round(random.uniform(-5, 5), 2)\n            self.params['std'] = round(random.uniform(1, 3), 2)\n        else:\n            self.params['lambda'] = round(random.uniform(0.5, 2.0), 2)\n\n    def get(self):\n        return 'pdfやcdf定義式利用'\n\n    def get(self, output_path):\n        return False\n\nclass j_sym(process_file):\n\n    def _variant_poisson(self):\n        super().__init__('joint_distribution_problem.tex')\n\n    def get(self):\n        A_and_B = round(random.uniform(0.05, 0.2), 2)\n        A_and_notB = round(random.uniform(0.05, 0.2), 2)\n        notA_and_B = round(random.uniform(0.05, 0.2), 2)\n        notA_and_notB = round(random.uniform(0.05, 0.2), 2)\n        total = A_and_B + A_and_notB + notA_and_B + notA_and_notB\n        A_and_B /= total\n        A_and_notB /= total\n        notA_and_B /= total\n        notA_and_notB /= total\n        self.params['joint_probabilities'] = {'A_and_B': round(A_and_B, 4), 'A_and_not_B': round(A_and_notB, 4), 'not_A_and_B': round(notA_and_B, 4), 'not_A_and_not_B': round(notA_and_notB, 4)}\n        P_A = A_and_B + A_and_notB\n        P_B = A_and_B + notA_and_B\n        P_BA = A_and_B / P_A if P_A > 0 else 0.0\n        self.params['P_A'] = round(P_A, 4)\n        self.params['P_B'] = round(P_B, 4)\n        self.params['P_B_given_A'] = round(P_BA, 4)\n\n    def get(self):\n        return '同時→周辺→条件付き確率'\n\n    def get(self, output_path):\n        try:\n            p = self.params['joint_probabilities']\n            matrix = np.array([[p['A_and_B'], p['A_and_not_B']], [p['not_A_and_B'], p['not_A_and_not_B']]])\n            plt.figure()\n            plt.imshow(matrix, cmap='Blues', interpolation='nearest')\n            plt.colorbar(label='Probability')\n            plt.xticks([0, 1], ['B', 'not B'])\n            plt.yticks([0, 1], ['A', 'not A'])\n            plt.title('Joint Distribution Heatmap')\n            plt.tight_layout()\n            plt.savefig(output_path)\n            plt.close()\n            return True\n        except:\n            return False"
    },
    {
      "path": "your_project\\obfuscated_latest\\統計検定1級\\problem_types\\problem_factory.py",
      "overview": "Pythonコード。\nクラス: process_file。\n関数: _variant_poisson, create_problem。\n",
      "content": "import logging, traceback\nfrom problem_types.problem import ProbabilityProblem, StatisticalInferenceProblem, RegressionAnalysisProblem, TimeSeriesAnalysisProblem, EconometricsProblem, LinearCombinationProblem, DistributionPropertiesProblem, HighMomentProblem, MultivariateNormalProblem, VarianceAnalysisProblem, NonParametricTestProblem, ProbabilityDefinitionProblem, ConditionalProbabilityProblem, DistributionFunctionsProblem, JointDistributionProblem\nfrom problem_types.conjugate_problems import BetaBinomialConjugateProblem, GammaPoissonConjugateProblem, DirichletMultinomialConjugateProblem, BinomialPoissonApproxProblem, PoissonNormalApproxProblem\n\nclass process_file:\n\n    def _variant_poisson(self):\n        self.problem_classes = {'probability': process_file, 'statistical_inference': setup_database, 'regression_analysis': run_command_in_env, 'time_series_analysis': topic_must_not_be_empty, 'econometrics': extract_text, 'linear_combination': load_topics, 'distribution_properties': detect_symbol_collisions, 'high_moment': h_sym, 'multivariate_normal': mvn_pdf, 'probability_definition': process_file, 'conditional_probability': create_problem, 'distribution_functions': detect_symbol_collisions, 'joint_distribution': j_sym, 't_test': setup_database, 'variance_analysis': visualize, 'nonparametric_test': n_sym, 'beta_binomial_conjugate': build_project, 'gamma_poisson_conjugate': get_value, 'dirichlet_multinomial_conjugate': detect_symbol_collisions, 'binomial_poisson_approx': build_project, 'poisson_normal_approx': process_file}\n\n    def create_problem(self, problem_type):\n        pc = self.problem_classes.get(problem_type)\n        if pc:\n            try:\n                return pc()\n            except Exception as e:\n                logging.error(f'{problem_type} problem generation error:{e}')\n                logging.error(traceback.format_exc())\n                raise\n        else:\n            raise ValueError(f'Unknown problem type:{problem_type}')"
    },
    {
      "path": "your_project\\obfuscated_latest\\統計検定1級\\templates\\distribution_properties_problem.tex",
      "overview": "TeXファイル。行数: 7\n",
      "content": "% distribution_properties_problem.tex\n{% if not show_solution %}\n{{ distribution }} の平均と分散を求めよ。\n{% else %}\n平均: ${{ properties.mean }}$, 分散: ${{ properties.variance }}$\n{{ explanation }}\n{% endif %}\n"
    },
    {
      "path": "your_project\\obfuscated_latest\\統計検定1級\\templates\\distribution_relations.tex",
      "overview": "TeXファイル。行数: 7\n",
      "content": "% distribution_relations.tex\n\\section*{Distribution Relations}\n- Beta+Binomial -> Beta-Binomial\n- Gamma+Poisson -> Negative Binomial\n- Dirichlet+Multinomial -> Dirichlet-Multinomial\n- Binomial->Poisson approximation\n- Poisson->Normal approximation\n"
    },
    {
      "path": "your_project\\obfuscated_latest\\統計検定1級\\templates\\econometrics_problem.tex",
      "overview": "TeXファイル。行数: 6\n",
      "content": "% econometrics_problem.tex\n{% if not show_solution %}\n計量経済学モデルに関する問題\n{% else %}\n解答と推定量\n{{ explanation }}\n{% endif %}"
    },
    {
      "path": "your_project\\obfuscated_latest\\統計検定1級\\templates\\high_moment_problem.tex",
      "overview": "TeXファイル。行数: 6\n",
      "content": "% high_moment_problem.tex\n{% if not show_solution %}\n正規分布の{{ n }}次モーメントは？(例: $E[X^{n}]$ )\n{% else %}\n$E[X^{n}]={{ moment }}$\n{{ explanation }}\n{% endif %}"
    },
    {
      "path": "your_project\\obfuscated_latest\\統計検定1級\\templates\\linear_combination_problem.tex",
      "overview": "TeXファイル。行数: 6\n",
      "content": "% linear_combination_problem.tex\n{% if not show_solution %}\nZ=aX+bY のE[Z],Var[Z]\n{% else %}\nE[Z],Var[Z]\n{{ explanation }}\n{% endif %}"
    },
    {
      "path": "your_project\\obfuscated_latest\\統計検定1級\\templates\\multivariate_normal_problem.tex",
      "overview": "TeXファイル。行数: 6\n",
      "content": "% multivariate_normal_problem.tex\n{% if not show_solution %}\n多変量正規に関する問題\n{% else %}\n解答\n{{ explanation }}\n{% endif %}"
    },
    {
      "path": "your_project\\obfuscated_latest\\統計検定1級\\templates\\nonparametric_test_problem.tex",
      "overview": "TeXファイル。行数: 6\n",
      "content": "% nonparametric_test_problem.tex\n{% if not show_solution %}\nノンパラ検定問題\n{% else %}\n検定結果\n{{ explanation }}\n{% endif %}"
    },
    {
      "path": "your_project\\obfuscated_latest\\統計検定1級\\templates\\poisson_normal_approx_problem.tex",
      "overview": "TeXファイル。行数: 6\n",
      "content": "% poisson_normal_approx_problem.tex\n{% if not show_solution %}\nPoisson→Normal近似\n{% else %}\nPoisson P=${{ poisson_p }}$, mean=${{mean}}$ var=${{variance}}$\n{{ explanation }}\n{% endif %}"
    },
    {
      "path": "your_project\\obfuscated_latest\\統計検定1級\\templates\\probability_definition_problem.tex",
      "overview": "TeXファイル。行数: 6\n",
      "content": "% probability_definition_problem.tex\n{% if not show_solution %}\nP(A∩B)求めよ\n{% else %}\n結果\n{{ explanation }}\n{% endif %}"
    },
    {
      "path": "your_project\\obfuscated_latest\\統計検定1級\\templates\\probability_problem.tex",
      "overview": "TeXファイル。行数: 7\n",
      "content": "% probability_problem.tex\n{% if not show_solution %}\n確率計算問題（例）\n問題タイプ: {{ problem_type }}\n{% else %}\n解答と説明: {{ explanation }}\n計算結果: P = {{ probability }}\n{% endif %}"
    },
    {
      "path": "your_project\\obfuscated_latest\\統計検定1級\\templates\\regression_analysis_problem.tex",
      "overview": "TeXファイル。行数: 6\n",
      "content": "% regression_analysis_problem.tex\n{% if not show_solution %}\n回帰分析問題\n{% else %}\n回帰係数結果\n{{ explanation }}\n{% endif %}"
    },
    {
      "path": "your_project\\obfuscated_latest\\統計検定1級\\templates\\statistical_inference_problem.tex",
      "overview": "TeXファイル。行数: 6\n",
      "content": "% statistical_inference_problem.tex\n{% if not show_solution %}\n統計的推定/検定問題\n{% else %}\n検定結果\n{{ explanation }}\n{% endif %}"
    },
    {
      "path": "your_project\\obfuscated_latest\\統計検定1級\\templates\\time_series_analysis_problem.tex",
      "overview": "TeXファイル。行数: 6\n",
      "content": "% time_series_analysis_problem.tex\n{% if not show_solution %}\n時系列分析問題\n{% else %}\n解答と説明\n{{ explanation }}\n{% endif %}"
    },
    {
      "path": "your_project\\obfuscated_latest\\統計検定1級\\templates\\variance_analysis_problem.tex",
      "overview": "TeXファイル。行数: 6\n",
      "content": "% variance_analysis_problem.tex\n{% if not show_solution %}\n分散分析問題\n{% else %}\nANOVA結果\n{{ explanation }}\n{% endif %}"
    },
    {
      "path": "your_project\\restored_latest\\統計検定1級\\config.py",
      "overview": "Pythonコード。\nクラス: Config。\n関数: __init__, load_config, get。\n",
      "content": "import json, os\n\nclass Config:\n\n    def __init__(self, config_file='config.json'):\n        script_dir = os.path.dirname(os.path.abspath(__file__))\n        self.config_file = os.path.join(script_dir, config_file)\n        self.settings = self.load_config()\n\n    def load_config(self):\n        if not os.path.exists(self.config_file):\n            return {}\n        try:\n            with open(self.config_file, 'r', encoding='utf-8') as f:\n                content = f.read().strip()\n                if not content:\n                    return {}\n                return json.loads(content)\n        except Exception:\n            return {}\n\n    def get(self, *keys, default=None):\n        data = self.settings\n        for k in keys:\n            if isinstance(data, dict) and k in data:\n                data = data[k]\n            else:\n                return default\n        return data\nconfig = Config()"
    },
    {
      "path": "your_project\\restored_latest\\統計検定1級\\database.py",
      "overview": "Pythonコード。\nクラス: DatabaseManager。\n関数: __init__, setup_database, save_problem。\n",
      "content": "import sqlite3, os\nfrom config import config\n\nclass DatabaseManager:\n\n    def __init__(self):\n        script_dir = os.path.dirname(os.path.abspath(__file__))\n        db_path = config.get('database_path', default='data/data.db')\n        if db_path is None:\n            db_path = 'data/data.db'\n        self.db_name = os.path.join(script_dir, db_path)\n        os.makedirs(os.path.dirname(self.db_name), exist_ok=True)\n\n    def setup_database(self):\n        conn = sqlite3.connect(self.db_name)\n        c = conn.cursor()\n        c.execute('CREATE TABLE IF NOT EXISTS problems (id TEXT PRIMARY KEY,date_created TEXT,problem_text TEXT,solution_text TEXT,problem_type TEXT)')\n        conn.commit()\n        conn.close()\n\n    def save_problem(self, problem_id, date_created, problem_text, solution_text, problem_type):\n        conn = sqlite3.connect(self.db_name)\n        c = conn.cursor()\n        c.execute('INSERT INTO problems VALUES (?,?,?,?,?)', (problem_id, date_created, problem_text, solution_text, problem_type))\n        conn.commit()\n        conn.close()"
    },
    {
      "path": "your_project\\restored_latest\\統計検定1級\\graph.py",
      "overview": "Pythonコード。\nクラス: ProbabilityDistributionVisualizer。\n関数: __init__, _safe_savefig, plot_probability, plot_t_test, plot_regression, plot_time_series, plot_econometrics, plot_multivariate_normal, plot_distribution_properties, plot_high_moment, plot_variance_analysis, plot_nonparametric_test, plot_linear_combination, confidence_ellipse, mvn_pdf, ecdf。\n",
      "content": "import matplotlib\nmatplotlib.use('Agg')\nimport matplotlib.pyplot as plt\nimport math\nimport numpy as np\nimport logging\nimport os\nfrom math import factorial, exp\nfrom scipy.stats import norm, t as t_dist, f as f_dist, poisson, expon\nfrom statsmodels.graphics.tsaplots import plot_acf\nimport traceback\n\nclass ProbabilityDistributionVisualizer:\n\n    def __init__(self):\n        pass\n\n    def _safe_savefig(self, output_path):\n        \"\"\"\n        画像を確実にPNGで出力し、ファイルサイズなどをログするヘルパー関数。\n        \"\"\"\n        try:\n            (_, ext) = os.path.splitext(output_path)\n            if ext.lower() == '.png':\n                plt.savefig(output_path, format='png')\n            else:\n                plt.savefig(output_path, format='png')\n            plt.close()\n            if os.path.exists(output_path):\n                fsize = os.path.getsize(output_path)\n                logging.info(f'Saved figure: {output_path} (size: {fsize} bytes)')\n            else:\n                logging.warning(f'File not found after saving: {output_path}')\n        except Exception as e:\n            logging.error(f'Failed to save figure to {output_path}, error={e}')\n            plt.close()\n\n    def plot_probability(self, params, output_path):\n        ptype = params.get('problem_type')\n        try:\n            if ptype == 'binomial':\n                p = params['p']\n                n = params['n']\n                k = params['k']\n                x = range(n + 1)\n                from math import comb\n                pmf = [comb(n, i) * p ** i * (1 - p) ** (n - i) for i in x]\n                cdf = np.cumsum(pmf)\n                plt.figure(figsize=(10, 5))\n                plt.subplot(1, 2, 1)\n                plt.bar(x, pmf, color='skyblue')\n                plt.title(f'Binomial PMF n={n}, p={p}')\n                plt.xlabel('k')\n                plt.ylabel('P(X=k)')\n                plt.axvline(k, color='red', linestyle='--', label='Target k')\n                plt.legend()\n                plt.subplot(1, 2, 2)\n                plt.step(x, cdf, where='post', color='green')\n                plt.title('CDF')\n                plt.xlabel('k')\n                plt.ylabel('P(X<=k)')\n                plt.axvline(k, color='red', linestyle='--')\n                plt.tight_layout()\n                self._safe_savefig(output_path)\n                return True\n            elif ptype == 'poisson':\n                lam = params['lambda']\n                k = params['k']\n                x = range(k + 10 + 1)\n                pmf = [lam ** i * exp(-lam) / factorial(i) for i in x]\n                cdf = np.cumsum(pmf)\n                plt.figure(figsize=(10, 5))\n                plt.subplot(1, 2, 1)\n                plt.bar(x, pmf, color='orange')\n                plt.title(f'Poisson(lambda={lam}) PMF')\n                plt.xlabel('k')\n                plt.ylabel('P(X=k)')\n                plt.axvline(k, color='red', linestyle='--', label='Target k')\n                plt.legend()\n                plt.subplot(1, 2, 2)\n                plt.step(x, cdf, where='post', color='green')\n                plt.title('CDF')\n                plt.xlabel('k')\n                plt.ylabel('P(X<=k)')\n                plt.axvline(k, color='red', linestyle='--')\n                plt.tight_layout()\n                self._safe_savefig(output_path)\n                return True\n            elif ptype == 'conditional_probability':\n                P_A = params['P_A']\n                P_BA = params['P_B_given_A']\n                P_AB = params['P_A_and_B']\n                plt.figure()\n                vals = [P_A, P_BA, P_AB]\n                labels = ['P(A)', 'P(B|A)', 'P(A∩B)']\n                plt.bar(labels, vals, color=['blue', 'green', 'red'])\n                plt.title('Conditional Probability Visualization')\n                plt.ylabel('Probability')\n                plt.tight_layout()\n                self._safe_savefig(output_path)\n                return True\n            else:\n                return False\n        except Exception as e:\n            logging.error(f'Error in plot_probability: {e}')\n            plt.close()\n            return False\n\n    def plot_t_test(self, params, output_path):\n        try:\n            alpha = params['alpha']\n            df = params['df']\n            t_stat = params['t_stat']\n            critical_value = params['critical_value']\n            x = np.linspace(t_dist.ppf(0.001, df), t_dist.ppf(0.999, df), 1000)\n            y = t_dist.pdf(x, df)\n            plt.figure(figsize=(10, 5))\n            plt.plot(x, y, 'b-', label=f't-dist(df={df})')\n            plt.axvline(x=critical_value, color='r', linestyle='--', label='critical +')\n            plt.axvline(x=-critical_value, color='r', linestyle='--', label='critical -')\n            plt.axvline(x=t_stat, color='g', label='t-stat')\n            p_area_x = x[x > critical_value]\n            plt.fill_between(p_area_x, t_dist.pdf(p_area_x, df), color='red', alpha=0.3)\n            p_area_x2 = x[x < -critical_value]\n            plt.fill_between(p_area_x2, t_dist.pdf(p_area_x2, df), color='red', alpha=0.3)\n            plt.title('t-test visualization')\n            plt.xlabel('t')\n            plt.ylabel('pdf')\n            plt.legend()\n            plt.tight_layout()\n            self._safe_savefig(output_path)\n            return True\n        except Exception as e:\n            logging.error(f't検定グラフ生成エラー: {e}')\n            plt.close()\n            return False\n\n    def plot_regression(self, x_values, y_values, beta_0_hat, beta_1_hat, output_path):\n        try:\n            import statsmodels.api as sm\n            X = sm.add_constant(x_values)\n            model = sm.OLS(y_values, X).fit()\n            residuals = model.resid\n            (fig, axes) = plt.subplots(2, 2, figsize=(10, 10))\n            ax = axes[0, 0]\n            ax.scatter(x_values, y_values, color='blue', label='data')\n            x_line = np.linspace(min(x_values), max(x_values), 100)\n            y_line = beta_0_hat + beta_1_hat * x_line\n            ax.plot(x_line, y_line, color='red', label='reg line')\n            ax.set_title('Data & Regression Line')\n            ax.set_xlabel('X')\n            ax.set_ylabel('Y')\n            ax.legend()\n            ax = axes[0, 1]\n            ax.hist(residuals, bins=20, color='green', alpha=0.7)\n            ax.set_title('Residual Histogram')\n            ax.set_xlabel('Residual')\n            ax.set_ylabel('Frequency')\n            sm.qqplot(residuals, line='45', ax=axes[1, 0], color='purple')\n            axes[1, 0].set_title('Q-Q plot of Residuals')\n            fitted = model.fittedvalues\n            ax = axes[1, 1]\n            ax.scatter(fitted, residuals, color='orange')\n            ax.axhline(y=0, color='red', linestyle='--')\n            ax.set_title('Residuals vs Fitted')\n            ax.set_xlabel('Fitted')\n            ax.set_ylabel('Residuals')\n            fig.tight_layout()\n            self._safe_savefig(output_path)\n            return True\n        except Exception as e:\n            logging.error(f'回帰分析グラフ生成中にエラー: {e}')\n            plt.close()\n            return False\n\n    def plot_time_series(self, params, output_path):\n        try:\n            ts = params['time_series']\n            (fig, axes) = plt.subplots(2, 1, figsize=(10, 8))\n            axes[0].plot(ts, color='blue')\n            axes[0].set_title('Time Series Data')\n            axes[0].set_xlabel('Time')\n            axes[0].set_ylabel('Value')\n            plot_acf(ts, ax=axes[1])\n            axes[1].set_title('Autocorrelation Function')\n            fig.tight_layout()\n            self._safe_savefig(output_path)\n            return True\n        except Exception as e:\n            logging.error(f'時系列分析グラフエラー: {e}')\n            plt.close()\n            return False\n\n    def plot_econometrics(self, params, output_path):\n        try:\n            import statsmodels.api as sm\n            X = np.column_stack((params['x1_values'], params['x2_values']))\n            Y = np.array(params['y_values'])\n            Xc = sm.add_constant(X)\n            model = sm.OLS(Y, Xc).fit()\n            residuals = model.resid\n            fitted = model.fittedvalues\n            (fig, axes) = plt.subplots(2, 2, figsize=(10, 10))\n            ax = axes[0, 0]\n            ax.scatter(params['x1_values'], Y, color='blue', alpha=0.7, label='X1-Y')\n            ax.set_title('X1 vs Y')\n            ax.set_xlabel('X1')\n            ax.set_ylabel('Y')\n            ax = axes[0, 1]\n            ax.scatter(params['x2_values'], Y, color='green', alpha=0.7, label='X2-Y')\n            ax.set_title('X2 vs Y')\n            ax.set_xlabel('X2')\n            ax.set_ylabel('Y')\n            ax = axes[1, 0]\n            ax.hist(residuals, bins=20, color='gray', alpha=0.7)\n            ax.set_title('Residuals Histogram')\n            ax.set_xlabel('Residual')\n            ax = axes[1, 1]\n            ax.scatter(fitted, residuals, color='red', alpha=0.7)\n            ax.axhline(y=0, color='black', linestyle='--')\n            ax.set_title('Residuals vs Fitted')\n            ax.set_xlabel('Fitted')\n            ax.set_ylabel('Residuals')\n            fig.tight_layout()\n            self._safe_savefig(output_path)\n            return True\n        except Exception as e:\n            logging.error(f'計量経済学グラフエラー: {e}')\n            plt.close()\n            return False\n\n    def plot_multivariate_normal(self, params, output_path):\n        try:\n            mu = params['mu']\n            sigma = np.array(params['sigma'])\n            fig = plt.figure(figsize=(10, 10))\n            from matplotlib.patches import Ellipse\n            import matplotlib.transforms as transforms\n\n            def confidence_ellipse(mu, cov, ax, n_std=1.96, facecolor='none', **kwargs):\n                pearson = cov[0, 1] / np.sqrt(cov[0, 0] * cov[1, 1])\n                ell_radius_x = np.sqrt(1 + pearson)\n                ell_radius_y = np.sqrt(1 - pearson)\n                ellipse = Ellipse((0, 0), width=ell_radius_x * 2, height=ell_radius_y * 2, facecolor=facecolor, **kwargs)\n                scale_x = np.sqrt(cov[0, 0]) * n_std\n                scale_y = np.sqrt(cov[1, 1]) * n_std\n                transf = transforms.Affine2D().rotate_deg(45).scale(scale_x, scale_y).translate(mu[0], mu[1])\n                ellipse.set_transform(transf + ax.transData)\n                return ax.add_patch(ellipse)\n            ax = fig.add_subplot(2, 2, 1)\n            ax.set_title('Confidence Ellipse')\n            confidence_ellipse(mu, sigma, ax, edgecolor='red')\n            ax.scatter(mu[0], mu[1], c='blue', marker='x', label='mean')\n            ax.legend()\n            ax.set_xlabel('X1')\n            ax.set_ylabel('X2')\n            ax2 = fig.add_subplot(2, 2, 2)\n            x = np.linspace(mu[0] - 4 * np.sqrt(sigma[0, 0]), mu[0] + 4 * np.sqrt(sigma[0, 0]), 100)\n            y = np.linspace(mu[1] - 4 * np.sqrt(sigma[1, 1]), mu[1] + 4 * np.sqrt(sigma[1, 1]), 100)\n            (X, Y) = np.meshgrid(x, y)\n            pos = np.dstack((X, Y))\n\n            def mvn_pdf(xarr, muarr, cov):\n                det = np.linalg.det(cov)\n                inv = np.linalg.inv(cov)\n                diff = xarr - muarr\n                return 1.0 / (2 * np.pi * np.sqrt(det)) * np.exp(-0.5 * (diff @ inv @ diff.T))\n            Z = np.empty(X.shape)\n            for i in range(X.shape[0]):\n                for j in range(X.shape[1]):\n                    Z[i, j] = mvn_pdf(np.array([X[i, j], Y[i, j]]), np.array(mu), sigma)\n            ax2.contour(X, Y, Z, levels=5, cmap='Blues')\n            ax2.set_title('Contour')\n            ax3 = fig.add_subplot(2, 2, 3)\n            X_marg = norm(loc=mu[0], scale=np.sqrt(sigma[0, 0]))\n            x_line = np.linspace(mu[0] - 4 * np.sqrt(sigma[0, 0]), mu[0] + 4 * np.sqrt(sigma[0, 0]), 100)\n            y_line = X_marg.pdf(x_line)\n            ax3.plot(x_line, y_line, 'r-')\n            ax3.set_title('Marginal X1 distribution')\n            ax3.set_xlabel('X1')\n            ax3.set_ylabel('pdf')\n            ax4 = fig.add_subplot(2, 2, 4)\n            Y_marg = norm(loc=mu[1], scale=np.sqrt(sigma[1, 1]))\n            y_line = Y_marg.pdf(x_line)\n            ax4.plot(x_line, y_line, 'g-')\n            ax4.set_title('Marginal X2 distribution')\n            ax4.set_xlabel('X2')\n            ax4.set_ylabel('pdf')\n            fig.tight_layout()\n            self._safe_savefig(output_path)\n            return True\n        except Exception as e:\n            logging.error(f'多変量正規分布グラフエラー: {e}')\n            plt.close()\n            return False\n\n    def plot_distribution_properties(self, params, output_path):\n        try:\n            dist = params['distribution']\n            plt.figure(figsize=(10, 5))\n            if dist == '正規分布':\n                mu = 0\n                sigma = 1\n                x = np.linspace(mu - 4 * sigma, mu + 4 * sigma, 200)\n                y = norm.pdf(x, mu, sigma)\n                plt.plot(x, y, 'b-')\n                plt.axvline(mu, color='r', linestyle='--', label='mean')\n                plt.axvline(mu + sigma, color='g', linestyle=':', label='mean+sigma')\n                plt.axvline(mu - sigma, color='g', linestyle=':')\n                plt.title('Normal Distribution (mu=0, sigma=1)')\n                plt.legend()\n            elif dist == 'ポアソン分布':\n                lam = 3\n                x = np.arange(0, 15)\n                y = poisson.pmf(x, lam)\n                plt.bar(x, y, color='skyblue')\n                plt.axvline(lam, color='r', linestyle='--', label='mean=lambda=3')\n                plt.title('Poisson(lambda=3)')\n                plt.legend()\n            elif dist == '指数分布':\n                lam = 1\n                x = np.linspace(0, 5, 200)\n                y = expon.pdf(x, scale=1 / lam)\n                plt.plot(x, y, 'b-')\n                plt.axvline(1 / lam, color='r', linestyle='--', label='mean=1/lambda')\n                plt.title('Exponential(lambda=1)')\n                plt.legend()\n            plt.tight_layout()\n            self._safe_savefig(output_path)\n            return True\n        except Exception as e:\n            logging.error(f'分布性質グラフエラー: {e}')\n            plt.close()\n            return False\n\n    def plot_high_moment(self, params, output_path):\n        try:\n            mu = params['mu']\n            sigma = params['sigma']\n            n = params['n']\n            moment = params['moment']\n            x = np.linspace(mu - 4 * sigma, mu + 4 * sigma, 200)\n            y = norm.pdf(x, mu, sigma)\n            plt.figure(figsize=(10, 5))\n            plt.plot(x, y, 'b-', label='Normal pdf')\n            plt.axvline(mu, color='r', linestyle='--', label='mean')\n            plt.axvline(mu + sigma, color='g', linestyle=':', label='mu±sigma')\n            plt.axvline(mu - sigma, color='g', linestyle=':')\n            plt.title(f'Normal distribution (mu={mu}, sigma={sigma}). n={n}-th moment={moment}')\n            plt.legend()\n            plt.tight_layout()\n            self._safe_savefig(output_path)\n            return True\n        except Exception as e:\n            logging.error(f'高次モーメントグラフエラー: {e}')\n            plt.close()\n            return False\n\n    def plot_variance_analysis(self, params, output_path):\n        try:\n            group_count = params['group_count']\n            sample_sizes = params['sample_sizes']\n            means = params['means']\n            variances = params['variances']\n            data = []\n            for i in range(group_count):\n                np.random.seed(i)\n                samples = np.random.normal(means[i], np.sqrt(variances[i]), sample_sizes[i])\n                data.append(samples)\n            plt.figure(figsize=(8, 6))\n            plt.boxplot(data, labels=[f'Group{i + 1}' for i in range(group_count)])\n            plt.title('ANOVA: Boxplots of groups')\n            plt.ylabel('Value')\n            plt.tight_layout()\n            self._safe_savefig(output_path)\n            return True\n        except Exception as e:\n            logging.error(f'分散分析グラフエラー: {e}')\n            plt.close()\n            return False\n\n    def plot_nonparametric_test(self, params, output_path):\n        try:\n            s1 = params['sample1']\n            s2 = params['sample2']\n\n            def ecdf(data):\n                d_sorted = np.sort(data)\n                y = np.arange(1, len(d_sorted) + 1) / len(d_sorted)\n                return (d_sorted, y)\n            (x1, y1) = ecdf(s1)\n            (x2, y2) = ecdf(s2)\n            plt.figure(figsize=(8, 6))\n            plt.step(x1, y1, where='post', label='Sample1 ECDF', color='blue')\n            plt.step(x2, y2, where='post', label='Sample2 ECDF', color='red')\n            plt.title('Nonparametric Test Visualization')\n            plt.xlabel('Value')\n            plt.ylabel('ECDF')\n            plt.legend()\n            plt.tight_layout()\n            self._safe_savefig(output_path)\n            return True\n        except Exception as e:\n            logging.error(f'ノンパラ検定グラフエラー: {e}')\n            plt.close()\n            return False\n\n    def plot_linear_combination(self, params, output_path):\n        try:\n            a = params['a']\n            b = params['b']\n            mu1 = params['mu1']\n            mu2 = params['mu2']\n            sig1 = params['sigma1_squared']\n            sig2 = params['sigma2_squared']\n            np.random.seed(123)\n            x = np.random.normal(mu1, np.sqrt(sig1), 1000)\n            y = np.random.normal(mu2, np.sqrt(sig2), 1000)\n            Z = a * x + b * y\n            plt.figure(figsize=(8, 6))\n            plt.hist(Z, bins=30, density=True, alpha=0.7, color='purple', label='Simulated Z')\n            E_Z = a * mu1 + b * mu2\n            Var_Z = a ** 2 * sig1 + b ** 2 * sig2\n            X_line = np.linspace(E_Z - 4 * np.sqrt(Var_Z), E_Z + 4 * np.sqrt(Var_Z), 200)\n            Y_line = norm.pdf(X_line, E_Z, np.sqrt(Var_Z))\n            plt.plot(X_line, Y_line, 'r-', label='Theoretical PDF')\n            plt.title('Linear Combination Distribution')\n            plt.xlabel('Z')\n            plt.ylabel('Density')\n            plt.legend()\n            plt.tight_layout()\n            self._safe_savefig(output_path)\n            return True\n        except Exception as e:\n            logging.error(f'線形結合グラフエラー: {e}')\n            plt.close()\n            return False"
    },
    {
      "path": "your_project\\restored_latest\\統計検定1級\\gui.py",
      "overview": "Pythonコード。\nクラス: InteractiveSolverGUI。\n関数: __init__, run。\n",
      "content": "import tkinter as tk\n\nclass InteractiveSolverGUI:\n\n    def __init__(self):\n        self.root = tk.Tk()\n        self.root.title('統計検定1級 インタラクティブ問題解答システム')\n\n    def run(self):\n        self.root.mainloop()"
    },
    {
      "path": "your_project\\restored_latest\\統計検定1級\\main.py",
      "overview": "Pythonコード。\n関数: main。\n",
      "content": "import sys\nfrom main_app import MainApp\nfrom gui import InteractiveSolverGUI\nfrom config import config\n\ndef main():\n    pdf_gen = config.get('pdf_generation', default={})\n    problem_count = pdf_gen.get('problem_count', 9)\n    if len(sys.argv) > 1 and sys.argv[1] == '--generate-pdf':\n        app = MainApp()\n        app.generate_and_compile(problem_count)\n    else:\n        gui = InteractiveSolverGUI()\n        gui.run()\nif __name__ == '__main__':\n    main()"
    },
    {
      "path": "your_project\\restored_latest\\統計検定1級\\main_app.py",
      "overview": "Pythonコード。\nクラス: MainApp。\n関数: __init__, generate_latex_header, generate_and_compile, compile_latex。\n",
      "content": "import os, subprocess\nfrom config import config\nfrom problem_generator import ProblemGenerator\nimport logging\nimport traceback\n\nclass MainApp:\n\n    def __init__(self):\n        self.output_tex_file = config.get('output_tex_file', default='practice_problems.tex')\n        if self.output_tex_file is None:\n            self.output_tex_file = 'practice_problems.tex'\n        self.enable_visualization = config.get('enable_visualization', default=True)\n        self.problem_templates_dir = config.get('problem_templates_directory', default='templates')\n        self.output_directory = config.get('output_directory', default='outputs')\n        self.latex_path = config.get('latex_path', default='xelatex')\n        self.generator = ProblemGenerator()\n\n    def generate_latex_header(self):\n        cjk_font = config.get('cjk_main_font', default='Yu Gothic')\n        if cjk_font is None:\n            cjk_font = 'Yu Gothic'\n        header = ['\\\\documentclass{article}', '\\\\usepackage{amsmath}', '\\\\usepackage{amssymb}', '\\\\usepackage{graphicx}', '\\\\usepackage{float}', '\\\\usepackage{geometry}', '\\\\usepackage{xeCJK}', '\\\\usepackage{fontspec}', '\\\\setmainfont{Times New Roman}', f'\\\\setCJKmainfont{{{cjk_font}}}', '\\\\geometry{a4paper, margin=1in}', '\\\\begin{document}']\n        return header\n\n    def generate_and_compile(self, problem_count):\n        script_dir = os.path.dirname(os.path.abspath(__file__))\n        latex_content = self.generate_latex_header()\n        out_dir = self.output_directory\n        if out_dir is None:\n            out_dir = 'outputs'\n        output_dir = os.path.join(script_dir, out_dir)\n        os.makedirs(output_dir, exist_ok=True)\n        tex_file_path = os.path.join(output_dir, self.output_tex_file)\n        for idx in range(1, problem_count + 1):\n            try:\n                result = self.generator.generate_problem()\n                if result:\n                    (problem_id, date_created, problem_type, problem_text, solution_text, graph_filename) = result\n                    latex_content.append(f'\\\\section*{{問題 {idx}}}')\n                    latex_content.append(problem_text)\n                    if self.enable_visualization and graph_filename:\n                        latex_content.append('\\\\begin{figure}[H]')\n                        latex_content.append('\\\\centering')\n                        graph_relative_path = os.path.join('graphs', graph_filename).replace('\\\\', '/')\n                        latex_content.append(f'\\\\includegraphics[width=0.8\\\\textwidth]{{{graph_relative_path}}}')\n                        latex_content.append('\\\\end{figure}')\n                    latex_content.append('\\\\subsection*{解答}')\n                    latex_content.append(solution_text)\n                    latex_content.append('\\\\newpage')\n                else:\n                    logging.warning(f'問題 {idx} の生成に失敗しました。')\n                    print(f'問題 {idx} の生成に失敗しました。')\n            except Exception as e:\n                logging.error(f'問題 {idx} の生成中にエラー: {e}')\n                logging.error(traceback.format_exc())\n                print(f'問題 {idx} 生成エラー。ログを確認')\n        templates_dir = self.problem_templates_dir\n        if templates_dir is None:\n            templates_dir = 'templates'\n        latex_content.append('\\\\clearpage')\n        latex_content.append('\\\\input{../' + templates_dir + '/distribution_relations.tex}')\n        latex_content.append('\\\\end{document}')\n        with open(tex_file_path, 'w', encoding='utf-8') as tex_file:\n            tex_file.write('\\n'.join(latex_content))\n        self.compile_latex(tex_file_path)\n\n    def compile_latex(self, tex_file_path):\n        tex_file_name = os.path.basename(tex_file_path)\n        latex_path = self.latex_path\n        if not latex_path or not os.path.exists(latex_path):\n            print(f'LaTeXコンパイラが見つかりません: {latex_path}')\n            return\n        process = subprocess.run([latex_path, '-interaction=nonstopmode', tex_file_name], cwd=os.path.dirname(tex_file_path), stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n        log_file_path = os.path.join(os.path.dirname(tex_file_path), 'latex_compile.log')\n        with open(log_file_path, 'w', encoding='utf-8') as f:\n            f.write(process.stdout or '')\n            f.write(process.stderr or '')\n        if process.returncode != 0:\n            print('LaTeXコンパイルでエラー')\n        else:\n            pdf_file = tex_file_path.replace('.tex', '.pdf')\n            if os.path.exists(pdf_file):\n                print(f'PDF生成成功: {pdf_file}')\n            else:\n                print('PDFファイル未発見')"
    },
    {
      "path": "your_project\\restored_latest\\統計検定1級\\problem_generator.py",
      "overview": "Pythonコード。\nクラス: ProblemGenerator。\n関数: __init__, load_topics, get_problem_types_by_topic, generate_problem。\n",
      "content": "import random, os, logging, uuid\nfrom datetime import datetime\nfrom config import config\nfrom database import DatabaseManager\nfrom problem_types.problem_factory import ProblemFactory\nimport json\nimport traceback\n\nclass ProblemGenerator:\n\n    def __init__(self):\n        self.db_path = config.get('database_path', default='data/data.db')\n        self.output_directory = config.get('output_directory', default='outputs')\n        self.enable_visualization = config.get('enable_visualization', default=True)\n        self.problem_types_weights = config.get('problem_types', default={})\n        self.factory = ProblemFactory()\n        self.db_manager = DatabaseManager()\n        self.db_manager.setup_database()\n        script_dir = os.path.dirname(os.path.abspath(__file__))\n        out_dir = self.output_directory\n        if out_dir is None:\n            out_dir = 'outputs'\n        self.output_dir = os.path.join(script_dir, out_dir, 'graphs')\n        os.makedirs(self.output_dir, exist_ok=True)\n        self.topics_data = self.load_topics()\n\n    def load_topics(self):\n        script_dir = os.path.dirname(os.path.abspath(__file__))\n        topics_file = os.path.join(script_dir, 'topics.json')\n        if not os.path.exists(topics_file):\n            raise FileNotFoundError(f\"'{topics_file}'がない\")\n        with open(topics_file, 'r', encoding='utf-8') as f:\n            return json.load(f)\n\n    def get_problem_types_by_topic(self, topic):\n        return self.topics_data['topics'].get(topic, [])\n\n    def generate_problem(self, selected_topic=None):\n        try:\n            ptypes = self.problem_types_weights\n            if selected_topic:\n                problem_types = self.get_problem_types_by_topic(selected_topic)\n                if not problem_types:\n                    return None\n                problem_type = random.choice(problem_types)\n            else:\n                pts = list(ptypes.keys())\n                pwt = list(ptypes.values())\n                if not pts:\n                    pts = ['probability']\n                    pwt = [1.0]\n                problem_type = random.choices(pts, weights=pwt, k=1)[0]\n            problem = self.factory.create_problem(problem_type)\n            problem.generate_parameters()\n            problem_text = problem.generate_problem_text()\n            solution_text = problem.generate_solution_text()\n            enable_vis = self.enable_visualization\n            if enable_vis is None:\n                enable_vis = True\n            graph_filename = None\n            if enable_vis:\n                graph_filename = f'graph_{uuid.uuid4().hex}.png'\n                graph_filepath = os.path.join(self.output_dir, graph_filename)\n                if not problem.generate_graph(graph_filepath):\n                    graph_filename = None\n            problem_id = uuid.uuid4().hex\n            date_created = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n            self.db_manager.save_problem(problem_id, date_created, problem_text, solution_text, problem_type)\n            return (problem_id, date_created, problem_type, problem_text, solution_text, graph_filename)\n        except Exception as e:\n            logging.error(f'問題生成エラー: {e}')\n            logging.error(traceback.format_exc())\n            return None"
    },
    {
      "path": "your_project\\restored_latest\\統計検定1級\\sympy_solver.py",
      "overview": "Pythonコード。\nクラス: SympySolver。\n関数: __init__, check_equivalence。\n",
      "content": "class SympySolver:\n\n    def __init__(self):\n        pass\n\n    def check_equivalence(self, user_input, correct_answer):\n        return (False, '')"
    },
    {
      "path": "your_project\\restored_latest\\統計検定1級\\topics.json",
      "overview": "JSONファイル (辞書)。キー: topics\n",
      "content": "{\n  \"topics\": {\n    \"確率論\": [\n      \"probability_definition\",\n      \"conditional_probability\",\n      \"distribution_functions\",\n      \"joint_distribution\",\n      \"probability\"\n    ],\n    \"統計的推定\": [\n      \"statistical_inference\",\n      \"t_test\"\n    ],\n    \"統計的検定\": [\n      \"statistical_inference\",\n      \"t_test\"\n    ],\n    \"回帰分析\": [\n      \"regression_analysis\"\n    ],\n    \"分散分析\": [\n      \"variance_analysis\"\n    ],\n    \"ノンパラメトリック検定\": [\n      \"nonparametric_test\"\n    ]\n  }\n}\n"
    },
    {
      "path": "your_project\\restored_latest\\統計検定1級\\.vscode\\launch.json",
      "overview": "JSONファイル (辞書)。キー: version, configurations\n",
      "content": "{\n    \"version\": \"0.2.0\",\n    \"configurations\": [\n        {\n            \"name\": \"Python: 現在のファイル\",\n            \"type\": \"python\",\n            \"request\": \"launch\",\n            \"program\": \"${file}\",\n            \"console\": \"integratedTerminal\",\n            \"justMyCode\": true,\n            \"args\": [\"--generate-pdf\"]\n\n        }\n    ]\n}"
    },
    {
      "path": "your_project\\restored_latest\\統計検定1級\\problem_types\\conjugate_problems.py",
      "overview": "Pythonコード。\nクラス: BetaBinomialConjugateProblem, GammaPoissonConjugateProblem, DirichletMultinomialConjugateProblem, BinomialPoissonApproxProblem, PoissonNormalApproxProblem。\n関数: __init__, generate_parameters, generate_explanation, generate_graph, __init__, generate_parameters, generate_explanation, generate_graph, __init__, generate_parameters, generate_explanation, generate_graph, __init__, generate_parameters, generate_explanation, generate_graph, __init__, generate_parameters, generate_explanation, generate_graph, B, B。\n",
      "content": "import math, random\nfrom problem_types.problem import Problem\nfrom math import comb, factorial, exp, gamma\nimport numpy as np\n\nclass BetaBinomialConjugateProblem(Problem):\n\n    def __init__(self):\n        super().__init__('beta_binomial_conjugate_problem.tex')\n\n    def generate_parameters(self):\n        self.params['alpha'] = random.randint(1, 5)\n        self.params['beta'] = random.randint(1, 5)\n        self.params['n'] = random.randint(5, 20)\n        self.params['k'] = random.randint(0, self.params['n'])\n\n        def B(x, y):\n            return gamma(x) * gamma(y) / gamma(x + y)\n        p_x = comb(self.params['n'], self.params['k']) * B(self.params['k'] + self.params['alpha'], self.params['n'] - self.params['k'] + self.params['beta']) / B(self.params['alpha'], self.params['beta'])\n        self.params['probability'] = round(p_x, 4)\n\n    def generate_explanation(self):\n        return 'Beta+Binomial->Beta-Binomial'\n\n    def generate_graph(self, o):\n        return False\n\nclass GammaPoissonConjugateProblem(Problem):\n\n    def __init__(self):\n        super().__init__('gamma_poisson_conjugate_problem.tex')\n\n    def generate_parameters(self):\n        self.params['alpha'] = random.randint(1, 5)\n        self.params['beta'] = random.uniform(0.5, 2.0)\n        self.params['k'] = random.randint(0, 20)\n        p = self.params['beta'] / (self.params['beta'] + 1)\n        q = 1 - p\n        negbin_p = comb(self.params['k'] + self.params['alpha'] - 1, self.params['k']) * q ** self.params['k'] * p ** self.params['alpha']\n        self.params['probability'] = round(negbin_p, 4)\n\n    def generate_explanation(self):\n        return 'Gamma+Poisson->Negative Binomial'\n\n    def generate_graph(self, o):\n        return False\n\nclass DirichletMultinomialConjugateProblem(Problem):\n\n    def __init__(self):\n        super().__init__('dirichlet_multinomial_conjugate_problem.tex')\n\n    def generate_parameters(self):\n        m = random.randint(2, 4)\n        self.params['m'] = m\n        self.params['n'] = random.randint(5, 20)\n        self.params['alpha_vec'] = [random.uniform(1, 3) for _ in range(m)]\n        counts = [0] * m\n        remain = self.params['n']\n        for i in range(m - 1):\n            c = random.randint(0, remain)\n            counts[i] = c\n            remain -= c\n        counts[-1] = remain\n        self.params['counts'] = counts\n\n        def B(alpha):\n            import numpy as np\n            return np.prod([gamma(a) for a in alpha]) / gamma(sum(alpha))\n        alpha_x = [self.params['alpha_vec'][i] + counts[i] for i in range(m)]\n        num = B(alpha_x)\n        den = B(self.params['alpha_vec'])\n        multinomial_coef = math.factorial(self.params['n'])\n        for c in counts:\n            multinomial_coef /= math.factorial(c)\n        p_x = multinomial_coef * (num / den)\n        self.params['probability'] = round(p_x, 4)\n\n    def generate_explanation(self):\n        return 'Dirichlet+Multinomial->Dirichlet-Multinomial'\n\n    def generate_graph(self, o):\n        return False\n\nclass BinomialPoissonApproxProblem(Problem):\n\n    def __init__(self):\n        super().__init__('binomial_poisson_approx_problem.tex')\n\n    def generate_parameters(self):\n        lam = random.uniform(2, 5)\n        n = random.randint(50, 200)\n        p = lam / n\n        k = random.randint(0, int(lam * 3))\n        binom_p = comb(n, k) * p ** k * (1 - p) ** (n - k)\n        poisson_p = lam ** k * exp(-lam) / math.factorial(k)\n        self.params['n'] = n\n        self.params['p'] = round(p, 6)\n        self.params['k'] = k\n        self.params['lambda'] = round(lam, 3)\n        self.params['binom_p'] = round(binom_p, 6)\n        self.params['poisson_p'] = round(poisson_p, 6)\n\n    def generate_explanation(self):\n        return 'Binomial->Poisson近似条件'\n\n    def generate_graph(self, o):\n        return False\n\nclass PoissonNormalApproxProblem(Problem):\n\n    def __init__(self):\n        super().__init__('poisson_normal_approx_problem.tex')\n\n    def generate_parameters(self):\n        lam = random.randint(30, 100)\n        low = max(0, int(lam - 3 * math.sqrt(lam)))\n        high = int(lam + 3 * math.sqrt(lam))\n        k = random.randint(low, high)\n        poisson_p = lam ** k * exp(-lam) / math.factorial(k)\n        self.params['lambda'] = lam\n        self.params['k'] = k\n        self.params['poisson_p'] = round(poisson_p, 6)\n        self.params['mean'] = lam\n        self.params['variance'] = lam\n\n    def generate_explanation(self):\n        return 'Poisson->Normal近似(λ大)'\n\n    def generate_graph(self, o):\n        return False"
    },
    {
      "path": "your_project\\restored_latest\\統計検定1級\\problem_types\\problem.py",
      "overview": "Pythonコード。\nクラス: Problem, ProbabilityProblem, StatisticalInferenceProblem, RegressionAnalysisProblem, TimeSeriesAnalysisProblem, EconometricsProblem, LinearCombinationProblem, DistributionPropertiesProblem, HighMomentProblem, MultivariateNormalProblem, VarianceAnalysisProblem, NonParametricTestProblem, ProbabilityDefinitionProblem, ConditionalProbabilityProblem, DistributionFunctionsProblem, JointDistributionProblem。\n関数: __init__, generate_parameters, generate_problem_text, generate_solution_text, generate_explanation, generate_graph, __init__, generate_parameters, _variant_binomial, _variant_poisson, _variant_conditional_probability, generate_explanation, generate_graph, __init__, generate_parameters, generate_graph, __init__, generate_parameters, generate_graph, __init__, generate_parameters, generate_graph, __init__, generate_parameters, generate_graph, __init__, generate_parameters, generate_explanation, generate_graph, __init__, generate_parameters, generate_graph, __init__, generate_parameters, generate_graph, __init__, generate_parameters, generate_explanation, generate_graph, __init__, generate_parameters, generate_explanation, generate_graph, __init__, generate_parameters, generate_explanation, generate_graph, __init__, generate_parameters, generate_explanation, generate_graph, __init__, generate_parameters, generate_explanation, generate_graph, __init__, generate_parameters, generate_explanation, generate_graph, __init__, generate_parameters, generate_explanation, generate_graph。\n",
      "content": "from abc import ABC, abstractmethod\nimport os\nimport random\nimport logging\nfrom config import config\nfrom jinja2 import Environment, FileSystemLoader\nfrom graph import ProbabilityDistributionVisualizer\nimport traceback\nfrom math import comb, exp, factorial\nfrom scipy.stats import norm, stats\n\nclass Problem(ABC):\n\n    def __init__(self, template_name):\n        self.params = {}\n        self.template_name = template_name\n        templates_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), '..', config.get('problem_templates_directory', default='templates'))\n        self.env = Environment(loader=FileSystemLoader(templates_dir), autoescape=False)\n        self.visualizer = ProbabilityDistributionVisualizer()\n\n    @abstractmethod\n    def generate_parameters(self):\n        pass\n\n    def generate_problem_text(self):\n        template = self.env.get_template(self.template_name)\n        return template.render(**self.params, show_solution=False)\n\n    def generate_solution_text(self):\n        template = self.env.get_template(self.template_name)\n        self.params['explanation'] = self.generate_explanation()\n        return template.render(**self.params, show_solution=True)\n\n    def generate_explanation(self):\n        return ''\n\n    @abstractmethod\n    def generate_graph(self, output_path):\n        pass\n\nclass ProbabilityProblem(Problem):\n\n    def __init__(self):\n        super().__init__('probability_problem.tex')\n\n    def generate_parameters(self):\n        variants = [self._variant_binomial, self._variant_poisson, self._variant_conditional_probability]\n        v = random.choice(variants)\n        v()\n\n    def _variant_binomial(self):\n        self.params['problem_type'] = 'binomial'\n        self.params['n'] = random.randint(5, 20)\n        self.params['p'] = round(random.uniform(0.1, 0.9), 2)\n        self.params['k'] = random.randint(0, self.params['n'])\n        from math import comb\n        prob = comb(self.params['n'], self.params['k']) * self.params['p'] ** self.params['k'] * (1 - self.params['p']) ** (self.params['n'] - self.params['k'])\n        self.params['probability'] = round(prob, 6)\n\n    def _variant_poisson(self):\n        self.params['problem_type'] = 'poisson'\n        self.params['lambda'] = round(random.uniform(0.5, 5.0), 2)\n        self.params['k'] = random.randint(0, 10)\n        lam = self.params['lambda']\n        k = self.params['k']\n        prob = lam ** k * exp(-lam) / factorial(k)\n        self.params['probability'] = round(prob, 6)\n\n    def _variant_conditional_probability(self):\n        self.params['problem_type'] = 'conditional_probability'\n        self.params['P_A'] = round(random.uniform(0.1, 0.9), 2)\n        self.params['P_B_given_A'] = round(random.uniform(0.1, 0.9), 2)\n        self.params['P_A_and_B'] = round(self.params['P_A'] * self.params['P_B_given_A'], 6)\n\n    def generate_explanation(self):\n        t = self.params['problem_type']\n        if t == 'binomial':\n            return '二項分布の公式を使用'\n        elif t == 'poisson':\n            return 'ポアソン分布の公式を使用'\n        elif t == 'conditional_probability':\n            return '条件付き確率P(A∩B)=P(A)*P(B|A)'\n\n    def generate_graph(self, output_path):\n        return self.visualizer.plot_probability(self.params, output_path)\n\nclass StatisticalInferenceProblem(Problem):\n\n    def __init__(self):\n        super().__init__('statistical_inference_problem.tex')\n\n    def generate_parameters(self):\n        self.params['sample_mean'] = round(random.uniform(50, 100), 2)\n        self.params['sample_std'] = round(random.uniform(5, 15), 2)\n        self.params['n'] = random.randint(30, 100)\n        self.params['population_mean'] = round(random.uniform(50, 100), 2)\n        self.params['alpha'] = round(random.uniform(0.01, 0.1), 2)\n        t_stat = (self.params['sample_mean'] - self.params['population_mean']) / (self.params['sample_std'] / self.params['n'] ** 0.5)\n        t_stat = round(t_stat, 4)\n        df = self.params['n'] - 1\n        cv = round(stats.t.ppf(1 - self.params['alpha'] / 2, df=df), 4)\n        reject = '棄却' if abs(t_stat) > cv else '棄却しない'\n        self.params['t_stat'] = t_stat\n        self.params['critical_value'] = cv\n        self.params['reject_null'] = reject\n        self.params['df'] = df\n\n    def generate_graph(self, output_path):\n        return self.visualizer.plot_t_test(self.params, output_path)\n\nclass RegressionAnalysisProblem(Problem):\n\n    def __init__(self):\n        super().__init__('regression_analysis_problem.tex')\n\n    def generate_parameters(self):\n        self.params['beta_0'] = round(random.uniform(0, 10), 2)\n        self.params['beta_1'] = round(random.uniform(-5, 5), 2)\n        self.params['n'] = random.randint(50, 200)\n        self.params['x_values'] = [round(random.uniform(0, 100), 2) for _ in range(self.params['n'])]\n        self.params['epsilon'] = [round(random.gauss(0, 10), 2) for _ in range(self.params['n'])]\n        self.params['y_values'] = [self.params['beta_0'] + self.params['beta_1'] * x + e for (x, e) in zip(self.params['x_values'], self.params['epsilon'])]\n        import numpy as np\n        X = np.array(self.params['x_values'])\n        Y = np.array(self.params['y_values'])\n        beta_1_hat = np.cov(X, Y, bias=True)[0, 1] / np.var(X)\n        beta_0_hat = np.mean(Y) - beta_1_hat * np.mean(X)\n        self.params['beta_0_hat'] = round(beta_0_hat, 4)\n        self.params['beta_1_hat'] = round(beta_1_hat, 4)\n\n    def generate_graph(self, output_path):\n        return self.visualizer.plot_regression(self.params['x_values'], self.params['y_values'], self.params['beta_0_hat'], self.params['beta_1_hat'], output_path)\n\nclass TimeSeriesAnalysisProblem(Problem):\n\n    def __init__(self):\n        super().__init__('time_series_analysis_problem.tex')\n\n    def generate_parameters(self):\n        self.params['phi'] = round(random.uniform(0.5, 0.9), 2)\n        self.params['theta'] = round(random.uniform(-0.5, 0.5), 2)\n        self.params['n'] = 100\n        self.params['epsilon'] = [random.gauss(0, 1) for _ in range(self.params['n'])]\n        self.params['time_series'] = [0] * self.params['n']\n        for t in range(1, self.params['n']):\n            self.params['time_series'][t] = self.params['phi'] * self.params['time_series'][t - 1] + self.params['epsilon'][t] + self.params['theta'] * self.params['epsilon'][t - 1]\n\n    def generate_graph(self, output_path):\n        return self.visualizer.plot_time_series(self.params, output_path)\n\nclass EconometricsProblem(Problem):\n\n    def __init__(self):\n        super().__init__('econometrics_problem.tex')\n\n    def generate_parameters(self):\n        self.params['beta_0'] = round(random.uniform(0, 5), 2)\n        self.params['beta_1'] = round(random.uniform(0, 1), 2)\n        self.params['beta_2'] = round(random.uniform(-1, 0), 2)\n        self.params['n'] = random.randint(50, 200)\n        self.params['x1_values'] = [round(random.uniform(0, 100), 2) for _ in range(self.params['n'])]\n        self.params['x2_values'] = [round(random.uniform(0, 100), 2) for _ in range(self.params['n'])]\n        self.params['epsilon'] = [round(random.gauss(0, 5), 2) for _ in range(self.params['n'])]\n        self.params['y_values'] = [self.params['beta_0'] + self.params['beta_1'] * x1 + self.params['beta_2'] * x2 + e for (x1, x2, e) in zip(self.params['x1_values'], self.params['x2_values'], self.params['epsilon'])]\n        import numpy as np\n        X = np.column_stack((np.ones(self.params['n']), self.params['x1_values'], self.params['x2_values']))\n        Y = np.array(self.params['y_values'])\n        beta_hat = np.linalg.inv(X.T @ X) @ X.T @ Y\n        self.params['beta_0_hat'] = round(beta_hat[0], 4)\n        self.params['beta_1_hat'] = round(beta_hat[1], 4)\n        self.params['beta_2_hat'] = round(beta_hat[2], 4)\n\n    def generate_graph(self, output_path):\n        return self.visualizer.plot_econometrics(self.params, output_path)\n\nclass LinearCombinationProblem(Problem):\n\n    def __init__(self):\n        super().__init__('linear_combination_problem.tex')\n\n    def generate_parameters(self):\n        self.params['a'] = random.randint(1, 5)\n        self.params['b'] = random.randint(1, 5)\n        self.params['mu1'] = round(random.uniform(0, 10), 2)\n        self.params['mu2'] = round(random.uniform(0, 10), 2)\n        self.params['sigma1_squared'] = round(random.uniform(1, 5), 2)\n        self.params['sigma2_squared'] = round(random.uniform(1, 5), 2)\n        E_Z = self.params['a'] * self.params['mu1'] + self.params['b'] * self.params['mu2']\n        Var_Z = self.params['a'] ** 2 * self.params['sigma1_squared'] + self.params['b'] ** 2 * self.params['sigma2_squared']\n        self.params['E_Z'] = round(E_Z, 4)\n        self.params['Var_Z'] = round(Var_Z, 4)\n\n    def generate_explanation(self):\n        return '線形結合の期待値・分散計算'\n\n    def generate_graph(self, output_path):\n        return self.visualizer.plot_linear_combination(self.params, output_path)\n\nclass DistributionPropertiesProblem(Problem):\n\n    def __init__(self):\n        super().__init__('distribution_properties_problem.tex')\n\n    def generate_parameters(self):\n        dist_choice = random.choice(['正規分布', 'ポアソン分布', '指数分布'])\n        self.params['distribution'] = dist_choice\n        if dist_choice == '正規分布':\n            self.params['properties'] = {'mean': '\\\\mu', 'variance': '\\\\sigma^2'}\n        elif dist_choice == 'ポアソン分布':\n            self.params['properties'] = {'mean': '\\\\lambda', 'variance': '\\\\lambda'}\n        else:\n            self.params['properties'] = {'mean': '1/\\\\lambda', 'variance': '1/\\\\lambda^2'}\n\n    def generate_graph(self, output_path):\n        return self.visualizer.plot_distribution_properties(self.params, output_path)\n\nclass HighMomentProblem(Problem):\n\n    def __init__(self):\n        super().__init__('high_moment_problem.tex')\n\n    def generate_parameters(self):\n        self.params['n'] = random.randint(3, 5)\n        self.params['mu'] = round(random.uniform(0, 10), 2)\n        self.params['sigma'] = round(random.uniform(1, 5), 2)\n        from scipy.stats import norm\n        m = norm.moment(self.params['n'], loc=self.params['mu'], scale=self.params['sigma'])\n        self.params['moment'] = round(m, 4)\n\n    def generate_graph(self, output_path):\n        return self.visualizer.plot_high_moment(self.params, output_path)\n\nclass MultivariateNormalProblem(Problem):\n\n    def __init__(self):\n        super().__init__('multivariate_normal_problem.tex')\n\n    def generate_parameters(self):\n        self.params['mu'] = [round(random.uniform(0, 10), 2) for _ in range(2)]\n        self.params['sigma'] = [[round(random.uniform(1, 5), 2), round(random.uniform(0, 2), 2)], [round(random.uniform(0, 2), 2), round(random.uniform(1, 5), 2)]]\n\n    def generate_explanation(self):\n        return '多変量正規分布の性質を利用'\n\n    def generate_graph(self, output_path):\n        return self.visualizer.plot_multivariate_normal(self.params, output_path)\n\nclass VarianceAnalysisProblem(Problem):\n\n    def __init__(self):\n        super().__init__('variance_analysis_problem.tex')\n\n    def generate_parameters(self):\n        self.params['group_count'] = random.randint(2, 4)\n        self.params['sample_sizes'] = [random.randint(5, 20) for _ in range(self.params['group_count'])]\n        self.params['means'] = [round(random.uniform(10, 50), 2) for _ in range(self.params['group_count'])]\n        self.params['variances'] = [round(random.uniform(1, 5), 2) for _ in range(self.params['group_count'])]\n        total_n = sum(self.params['sample_sizes'])\n        group_count = self.params['group_count']\n        means = self.params['means']\n        variances = self.params['variances']\n        sample_sizes = self.params['sample_sizes']\n        grand_mean = sum([means[i] * sample_sizes[i] for i in range(group_count)]) / total_n\n        ssb = sum([sample_sizes[i] * (means[i] - grand_mean) ** 2 for i in range(group_count)])\n        ssw = sum([(sample_sizes[i] - 1) * variances[i] for i in range(group_count)])\n        df_between = group_count - 1\n        df_within = total_n - group_count\n        msb = ssb / df_between\n        msw = ssw / df_within\n        F = msb / msw\n        alpha = 0.05\n        from scipy.stats import f\n        F_critical = f.ppf(1 - alpha, df_between, df_within)\n        reject = '棄却する' if F > F_critical else '棄却しない'\n        self.params['F_value'] = round(F, 4)\n        self.params['F_critical'] = round(F_critical, 4)\n        self.params['reject_null'] = reject\n        self.params['df_between'] = df_between\n        self.params['df_within'] = df_within\n\n    def generate_explanation(self):\n        return '一元配置分散分析による検定'\n\n    def generate_graph(self, output_path):\n        return self.visualizer.plot_variance_analysis(self.params, output_path)\n\nclass NonParametricTestProblem(Problem):\n\n    def __init__(self):\n        super().__init__('nonparametric_test_problem.tex')\n\n    def generate_parameters(self):\n        self.params['test_type'] = random.choice(['Mann-Whitney U', 'Kruskal-Wallis', 'Wilcoxon Signed-Rank'])\n        self.params['sample1'] = [round(random.uniform(10, 100), 2) for _ in range(random.randint(5, 20))]\n        self.params['sample2'] = [round(random.uniform(10, 100), 2) for _ in range(random.randint(5, 20))]\n        self.params['test_result'] = '有意差なし(例)'\n\n    def generate_explanation(self):\n        return 'ノンパラ検定で中央値差を検定'\n\n    def generate_graph(self, output_path):\n        return self.visualizer.plot_nonparametric_test(self.params, output_path)\n\nclass ProbabilityDefinitionProblem(Problem):\n\n    def __init__(self):\n        super().__init__('probability_definition_problem.tex')\n\n    def generate_parameters(self):\n        self.params['P_A'] = round(random.uniform(0.1, 0.9), 2)\n        self.params['P_B'] = round(random.uniform(0.1, 0.9), 2)\n        self.params['P_A_and_B'] = round(self.params['P_A'] * self.params['P_B'], 4)\n\n    def generate_explanation(self):\n        return '独立性利用 P(A∩B)=P(A)*P(B)'\n\n    def generate_graph(self, output_path):\n        try:\n            PA = self.params['P_A']\n            PB = self.params['P_B']\n            PAB = self.params['P_A_and_B']\n            plt.figure()\n            plt.bar(['P(A)', 'P(B)', 'P(A∩B)'], [PA, PB, PAB], color=['blue', 'green', 'red'])\n            plt.title('Probability Definition')\n            plt.tight_layout()\n            plt.savefig(output_path)\n            plt.close()\n            return True\n        except:\n            return False\n\nclass ConditionalProbabilityProblem(Problem):\n\n    def __init__(self):\n        super().__init__('conditional_probability_problem.tex')\n\n    def generate_parameters(self):\n        self.params['P_A'] = round(random.uniform(0.2, 0.8), 2)\n        self.params['P_B_given_A'] = round(random.uniform(0.2, 0.8), 2)\n        self.params['P_A_and_B'] = round(self.params['P_A'] * self.params['P_B_given_A'], 4)\n\n    def generate_explanation(self):\n        return 'P(A∩B)=P(A)*P(B|A)'\n\n    def generate_graph(self, output_path):\n        from graph import ProbabilityDistributionVisualizer\n        vis = ProbabilityDistributionVisualizer()\n        return vis.plot_probability(self.params, output_path)\n\nclass DistributionFunctionsProblem(Problem):\n\n    def __init__(self):\n        super().__init__('distribution_functions_problem.tex')\n\n    def generate_parameters(self):\n        self.params['function_type'] = random.choice(['pdf', 'cdf'])\n        self.params['distribution'] = random.choice(['正規分布', '指数分布'])\n        if self.params['distribution'] == '正規分布':\n            self.params['mean'] = round(random.uniform(-5, 5), 2)\n            self.params['std'] = round(random.uniform(1, 3), 2)\n        else:\n            self.params['lambda'] = round(random.uniform(0.5, 2.0), 2)\n\n    def generate_explanation(self):\n        return 'pdfやcdf定義式利用'\n\n    def generate_graph(self, output_path):\n        return False\n\nclass JointDistributionProblem(Problem):\n\n    def __init__(self):\n        super().__init__('joint_distribution_problem.tex')\n\n    def generate_parameters(self):\n        A_and_B = round(random.uniform(0.05, 0.2), 2)\n        A_and_notB = round(random.uniform(0.05, 0.2), 2)\n        notA_and_B = round(random.uniform(0.05, 0.2), 2)\n        notA_and_notB = round(random.uniform(0.05, 0.2), 2)\n        total = A_and_B + A_and_notB + notA_and_B + notA_and_notB\n        A_and_B /= total\n        A_and_notB /= total\n        notA_and_B /= total\n        notA_and_notB /= total\n        self.params['joint_probabilities'] = {'A_and_B': round(A_and_B, 4), 'A_and_not_B': round(A_and_notB, 4), 'not_A_and_B': round(notA_and_B, 4), 'not_A_and_not_B': round(notA_and_notB, 4)}\n        P_A = A_and_B + A_and_notB\n        P_B = A_and_B + notA_and_B\n        P_BA = A_and_B / P_A if P_A > 0 else 0.0\n        self.params['P_A'] = round(P_A, 4)\n        self.params['P_B'] = round(P_B, 4)\n        self.params['P_B_given_A'] = round(P_BA, 4)\n\n    def generate_explanation(self):\n        return '同時→周辺→条件付き確率'\n\n    def generate_graph(self, output_path):\n        try:\n            p = self.params['joint_probabilities']\n            matrix = np.array([[p['A_and_B'], p['A_and_not_B']], [p['not_A_and_B'], p['not_A_and_not_B']]])\n            plt.figure()\n            plt.imshow(matrix, cmap='Blues', interpolation='nearest')\n            plt.colorbar(label='Probability')\n            plt.xticks([0, 1], ['B', 'not B'])\n            plt.yticks([0, 1], ['A', 'not A'])\n            plt.title('Joint Distribution Heatmap')\n            plt.tight_layout()\n            plt.savefig(output_path)\n            plt.close()\n            return True\n        except:\n            return False"
    },
    {
      "path": "your_project\\restored_latest\\統計検定1級\\problem_types\\problem_factory.py",
      "overview": "Pythonコード。\nクラス: ProblemFactory。\n関数: __init__, create_problem。\n",
      "content": "import logging, traceback\nfrom problem_types.problem import ProbabilityProblem, StatisticalInferenceProblem, RegressionAnalysisProblem, TimeSeriesAnalysisProblem, EconometricsProblem, LinearCombinationProblem, DistributionPropertiesProblem, HighMomentProblem, MultivariateNormalProblem, VarianceAnalysisProblem, NonParametricTestProblem, ProbabilityDefinitionProblem, ConditionalProbabilityProblem, DistributionFunctionsProblem, JointDistributionProblem\nfrom problem_types.conjugate_problems import BetaBinomialConjugateProblem, GammaPoissonConjugateProblem, DirichletMultinomialConjugateProblem, BinomialPoissonApproxProblem, PoissonNormalApproxProblem\n\nclass ProblemFactory:\n\n    def __init__(self):\n        self.problem_classes = {'probability': ProbabilityProblem, 'statistical_inference': StatisticalInferenceProblem, 'regression_analysis': RegressionAnalysisProblem, 'time_series_analysis': TimeSeriesAnalysisProblem, 'econometrics': EconometricsProblem, 'linear_combination': LinearCombinationProblem, 'distribution_properties': DistributionPropertiesProblem, 'high_moment': HighMomentProblem, 'multivariate_normal': MultivariateNormalProblem, 'probability_definition': ProbabilityDefinitionProblem, 'conditional_probability': ConditionalProbabilityProblem, 'distribution_functions': DistributionFunctionsProblem, 'joint_distribution': JointDistributionProblem, 't_test': StatisticalInferenceProblem, 'variance_analysis': VarianceAnalysisProblem, 'nonparametric_test': NonParametricTestProblem, 'beta_binomial_conjugate': BetaBinomialConjugateProblem, 'gamma_poisson_conjugate': GammaPoissonConjugateProblem, 'dirichlet_multinomial_conjugate': DirichletMultinomialConjugateProblem, 'binomial_poisson_approx': BinomialPoissonApproxProblem, 'poisson_normal_approx': PoissonNormalApproxProblem}\n\n    def create_problem(self, problem_type):\n        pc = self.problem_classes.get(problem_type)\n        if pc:\n            try:\n                return pc()\n            except Exception as e:\n                logging.error(f'{problem_type} problem generation error:{e}')\n                logging.error(traceback.format_exc())\n                raise\n        else:\n            raise ValueError(f'Unknown problem type:{problem_type}')"
    },
    {
      "path": "your_project\\restored_latest\\統計検定1級\\templates\\distribution_properties_problem.tex",
      "overview": "TeXファイル。行数: 7\n",
      "content": "% distribution_properties_problem.tex\n{% if not show_solution %}\n{{ distribution }} の平均と分散を求めよ。\n{% else %}\n平均: ${{ properties.mean }}$, 分散: ${{ properties.variance }}$\n{{ explanation }}\n{% endif %}\n"
    },
    {
      "path": "your_project\\restored_latest\\統計検定1級\\templates\\distribution_relations.tex",
      "overview": "TeXファイル。行数: 7\n",
      "content": "% distribution_relations.tex\n\\section*{Distribution Relations}\n- Beta+Binomial -> Beta-Binomial\n- Gamma+Poisson -> Negative Binomial\n- Dirichlet+Multinomial -> Dirichlet-Multinomial\n- Binomial->Poisson approximation\n- Poisson->Normal approximation\n"
    },
    {
      "path": "your_project\\restored_latest\\統計検定1級\\templates\\econometrics_problem.tex",
      "overview": "TeXファイル。行数: 6\n",
      "content": "% econometrics_problem.tex\n{% if not show_solution %}\n計量経済学モデルに関する問題\n{% else %}\n解答と推定量\n{{ explanation }}\n{% endif %}"
    },
    {
      "path": "your_project\\restored_latest\\統計検定1級\\templates\\high_moment_problem.tex",
      "overview": "TeXファイル。行数: 6\n",
      "content": "% high_moment_problem.tex\n{% if not show_solution %}\n正規分布の{{ n }}次モーメントは？(例: $E[X^{n}]$ )\n{% else %}\n$E[X^{n}]={{ moment }}$\n{{ explanation }}\n{% endif %}"
    },
    {
      "path": "your_project\\restored_latest\\統計検定1級\\templates\\linear_combination_problem.tex",
      "overview": "TeXファイル。行数: 6\n",
      "content": "% linear_combination_problem.tex\n{% if not show_solution %}\nZ=aX+bY のE[Z],Var[Z]\n{% else %}\nE[Z],Var[Z]\n{{ explanation }}\n{% endif %}"
    },
    {
      "path": "your_project\\restored_latest\\統計検定1級\\templates\\multivariate_normal_problem.tex",
      "overview": "TeXファイル。行数: 6\n",
      "content": "% multivariate_normal_problem.tex\n{% if not show_solution %}\n多変量正規に関する問題\n{% else %}\n解答\n{{ explanation }}\n{% endif %}"
    },
    {
      "path": "your_project\\restored_latest\\統計検定1級\\templates\\nonparametric_test_problem.tex",
      "overview": "TeXファイル。行数: 6\n",
      "content": "% nonparametric_test_problem.tex\n{% if not show_solution %}\nノンパラ検定問題\n{% else %}\n検定結果\n{{ explanation }}\n{% endif %}"
    },
    {
      "path": "your_project\\restored_latest\\統計検定1級\\templates\\poisson_normal_approx_problem.tex",
      "overview": "TeXファイル。行数: 6\n",
      "content": "% poisson_normal_approx_problem.tex\n{% if not show_solution %}\nPoisson→Normal近似\n{% else %}\nPoisson P=${{ poisson_p }}$, mean=${{mean}}$ var=${{variance}}$\n{{ explanation }}\n{% endif %}"
    },
    {
      "path": "your_project\\restored_latest\\統計検定1級\\templates\\probability_definition_problem.tex",
      "overview": "TeXファイル。行数: 6\n",
      "content": "% probability_definition_problem.tex\n{% if not show_solution %}\nP(A∩B)求めよ\n{% else %}\n結果\n{{ explanation }}\n{% endif %}"
    },
    {
      "path": "your_project\\restored_latest\\統計検定1級\\templates\\probability_problem.tex",
      "overview": "TeXファイル。行数: 7\n",
      "content": "% probability_problem.tex\n{% if not show_solution %}\n確率計算問題（例）\n問題タイプ: {{ problem_type }}\n{% else %}\n解答と説明: {{ explanation }}\n計算結果: P = {{ probability }}\n{% endif %}"
    },
    {
      "path": "your_project\\restored_latest\\統計検定1級\\templates\\regression_analysis_problem.tex",
      "overview": "TeXファイル。行数: 6\n",
      "content": "% regression_analysis_problem.tex\n{% if not show_solution %}\n回帰分析問題\n{% else %}\n回帰係数結果\n{{ explanation }}\n{% endif %}"
    },
    {
      "path": "your_project\\restored_latest\\統計検定1級\\templates\\statistical_inference_problem.tex",
      "overview": "TeXファイル。行数: 6\n",
      "content": "% statistical_inference_problem.tex\n{% if not show_solution %}\n統計的推定/検定問題\n{% else %}\n検定結果\n{{ explanation }}\n{% endif %}"
    },
    {
      "path": "your_project\\restored_latest\\統計検定1級\\templates\\time_series_analysis_problem.tex",
      "overview": "TeXファイル。行数: 6\n",
      "content": "% time_series_analysis_problem.tex\n{% if not show_solution %}\n時系列分析問題\n{% else %}\n解答と説明\n{{ explanation }}\n{% endif %}"
    },
    {
      "path": "your_project\\restored_latest\\統計検定1級\\templates\\variance_analysis_problem.tex",
      "overview": "TeXファイル。行数: 6\n",
      "content": "% variance_analysis_problem.tex\n{% if not show_solution %}\n分散分析問題\n{% else %}\nANOVA結果\n{{ explanation }}\n{% endif %}"
    },
    {
      "path": "your_project\\ConfigManager.py",
      "overview": "Pythonコード。\nクラス: Config。\n関数: __init__, load_config, get。\n",
      "content": "import json\nimport os\nimport logging\nfrom typing import Any\n\nclass Config:\n\n    def __init__(self, config_file='config.json'):\n        script_dir = os.path.dirname(os.path.abspath(__file__))\n        self.config_file = os.path.join(script_dir, config_file)\n        self.settings = self.load_config()\n\n    def load_config(self):\n        if not os.path.exists(self.config_file):\n            return {}\n        try:\n            with open(self.config_file, 'r', encoding='utf-8') as f:\n                content = f.read().strip()\n                if not content:\n                    return {}\n                return json.loads(content)\n        except Exception:\n            return {}\n\n    def get(self, *keys: str, default: Any=None) -> Any:\n        data = self.settings\n        data = data['settings']\n        for k in keys:\n            if isinstance(data, dict) and k in data:\n                data = data[k]['value']\n            elif 'value' in data:\n                return data['value']\n            else:\n                return default\n        if isinstance(data, dict) and 'value' in data:\n            return data['value']\n        return data\nconfig = Config()"
    },
    {
      "path": "your_project\\launch.json",
      "overview": "JSONファイル (辞書)。キー: version, configurations\n",
      "content": "{\n    \"version\": \"0.2.0\",\n    \"configurations\": [\n        {\n            \"name\": \"Python: 現在のファイル\",\n            \"type\": \"python\",\n            \"request\": \"launch\",\n            \"program\": \"${file}\",\n            \"console\": \"integratedTerminal\",\n            \"justMyCode\": true,\n            \"args\": [\"--generate-pdf\"]\n\n        }\n    ]\n}"
    },
    {
      "path": "your_project\\interactive_solver_ui.py",
      "overview": "Pythonコード。\nクラス: InteractiveSolverGUI。\n関数: log_info, qa_ui, add_answer_field, next_question, submit_answer, __init__, run。\n",
      "content": "import tkinter as tk\nimport tkinter.font as tkfont\nimport logging\nfrom problem_generator import ProblemGenerator\nfrom config import config\n\ndef log_info(widget, msg):\n    if widget:\n        widget.insert('end', msg + '\\n')\n        widget.see('end')\n    logging.info(msg)\n\ndef qa_ui(parent_frame, log_widget=None, generator=None):\n    ui_font_size = config.get('gui_settings', 'font_size', default=14)\n    if ui_font_size is None:\n        ui_font_size = 14\n    font_style = ('Helvetica', ui_font_size)\n    frm_qa = tk.Frame(parent_frame)\n    frm_qa.pack(side=tk.TOP, fill=tk.X, padx=5, pady=5)\n    question_var = tk.StringVar()\n    question_label = tk.Label(frm_qa, textvariable=question_var, wraplength=600, justify='left', font=font_style)\n    question_label.pack(anchor='w')\n    answer_frame = tk.Frame(frm_qa)\n    answer_frame.pack(anchor='w', pady=5)\n    answer_entries = []\n    questions_list = ['正規分布の n 次モーメントは何ですか？ (例: E[X^n])', 'Z = aX + bY の E[Z] と Var[Z] を求めよ。', 'P(A∩B) を求めよ。'] if generator is None else []\n    curr_index = 0\n    curr_question_text = None\n    if generator is not None:\n        try:\n            result = generator.generate_problem()\n        except Exception as e:\n            result = None\n            log_info(log_widget, f'問題の生成に失敗: {e}')\n            logging.error(f'Failed to generate problem: {e}', exc_info=True)\n        if result is not None:\n            curr_question_text = result[3] if len(result) > 3 else ''\n        if not curr_question_text:\n            curr_question_text = '問題の取得に失敗しました。'\n        question_var.set(curr_question_text)\n    else:\n        curr_question_text = questions_list[curr_index]\n        question_var.set(curr_question_text)\n\n    def add_answer_field():\n        entry = tk.Entry(answer_frame, width=50, font=font_style)\n        entry.pack(pady=2, anchor='w')\n        answer_entries.append(entry)\n    fields = 1\n    if curr_question_text and any((x in curr_question_text for x in ('と', '、', ','))):\n        fields = 2\n    for _ in range(fields):\n        add_answer_field()\n\n    def next_question():\n        nonlocal curr_index, curr_question_text\n        if generator is not None:\n            curr_index += 1\n            try:\n                result = generator.generate_problem()\n            except Exception as e:\n                result = None\n                log_info(log_widget, f'問題の生成に失敗: {e}')\n                logging.error(f'Failed to generate problem: {e}', exc_info=True)\n            if result is not None:\n                curr_question_text = result[3] if len(result) > 3 else ''\n            else:\n                curr_question_text = ''\n            if not curr_question_text:\n                curr_question_text = '問題の取得に失敗しました。'\n        else:\n            curr_index = (curr_index + 1) % len(questions_list)\n            curr_question_text = questions_list[curr_index]\n        question_var.set(curr_question_text)\n        for e in answer_entries:\n            e.destroy()\n        answer_entries.clear()\n        fields = 1\n        if curr_question_text and any((x in curr_question_text for x in ('と', '…', ','))):\n            fields = 2\n        for _ in range(fields):\n            add_answer_field()\n\n    def submit_answer():\n        answers = [e.get() for e in answer_entries]\n        msg = f'解答{curr_index + 1}: {answers}'\n        if log_widget:\n            log_info(log_widget, msg)\n        else:\n            logging.info(msg)\n    btn_frame = tk.Frame(frm_qa)\n    btn_frame.pack(anchor='w', pady=5)\n    tk.Button(btn_frame, text='次の問題', command=next_question, font=font_style).pack(side=tk.LEFT, padx=5)\n    tk.Button(btn_frame, text='解答欄追加', command=add_answer_field, font=font_style).pack(side=tk.LEFT, padx=5)\n    tk.Button(btn_frame, text='解答送信', command=submit_answer, font=font_style).pack(side=tk.LEFT, padx=5)\n\nclass InteractiveSolverGUI:\n\n    def __init__(self):\n        self.root = tk.Tk()\n        title = config.get('gui_settings', 'window_title', default='統計検定1級 インタラクティブ問題解答システム')\n        if title is None:\n            title = '統計検定1級 インタラクティブ問題解答システム'\n        self.root.title(title)\n        font_size = config.get('gui_settings', 'font_size', default=14)\n        if font_size is None:\n            font_size = 14\n        tkfont.nametofont('TkDefaultFont').configure(size=font_size)\n        self.frm_main = tk.Frame(self.root)\n        self.frm_main.pack(fill=tk.BOTH, expand=True, padx=10, pady=10)\n        self.log_widget = tk.Text(self.root, height=8, wrap=tk.WORD, font=('Helvetica', 10))\n        self.log_widget.pack(fill=tk.X, padx=10, pady=5)\n        self.generator = None\n        try:\n            self.generator = ProblemGenerator()\n        except Exception as e:\n            log_info(self.log_widget, f'問題ジェネレータ初期化エラー: {e}')\n            logging.error(f'ProblemGenerator init failed: {e}', exc_info=True)\n        qa_ui(self.frm_main, self.log_widget, self.generator)\n\n    def run(self):\n        self.root.mainloop()"
    },
    {
      "path": "your_project\\util\\config_manager.py",
      "overview": "Pythonコード。\nクラス: ConfigManager。\n関数: __new__, load_config, get。\n",
      "content": "import json\nimport os\nimport logging\nfrom typing import Any\n\nclass ConfigManager:\n    _instance = None\n\n    def __new__(cls):\n        if cls._instance is None:\n            cls._instance = super(ConfigManager, cls).__new__(cls)\n            cls._instance._config = {}\n        return cls._instance\n\n    def load_config(self, config_path: str) -> None:\n        \"\"\"設定ファイル（JSON）を読み込み、内部設定に取り込む。\"\"\"\n        if not os.path.exists(config_path):\n            logging.warning(f'設定ファイルが見つかりません: {config_path}')\n            self._config = {}\n        else:\n            try:\n                with open(config_path, 'r', encoding='utf-8') as f:\n                    content = f.read().strip()\n                    data = json.loads(content) if content else {}\n                if isinstance(data, dict) and 'settings' in data:\n                    data = data['settings']\n                if isinstance(data, dict) and len(data) == 1:\n                    sole_key = next(iter(data))\n                    if isinstance(data[sole_key], dict):\n                        data = data[sole_key]\n                self._config = data\n                logging.info(f'設定ファイル読込完了: {config_path}')\n            except Exception as e:\n                logging.warning(f'設定ファイル読込中にエラー: {e}', exc_info=True)\n                self._config = {}\n\n    def get(self, *keys: str, default: Any=None) -> Any:\n        \"\"\"\n        可変長のキー列で設定値を取得（ネスト構造にも対応）。\n        例) config_manager.get(\"log_level\")\n        \"\"\"\n        data = self._config\n        for k in keys:\n            if isinstance(data, dict) and k in data:\n                data = data[k]\n            else:\n                return default\n        if isinstance(data, dict) and 'value' in data:\n            return data['value']\n        return data"
    },
    {
      "path": "aum.py.py",
      "overview": "Pythonコード。\nクラス: AUMAppState, ImportCollector, Mapper。\n関数: get_file_lock, with_file_lock, retry_operation, setup_logging, log_info, log_error, with_io_throttling, read_file_content, write_file_content, generate_summary, detect_symbol_collisions, auto_rename_collisions_in_name_map, auto_repair_collected_scripts, merge_collected_scripts, _merge_system_and_settings, _merge_single_script, ensure_init_py_for_python_dirs, restore_settings_from_config_json, collect, collect_python_symbols_for_map, generate_short_symbol_name, ensure_name_map_csv_exists, load_name_mappings, invert_name_map, apply_name_mappings_ast, analyze_dependencies_and_sort, build_in_topological_order, verify_build_results, _build_config_json, build_project, finalize_build, token_saving_build_unified, restore_build, run_main_py, gather_run_artifacts, generate_summary_report, apply_code_patch, apply_system_patch, patch_ui, clean, enhanced_smart_paste, patch_ui_placeholder, gui_main, __init__, wrapper, decorator, wrapper, extract_problematic_identifier, extract_imports, on_code_edit, on_apply_patch, on_system_patch, on_collect, on_token_saving, on_restore, on_clean, on_run, on_paste, wrapper, __init__, visit_Import, visit_ImportFrom, __init__, visit_Name, visit_FunctionDef, visit_ClassDef, visit_Attribute, visit_Import, visit_ImportFrom。\n",
      "content": "import os\nimport sys\nimport csv\nimport json\nimport ast\nimport shutil\nimport logging\nimport re\nimport datetime\nimport tkinter as tk\nfrom tkinter import messagebox, simpledialog, filedialog\nfrom tkinter.scrolledtext import ScrolledText\nfrom typing import Any, Dict, List, Optional, Tuple, Set\nfrom collections import defaultdict, deque\nimport subprocess\nimport threading\nimport functools\nimport time\nimport psutil\n\n###############################################################################\n# グローバル定数\n###############################################################################\nLOG_LEVEL = 'ERROR'\nDATA_FOLDER_NAME = \"data\"\nMAX_REBUILD_ATTEMPTS = 3  # ビルド再試行の最大回数\n\n# 特定のクラスやシンボルを常にリネーム対象から除外するリスト\nTOKEN_SAVING_EXCLUDE_LIST = {\n    \"ConfigManager\",\n    \"CsvPlatformFeatureExtractor\",\n    \"JsonPlatformFeatureExtractor\",\n    \"PlatformSampleManager\",\n    \"PlatformIdentifier\",\n}\n\n# 特定のモジュールパス(=ファイル)をまるごとリネーム対象から除外する\nABSOLUTE_EXCLUDE_MODULES = {\n    \"utils/file_identifier.py\",\n    \"parsers/base_parser.py\",\n}\n\n###############################################################################\n# I/Oスロットリング用の定数\n###############################################################################\nWRITE_RATE_THRESHOLD = 10 * 1024 * 1024  # 10MB/s（しきい値、環境に合わせて調整）\nTHROTTLE_SLEEP_TIME = 0.5  # 待機時間（秒）\n\n###############################################################################\n# アプリ状態管理\n###############################################################################\nclass AUMAppState:\n    def __init__(self):\n        self.last_build_folder: Optional[str] = None\n\n###############################################################################\n# グローバルファイルロック（排他制御用）\n###############################################################################\n_file_lock_dict: Dict[str, threading.Lock] = {}\n_lock_dict_lock = threading.Lock()\n\ndef get_file_lock(file_path: str) -> threading.Lock:\n    with _lock_dict_lock:\n        if file_path not in _file_lock_dict:\n            _file_lock_dict[file_path] = threading.Lock()\n        return _file_lock_dict[file_path]\n\ndef with_file_lock(func):\n    @functools.wraps(func)\n    def wrapper(file_path, *args, **kwargs):\n        lock = get_file_lock(file_path)\n        with lock:\n            return func(file_path, *args, **kwargs)\n    return wrapper\n\n###############################################################################\n# リトライ処理の共通ラッパー\n###############################################################################\ndef retry_operation(max_attempts: int = 3):\n    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            attempt = 0\n            while attempt < max_attempts:\n                try:\n                    return func(*args, **kwargs)\n                except Exception as e:\n                    attempt += 1\n                    logging.error(f\"{func.__name__} attempt {attempt} failed: {e}\")\n                    if attempt >= max_attempts:\n                        raise\n        return wrapper\n    return decorator\n\n###############################################################################\n# ログ設定\n###############################################################################\ndef setup_logging(level='ERROR'):\n    logging.basicConfig(\n        filename='aum_error.log',\n        level=getattr(logging, level.upper(), logging.ERROR),\n        format='%(asctime)s:%(levelname)s:%(message)s',\n        encoding='utf-8'\n    )\n\nsetup_logging(level=LOG_LEVEL)\n\ndef log_info(widget: Optional[tk.Text], msg: str):\n    logging.info(msg)\n    if widget:\n        widget.insert(\"end\", msg + \"\\n\")\n        widget.see(\"end\")\n\ndef log_error(widget: Optional[tk.Text], msg: str):\n    logging.error(msg)\n    if widget:\n        widget.insert(\"end\", \"[ERROR] \" + msg + \"\\n\")\n        widget.see(\"end\")\n\n###############################################################################\n# with_io_throttling: 書き込み前にディスクI/O状況を監視してスロットリングするデコレータ\n###############################################################################\ndef with_io_throttling(func):\n    @functools.wraps(func)\n    def wrapper(file_path, *args, **kwargs):\n        io_before = psutil.disk_io_counters().write_bytes\n        time.sleep(THROTTLE_SLEEP_TIME)\n        io_after = psutil.disk_io_counters().write_bytes\n        write_rate = (io_after - io_before) / THROTTLE_SLEEP_TIME\n\n        if write_rate > WRITE_RATE_THRESHOLD:\n            logging.info(f\"High disk write rate detected: {write_rate / (1024*1024):.2f} MB/s. Throttling...\")\n            while write_rate > WRITE_RATE_THRESHOLD:\n                time.sleep(THROTTLE_SLEEP_TIME)\n                io_before = psutil.disk_io_counters().write_bytes\n                time.sleep(THROTTLE_SLEEP_TIME)\n                io_after = psutil.disk_io_counters().write_bytes\n                write_rate = (io_after - io_before) / THROTTLE_SLEEP_TIME\n                logging.info(f\"Throttling... current write rate: {write_rate / (1024*1024):.2f} MB/s\")\n        return func(file_path, *args, **kwargs)\n    return wrapper\n\n###############################################################################\n# ユーティリティ（ファイルI/O：排他制御＋スロットリング付き）\n###############################################################################\n@with_file_lock\n@with_io_throttling\ndef read_file_content(file_path: str, log_widget=None) -> str:\n    try:\n        with open(file_path, 'r', encoding='utf-8') as fr:\n            return fr.read()\n    except Exception as e:\n        log_error(log_widget, f\"ファイル読込失敗: {file_path}: {e}\")\n    return \"\"\n\n@with_file_lock\n@with_io_throttling\ndef write_file_content(file_path: str, content: str, log_widget=None):\n    try:\n        os.makedirs(os.path.dirname(file_path), exist_ok=True)\n        with open(file_path, 'w', encoding='utf-8') as fw:\n            fw.write(content)\n    except Exception as e:\n        log_error(log_widget, f\"ファイル書込み失敗: {file_path}: {e}\")\n\n###############################################################################\n# generate_summary: ファイル種別に応じたサマリー生成\n###############################################################################\ndef generate_summary(file_path: str, content: str) -> str:\n    if file_path.endswith('.py'):\n        try:\n            tree = ast.parse(content)\n            funcs = [n.name for n in ast.walk(tree) if isinstance(n, ast.FunctionDef)]\n            clss = [n.name for n in ast.walk(tree) if isinstance(n, ast.ClassDef)]\n            docstrings = ast.get_docstring(tree)\n            s = \"Pythonコード。\\n\"\n            if clss:\n                s += f\"クラス: {', '.join(clss)}。\\n\"\n            if funcs:\n                s += f\"関数: {', '.join(funcs)}。\\n\"\n            if docstrings:\n                s += f\"Docstring(冒頭60文字): {docstrings[:60]}...\\n\"\n            return s\n        except Exception as e:\n            logging.error(f\"AST解析失敗: {file_path}: {e}\", exc_info=True)\n            return f\"解析エラー: {e}\\n\"\n    elif file_path.endswith('.ipynb'):\n        try:\n            nb = json.loads(content)\n            cells = nb.get('cells', [])\n            m = sum(1 for c in cells if c.get('cell_type') == 'markdown')\n            c = sum(1 for c in cells if c.get('cell_type') == 'code')\n            return f\"Jupyter Notebook。\\nセル合計={len(cells)}, Markdown={m}, Code={c}\\n\"\n        except Exception as e:\n            return f\"Jupyter Notebook。\\nNotebook解析エラー: {e}\\n\"\n    elif file_path.endswith('.json'):\n        try:\n            obj = json.loads(content)\n            if isinstance(obj, dict):\n                keys = list(obj.keys())\n                keys_display = \", \".join(keys[:10])\n                if len(keys) > 10:\n                    keys_display += \"...\"\n                return f\"JSONファイル (辞書)。キー: {keys_display}\\n\"\n            elif isinstance(obj, list):\n                return f\"JSONファイル (リスト)。要素数: {len(obj)}\\n\"\n            else:\n                return \"JSONファイル (その他形式)。\\n\"\n        except Exception as e:\n            return f\"JSON解析エラー: {e}\\n\"\n    elif file_path.endswith('.tex'):\n        lines = content.count('\\n')\n        return f\"TeXファイル。行数: {lines}\\n\"\n    else:\n        lines = content.count('\\n')\n        return f\"その他テキスト。\\n行数: {lines}\\n\"\n\n###############################################################################\n# name_map.csv の衝突修正\n###############################################################################\ndef detect_symbol_collisions(mapping: Dict[str, str]) -> set:\n    from collections import defaultdict\n    rev = defaultdict(list)\n    for orig, short_ in mapping.items():\n        rev[short_].append(orig)\n    return {k for k, v in rev.items() if len(v) > 1}\n\ndef auto_rename_collisions_in_name_map(csv_path: str, log_widget=None) -> bool:\n    if not os.path.exists(csv_path):\n        return False\n    rows = []\n    try:\n        with open(csv_path, 'r', encoding='utf-8') as f:\n            rd = csv.DictReader(f)\n            for r in rd:\n                rows.append(r)\n    except Exception as e:\n        log_error(log_widget, f\"name_map.csv 読込失敗: {e}\")\n        return False\n    mapping = {r['original_name']: r['short_name'] for r in rows}\n    collisions = detect_symbol_collisions(mapping)\n    if not collisions:\n        return False\n    short_name_counter = {}\n    renamed_count = 0\n    for r in rows:\n        sh = r['short_name']\n        if sh in collisions:\n            base = sh\n            short_name_counter.setdefault(base, 0)\n            short_name_counter[base] += 1\n            r['short_name'] = base + str(short_name_counter[base])\n            renamed_count += 1\n    if renamed_count:\n        try:\n            with open(csv_path, 'w', encoding='utf-8', newline='') as fw:\n                w = csv.writer(fw)\n                w.writerow([\"original_name\", \"short_name\"])\n                for row in rows:\n                    w.writerow([row['original_name'], row['short_name']])\n            log_info(log_widget, f\"name_map.csv 衝突を {renamed_count} 件リネーム修正\")\n            return True\n        except Exception as e:\n            log_error(log_widget, f\"name_map.csv 修正書込失敗: {e}\")\n    return False\n\n###############################################################################\n# collected_scripts.json 修復\n###############################################################################\ndef auto_repair_collected_scripts(base_dir: str, log_widget) -> None:\n    cjson_path = os.path.join(base_dir, \"collected_scripts.json\")\n    if not os.path.exists(cjson_path):\n        return\n    try:\n        with open(cjson_path, 'r', encoding='utf-8') as fr:\n            content = fr.read().strip()\n            if not content:\n                raise ValueError(\"Empty file\")\n            data = json.loads(content)\n        if not isinstance(data, dict):\n            raise ValueError(\"トップレベルがdictではない\")\n        if \"scripts\" in data and not isinstance(data[\"scripts\"], list):\n            raise ValueError(\"'scripts'がlistではない\")\n    except Exception as e:\n        log_error(log_widget, f\"collected_scripts.json 破損: {e}\")\n        backup = cjson_path + \".bak\"\n        try:\n            shutil.move(cjson_path, backup)\n            log_info(log_widget, f\"破損ファイルをバックアップ→ {backup}\")\n        except Exception as e_backup:\n            log_error(log_widget, f\"バックアップ失敗: {e_backup}\")\n        data = {\n            \"system_overview\": \"このシステムは...（必要に応じて書き換え）\",\n            \"settings\": {},\n            \"scripts\": []\n        }\n        try:\n            with open(cjson_path, 'w', encoding='utf-8') as fw:\n                json.dump(data, fw, ensure_ascii=False, indent=2)\n            log_info(log_widget, \"collected_scripts.json を新規作成\")\n        except Exception as e_write:\n            log_error(log_widget, f\"新規作成失敗: {e_write}\")\n\n###############################################################################\n# 三方比較マージ\n###############################################################################\ndef merge_collected_scripts(original: dict, new: dict, file_data: Dict[str, dict], log_widget) -> dict:\n    _merge_system_and_settings(original, new)\n    old_scripts = original.get(\"scripts\", [])\n    old_map = {s['path']: s for s in old_scripts if 'path' in s}\n    new_scripts = new.get(\"scripts\", [])\n    new_map = {s.get('path'): s for s in new_scripts if s.get('path')}\n    for p, new_sc in new_map.items():\n        if p not in old_map:\n            old_map[p] = new_sc\n            log_info(log_widget, f\"新規追加: {p}\")\n        else:\n            old_sc = old_map[p]\n            _merge_single_script(old_sc, new_sc, file_data, p, log_widget)\n    merged_list = list(old_map.values())\n    original[\"scripts\"] = merged_list\n    return original\n\ndef _merge_system_and_settings(old: dict, new: dict):\n    if new.get(\"system_overview\"):\n        old[\"system_overview\"] = new[\"system_overview\"]\n    if \"settings\" in new and isinstance(new[\"settings\"], dict):\n        old.setdefault(\"settings\", {}).update(new[\"settings\"])\n\ndef _merge_single_script(old_sc: dict, new_sc: dict, file_data: Dict[str, dict], p: str, log_widget):\n    old_cont = old_sc.get(\"content\", \"\")\n    new_cont = new_sc.get(\"content\", \"\")\n    disk_cont = file_data.get(p, {}).get(\"content\", \"\")\n    conflict = (old_cont != disk_cont and new_cont != disk_cont and old_cont != new_cont)\n    if conflict:\n        log_info(log_widget, f\"衝突あり: {p}\")\n    else:\n        for k, v in new_sc.items():\n            if k != \"path\":\n                old_sc[k] = v\n        log_info(log_widget, f\"上書き: {p}\")\n\n###############################################################################\n# __init__.py 自動生成\n###############################################################################\ndef ensure_init_py_for_python_dirs(base_dir: str, log_widget):\n    exclude = {'venv', '__pycache__', '.git', 'outputs', 'temp_build', 'backups', DATA_FOLDER_NAME}\n    py_dirs = set()\n    for root, dirs, files in os.walk(base_dir):\n        dirs[:] = [d for d in dirs if d not in exclude]\n        if any(f.endswith('.py') for f in files):\n            py_dirs.add(root)\n    for d in py_dirs:\n        init_file = os.path.join(d, \"__init__.py\")\n        if not os.path.isfile(init_file):\n            write_file_content(init_file, \"\", log_widget)\n            log_info(log_widget, f\"__init__.py 自動生成: {os.path.relpath(init_file, base_dir)}\")\n\n###############################################################################\n# config.json => collected_scripts.json の更新（設定取り込み）\n###############################################################################\ndef restore_settings_from_config_json(base_dir: str, log_widget):\n    \"\"\"\n    your_project直下または your_project/your_project にある config.json を探し、\n    設定情報を collected_scripts.json の settings に反映する。\n    ※設定ファイル内に \"settings\" キーがあれば、その中身を使用（なければファイル全体を設定として取り込む）\n    \"\"\"\n    possible_cfg_paths = [\n        os.path.join(base_dir, \"your_project\", \"config.json\"),\n        os.path.join(base_dir, \"your_project\", \"your_project\", \"config.json\"),\n    ]\n\n    cfg_path = None\n    for pth in possible_cfg_paths:\n        if os.path.isfile(pth):\n            cfg_path = pth\n            break\n\n    if not cfg_path:\n        log_info(log_widget, \"config.jsonが見つからないためスキップ\")\n        return\n\n    cjson_path = os.path.join(base_dir, \"collected_scripts.json\")\n    if not os.path.isfile(cjson_path):\n        log_info(log_widget, f\"collected_scripts.jsonが無い: {cjson_path}\")\n        return\n\n    try:\n        with open(cfg_path, 'r', encoding='utf-8') as f:\n            cfg_data = json.load(f)\n    except Exception as e:\n        log_error(log_widget, f\"config.json 読込失敗: {e}\")\n        return\n\n    if not isinstance(cfg_data, dict):\n        log_info(log_widget, \"config.jsonの形式が不正です。\")\n        return\n\n    # 設定ファイル内に \"settings\" キーがあれば、その中身を使用（なければ全体を設定として取り込む）\n    new_settings = cfg_data.get(\"settings\", cfg_data)\n\n    try:\n        with open(cjson_path, 'r', encoding='utf-8') as fr:\n            cjson_data = json.load(fr)\n        if not isinstance(cjson_data, dict):\n            cjson_data = {\"system_overview\": \"\", \"settings\": {}, \"scripts\": []}\n    except Exception as e:\n        log_error(log_widget, f\"collected_scripts.json 読込失敗: {e}\")\n        return\n\n    # 既存の設定に新しい設定をマージする\n    cjson_data.setdefault(\"settings\", {}).update(new_settings)\n    try:\n        with open(cjson_path, 'w', encoding='utf-8') as fw:\n            json.dump(cjson_data, fw, ensure_ascii=False, indent=2)\n        log_info(log_widget, f\"{cjson_path} の settings を更新しました\")\n    except Exception as e:\n        log_error(log_widget, f\"config→collected_scripts 反映失敗: {e}\")\n\n###############################################################################\n# コード収集\n###############################################################################\ndef collect(base_dir: str, log_widget):\n    log_info(log_widget, \"コード収集を開始...\")\n    auto_repair_collected_scripts(base_dir, log_widget)\n    output_folder = os.path.join(base_dir, \"your_project\")\n    cjson = os.path.join(base_dir, \"collected_scripts.json\")\n    if os.path.isfile(cjson):\n        try:\n            with open(cjson, 'r', encoding='utf-8') as fr:\n                old_data = json.load(fr)\n            if not isinstance(old_data, dict):\n                old_data = {\"system_overview\": \"\", \"settings\": {}, \"scripts\": []}\n        except Exception as e:\n            log_error(log_widget, f\"collected_scripts.json 読込失敗: {e}\")\n            old_data = {\"system_overview\": \"\", \"settings\": {}, \"scripts\": []}\n    else:\n        old_data = {\"system_overview\": \"\", \"settings\": {}, \"scripts\": []}\n\n    ensure_init_py_for_python_dirs(base_dir, log_widget)\n\n    target_ext = ('.py', '.json', '.tex')\n    exclude_dirs = {'venv', '__pycache__', '.git', 'outputs', 'temp_build', 'backups', DATA_FOLDER_NAME}\n    exclude_files = {'collected_scripts.json', 'config.json', os.path.basename(__file__)}\n    new_list = []\n    for root, dirs, files in os.walk(base_dir):\n        abs_root = os.path.abspath(root)\n        abs_output = os.path.abspath(output_folder)\n        if abs_root.startswith(abs_output) and \"your_project\" not in os.path.relpath(abs_root, base_dir).split(os.sep)[0]:\n            continue\n        dirs[:] = [d for d in dirs if d not in exclude_dirs]\n        for fn in files:\n            if fn in exclude_files:\n                continue\n            if any(fn.endswith(e) for e in target_ext):\n                fp = os.path.join(root, fn)\n                if fp == os.path.abspath(__file__):\n                    continue\n                content = read_file_content(fp, log_widget)\n                if not content:\n                    continue\n                relp = os.path.relpath(fp, base_dir)\n                summ = generate_summary(fp, content)\n                new_list.append({\"path\": relp, \"overview\": summ, \"content\": content})\n                log_info(log_widget, f\"収集: {relp}\")\n\n    file_map = {s[\"path\"]: {\"content\": s[\"content\"], \"overview\": s[\"overview\"]} for s in new_list}\n    merged = merge_collected_scripts(\n        old_data,\n        {\n            \"system_overview\": old_data.get(\"system_overview\", \"\"),\n            \"settings\": old_data.get(\"settings\", {}),\n            \"scripts\": new_list\n        },\n        file_map,\n        log_widget\n    )\n    try:\n        with open(cjson, 'w', encoding='utf-8') as fw:\n            json.dump(merged, fw, ensure_ascii=False, indent=2)\n        log_info(log_widget, f\"出力完了: {cjson}\")\n    except Exception as e:\n        log_error(log_widget, f\"書き込み失敗: {e}\")\n    # 変更：ここで設定取り込み処理を呼び出して、config.json の変更を反映\n    restore_settings_from_config_json(base_dir, log_widget)\n\n###############################################################################\n# Pythonシンボル名 短縮（name_map.csv生成）\n###############################################################################\ndef collect_python_symbols_for_map(base_dir: str) -> set:\n    import ast\n    exclude = {'venv', '__pycache__', '.git', 'temp_build', 'backups', DATA_FOLDER_NAME}\n    found = set()\n    for root, dirs, files in os.walk(base_dir):\n        if os.path.basename(root).lower() == DATA_FOLDER_NAME.lower():\n            continue\n        abs_root = os.path.abspath(root)\n        abs_output = os.path.abspath(os.path.join(base_dir, \"your_project\"))\n        if abs_root.startswith(abs_output):\n            continue\n        dirs[:] = [d for d in dirs if d not in exclude]\n        for f in files:\n            if f.endswith('.py') and f != os.path.basename(__file__):\n                fp = os.path.join(root, f)\n                txt = read_file_content(fp, log_widget=None)\n                if not txt:\n                    continue\n                try:\n                    tree = ast.parse(txt)\n                    for node in ast.walk(tree):\n                        if isinstance(node, ast.FunctionDef):\n                            found.add(node.name)\n                        elif isinstance(node, ast.ClassDef):\n                            found.add(node.name)\n                    # Note: 変数名などは収集しない\n                except Exception as e:\n                    logging.error(f\"AST解析失敗: {fp}: {e}\", exc_info=True)\n    return found\n\ndef generate_short_symbol_name(orig_name: str) -> str:\n    if not orig_name:\n        return \"x\"\n    return orig_name[0].lower() + \"_sym\"\n\ndef ensure_name_map_csv_exists(base_dir: str) -> str:\n    csv_path = os.path.join(base_dir, \"name_map.csv\")\n    syms = collect_python_symbols_for_map(base_dir)\n    existing_map = {}\n    if os.path.isfile(csv_path):\n        try:\n            with open(csv_path, 'r', encoding='utf-8') as fr:\n                rd = csv.DictReader(fr)\n                for r in rd:\n                    o = r['original_name']\n                    s = r['short_name']\n                    existing_map[o] = s\n        except Exception as e:\n            logging.error(f\"name_map.csv 読込失敗: {e}\")\n    else:\n        logging.info(\"name_map.csvがないため新規作成\")\n    for s in syms:\n        if s in TOKEN_SAVING_EXCLUDE_LIST:\n            existing_map[s] = s\n        else:\n            if s not in existing_map:\n                existing_map[s] = generate_short_symbol_name(s)\n    with open(csv_path, 'w', encoding='utf-8', newline='') as fw:\n        w = csv.writer(fw)\n        w.writerow([\"original_name\", \"short_name\"])\n        for k, v in sorted(existing_map.items()):\n            w.writerow([k, v])\n    return csv_path\n\ndef load_name_mappings(csv_path: str) -> dict:\n    mp = {}\n    if not os.path.isfile(csv_path):\n        return mp\n    try:\n        with open(csv_path, 'r', encoding='utf-8') as fr:\n            rd = csv.DictReader(fr)\n            for r in rd:\n                mp[r['original_name']] = r['short_name']\n    except Exception as e:\n        logging.error(f\"name_map.csv 読込失敗: {e}\")\n    return mp\n\ndef invert_name_map(mp: dict) -> dict:\n    return {v: k for k, v in mp.items()}\n\n###############################################################################\n# AST変換 (識別子置換)\n###############################################################################\nGLOBAL_EXCLUDE_MODULES = ABSOLUTE_EXCLUDE_MODULES\n\ndef apply_name_mappings_ast(content: str, name_map: dict, path_for_check: str) -> str:\n    norm_path = os.path.normpath(path_for_check).replace(\"\\\\\", \"/\")\n    for exc_subpath in GLOBAL_EXCLUDE_MODULES:\n        if exc_subpath in norm_path:\n            return content\n\n    FIXED_EXCLUDE = TOKEN_SAVING_EXCLUDE_LIST.copy()\n    current_exclude = set()\n    attempt = 0\n\n    def extract_problematic_identifier(e: Exception) -> Optional[str]:\n        msg = str(e)\n        m = re.search(r\"object has no attribute '(\\w+)'\", msg)\n        if m:\n            return m.group(1)\n        m = re.search(r\"cannot import name '(\\w+)'\", msg)\n        if m:\n            return m.group(1)\n        return None\n\n    class ImportCollector(ast.NodeVisitor):\n        def __init__(self):\n            self.imported = set()\n        def visit_Import(self, node):\n            for alias in node.names:\n                name = alias.asname if alias.asname else alias.name.split('.')[0]\n                self.imported.add(name)\n            self.generic_visit(node)\n        def visit_ImportFrom(self, node):\n            if node.module:\n                self.imported.add(node.module.split('.')[0])\n            for alias in node.names:\n                name = alias.asname if alias.asname else alias.name\n                self.imported.add(name)\n            self.generic_visit(node)\n\n    class Mapper(ast.NodeTransformer):\n        def __init__(self, exclude_set, nmap):\n            self.exclude = exclude_set\n            self.nmap = nmap\n            super().__init__()\n        def visit_Name(self, node):\n            if node.id.startswith(\"__\") and node.id.endswith(\"__\"):\n                return node\n            if node.id in self.exclude:\n                return node\n            if node.id in self.nmap:\n                node.id = self.nmap[node.id]\n            return self.generic_visit(node)\n        def visit_FunctionDef(self, node):\n            if not (node.name.startswith(\"__\") and node.name.endswith(\"__\")):\n                if node.name not in self.exclude and node.name in self.nmap:\n                    node.name = self.nmap[node.name]\n            self.generic_visit(node)\n            return node\n        def visit_ClassDef(self, node):\n            if not (node.name.startswith(\"__\") and node.name.endswith(\"__\")):\n                if node.name not in self.exclude and node.name in self.nmap:\n                    node.name = self.nmap[node.name]\n            self.generic_visit(node)\n            return node\n        def visit_Attribute(self, node):\n            self.generic_visit(node)\n            if isinstance(node.value, ast.Name) and node.value.id == \"self\":\n                return node\n            if node.attr.startswith(\"__\") and node.attr.endswith(\"__\"):\n                return node\n            if node.attr in self.exclude:\n                return node\n            if node.attr in self.nmap:\n                node.attr = self.nmap[node.attr]\n            return node\n        def visit_Import(self, node):\n            for alias in node.names:\n                if alias.name in self.exclude:\n                    continue\n                if alias.name in self.nmap:\n                    alias.name = self.nmap[alias.name]\n                if alias.asname and alias.asname in self.nmap:\n                    alias.asname = self.nmap[alias.asname]\n            return node\n        def visit_ImportFrom(self, node):\n            if node.module in self.exclude:\n                return node\n            if node.module and (node.module in self.nmap):\n                node.module = self.nmap[node.module]\n            for alias in node.names:\n                if alias.name in self.exclude:\n                    continue\n                if alias.name in self.nmap:\n                    alias.name = self.nmap[alias.name]\n                if alias.asname and alias.asname in self.nmap:\n                    alias.asname = self.nmap[alias.asname]\n            return node\n\n    while True:\n        attempt += 1\n        try:\n            tree = ast.parse(content)\n            collector = ImportCollector()\n            collector.visit(tree)\n            auto_exclude = collector.imported\n            total_exclude = FIXED_EXCLUDE.union(auto_exclude).union(current_exclude)\n            mapper = Mapper(total_exclude, name_map)\n            new_tree = mapper.visit(tree)\n            ast.fix_missing_locations(new_tree)\n            if hasattr(ast, 'unparse'):\n                out_code = ast.unparse(new_tree)\n            else:\n                import astor\n                out_code = astor.to_source(new_tree)\n            return out_code\n        except Exception as e:\n            problematic = extract_problematic_identifier(e)\n            if problematic:\n                if problematic not in current_exclude:\n                    current_exclude.add(problematic)\n                    # 問題の識別子を除外リストに加えて再試行\n                    continue\n                else:\n                    return content\n            else:\n                return content\n\n###############################################################################\n# 依存解析 & トポロジカルソート\n###############################################################################\ndef analyze_dependencies_and_sort(scripts: List[Dict[str, str]], log_widget=None) -> List[Dict[str, str]]:\n    graph = {}\n    script_map = {}\n    for sc in scripts:\n        p = sc[\"path\"]\n        script_map[p] = sc\n        graph[p] = set()\n\n    path_by_modname = {}\n    for sc in scripts:\n        p = sc[\"path\"].replace(\"\\\\\", \"/\")\n        if p.endswith(\".py\"):\n            mod_parts = p.split(\"/\")\n            if mod_parts[0] == \"your_project\":\n                mod_parts = mod_parts[1:]\n            py_base = mod_parts[-1][:-3]\n            mod_parts[-1] = py_base\n            dotted = \".\".join(mod_parts)\n            path_by_modname[dotted] = p\n\n    def extract_imports(p: str, code: str) -> Set[str]:\n        import_list = set()\n        try:\n            tree = ast.parse(code)\n            for node in ast.walk(tree):\n                if isinstance(node, ast.Import):\n                    for alias in node.names:\n                        mod = alias.name.split(\".\")[0]\n                        import_list.add(mod)\n                elif isinstance(node, ast.ImportFrom):\n                    if node.module:\n                        base = node.module.split(\".\")[0]\n                        import_list.add(base)\n        except Exception as e:\n            logging.error(f\"依存関係解析: {p} の import抽出失敗: {e}\", exc_info=True)\n            log_error(log_widget, f\"依存関係解析: {p} の import抽出失敗: {e}\")\n        return import_list\n\n    for sc in scripts:\n        p = sc[\"path\"]\n        code = sc[\"content\"]\n        imported = extract_imports(p, code)\n        for imp in imported:\n            for k, v in path_by_modname.items():\n                if k.startswith(imp):\n                    graph[p].add(v)\n\n    in_degree = {p: 0 for p in graph}\n    for p in graph:\n        for dep in graph[p]:\n            if dep in in_degree:\n                in_degree[dep] += 1\n\n    queue = deque([p for p in in_degree if in_degree[p] == 0])\n    sorted_paths = []\n    while queue:\n        u = queue.popleft()\n        sorted_paths.append(u)\n        for dep in graph[u]:\n            if dep in in_degree:\n                in_degree[dep] -= 1\n                if in_degree[dep] == 0:\n                    queue.append(dep)\n\n    if len(sorted_paths) < len(graph):\n        logging.error(\"循環依存関係が検出されました。依存順序の保証ができません。\")\n        log_error(log_widget, \"循環依存関係が検出されました。依存順序の保証ができません。\")\n        return scripts\n\n    path_to_index = {p: i for i, p in enumerate(sorted_paths)}\n    scripts_sorted = sorted(scripts, key=lambda sc: path_to_index.get(sc[\"path\"], 999999))\n    return scripts_sorted\n\n###############################################################################\n# ビルド (順序づけ + AST変換 + 出力)\n###############################################################################\ndef build_in_topological_order(\n    scripts: List[Dict[str, str]],\n    out_dir: str,\n    name_map: Dict[str, str],\n    log_widget\n) -> Tuple[int, List[str]]:\n    sorted_scripts = analyze_dependencies_and_sort(scripts, log_widget)\n    logs = []\n    built_count = 0\n    for sc in sorted_scripts:\n        path_ = sc[\"path\"]\n        content_ = sc[\"content\"]\n        if path_.endswith(\".py\"):\n            new_content = apply_name_mappings_ast(content_, name_map, path_for_check=path_)\n        elif path_.endswith(\".json\") or path_.endswith(\".tex\"):\n            new_content = content_\n        else:\n            continue\n        rel_path = path_\n        if rel_path.startswith(\"your_project\" + os.path.sep):\n            rel_path = rel_path[len(\"your_project\" + os.path.sep):]\n        target_file = os.path.join(out_dir, rel_path)\n        try:\n            os.makedirs(os.path.dirname(target_file), exist_ok=True)\n            with open(target_file, 'w', encoding='utf-8') as fw:\n                fw.write(new_content)\n            logs.append(f\"出力: {os.path.relpath(target_file, out_dir)}\")\n            built_count += 1\n        except Exception as e:\n            logs.append(f\"書き込みエラー: {target_file}: {e}\")\n    return built_count, logs\n\ndef verify_build_results(build_dir: str, collected_json: str, log_widget) -> bool:\n    try:\n        with open(collected_json, 'r', encoding='utf-8') as fr:\n            data = json.load(fr)\n    except Exception as e:\n        log_error(log_widget, f\"収集結果の読込失敗: {e}\")\n        return False\n    scripts = data.get(\"scripts\", [])\n    all_ok = True\n    for script in scripts:\n        p = script.get(\"path\", \"\")\n        if not (p.endswith(\".py\") or p.endswith(\".json\") or p.endswith(\".tex\")):\n            continue\n        rel_path = p\n        if rel_path.startswith(\"your_project\" + os.path.sep):\n            rel_path = rel_path[len(\"your_project\" + os.path.sep):]\n        build_file = os.path.join(build_dir, rel_path)\n        if not os.path.exists(build_file):\n            log_error(log_widget, f\"ビルド結果に存在しない: {build_file}\")\n            all_ok = False\n            continue\n        with open(build_file, 'r', encoding='utf-8') as bf:\n            build_content = bf.read()\n        if build_content.strip() != script.get(\"content\", \"\").strip():\n            log_error(log_widget, f\"内容不一致: {build_file}\")\n            all_ok = False\n        else:\n            log_info(log_widget, f\"チェックOK: {build_file}\")\n    return all_ok\n\ndef _build_config_json(settings: dict, out_dir: str, log_widget):\n    cfg_path = os.path.join(out_dir, \"config.json\")\n    try:\n        os.makedirs(os.path.dirname(cfg_path), exist_ok=True)\n        with open(cfg_path, 'w', encoding='utf-8') as fw:\n            json.dump(settings, fw, ensure_ascii=False, indent=2)\n        log_info(log_widget, f\"config.json生成: {cfg_path}\")\n    except Exception as e:\n        log_error(log_widget, f\"config.json 書込失敗: {e}\")\n\ndef build_project(base_dir: str, log_widget, is_obfuscate: bool = True) -> Optional[str]:\n    auto_repair_collected_scripts(base_dir, log_widget)\n    cjson = os.path.join(base_dir, \"collected_scripts.json\")\n    if not os.path.isfile(cjson):\n        messagebox.showerror(\"エラー\", f\"{cjson} が見つかりません\")\n        return None\n    csv_path = ensure_name_map_csv_exists(base_dir)\n    direct_map = load_name_mappings(csv_path)\n    final_map = direct_map if is_obfuscate else invert_name_map(direct_map)\n    tmp_build_root = os.path.join(base_dir, \"temp_build\")\n    os.makedirs(tmp_build_root, exist_ok=True)\n    out_dir = os.path.join(tmp_build_root, \"your_project_temp\")\n    if os.path.exists(out_dir):\n        shutil.rmtree(out_dir)\n    os.makedirs(out_dir, exist_ok=True)\n    log_info(log_widget, f\"一時出力先: {out_dir}\")\n    # collected_scripts.json の読み込み（破損時は例外を補足）\n    try:\n        with open(cjson, 'r', encoding='utf-8') as fr:\n            data = json.load(fr)\n    except Exception as e:\n        log_error(log_widget, f\"collected_scripts.json 読込失敗: {e}\")\n        return None\n    scripts = data.get(\"scripts\", [])\n    settings = data.get(\"settings\", {})\n    py_scripts = [s for s in scripts if s.get(\"path\", \"\").endswith((\".py\", \".json\", \".tex\"))]\n    built_count, logs_texts = build_in_topological_order(py_scripts, out_dir, final_map, log_widget)\n    _build_config_json(settings, out_dir, log_widget)\n    built_count += 1\n    if logs_texts:\n        for l in logs_texts:\n            log_info(log_widget, l)\n    log_info(log_widget, f\"ビルド完了: {built_count}ファイル\")\n    if not verify_build_results(out_dir, cjson, log_widget):\n        log_error(log_widget, \"収集結果とビルド結果が一致しません\")\n    else:\n        log_info(log_widget, \"ビルド結果と収集結果は一致しています\")\n    # 変更：ビルド後にも設定取り込みを実施\n    restore_settings_from_config_json(base_dir, log_widget)\n    return out_dir\n\ndef finalize_build(temp_out_dir: str, base_dir: str, log_widget, app_state: AUMAppState) -> bool:\n    if not temp_out_dir or not os.path.isdir(temp_out_dir):\n        log_error(log_widget, f\"一時フォルダが無い: {temp_out_dir}\")\n        return False\n    final_dir = os.path.join(base_dir, \"your_project\")\n    if os.path.isdir(final_dir):\n        backups_dir = os.path.join(base_dir, \"backups\")\n        os.makedirs(backups_dir, exist_ok=True)\n        stamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        zip_name = os.path.join(backups_dir, f\"your_project_old_{stamp}\")\n        try:\n            shutil.make_archive(zip_name, 'zip', final_dir)\n            log_info(log_widget, f\"既存フォルダをzipバックアップ: {zip_name}.zip\")\n        except Exception as e:\n            log_error(log_widget, f\"バックアップ失敗: {e}\")\n        try:\n            shutil.rmtree(final_dir)\n            log_info(log_widget, f\"既存フォルダ削除: {final_dir}\")\n        except Exception as e:\n            log_error(log_widget, f\"削除失敗: {e}\")\n            return False\n    try:\n        shutil.move(temp_out_dir, final_dir)\n        log_info(log_widget, f\"上書き完了: {final_dir}\")\n        app_state.last_build_folder = base_dir\n    except Exception as e:\n        log_error(log_widget, f\"移動失敗: {e}\")\n        return False\n    tmpb = os.path.join(base_dir, \"temp_build\")\n    if os.path.isdir(tmpb):\n        try:\n            shutil.rmtree(tmpb)\n            log_info(log_widget, f\"temp_build削除: {tmpb}\")\n        except Exception as ex:\n            log_error(log_widget, f\"temp_build削除失敗: {ex}\")\n    log_info(log_widget, f\"last_build_folder={app_state.last_build_folder}\")\n    return True\n\n###############################################################################\n# トークン節約ビルド（衝突検知＆リトライ処理）\n###############################################################################\ndef token_saving_build_unified(base_dir: str, log_widget, app_state: AUMAppState):\n    tries = 0\n    while True:  # リトライ（最大 MAX_REBUILD_ATTEMPTS 回）\n        tries += 1\n        log_info(log_widget, f\"ビルド試行 {tries} 回目\")\n        tmp_out = None\n        try:\n            tmp_out = build_project(base_dir, log_widget, is_obfuscate=True)\n        except Exception as e:\n            log_error(log_widget, f\"ビルド処理で例外発生: {e}\")\n            break\n        if not tmp_out:\n            log_error(log_widget, \"build_project で出力が得られませんでした。\")\n            if tries >= MAX_REBUILD_ATTEMPTS:\n                log_error(log_widget, \"ビルドを中断します。\")\n                break\n            continue\n        result = finalize_build(tmp_out, base_dir, log_widget, app_state)\n        if not result:\n            log_error(log_widget, \"ビルドの適用に失敗しました。処理を中断します。\")\n            break\n        run_main_py(base_dir, log_widget, app_state)\n        all_logs = log_widget.get(\"1.0\", \"end\")\n        if (\"ImportError:\" in all_logs) or (\"NameError:\" in all_logs) or (\"ModuleNotFoundError:\" in all_logs):\n            log_info(log_widget, \"エラー検知（名前衝突等）。name_map.csv を修正して再試行します。\")\n            csvp = os.path.join(base_dir, \"name_map.csv\")\n            fixed = auto_rename_collisions_in_name_map(csvp, log_widget)\n            if fixed:\n                continue\n            else:\n                log_error(log_widget, \"name_map.csv の自動修正に失敗しました。再試行します。\")\n                continue\n        else:\n            log_info(log_widget, \"ビルド成功：エラーなし。\")\n            break\n\ndef restore_build(base_dir: str, log_widget, app_state: AUMAppState):\n    try:\n        tmp_out = build_project(base_dir, log_widget, is_obfuscate=False)\n    except Exception as e:\n        log_error(log_widget, f\"ビルド処理で例外発生: {e}\")\n        return\n    if tmp_out:\n        finalize_build(tmp_out, base_dir, log_widget, app_state)\n\ndef run_main_py(base_dir: str, log_widget, app_state: AUMAppState):\n    final_dir = os.path.join(base_dir, \"your_project\")\n    main_py = os.path.join(final_dir, \"main.py\")\n    data_path = os.path.join(final_dir, DATA_FOLDER_NAME)\n    log_info(log_widget, f\"main.py => {main_py}\")\n    log_info(log_widget, f\"--input => {data_path}\")\n    if not os.path.isfile(main_py):\n        log_error(log_widget, \"main.py が見つからない\")\n        return\n    if not os.path.isdir(data_path):\n        log_info(log_widget, f\"dataフォルダなし: {data_path}\")\n    cmd = [sys.executable, main_py, \"--input\", data_path, \"--skip-parent-dir\"]\n    try:\n        ret = subprocess.run(cmd, capture_output=True, text=True)\n        if ret.returncode != 0:\n            log_error(log_widget, f\"エラー({ret.returncode}): {ret.stderr}\")\n        else:\n            log_info(log_widget, f\"実行完了:\\n{ret.stdout}\")\n    except Exception as e:\n        log_error(log_widget, f\"例外: {e}\")\n    gather_run_artifacts(base_dir, log_widget, app_state)\n    generate_summary_report(base_dir, log_widget)\n\ndef gather_run_artifacts(base_dir: str, log_widget, app_state: AUMAppState):\n    final_dir = os.path.join(base_dir, \"your_project\")\n    if not os.path.isdir(final_dir):\n        log_info(log_widget, \"your_projectがない\")\n        return\n    runs = []\n    for d in os.listdir(final_dir):\n        if d.startswith(\"run_\"):\n            fp = os.path.join(final_dir, d)\n            if os.path.isdir(fp):\n                runs.append(fp)\n    if not runs:\n        log_info(log_widget, \"run_*** フォルダなし\")\n        return\n    runs.sort(key=lambda x: os.path.getmtime(x), reverse=True)\n    latest = runs[0]\n    log_info(log_widget, f\"最新run: {latest}\")\n    fb_src = os.path.join(latest, \"feedback.json\")\n    fb_dst = os.path.join(base_dir, \"feedback.json\")\n    if os.path.isfile(fb_src):\n        shutil.copy2(fb_src, fb_dst)\n        log_info(log_widget, f\"feedback.json-> {fb_dst}\")\n    logs_src = os.path.join(latest, \"logs\")\n    logs_dst = os.path.join(base_dir, \"logs\")\n    if os.path.isdir(logs_src):\n        if os.path.isdir(logs_dst):\n            shutil.rmtree(logs_dst)\n        shutil.copytree(logs_src, logs_dst)\n        log_info(log_widget, f\"logs-> {logs_dst}\")\n    else:\n        log_info(log_widget, \"logsなし\")\n\ndef generate_summary_report(base_dir: str, log_widget):\n    log_info(log_widget, \"レポート生成...\")\n    fb = os.path.join(base_dir, \"feedback.json\")\n    logsdir = os.path.join(base_dir, \"logs\")\n    outp = os.path.join(base_dir, \"summary_report.md\")\n    data = {}\n    if os.path.isfile(fb):\n        try:\n            with open(fb, 'r', encoding='utf-8') as fr:\n                data = json.load(fr)\n        except Exception as e:\n            log_error(log_widget, f\"feedback.json 読込失敗: {e}\")\n    tstamp = data.get(\"timestamp\", \"N/A\")\n    st = data.get(\"status\", \"N/A\")\n    remarks = data.get(\"remarks\", \"\")\n    improvs = data.get(\"possible_improvements\", [])\n    logs_sum = []\n    if os.path.isdir(logsdir):\n        for root, dirs, files in os.walk(logsdir):\n            for f in files:\n                if f.endswith(\".log\"):\n                    fp = os.path.join(root, f)\n                    try:\n                        with open(fp, 'r', encoding='utf-8', errors='replace') as ff:\n                            lines = ff.readlines()\n                        errwarn = [ln.strip() for ln in lines if (\"ERROR\" in ln or \"WARN\" in ln)]\n                        relp = os.path.relpath(fp, logsdir)\n                        logs_sum.append((relp, errwarn))\n                    except Exception as e:\n                        log_error(log_widget, f\"ログファイル読込失敗: {fp}: {e}\")\n    try:\n        with open(outp, 'w', encoding='utf-8') as fw:\n            fw.write(\"# Analysis Summary Report\\n\\n\")\n            fw.write(f\"- **Timestamp**: {tstamp}\\n\")\n            fw.write(f\"- **Status**: {st}\\n\")\n            fw.write(f\"- **Remarks**: {remarks}\\n\\n\")\n            fw.write(\"## Possible Improvements\\n\")\n            if improvs:\n                for x in improvs:\n                    fw.write(f\"- {x}\\n\")\n            else:\n                fw.write(\"- (No improvements)\\n\")\n            fw.write(\"\\n## Logs Summary (ERROR/WARN lines)\\n\")\n            if logs_sum:\n                for (fn, lines) in logs_sum:\n                    fw.write(f\"### {fn}\\n\")\n                    if lines:\n                        for ln in lines:\n                            fw.write(f\"- {ln}\\n\")\n                    else:\n                        fw.write(\"- (No ERROR/WARN)\\n\")\n            else:\n                fw.write(\"(No logs found or no ERROR/WARN lines)\\n\")\n        log_info(log_widget, f\"summary_report.md 生成: {outp}\")\n    except Exception as e:\n        log_error(log_widget, f\"summary_report.md 書込失敗: {e}\")\n\n###############################################################################\n# 新機能：コードパッチ適用\n###############################################################################\ndef apply_code_patch(file_path: str, target: str, new_code: str, log_widget=None, preview: bool = False):\n    \"\"\"\n    指定ファイル内の target（\"クラス名.関数名\" または \"クラス名\" または \"関数名\"）の定義を\n    new_code で置き換える。preview=True の場合は適用後の全文を返す（ファイルは更新しない）。\n    \"\"\"\n    content = read_file_content(file_path, log_widget)\n    if not content:\n        log_error(log_widget, f\"{file_path} の読込に失敗しました\")\n        return None if preview else False\n    lines = content.splitlines(keepends=True)\n    start_idx = None\n    target_indent = 0\n\n    # targetにドットが含まれる場合はクラス内メソッドを対象とする\n    if \".\" in target:\n        class_name, func_name = target.split(\".\", 1)\n        for i, line in enumerate(lines):\n            if line.lstrip().startswith(\"class \") and f\"class {class_name}\" in line:\n                base_indent = len(line) - len(line.lstrip())\n                for j in range(i+1, len(lines)):\n                    lj = lines[j]\n                    if lj.strip() != \"\" and (len(lj) - len(lj.lstrip()) <= base_indent):\n                        break\n                    if lj.lstrip().startswith(\"def \") and f\"def {func_name}\" in lj:\n                        start_idx = j\n                        target_indent = len(lj) - len(lj.lstrip())\n                        break\n                break\n    else:\n        # 単体の関数またはクラス定義を対象とする\n        for i, line in enumerate(lines):\n            if (line.lstrip().startswith(\"def \") or line.lstrip().startswith(\"class \")) and f\" {target}\" in line:\n                start_idx = i\n                target_indent = len(line) - len(line.lstrip())\n                break\n\n    if start_idx is None:\n        log_error(log_widget, f\"対象が見つかりません: {target}\")\n        return None if preview else False\n\n    # 終了位置の検出（同じインデント以下の行が来たら終了とする）\n    end_idx = None\n    for i in range(start_idx+1, len(lines)):\n        if lines[i].strip() == \"\":\n            continue\n        if (len(lines[i]) - len(lines[i].lstrip())) <= target_indent:\n            end_idx = i\n            break\n    if end_idx is None:\n        end_idx = len(lines)\n\n    # new_code の各行に target のインデントを付与\n    indent_str = \" \" * target_indent\n    patched_lines = []\n    for ln in new_code.splitlines():\n        if ln.strip() == \"\":\n            patched_lines.append(\"\\n\")\n        else:\n            patched_lines.append(indent_str + ln.lstrip() + \"\\n\")\n    new_block = \"\".join(patched_lines)\n    new_content = \"\".join(lines[:start_idx] + [new_block] + lines[end_idx:])\n    if preview:\n        return new_content\n    else:\n        write_file_content(file_path, new_content, log_widget)\n        log_info(log_widget, f\"{file_path} の {target} にパッチ適用しました\")\n        return True\n\n###############################################################################\n# 新機能：システムパッチ適用（収集・ビルド・反映の一括実行）\n###############################################################################\ndef apply_system_patch(base_dir: str, log_widget, app_state: AUMAppState) -> bool:\n    log_info(log_widget, \"=== システムパッチ適用開始 ===\")\n    collect(base_dir, log_widget)\n    tmp_out = build_project(base_dir, log_widget, is_obfuscate=True)\n    if not tmp_out:\n        log_error(log_widget, \"ビルドに失敗しました。パッチ適用を中止します。\")\n        return False\n    result = finalize_build(tmp_out, base_dir, log_widget, app_state)\n    if not result:\n        log_error(log_widget, \"システムパッチの最終適用に失敗しました。\")\n        return False\n    log_info(log_widget, \"=== システムパッチ適用完了 ===\")\n    return True\n\n###############################################################################\n# 新機能：パッチ関連UIの実装\n###############################################################################\ndef patch_ui(parent_frame, base_dir: str, log_widget, app_state: AUMAppState):\n    frm_patch = tk.Frame(parent_frame)\n    frm_patch.pack(side=tk.TOP, fill=tk.X, pady=5)\n\n    def on_code_edit():\n        file_path = filedialog.askopenfilename(title=\"パッチ対象のファイルを選択\",\n                                               initialdir=base_dir,\n                                               filetypes=[(\"Python files\", \"*.py\")])\n        if not file_path:\n            return\n        target = simpledialog.askstring(\"対象指定\", \"編集するクラス.関数 または 関数/クラス名を入力してください:\")\n        if not target:\n            return\n        original_code = read_file_content(file_path, log_widget)\n        if not original_code:\n            return\n        edited_code = simpledialog.askstring(\"コード編集\", f\"{target} の新しいコードを入力してください:\",\n                                              initialvalue=\"\")\n        if edited_code:\n            # 一時的に保存\n            patch_ui.last_edit = (file_path, target, edited_code)\n            log_info(log_widget, f\"コード編集内容を一時保存しました。（対象: {target}）\")\n\n    def on_apply_patch():\n        if not hasattr(patch_ui, \"last_edit\"):\n            messagebox.showinfo(\"情報\", \"先に「コード編集」で変更内容を入力してください\")\n            return\n        file_path, target, new_code = patch_ui.last_edit\n        preview_content = apply_code_patch(file_path, target, new_code, log_widget=None, preview=True)\n        if preview_content is None:\n            messagebox.showerror(\"パッチ適用エラー\", \"コードのプレビュー生成に失敗しました\")\n            return\n        if messagebox.askyesno(\"変更後コードプレビュー\", f\"以下の内容で適用します。\\n\\n{preview_content}\\n\\n適用してよろしいですか？\"):\n            result = apply_code_patch(file_path, target, new_code, log_widget)\n            if result:\n                messagebox.showinfo(\"成功\", \"パッチ適用に成功しました\")\n            else:\n                messagebox.showerror(\"失敗\", \"パッチ適用に失敗しました\")\n        else:\n            log_info(log_widget, \"パッチ適用をキャンセルしました。\")\n\n    def on_system_patch():\n        apply_system_patch(base_dir, log_widget, app_state)\n\n    tk.Button(frm_patch, text=\"コード編集\", command=on_code_edit).pack(side=tk.LEFT, padx=5)\n    tk.Button(frm_patch, text=\"パッチ適用\", command=on_apply_patch).pack(side=tk.LEFT, padx=5)\n    tk.Button(frm_patch, text=\"システムパッチ適用\", command=on_system_patch).pack(side=tk.LEFT, padx=5)\n\n###############################################################################\n# Additional GUI Functions\n###############################################################################\ndef clean(base_dir: str, log_widget):\n    log_info(log_widget, \"クリーンアップを開始...\")\n    current_script = os.path.abspath(__file__)\n    files_to_keep = { current_script, os.path.join(base_dir, 'collected_scripts.json') }\n    if not messagebox.askyesno(\"確認\", \"スクリプト自身とcollected_scripts.jsonを除く全てのファイル/フォルダを削除します。よろしいですか？\"):\n        log_info(log_widget, \"クリーンアップをキャンセルしました。\")\n        return\n    for item in os.listdir(base_dir):\n        item_path = os.path.join(base_dir, item)\n        if item_path in files_to_keep:\n            continue\n        try:\n            if os.path.isfile(item_path) or os.path.islink(item_path):\n                os.remove(item_path)\n                log_info(log_widget, f\"削除: {item_path}\")\n            elif os.path.isdir(item_path):\n                shutil.rmtree(item_path)\n                log_info(log_widget, f\"削除: {item_path}\")\n        except Exception as e:\n            log_error(log_widget, f\"削除失敗: {item_path}: {e}\")\n    log_info(log_widget, \"クリーンアップが完了しました。\")\n\ndef enhanced_smart_paste(base_dir: str, log_widget):\n    try:\n        content = log_widget.clipboard_get()\n    except Exception as e:\n        log_error(log_widget, f\"クリップボード取得失敗: {e}\")\n        return\n    if not content:\n        log_info(log_widget, \"クリップボードが空です。\")\n        return\n    fname = simpledialog.askstring(\"貼り付け\", \"ファイル名を入力してください:\")\n    if not fname:\n        log_info(log_widget, \"貼り付けをキャンセルしました。\")\n        return\n    if '.' not in fname:\n        fname += '.py'\n    file_path = os.path.normpath(os.path.join(base_dir, fname))\n    if not file_path.startswith(os.path.normpath(base_dir)):\n        log_error(log_widget, \"無効なパス指定です。\")\n        return\n    if os.path.exists(file_path):\n        if not messagebox.askyesno(\"確認\", f\"{file_path} は既に存在します。上書きしますか？\"):\n            log_info(log_widget, \"貼り付けをキャンセルしました。\")\n            return\n    write_file_content(file_path, content, log_widget)\n    log_info(log_widget, f\"ファイル作成: {os.path.relpath(file_path, base_dir)}\")\n    collect(base_dir, log_widget)\n\ndef patch_ui_placeholder(parent_frame, base_dir: str, log_widget):\n    # すでに patch_ui にて実装済み\n    pass\n\n###############################################################################\n# GUIエントリ\n###############################################################################\ndef gui_main():\n    base_dir = os.path.dirname(os.path.abspath(__file__))\n    app_state = AUMAppState()\n    root = tk.Tk()\n    root.title(\"AUM GUI with Ordered Build\")\n    frm_main = tk.Frame(root)\n    frm_main.pack(fill=tk.BOTH, expand=True)\n\n    log_widget = ScrolledText(frm_main, width=90, height=25)\n    log_widget.pack(side=tk.BOTTOM, fill=tk.BOTH, expand=True)\n\n    frm_top = tk.Frame(frm_main)\n    frm_top.pack(side=tk.TOP, fill=tk.X)\n\n    def on_collect():\n        collect(base_dir, log_widget)\n    def on_token_saving():\n        token_saving_build_unified(base_dir, log_widget, app_state)\n    def on_restore():\n        restore_build(base_dir, log_widget, app_state)\n    def on_clean():\n        clean(base_dir, log_widget)\n    def on_run():\n        run_main_py(base_dir, log_widget, app_state)\n    def on_paste():\n        enhanced_smart_paste(base_dir, log_widget)\n\n    tk.Button(frm_top, text=\"コード収集\", command=on_collect).pack(side=tk.LEFT, padx=5, pady=5)\n    tk.Button(frm_top, text=\"ビルド(復元)\", command=on_restore).pack(side=tk.LEFT, padx=5, pady=5)\n    tk.Button(frm_top, text=\"トークン節約(衝突検知)\", command=on_token_saving).pack(side=tk.LEFT, padx=5, pady=5)\n    tk.Button(frm_top, text=\"クリーン\", command=on_clean).pack(side=tk.LEFT, padx=5, pady=5)\n    tk.Button(frm_top, text=\"実行\", command=on_run).pack(side=tk.LEFT, padx=5, pady=5)\n    tk.Button(frm_top, text=\"貼り付け\", command=on_paste).pack(side=tk.LEFT, padx=5, pady=5)\n\n    # 新たにパッチ適用用のUIを統合\n    patch_ui(frm_main, base_dir, log_widget, app_state)\n\n    root.mainloop()\n\nif __name__ == \"__main__\":\n    gui_main()\n"
    },
    {
      "path": "aum copy.py",
      "overview": "Pythonコード。\nクラス: AUMAppState, ImportCollector, Mapper。\n関数: get_file_lock, with_file_lock, retry_operation, setup_logging, log_info, log_error, with_io_throttling, _read_file_binary, read_file_content, write_file_content, generate_summary, detect_symbol_collisions, auto_rename_collisions_in_name_map, auto_repair_collected_scripts, merge_collected_scripts, _merge_system_and_settings, _merge_single_script, ensure_init_py_for_python_dirs, restore_settings_from_config_json, collect, _parse_file_for_symbols, collect_python_symbols_for_map, generate_short_symbol_name, ensure_name_map_csv_exists, load_name_mappings, invert_name_map, apply_name_mappings_ast, analyze_dependencies_and_sort, build_in_topological_order, verify_build_results, _build_config_json, build_project, finalize_build, token_saving_build_unified, restore_build, run_main_py, gather_run_artifacts, generate_summary_report, apply_code_patch, apply_system_patch, patch_ui, clean, enhanced_smart_paste, patch_ui_placeholder, gui_main, __init__, wrapper, decorator, wrapper, _collect_task, extract_problematic_identifier, extract_imports, on_code_edit, on_apply_patch, on_system_patch, on_collect, on_token_saving, on_restore, on_clean, on_run, on_paste, wrapper, __init__, visit_Import, visit_ImportFrom, __init__, visit_Name, visit_FunctionDef, visit_ClassDef, visit_Attribute, visit_Import, visit_ImportFrom。\n",
      "content": "import os\nimport sys\nimport csv\nimport json\nimport ast\nimport shutil\nimport logging\nimport re\nimport datetime\nimport tkinter as tk\nfrom tkinter import messagebox, simpledialog, filedialog\nfrom tkinter.scrolledtext import ScrolledText\nfrom typing import Any, Dict, List, Optional, Tuple, Set\nfrom collections import defaultdict, deque\nimport subprocess\nimport threading\nimport functools\nimport time\nimport psutil\nimport asyncio\nimport mmap\nfrom concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor\nfrom functools import lru_cache\nfrom joblib import Memory\n\n# ジョブリブのキャッシュ設定（AST解析結果などをキャッシュ）\nmemory = Memory(location=\"joblib_cache\", verbose=0)\n\n###############################################################################\n# グローバル定数\n###############################################################################\nLOG_LEVEL = 'ERROR'\nDATA_FOLDER_NAME = \"data\"\nMAX_REBUILD_ATTEMPTS = 3  # ビルド再試行の最大回数\n\n# 特定のクラスやシンボルを常にリネーム対象から除外するリスト\nTOKEN_SAVING_EXCLUDE_LIST = {\n    \"ConfigManager\",\n    \"CsvPlatformFeatureExtractor\",\n    \"JsonPlatformFeatureExtractor\",\n    \"PlatformSampleManager\",\n    \"PlatformIdentifier\",\n}\n\n# 特定のモジュールパス(=ファイル)をまるごとリネーム対象から除外する\nABSOLUTE_EXCLUDE_MODULES = {\n    \"utils/file_identifier.py\",\n    \"parsers/base_parser.py\",\n}\n\n###############################################################################\n# I/Oスロットリング用の定数\n###############################################################################\nWRITE_RATE_THRESHOLD = 10 * 1024 * 1024  # 10MB/s（I/Oスロットリングの閾値）\nTHROTTLE_SLEEP_TIME = 0.5  # スロットリング時の待機時間（秒）\nLARGE_FILE_THRESHOLD = 5 * 1024 * 1024   # 大容量ファイルとみなすしきい値（5MB）\n\n###############################################################################\n# アプリ状態管理\n###############################################################################\nclass AUMAppState:\n    def __init__(self):\n        self.last_build_folder: Optional[str] = None\n\n###############################################################################\n# グローバルファイルロック（排他制御用）\n###############################################################################\n_file_lock_dict: Dict[str, threading.Lock] = {}\n_lock_dict_lock = threading.Lock()\n\ndef get_file_lock(file_path: str) -> threading.Lock:\n    \"\"\"指定ファイルパスごとのグローバルロックを取得\"\"\"\n    with _lock_dict_lock:\n        if file_path not in _file_lock_dict:\n            _file_lock_dict[file_path] = threading.Lock()\n        return _file_lock_dict[file_path]\n\ndef with_file_lock(func):\n    \"\"\"ファイルI/O用グローバルロックを取るデコレータ\"\"\"\n    @functools.wraps(func)\n    def wrapper(file_path, *args, **kwargs):\n        lock = get_file_lock(file_path)\n        with lock:\n            return func(file_path, *args, **kwargs)\n    return wrapper\n\n###############################################################################\n# リトライ処理の共通ラッパー\n###############################################################################\ndef retry_operation(max_attempts: int = 3):\n    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            attempt = 0\n            while attempt < max_attempts:\n                try:\n                    return func(*args, **kwargs)\n                except Exception as e:\n                    attempt += 1\n                    logging.error(f\"{func.__name__} attempt {attempt} failed: {e}\")\n                    if attempt >= max_attempts:\n                        raise\n        return wrapper\n    return decorator\n\n###############################################################################\n# ログ設定\n###############################################################################\ndef setup_logging(level='ERROR'):\n    logging.basicConfig(\n        filename='aum_error.log',\n        level=getattr(logging, level.upper(), logging.ERROR),\n        format='%(asctime)s:%(levelname)s:%(message)s',\n        encoding='utf-8'\n    )\n\nsetup_logging(level=LOG_LEVEL)\n\ndef log_info(widget: Optional[tk.Text], msg: str):\n    logging.info(msg)\n    if widget:\n        widget.insert(\"end\", msg + \"\\n\")\n        widget.see(\"end\")\n\ndef log_error(widget: Optional[tk.Text], msg: str):\n    logging.error(msg)\n    if widget:\n        widget.insert(\"end\", \"[ERROR] \" + msg + \"\\n\")\n        widget.see(\"end\")\n\n###############################################################################\n# with_io_throttling: ディスクI/Oを監視してスロットリングするデコレータ\n###############################################################################\ndef with_io_throttling(func):\n    @functools.wraps(func)\n    def wrapper(file_path, *args, **kwargs):\n        io_before = psutil.disk_io_counters().write_bytes\n        time.sleep(THROTTLE_SLEEP_TIME)\n        io_after = psutil.disk_io_counters().write_bytes\n        write_rate = (io_after - io_before) / THROTTLE_SLEEP_TIME\n\n        # しきい値超えの場合はスロットリング\n        if write_rate > WRITE_RATE_THRESHOLD:\n            logging.info(f\"High disk write rate: {write_rate / (1024*1024):.2f} MB/s. Throttling...\")\n            while write_rate > WRITE_RATE_THRESHOLD:\n                time.sleep(THROTTLE_SLEEP_TIME)\n                io_before = psutil.disk_io_counters().write_bytes\n                time.sleep(THROTTLE_SLEEP_TIME)\n                io_after = psutil.disk_io_counters().write_bytes\n                write_rate = (io_after - io_before) / THROTTLE_SLEEP_TIME\n                logging.info(f\"Throttling... current write rate: {write_rate / (1024*1024):.2f} MB/s\")\n        return func(file_path, *args, **kwargs)\n    return wrapper\n\n###############################################################################\n# ファイル読込内部関数（大容量ファイルはmmapで効率良く読み込み、結果をキャッシュ）\n###############################################################################\n@lru_cache(maxsize=None)\ndef _read_file_binary(file_path: str) -> str:\n    \"\"\"ファイルをバイナリで読み込みUTF-8デコード。大容量ファイルはmmapを使用\"\"\"\n    with open(file_path, 'rb') as fr:\n        size = os.path.getsize(file_path)\n        if size > LARGE_FILE_THRESHOLD:\n            mm = mmap.mmap(fr.fileno(), 0, access=mmap.ACCESS_READ)\n            data = mm.read()\n            mm.close()\n        else:\n            data = fr.read()\n    return data.decode('utf-8', errors='ignore')\n\n###############################################################################\n# ファイルI/Oユーティリティ：排他制御＋スロットリング付き\n###############################################################################\n@with_file_lock\n@with_io_throttling\ndef read_file_content(file_path: str, log_widget=None) -> str:\n    try:\n        return _read_file_binary(file_path)\n    except Exception as e:\n        log_error(log_widget, f\"ファイル読込失敗: {file_path}: {e}\")\n        return \"\"\n\n@with_file_lock\n@with_io_throttling\ndef write_file_content(file_path: str, content: str, log_widget=None):\n    try:\n        os.makedirs(os.path.dirname(file_path), exist_ok=True)\n        with open(file_path, 'w', encoding='utf-8') as fw:\n            fw.write(content)\n    except Exception as e:\n        log_error(log_widget, f\"ファイル書込み失敗: {file_path}: {e}\")\n\n###############################################################################\n# generate_summary: ファイル種別に応じたサマリー生成\n###############################################################################\ndef generate_summary(file_path: str, content: str) -> str:\n    if file_path.endswith('.py'):\n        try:\n            tree = ast.parse(content)\n            funcs = [n.name for n in ast.walk(tree) if isinstance(n, ast.FunctionDef)]\n            clss = [n.name for n in ast.walk(tree) if isinstance(n, ast.ClassDef)]\n            docstrings = ast.get_docstring(tree)\n            s = \"Pythonコード。\\n\"\n            if clss:\n                s += f\"クラス: {', '.join(clss)}。\\n\"\n            if funcs:\n                s += f\"関数: {', '.join(funcs)}。\\n\"\n            if docstrings:\n                s += f\"Docstring(冒頭60文字): {docstrings[:60]}...\\n\"\n            return s\n        except Exception as e:\n            logging.error(f\"AST解析失敗: {file_path}: {e}\", exc_info=True)\n            return f\"解析エラー: {e}\\n\"\n    elif file_path.endswith('.ipynb'):\n        try:\n            nb = json.loads(content)\n            cells = nb.get('cells', [])\n            m = sum(1 for c in cells if c.get('cell_type') == 'markdown')\n            c = sum(1 for c in cells if c.get('cell_type') == 'code')\n            return f\"Jupyter Notebook。\\nセル合計={len(cells)}, Markdown={m}, Code={c}\\n\"\n        except Exception as e:\n            return f\"Jupyter Notebook。\\nNotebook解析エラー: {e}\\n\"\n    elif file_path.endswith('.json'):\n        try:\n            obj = json.loads(content)\n            if isinstance(obj, dict):\n                keys = list(obj.keys())\n                keys_display = \", \".join(keys[:10])\n                if len(keys) > 10:\n                    keys_display += \"...\"\n                return f\"JSONファイル (辞書)。キー: {keys_display}\\n\"\n            elif isinstance(obj, list):\n                return f\"JSONファイル (リスト)。要素数: {len(obj)}\\n\"\n            else:\n                return \"JSONファイル (その他形式)。\\n\"\n        except Exception as e:\n            return f\"JSON解析エラー: {e}\\n\"\n    elif file_path.endswith('.tex'):\n        lines = content.count('\\n')\n        return f\"TeXファイル。行数: {lines}\\n\"\n    else:\n        lines = content.count('\\n')\n        return f\"その他テキスト。\\n行数: {lines}\\n\"\n\n###############################################################################\n# name_map.csv の衝突修正ユーティリティ\n###############################################################################\ndef detect_symbol_collisions(mapping: Dict[str, str]) -> set:\n    from collections import defaultdict\n    rev = defaultdict(list)\n    for orig, short_ in mapping.items():\n        rev[short_].append(orig)\n    return {k for k, v in rev.items() if len(v) > 1}\n\ndef auto_rename_collisions_in_name_map(csv_path: str, log_widget=None) -> bool:\n    if not os.path.exists(csv_path):\n        return False\n    rows = []\n    try:\n        with open(csv_path, 'r', encoding='utf-8') as f:\n            rd = csv.DictReader(f)\n            for r in rd:\n                rows.append(r)\n    except Exception as e:\n        log_error(log_widget, f\"name_map.csv 読込失敗: {e}\")\n        return False\n    mapping = {r['original_name']: r['short_name'] for r in rows}\n    collisions = detect_symbol_collisions(mapping)\n    if not collisions:\n        return False\n    short_name_counter = {}\n    renamed_count = 0\n    for r in rows:\n        sh = r['short_name']\n        if sh in collisions:\n            base = sh\n            short_name_counter.setdefault(base, 0)\n            short_name_counter[base] += 1\n            r['short_name'] = base + str(short_name_counter[base])\n            renamed_count += 1\n    if renamed_count:\n        try:\n            with open(csv_path, 'w', encoding='utf-8', newline='') as fw:\n                w = csv.writer(fw)\n                w.writerow([\"original_name\", \"short_name\"])\n                for row in rows:\n                    w.writerow([row['original_name'], row['short_name']])\n            log_info(log_widget, f\"name_map.csv 衝突を {renamed_count} 件リネーム修正\")\n            return True\n        except Exception as e:\n            log_error(log_widget, f\"name_map.csv 修正書込失敗: {e}\")\n    return False\n\n###############################################################################\n# collected_scripts.json 修復\n###############################################################################\ndef auto_repair_collected_scripts(base_dir: str, log_widget) -> None:\n    cjson_path = os.path.join(base_dir, \"collected_scripts.json\")\n    if not os.path.exists(cjson_path):\n        return\n    try:\n        with open(cjson_path, 'r', encoding='utf-8') as fr:\n            content = fr.read().strip()\n            if not content:\n                raise ValueError(\"Empty file\")\n            data = json.loads(content)\n        if not isinstance(data, dict):\n            raise ValueError(\"トップレベルがdictではない\")\n        if \"scripts\" in data and not isinstance(data[\"scripts\"], list):\n            raise ValueError(\"'scripts'がlistではない\")\n    except Exception as e:\n        log_error(log_widget, f\"collected_scripts.json 破損: {e}\")\n        backup = cjson_path + \".bak\"\n        try:\n            shutil.move(cjson_path, backup)\n            log_info(log_widget, f\"破損ファイルをバックアップ→ {backup}\")\n        except Exception as e_backup:\n            log_error(log_widget, f\"バックアップ失敗: {e_backup}\")\n        data = {\n            \"system_overview\": \"このシステムは...（必要に応じて書き換え）\",\n            \"settings\": {},\n            \"scripts\": []\n        }\n        try:\n            with open(cjson_path, 'w', encoding='utf-8') as fw:\n                json.dump(data, fw, ensure_ascii=False, indent=2)\n            log_info(log_widget, \"collected_scripts.json を新規作成\")\n        except Exception as e_write:\n            log_error(log_widget, f\"新規作成失敗: {e_write}\")\n\n###############################################################################\n# 三方比較マージ（collected_scripts.jsonの差分統合）\n###############################################################################\ndef merge_collected_scripts(original: dict, new: dict, file_data: Dict[str, dict], log_widget) -> dict:\n    _merge_system_and_settings(original, new)\n    old_scripts = original.get(\"scripts\", [])\n    old_map = {s['path']: s for s in old_scripts if 'path' in s}\n    new_scripts = new.get(\"scripts\", [])\n    new_map = {s.get('path'): s for s in new_scripts if s.get('path')}\n    for p, new_sc in new_map.items():\n        if p not in old_map:\n            old_map[p] = new_sc\n            log_info(log_widget, f\"新規追加: {p}\")\n        else:\n            old_sc = old_map[p]\n            _merge_single_script(old_sc, new_sc, file_data, p, log_widget)\n    merged_list = list(old_map.values())\n    original[\"scripts\"] = merged_list\n    return original\n\ndef _merge_system_and_settings(old: dict, new: dict):\n    if new.get(\"system_overview\"):\n        old[\"system_overview\"] = new[\"system_overview\"]\n    if \"settings\" in new and isinstance(new[\"settings\"], dict):\n        old.setdefault(\"settings\", {}).update(new[\"settings\"])\n\ndef _merge_single_script(old_sc: dict, new_sc: dict, file_data: Dict[str, dict], p: str, log_widget):\n    old_cont = old_sc.get(\"content\", \"\")\n    new_cont = new_sc.get(\"content\", \"\")\n    disk_cont = file_data.get(p, {}).get(\"content\", \"\")\n    conflict = (old_cont != disk_cont and new_cont != disk_cont and old_cont != new_cont)\n    if conflict:\n        log_info(log_widget, f\"衝突あり: {p}\")\n    else:\n        for k, v in new_sc.items():\n            if k != \"path\":\n                old_sc[k] = v\n        log_info(log_widget, f\"上書き: {p}\")\n\n###############################################################################\n# __init__.py 自動生成（Pythonパッケージディレクトリに__init__.pyを追加）\n###############################################################################\ndef ensure_init_py_for_python_dirs(base_dir: str, log_widget):\n    exclude = {'venv', '__pycache__', '.git', 'outputs', 'temp_build', 'backups', DATA_FOLDER_NAME}\n    py_dirs = set()\n    for root, dirs, files in os.walk(base_dir):\n        dirs[:] = [d for d in dirs if d not in exclude]\n        if any(f.endswith('.py') for f in files):\n            py_dirs.add(root)\n    for d in py_dirs:\n        init_file = os.path.join(d, \"__init__.py\")\n        if not os.path.isfile(init_file):\n            write_file_content(init_file, \"\", log_widget)\n            log_info(log_widget, f\"__init__.py 自動生成: {os.path.relpath(init_file, base_dir)}\")\n\n###############################################################################\n# config.json => collected_scripts.json の更新（設定取り込み）\n###############################################################################\ndef restore_settings_from_config_json(base_dir: str, log_widget):\n    \"\"\"\n    your_project直下または your_project/your_project にある config.json を探し、\n    設定情報を collected_scripts.json の settings に反映する。\n    ※設定ファイル内に \"settings\" キーがあれば、その中身を使用（なければファイル全体を設定として取り込む）\n    \"\"\"\n    possible_cfg_paths = [\n        os.path.join(base_dir, \"your_project\", \"config.json\"),\n        os.path.join(base_dir, \"your_project\", \"your_project\", \"config.json\"),\n    ]\n\n    cfg_path = None\n    for pth in possible_cfg_paths:\n        if os.path.isfile(pth):\n            cfg_path = pth\n            break\n\n    if not cfg_path:\n        log_info(log_widget, \"config.jsonが見つからないためスキップ\")\n        return\n\n    cjson_path = os.path.join(base_dir, \"collected_scripts.json\")\n    if not os.path.isfile(cjson_path):\n        log_info(log_widget, f\"collected_scripts.jsonが無い: {cjson_path}\")\n        return\n\n    try:\n        with open(cfg_path, 'r', encoding='utf-8') as f:\n            cfg_data = json.load(f)\n    except Exception as e:\n        log_error(log_widget, f\"config.json 読込失敗: {e}\")\n        return\n\n    if not isinstance(cfg_data, dict):\n        log_info(log_widget, \"config.jsonの形式が不正です。\")\n        return\n\n    # \"settings\" キーがあればその中身、なければ全体を設定情報とみなす\n    new_settings = cfg_data.get(\"settings\", cfg_data)\n\n    try:\n        with open(cjson_path, 'r', encoding='utf-8') as fr:\n            cjson_data = json.load(fr)\n        if not isinstance(cjson_data, dict):\n            cjson_data = {\"system_overview\": \"\", \"settings\": {}, \"scripts\": []}\n    except Exception as e:\n        log_error(log_widget, f\"collected_scripts.json 読込失敗: {e}\")\n        return\n\n    # 既存の設定に新しい設定をマージして反映\n    cjson_data.setdefault(\"settings\", {}).update(new_settings)\n    try:\n        with open(cjson_path, 'w', encoding='utf-8') as fw:\n            json.dump(cjson_data, fw, ensure_ascii=False, indent=2)\n        log_info(log_widget, f\"{cjson_path} の settings を更新しました\")\n    except Exception as e:\n        log_error(log_widget, f\"config→collected_scripts 反映失敗: {e}\")\n\n###############################################################################\n# コード収集\n###############################################################################\ndef collect(base_dir: str, log_widget):\n    log_info(log_widget, \"コード収集を開始...\")\n    auto_repair_collected_scripts(base_dir, log_widget)\n    output_folder = os.path.join(base_dir, \"your_project\")\n    cjson = os.path.join(base_dir, \"collected_scripts.json\")\n    if os.path.isfile(cjson):\n        try:\n            with open(cjson, 'r', encoding='utf-8') as fr:\n                old_data = json.load(fr)\n            if not isinstance(old_data, dict):\n                old_data = {\"system_overview\": \"\", \"settings\": {}, \"scripts\": []}\n        except Exception as e:\n            log_error(log_widget, f\"collected_scripts.json 読込失敗: {e}\")\n            old_data = {\"system_overview\": \"\", \"settings\": {}, \"scripts\": []}\n    else:\n        old_data = {\"system_overview\": \"\", \"settings\": {}, \"scripts\": []}\n\n    ensure_init_py_for_python_dirs(base_dir, log_widget)\n\n    target_ext = ('.py', '.json', '.tex')\n    exclude_dirs = {'venv', '__pycache__', '.git', 'outputs', 'temp_build', 'backups', DATA_FOLDER_NAME}\n    exclude_files = {'collected_scripts.json', 'config.json', os.path.basename(__file__)}\n    new_list: List[Dict[str, str]] = []\n    files_to_collect: List[str] = []\n    for root, dirs, files in os.walk(base_dir):\n        abs_root = os.path.abspath(root)\n        abs_output = os.path.abspath(output_folder)\n        # 生成済みyour_project配下のサブディレクトリは除外\n        if abs_root.startswith(abs_output) and \"your_project\" not in os.path.relpath(abs_root, base_dir).split(os.sep)[0]:\n            continue\n        dirs[:] = [d for d in dirs if d not in exclude_dirs]\n        for fn in files:\n            if fn in exclude_files:\n                continue\n            if any(fn.endswith(e) for e in target_ext):\n                fp = os.path.join(root, fn)\n                if fp == os.path.abspath(__file__):\n                    continue\n                files_to_collect.append(fp)\n    # 並列処理（非同期I/O）でファイル読み込みとサマリー生成を実行\n    def _collect_task(fp: str):\n        content = read_file_content(fp, log_widget=None)\n        if not content:\n            return None\n        relp = os.path.relpath(fp, base_dir)\n        summ = generate_summary(fp, content)\n        return {\"path\": relp, \"overview\": summ, \"content\": content}\n    if files_to_collect:\n        async def run_tasks():\n            tasks = [asyncio.to_thread(_collect_task, fp) for fp in files_to_collect]\n            return await asyncio.gather(*tasks)\n        results = asyncio.run(run_tasks())\n        for res in results:\n            if res:\n                new_list.append(res)\n                log_info(log_widget, f\"収集: {res['path']}\")\n\n    file_map = {s[\"path\"]: {\"content\": s[\"content\"], \"overview\": s[\"overview\"]} for s in new_list}\n    merged = merge_collected_scripts(\n        old_data,\n        {\n            \"system_overview\": old_data.get(\"system_overview\", \"\"),\n            \"settings\": old_data.get(\"settings\", {}),\n            \"scripts\": new_list\n        },\n        file_map,\n        log_widget\n    )\n    try:\n        with open(cjson, 'w', encoding='utf-8') as fw:\n            json.dump(merged, fw, ensure_ascii=False, indent=2)\n        log_info(log_widget, f\"出力完了: {cjson}\")\n    except Exception as e:\n        log_error(log_widget, f\"書き込み失敗: {e}\")\n    # 設定ファイルの変更を反映\n    restore_settings_from_config_json(base_dir, log_widget)\n\n###############################################################################\n# Pythonシンボル名 短縮（name_map.csv生成）\n###############################################################################\ndef _parse_file_for_symbols(file_path: str) -> Set[str]:\n    \"\"\"指定Pythonファイルから関数名・クラス名を抽出\"\"\"\n    try:\n        with open(file_path, 'r', encoding='utf-8') as fr:\n            content = fr.read()\n        tree = ast.parse(content)\n        symbols: Set[str] = set()\n        for node in ast.walk(tree):\n            if isinstance(node, ast.FunctionDef) or isinstance(node, ast.ClassDef):\n                symbols.add(node.name)\n        return symbols\n    except Exception as e:\n        logging.error(f\"AST解析失敗: {file_path}: {e}\", exc_info=True)\n        return set()\n\n# AST解析結果をキャッシュして高速化\nif memory:\n    _parse_file_for_symbols = memory.cache(_parse_file_for_symbols)\n\ndef collect_python_symbols_for_map(base_dir: str) -> set:\n    exclude = {'venv', '__pycache__', '.git', 'temp_build', 'backups', DATA_FOLDER_NAME}\n    found: Set[str] = set()\n    files_to_scan: List[str] = []\n    for root, dirs, files in os.walk(base_dir):\n        if os.path.basename(root).lower() == DATA_FOLDER_NAME.lower():\n            continue\n        abs_root = os.path.abspath(root)\n        abs_output = os.path.abspath(os.path.join(base_dir, \"your_project\"))\n        if abs_root.startswith(abs_output):\n            continue\n        dirs[:] = [d for d in dirs if d not in exclude]\n        for f in files:\n            if f.endswith('.py') and f != os.path.basename(__file__):\n                files_to_scan.append(os.path.join(root, f))\n    # ProcessPoolExecutorでAST解析を並列実行\n    if files_to_scan:\n        with ProcessPoolExecutor() as executor:\n            for symbols in executor.map(_parse_file_for_symbols, files_to_scan):\n                found |= symbols\n    return found\n\ndef generate_short_symbol_name(orig_name: str) -> str:\n    if not orig_name:\n        return \"x\"\n    return orig_name[0].lower() + \"_sym\"\n\ndef ensure_name_map_csv_exists(base_dir: str) -> str:\n    csv_path = os.path.join(base_dir, \"name_map.csv\")\n    syms = collect_python_symbols_for_map(base_dir)\n    existing_map = {}\n    if os.path.isfile(csv_path):\n        try:\n            with open(csv_path, 'r', encoding='utf-8') as fr:\n                rd = csv.DictReader(fr)\n                for r in rd:\n                    o = r['original_name']; s = r['short_name']\n                    existing_map[o] = s\n        except Exception as e:\n            logging.error(f\"name_map.csv 読込失敗: {e}\")\n    else:\n        logging.info(\"name_map.csvがないため新規作成\")\n    for s in syms:\n        if s in TOKEN_SAVING_EXCLUDE_LIST:\n            existing_map[s] = s\n        else:\n            if s not in existing_map:\n                existing_map[s] = generate_short_symbol_name(s)\n    with open(csv_path, 'w', encoding='utf-8', newline='') as fw:\n        w = csv.writer(fw)\n        w.writerow([\"original_name\", \"short_name\"])\n        for k, v in sorted(existing_map.items()):\n            w.writerow([k, v])\n    return csv_path\n\ndef load_name_mappings(csv_path: str) -> dict:\n    mp = {}\n    if not os.path.isfile(csv_path):\n        return mp\n    try:\n        with open(csv_path, 'r', encoding='utf-8') as fr:\n            rd = csv.DictReader(fr)\n            for r in rd:\n                mp[r['original_name']] = r['short_name']\n    except Exception as e:\n        logging.error(f\"name_map.csv 読込失敗: {e}\")\n    return mp\n\ndef invert_name_map(mp: dict) -> dict:\n    # 短縮名→元名 のマッピングに変換\n    return {v: k for k, v in mp.items()}\n\n###############################################################################\n# AST変換 (識別子置換：名前をname_mapに基づき変換)\n###############################################################################\nGLOBAL_EXCLUDE_MODULES = ABSOLUTE_EXCLUDE_MODULES\n\ndef apply_name_mappings_ast(content: str, name_map: dict, path_for_check: str) -> str:\n    norm_path = os.path.normpath(path_for_check).replace(\"\\\\\", \"/\")\n    for exc_subpath in GLOBAL_EXCLUDE_MODULES:\n        if exc_subpath in norm_path:\n            return content\n\n    FIXED_EXCLUDE = TOKEN_SAVING_EXCLUDE_LIST.copy()\n    current_exclude = set()\n    attempt = 0\n\n    def extract_problematic_identifier(e: Exception) -> Optional[str]:\n        msg = str(e)\n        m = re.search(r\"object has no attribute '(\\w+)'\", msg)\n        if m:\n            return m.group(1)\n        m = re.search(r\"cannot import name '(\\w+)'\", msg)\n        if m:\n            return m.group(1)\n        return None\n\n    class ImportCollector(ast.NodeVisitor):\n        def __init__(self):\n            self.imported = set()\n        def visit_Import(self, node):\n            for alias in node.names:\n                name = alias.asname if alias.asname else alias.name.split('.')[0]\n                self.imported.add(name)\n            self.generic_visit(node)\n        def visit_ImportFrom(self, node):\n            if node.module:\n                self.imported.add(node.module.split('.')[0])\n            for alias in node.names:\n                name = alias.asname if alias.asname else alias.name\n                self.imported.add(name)\n            self.generic_visit(node)\n\n    class Mapper(ast.NodeTransformer):\n        def __init__(self, exclude_set, nmap):\n            self.exclude = exclude_set\n            self.nmap = nmap\n            super().__init__()\n        def visit_Name(self, node):\n            if node.id.startswith(\"__\") and node.id.endswith(\"__\"):\n                return node\n            if node.id in self.exclude:\n                return node\n            if node.id in self.nmap:\n                node.id = self.nmap[node.id]\n            return self.generic_visit(node)\n        def visit_FunctionDef(self, node):\n            if not (node.name.startswith(\"__\") and node.name.endswith(\"__\")):\n                if node.name not in self.exclude and node.name in self.nmap:\n                    node.name = self.nmap[node.name]\n            self.generic_visit(node)\n            return node\n        def visit_ClassDef(self, node):\n            if not (node.name.startswith(\"__\") and node.name.endswith(\"__\")):\n                if node.name not in self.exclude and node.name in self.nmap:\n                    node.name = self.nmap[node.name]\n            self.generic_visit(node)\n            return node\n        def visit_Attribute(self, node):\n            self.generic_visit(node)\n            if isinstance(node.value, ast.Name) and node.value.id == \"self\":\n                return node\n            if node.attr.startswith(\"__\") and node.attr.endswith(\"__\"):\n                return node\n            if node.attr in self.exclude:\n                return node\n            if node.attr in self.nmap:\n                node.attr = self.nmap[node.attr]\n            return node\n        def visit_Import(self, node):\n            for alias in node.names:\n                if alias.name in self.exclude:\n                    continue\n                if alias.name in self.nmap:\n                    alias.name = self.nmap[alias.name]\n                if alias.asname and alias.asname in self.nmap:\n                    alias.asname = self.nmap[alias.asname]\n            return node\n        def visit_ImportFrom(self, node):\n            if node.module in self.exclude:\n                return node\n            if node.module and (node.module in self.nmap):\n                node.module = self.nmap[node.module]\n            for alias in node.names:\n                if alias.name in self.exclude:\n                    continue\n                if alias.name in self.nmap:\n                    alias.name = self.nmap[alias.name]\n                if alias.asname and alias.asname in self.nmap:\n                    alias.asname = self.nmap[alias.asname]\n            return node\n\n    while True:\n        attempt += 1\n        try:\n            tree = ast.parse(content)\n            collector = ImportCollector()\n            collector.visit(tree)\n            auto_exclude = collector.imported\n            total_exclude = FIXED_EXCLUDE.union(auto_exclude).union(current_exclude)\n            mapper = Mapper(total_exclude, name_map)\n            new_tree = mapper.visit(tree)\n            ast.fix_missing_locations(new_tree)\n            if hasattr(ast, 'unparse'):\n                out_code = ast.unparse(new_tree)\n            else:\n                import astor\n                out_code = astor.to_source(new_tree)\n            return out_code\n        except Exception as e:\n            problematic = extract_problematic_identifier(e)\n            if problematic:\n                if problematic not in current_exclude:\n                    current_exclude.add(problematic)\n                    # 問題の識別子を除外リストに加えて再試行\n                    continue\n                else:\n                    return content\n            else:\n                return content\n\n###############################################################################\n# 依存解析 & トポロジカルソート\n###############################################################################\ndef analyze_dependencies_and_sort(scripts: List[Dict[str, str]], log_widget=None) -> List[Dict[str, str]]:\n    graph = {}\n    script_map = {}\n    for sc in scripts:\n        p = sc[\"path\"]\n        script_map[p] = sc\n        graph[p] = set()\n\n    path_by_modname = {}\n    for sc in scripts:\n        p = sc[\"path\"].replace(\"\\\\\", \"/\")\n        if p.endswith(\".py\"):\n            mod_parts = p.split(\"/\")\n            if mod_parts[0] == \"your_project\":\n                mod_parts = mod_parts[1:]\n            py_base = mod_parts[-1][:-3]  # \"file.py\" -> \"file\"\n            mod_parts[-1] = py_base\n            dotted = \".\".join(mod_parts)\n            path_by_modname[dotted] = p\n\n    def extract_imports(p: str, code: str) -> Set[str]:\n        import_list = set()\n        try:\n            tree = ast.parse(code)\n            for node in ast.walk(tree):\n                if isinstance(node, ast.Import):\n                    for alias in node.names:\n                        mod = alias.name.split(\".\")[0]\n                        import_list.add(mod)\n                elif isinstance(node, ast.ImportFrom):\n                    if node.module:\n                        base = node.module.split(\".\")[0]\n                        import_list.add(base)\n        except Exception as e:\n            logging.error(f\"依存関係解析: {p} の import抽出失敗: {e}\", exc_info=True)\n            log_error(log_widget, f\"依存関係解析: {p} の import抽出失敗: {e}\")\n        return import_list\n\n    for sc in scripts:\n        p = sc[\"path\"]\n        code = sc[\"content\"]\n        imported = extract_imports(p, code)\n        for imp in imported:\n            for k, v in path_by_modname.items():\n                if k.startswith(imp):\n                    graph[p].add(v)\n\n    in_degree = {p: 0 for p in graph}\n    for p in graph:\n        for dep in graph[p]:\n            if dep in in_degree:\n                in_degree[dep] += 1\n\n    queue = deque([p for p in in_degree if in_degree[p] == 0])\n    sorted_paths = []\n    while queue:\n        u = queue.popleft()\n        sorted_paths.append(u)\n        for dep in graph[u]:\n            if dep in in_degree:\n                in_degree[dep] -= 1\n                if in_degree[dep] == 0:\n                    queue.append(dep)\n\n    if len(sorted_paths) < len(graph):\n        logging.error(\"循環依存関係が検出されました。依存順序の保証ができません。\")\n        log_error(log_widget, \"循環依存関係が検出されました。依存順序の保証ができません。\")\n        return scripts\n\n    path_to_index = {p: i for i, p in enumerate(sorted_paths)}\n    scripts_sorted = sorted(scripts, key=lambda sc: path_to_index.get(sc[\"path\"], float('inf')))\n    return scripts_sorted\n\n###############################################################################\n# ビルド (トポロジカルソート + AST変換 + 出力)\n###############################################################################\ndef build_in_topological_order(\n    scripts: List[Dict[str, str]],\n    out_dir: str,\n    name_map: Dict[str, str],\n    log_widget\n) -> Tuple[int, List[str]]:\n    sorted_scripts = analyze_dependencies_and_sort(scripts, log_widget)\n    logs: List[str] = []\n    built_count = 0\n    for sc in sorted_scripts:\n        path_ = sc[\"path\"]\n        content_ = sc[\"content\"]\n        if path_.endswith(\".py\"):\n            new_content = apply_name_mappings_ast(content_, name_map, path_for_check=path_)\n        elif path_.endswith(\".json\") or path_.endswith(\".tex\"):\n            new_content = content_\n        else:\n            continue  # サポート対象外ファイルは無視\n        rel_path = path_\n        if rel_path.startswith(\"your_project\" + os.path.sep):\n            rel_path = rel_path[len(\"your_project\" + os.path.sep):]\n        target_file = os.path.join(out_dir, rel_path)\n        try:\n            os.makedirs(os.path.dirname(target_file), exist_ok=True)\n            with open(target_file, 'w', encoding='utf-8') as fw:\n                fw.write(new_content)\n            logs.append(f\"出力: {os.path.relpath(target_file, out_dir)}\")\n            built_count += 1\n        except Exception as e:\n            logs.append(f\"書き込みエラー: {target_file}: {e}\")\n    return built_count, logs\n\ndef verify_build_results(build_dir: str, collected_json: str, log_widget) -> bool:\n    try:\n        with open(collected_json, 'r', encoding='utf-8') as fr:\n            data = json.load(fr)\n    except Exception as e:\n        log_error(log_widget, f\"収集結果の読込失敗: {e}\")\n        return False\n    scripts = data.get(\"scripts\", [])\n    all_ok = True\n    for script in scripts:\n        p = script.get(\"path\", \"\")\n        if not (p.endswith(\".py\") or p.endswith(\".json\") or p.endswith(\".tex\")):\n            continue\n        rel_path = p\n        if rel_path.startswith(\"your_project\" + os.path.sep):\n            rel_path = rel_path[len(\"your_project\" + os.path.sep):]\n        build_file = os.path.join(build_dir, rel_path)\n        if not os.path.exists(build_file):\n            log_error(log_widget, f\"ビルド結果に存在しない: {build_file}\")\n            all_ok = False\n            continue\n        with open(build_file, 'r', encoding='utf-8') as bf:\n            build_content = bf.read()\n        if build_content.strip() != script.get(\"content\", \"\").strip():\n            log_error(log_widget, f\"内容不一致: {build_file}\")\n            all_ok = False\n        else:\n            log_info(log_widget, f\"チェックOK: {build_file}\")\n    return all_ok\n\ndef _build_config_json(settings: dict, out_dir: str, log_widget):\n    cfg_path = os.path.join(out_dir, \"config.json\")\n    try:\n        os.makedirs(os.path.dirname(cfg_path), exist_ok=True)\n        with open(cfg_path, 'w', encoding='utf-8') as fw:\n            json.dump(settings, fw, ensure_ascii=False, indent=2)\n        log_info(log_widget, f\"config.json生成: {cfg_path}\")\n    except Exception as e:\n        log_error(log_widget, f\"config.json 書込失敗: {e}\")\n\ndef build_project(base_dir: str, log_widget, is_obfuscate: bool = True) -> Optional[str]:\n    auto_repair_collected_scripts(base_dir, log_widget)\n    cjson = os.path.join(base_dir, \"collected_scripts.json\")\n    if not os.path.isfile(cjson):\n        messagebox.showerror(\"エラー\", f\"{cjson} が見つかりません\")\n        return None\n    csv_path = ensure_name_map_csv_exists(base_dir)\n    direct_map = load_name_mappings(csv_path)\n    final_map = direct_map if is_obfuscate else invert_name_map(direct_map)\n    tmp_build_root = os.path.join(base_dir, \"temp_build\")\n    os.makedirs(tmp_build_root, exist_ok=True)\n    out_dir = os.path.join(tmp_build_root, \"your_project_temp\")\n    if os.path.exists(out_dir):\n        shutil.rmtree(out_dir)\n    os.makedirs(out_dir, exist_ok=True)\n    log_info(log_widget, f\"一時出力先: {out_dir}\")\n    # collected_scripts.json の読み込み\n    try:\n        with open(cjson, 'r', encoding='utf-8') as fr:\n            data = json.load(fr)\n    except Exception as e:\n        log_error(log_widget, f\"collected_scripts.json 読込失敗: {e}\")\n        return None\n    scripts = data.get(\"scripts\", [])\n    settings = data.get(\"settings\", {})\n    py_scripts = [s for s in scripts if s.get(\"path\", \"\").endswith((\".py\", \".json\", \".tex\"))]\n    built_count, logs_texts = build_in_topological_order(py_scripts, out_dir, final_map, log_widget)\n    _build_config_json(settings, out_dir, log_widget)\n    built_count += 1  # config.jsonも生成した\n    if logs_texts:\n        for l in logs_texts:\n            log_info(log_widget, l)\n    log_info(log_widget, f\"ビルド完了: {built_count}ファイル\")\n    if not verify_build_results(out_dir, cjson, log_widget):\n        log_error(log_widget, \"収集結果とビルド結果が一致しません\")\n    else:\n        log_info(log_widget, \"ビルド結果と収集結果は一致しています\")\n    restore_settings_from_config_json(base_dir, log_widget)\n    return out_dir\n\ndef finalize_build(temp_out_dir: str, base_dir: str, log_widget, app_state: AUMAppState) -> bool:\n    if not temp_out_dir or not os.path.isdir(temp_out_dir):\n        log_error(log_widget, f\"一時フォルダが無い: {temp_out_dir}\")\n        return False\n    final_dir = os.path.join(base_dir, \"your_project\")\n    if os.path.isdir(final_dir):\n        backups_dir = os.path.join(base_dir, \"backups\")\n        os.makedirs(backups_dir, exist_ok=True)\n        stamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        zip_name = os.path.join(backups_dir, f\"your_project_old_{stamp}\")\n        try:\n            shutil.make_archive(zip_name, 'zip', final_dir)\n            log_info(log_widget, f\"既存フォルダをzipバックアップ: {zip_name}.zip\")\n        except Exception as e:\n            log_error(log_widget, f\"バックアップ失敗: {e}\")\n        try:\n            shutil.rmtree(final_dir)\n            log_info(log_widget, f\"既存フォルダ削除: {final_dir}\")\n        except Exception as e:\n            log_error(log_widget, f\"削除失敗: {e}\")\n            return False\n    try:\n        shutil.move(temp_out_dir, final_dir)\n        log_info(log_widget, f\"上書き完了: {final_dir}\")\n        app_state.last_build_folder = base_dir\n    except Exception as e:\n        log_error(log_widget, f\"移動失敗: {e}\")\n        return False\n    tmpb = os.path.join(base_dir, \"temp_build\")\n    if os.path.isdir(tmpb):\n        try:\n            shutil.rmtree(tmpb)\n            log_info(log_widget, f\"temp_build削除: {tmpb}\")\n        except Exception as ex:\n            log_error(log_widget, f\"temp_build削除失敗: {ex}\")\n    log_info(log_widget, f\"last_build_folder={app_state.last_build_folder}\")\n    return True\n\n###############################################################################\n# トークン節約ビルド（衝突検知＆リトライ処理）\n###############################################################################\ndef token_saving_build_unified(base_dir: str, log_widget, app_state: AUMAppState):\n    tries = 0\n    while True:\n        tries += 1\n        log_info(log_widget, f\"ビルド試行 {tries} 回目\")\n        tmp_out = None\n        try:\n            tmp_out = build_project(base_dir, log_widget, is_obfuscate=True)\n        except Exception as e:\n            log_error(log_widget, f\"ビルド処理で例外発生: {e}\")\n            break\n        if not tmp_out:\n            log_error(log_widget, \"build_project で出力が得られませんでした。\")\n            if tries >= MAX_REBUILD_ATTEMPTS:\n                log_error(log_widget, \"ビルドを中断します。\")\n                break\n            continue\n        result = finalize_build(tmp_out, base_dir, log_widget, app_state)\n        if not result:\n            log_error(log_widget, \"ビルドの適用に失敗しました。処理を中断します。\")\n            break\n        run_main_py(base_dir, log_widget, app_state)\n        all_logs = log_widget.get(\"1.0\", \"end\")\n        if (\"ImportError:\" in all_logs) or (\"NameError:\" in all_logs) or (\"ModuleNotFoundError:\" in all_logs):\n            log_info(log_widget, \"エラー検知（名前衝突等）。name_map.csv を修正して再試行します。\")\n            csvp = os.path.join(base_dir, \"name_map.csv\")\n            fixed = auto_rename_collisions_in_name_map(csvp, log_widget)\n            if fixed:\n                continue\n            else:\n                log_error(log_widget, \"name_map.csv の自動修正に失敗しました。再試行します。\")\n                continue\n        else:\n            log_info(log_widget, \"ビルド成功：エラーなし。\")\n            break\n\ndef restore_build(base_dir: str, log_widget, app_state: AUMAppState):\n    try:\n        tmp_out = build_project(base_dir, log_widget, is_obfuscate=False)\n    except Exception as e:\n        log_error(log_widget, f\"ビルド処理で例外発生: {e}\")\n        return\n    if tmp_out:\n        finalize_build(tmp_out, base_dir, log_widget, app_state)\n\ndef run_main_py(base_dir: str, log_widget, app_state: AUMAppState):\n    final_dir = os.path.join(base_dir, \"your_project\")\n    main_py = os.path.join(final_dir, \"main.py\")\n    data_path = os.path.join(final_dir, DATA_FOLDER_NAME)\n    log_info(log_widget, f\"main.py => {main_py}\")\n    log_info(log_widget, f\"--input => {data_path}\")\n    if not os.path.isfile(main_py):\n        log_error(log_widget, \"main.py が見つからない\")\n        return\n    if not os.path.isdir(data_path):\n        log_info(log_widget, f\"dataフォルダなし: {data_path}\")\n    cmd = [sys.executable, main_py, \"--input\", data_path, \"--skip-parent-dir\"]\n    try:\n        ret = subprocess.run(cmd, capture_output=True, text=True)\n        if ret.returncode != 0:\n            log_error(log_widget, f\"エラー({ret.returncode}): {ret.stderr}\")\n        else:\n            log_info(log_widget, f\"実行完了:\\n{ret.stdout}\")\n    except Exception as e:\n        log_error(log_widget, f\"例外: {e}\")\n    gather_run_artifacts(base_dir, log_widget, app_state)\n    generate_summary_report(base_dir, log_widget)\n\ndef gather_run_artifacts(base_dir: str, log_widget, app_state: AUMAppState):\n    final_dir = os.path.join(base_dir, \"your_project\")\n    if not os.path.isdir(final_dir):\n        log_info(log_widget, \"your_projectがない\")\n        return\n    runs = []\n    for d in os.listdir(final_dir):\n        if d.startswith(\"run_\"):\n            fp = os.path.join(final_dir, d)\n            if os.path.isdir(fp):\n                runs.append(fp)\n    if not runs:\n        log_info(log_widget, \"run_*** フォルダなし\")\n        return\n    runs.sort(key=lambda x: os.path.getmtime(x), reverse=True)\n    latest = runs[0]\n    log_info(log_widget, f\"最新run: {latest}\")\n    fb_src = os.path.join(latest, \"feedback.json\")\n    fb_dst = os.path.join(base_dir, \"feedback.json\")\n    if os.path.isfile(fb_src):\n        shutil.copy2(fb_src, fb_dst)\n        log_info(log_widget, f\"feedback.json-> {fb_dst}\")\n    logs_src = os.path.join(latest, \"logs\")\n    logs_dst = os.path.join(base_dir, \"logs\")\n    if os.path.isdir(logs_src):\n        if os.path.isdir(logs_dst):\n            shutil.rmtree(logs_dst)\n        shutil.copytree(logs_src, logs_dst)\n        log_info(log_widget, f\"logs-> {logs_dst}\")\n    else:\n        log_info(log_widget, \"logsなし\")\n\ndef generate_summary_report(base_dir: str, log_widget):\n    log_info(log_widget, \"レポート生成...\")\n    fb = os.path.join(base_dir, \"feedback.json\")\n    logsdir = os.path.join(base_dir, \"logs\")\n    outp = os.path.join(base_dir, \"summary_report.md\")\n    data = {}\n    if os.path.isfile(fb):\n        try:\n            with open(fb, 'r', encoding='utf-8') as fr:\n                data = json.load(fr)\n        except Exception as e:\n            log_error(log_widget, f\"feedback.json 読込失敗: {e}\")\n    tstamp = data.get(\"timestamp\", \"N/A\")\n    st = data.get(\"status\", \"N/A\")\n    remarks = data.get(\"remarks\", \"\")\n    improvs = data.get(\"possible_improvements\", [])\n    logs_sum = []\n    if os.path.isdir(logsdir):\n        for root, dirs, files in os.walk(logsdir):\n            for f in files:\n                if f.endswith(\".log\"):\n                    fp = os.path.join(root, f)\n                    try:\n                        with open(fp, 'r', encoding='utf-8', errors='replace') as ff:\n                            lines = ff.readlines()\n                        errwarn = [ln.strip() for ln in lines if (\"ERROR\" in ln or \"WARN\" in ln)]\n                        relp = os.path.relpath(fp, logsdir)\n                        logs_sum.append((relp, errwarn))\n                    except Exception as e:\n                        log_error(log_widget, f\"ログファイル読込失敗: {fp}: {e}\")\n    try:\n        with open(outp, 'w', encoding='utf-8') as fw:\n            fw.write(\"# Analysis Summary Report\\n\\n\")\n            fw.write(f\"- **Timestamp**: {tstamp}\\n\")\n            fw.write(f\"- **Status**: {st}\\n\")\n            fw.write(f\"- **Remarks**: {remarks}\\n\\n\")\n            fw.write(\"## Possible Improvements\\n\")\n            if improvs:\n                for x in improvs:\n                    fw.write(f\"- {x}\\n\")\n            else:\n                fw.write(\"- (No improvements)\\n\")\n            fw.write(\"\\n## Logs Summary (ERROR/WARN lines)\\n\")\n            if logs_sum:\n                for (fn, lines) in logs_sum:\n                    fw.write(f\"### {fn}\\n\")\n                    if lines:\n                        for ln in lines:\n                            fw.write(f\"- {ln}\\n\")\n                    else:\n                        fw.write(\"- (No ERROR/WARN)\\n\")\n            else:\n                fw.write(\"(No logs found or no ERROR/WARN lines)\\n\")\n        log_info(log_widget, f\"summary_report.md 生成: {outp}\")\n    except Exception as e:\n        log_error(log_widget, f\"summary_report.md 書込失敗: {e}\")\n\n###############################################################################\n# 新機能：コードパッチ適用\n###############################################################################\ndef apply_code_patch(file_path: str, target: str, new_code: str, log_widget=None, preview: bool = False):\n    \"\"\"\n    指定ファイル内の target（\"クラス名.関数名\" または \"クラス名\" または \"関数名\"）の定義を\n    new_code で置き換える。preview=True の場合は適用後の全文を返す（ファイルは更新しない）。\n    \"\"\"\n    content = read_file_content(file_path, log_widget)\n    if not content:\n        log_error(log_widget, f\"{file_path} の読込に失敗しました\")\n        return None if preview else False\n    lines = content.splitlines(keepends=True)\n    start_idx = None\n    target_indent = 0\n\n    # クラス名.関数名 の形式か判定\n    if \".\" in target:\n        class_name, func_name = target.split(\".\", 1)\n        for i, line in enumerate(lines):\n            if line.lstrip().startswith(\"class \") and f\"class {class_name}\" in line:\n                base_indent = len(line) - len(line.lstrip())\n                for j in range(i+1, len(lines)):\n                    lj = lines[j]\n                    if lj.strip() != \"\" and (len(lj) - len(lj.lstrip()) <= base_indent):\n                        break\n                    if lj.lstrip().startswith(\"def \") and f\"def {func_name}\" in lj:\n                        start_idx = j\n                        target_indent = len(lj) - len(lj.lstrip())\n                        break\n                break\n    else:\n        # 単体の関数またはクラス定義を対象に\n        for i, line in enumerate(lines):\n            if (line.lstrip().startswith(\"def \") or line.lstrip().startswith(\"class \")) and f\" {target}\" in line:\n                start_idx = i\n                target_indent = len(line) - len(line.lstrip())\n                break\n\n    if start_idx is None:\n        log_error(log_widget, f\"対象が見つかりません: {target}\")\n        return None if preview else False\n\n    # 終了位置を検出（同じか小さいインデントが出現したら終了と判断）\n    end_idx = None\n    for i in range(start_idx+1, len(lines)):\n        if lines[i].strip() == \"\":\n            continue\n        if (len(lines[i]) - len(lines[i].lstrip())) <= target_indent:\n            end_idx = i\n            break\n    if end_idx is None:\n        end_idx = len(lines)\n\n    # new_code各行に適切なインデントを付与\n    indent_str = \" \" * target_indent\n    patched_lines = []\n    for ln in new_code.splitlines():\n        if ln.strip() == \"\":\n            patched_lines.append(\"\\n\")\n        else:\n            patched_lines.append(indent_str + ln.lstrip() + \"\\n\")\n    new_block = \"\".join(patched_lines)\n    new_content = \"\".join(lines[:start_idx] + [new_block] + lines[end_idx:])\n    if preview:\n        return new_content\n    else:\n        write_file_content(file_path, new_content, log_widget)\n        log_info(log_widget, f\"{file_path} の {target} にパッチ適用しました\")\n        return True\n\n###############################################################################\n# 新機能：システムパッチ適用（収集・ビルド・反映を一括実行）\n###############################################################################\ndef apply_system_patch(base_dir: str, log_widget, app_state: AUMAppState) -> bool:\n    log_info(log_widget, \"=== システムパッチ適用開始 ===\")\n    collect(base_dir, log_widget)\n    tmp_out = build_project(base_dir, log_widget, is_obfuscate=True)\n    if not tmp_out:\n        log_error(log_widget, \"ビルドに失敗しました。パッチ適用を中止します。\")\n        return False\n    result = finalize_build(tmp_out, base_dir, log_widget, app_state)\n    if not result:\n        log_error(log_widget, \"システムパッチの最終適用に失敗しました。\")\n        return False\n    log_info(log_widget, \"=== システムパッチ適用完了 ===\")\n    return True\n\n###############################################################################\n# 新機能：パッチ関連UIの実装\n###############################################################################\ndef patch_ui(parent_frame, base_dir: str, log_widget, app_state: AUMAppState):\n    frm_patch = tk.Frame(parent_frame)\n    frm_patch.pack(side=tk.TOP, fill=tk.X, pady=5)\n\n    def on_code_edit():\n        file_path = filedialog.askopenfilename(\n            title=\"パッチ対象のファイルを選択\", initialdir=base_dir,\n            filetypes=[(\"Python files\", \"*.py\")])\n        if not file_path:\n            return\n        target = simpledialog.askstring(\"対象指定\", \"編集するクラス.関数 または 関数/クラス名を入力してください:\")\n        if not target:\n            return\n        original_code = read_file_content(file_path, log_widget)\n        if not original_code:\n            return\n        edited_code = simpledialog.askstring(\"コード編集\", f\"{target} の新しいコードを入力してください:\", initialvalue=\"\")\n        if edited_code:\n            # 入力内容を一時保存\n            patch_ui.last_edit = (file_path, target, edited_code)\n            log_info(log_widget, f\"コード編集内容を一時保存しました。（対象: {target}）\")\n\n    def on_apply_patch():\n        if not hasattr(patch_ui, \"last_edit\"):\n            messagebox.showinfo(\"情報\", \"先に「コード編集」で変更内容を入力してください\")\n            return\n        file_path, target, new_code = patch_ui.last_edit\n        preview_content = apply_code_patch(file_path, target, new_code, log_widget=None, preview=True)\n        if preview_content is None:\n            messagebox.showerror(\"パッチ適用エラー\", \"コードのプレビュー生成に失敗しました\")\n            return\n        if messagebox.askyesno(\"変更後コードプレビュー\", f\"以下の内容で適用します。\\n\\n{preview_content}\\n\\n適用してよろしいですか？\"):\n            result = apply_code_patch(file_path, target, new_code, log_widget)\n            if result:\n                messagebox.showinfo(\"成功\", \"パッチ適用に成功しました\")\n            else:\n                messagebox.showerror(\"失敗\", \"パッチ適用に失敗しました\")\n        else:\n            log_info(log_widget, \"パッチ適用をキャンセルしました。\")\n\n    def on_system_patch():\n        apply_system_patch(base_dir, log_widget, app_state)\n\n    tk.Button(frm_patch, text=\"コード編集\", command=on_code_edit).pack(side=tk.LEFT, padx=5)\n    tk.Button(frm_patch, text=\"パッチ適用\", command=on_apply_patch).pack(side=tk.LEFT, padx=5)\n    tk.Button(frm_patch, text=\"システムパッチ適用\", command=on_system_patch).pack(side=tk.LEFT, padx=5)\n\n###############################################################################\n# Additional GUI Functions\n###############################################################################\ndef clean(base_dir: str, log_widget):\n    log_info(log_widget, \"クリーンアップを開始...\")\n    current_script = os.path.abspath(__file__)\n    files_to_keep = { current_script, os.path.join(base_dir, 'collected_scripts.json') }\n    if not messagebox.askyesno(\"確認\", \"スクリプト自身とcollected_scripts.jsonを除く全てのファイル/フォルダを削除します。よろしいですか？\"):\n        log_info(log_widget, \"クリーンアップをキャンセルしました。\")\n        return\n    for item in os.listdir(base_dir):\n        item_path = os.path.join(base_dir, item)\n        if item_path in files_to_keep:\n            continue\n        try:\n            if os.path.isfile(item_path) or os.path.islink(item_path):\n                os.remove(item_path)\n                log_info(log_widget, f\"削除: {item_path}\")\n            elif os.path.isdir(item_path):\n                shutil.rmtree(item_path)\n                log_info(log_widget, f\"削除: {item_path}\")\n        except Exception as e:\n            log_error(log_widget, f\"削除失敗: {item_path}: {e}\")\n    log_info(log_widget, \"クリーンアップが完了しました。\")\n\ndef enhanced_smart_paste(base_dir: str, log_widget):\n    try:\n        content = log_widget.clipboard_get()\n    except Exception as e:\n        log_error(log_widget, f\"クリップボード取得失敗: {e}\")\n        return\n    if not content:\n        log_info(log_widget, \"クリップボードが空です。\")\n        return\n    fname = simpledialog.askstring(\"貼り付け\", \"ファイル名を入力してください:\")\n    if not fname:\n        log_info(log_widget, \"貼り付けをキャンセルしました。\")\n        return\n    if '.' not in fname:\n        fname += '.py'\n    file_path = os.path.normpath(os.path.join(base_dir, fname))\n    if not file_path.startswith(os.path.normpath(base_dir)):\n        log_error(log_widget, \"無効なパス指定です。\")\n        return\n    if os.path.exists(file_path):\n        if not messagebox.askyesno(\"確認\", f\"{file_path} は既に存在します。上書きしますか？\"):\n            log_info(log_widget, \"貼り付けをキャンセルしました。\")\n            return\n    write_file_content(file_path, content, log_widget)\n    log_info(log_widget, f\"ファイル作成: {os.path.relpath(file_path, base_dir)}\")\n    collect(base_dir, log_widget)\n\ndef patch_ui_placeholder(parent_frame, base_dir: str, log_widget):\n    # すでに patch_ui にて実装済み\n    pass\n\n###############################################################################\n# GUIエントリポイント\n###############################################################################\ndef gui_main():\n    base_dir = os.path.dirname(os.path.abspath(__file__))\n    app_state = AUMAppState()\n    root = tk.Tk()\n    root.title(\"AUM GUI with Ordered Build\")\n    frm_main = tk.Frame(root)\n    frm_main.pack(fill=tk.BOTH, expand=True)\n\n    log_widget = ScrolledText(frm_main, width=90, height=25)\n    log_widget.pack(side=tk.BOTTOM, fill=tk.BOTH, expand=True)\n\n    frm_top = tk.Frame(frm_main)\n    frm_top.pack(side=tk.TOP, fill=tk.X)\n\n    def on_collect():\n        collect(base_dir, log_widget)\n    def on_token_saving():\n        token_saving_build_unified(base_dir, log_widget, app_state)\n    def on_restore():\n        restore_build(base_dir, log_widget, app_state)\n    def on_clean():\n        clean(base_dir, log_widget)\n    def on_run():\n        run_main_py(base_dir, log_widget, app_state)\n    def on_paste():\n        enhanced_smart_paste(base_dir, log_widget)\n\n    tk.Button(frm_top, text=\"コード収集\", command=on_collect).pack(side=tk.LEFT, padx=5, pady=5)\n    tk.Button(frm_top, text=\"ビルド(復元)\", command=on_restore).pack(side=tk.LEFT, padx=5, pady=5)\n    tk.Button(frm_top, text=\"トークン節約(衝突検知)\", command=on_token_saving).pack(side=tk.LEFT, padx=5, pady=5)\n    tk.Button(frm_top, text=\"クリーン\", command=on_clean).pack(side=tk.LEFT, padx=5, pady=5)\n    tk.Button(frm_top, text=\"実行\", command=on_run).pack(side=tk.LEFT, padx=5, pady=5)\n    tk.Button(frm_top, text=\"貼り付け\", command=on_paste).pack(side=tk.LEFT, padx=5, pady=5)\n\n    # パッチ適用用のUIを統合\n    patch_ui(frm_main, base_dir, log_widget, app_state)\n\n    root.mainloop()\n\nif __name__ == \"__main__\":\n    gui_main()\n"
    },
    {
      "path": "your_project\\aum copy.py",
      "overview": "Pythonコード。\nクラス: AUMAppState, ImportCollector, Mapper。\n関数: get_file_lock, with_file_lock, retry_operation, setup_logging, log_info, log_error, with_io_throttling, _read_file_binary, read_file_content, write_file_content, generate_summary, detect_symbol_collisions, auto_rename_collisions_in_name_map, auto_repair_collected_scripts, merge_collected_scripts, _merge_system_and_settings, _merge_single_script, ensure_init_py_for_python_dirs, restore_settings_from_config_json, collect, _parse_file_for_symbols, collect_python_symbols_for_map, generate_short_symbol_name, ensure_name_map_csv_exists, load_name_mappings, invert_name_map, apply_name_mappings_ast, analyze_dependencies_and_sort, build_in_topological_order, verify_build_results, _build_config_json, build_project, finalize_build, token_saving_build_unified, restore_build, run_main_py, gather_run_artifacts, generate_summary_report, apply_code_patch, apply_system_patch, patch_ui, clean, enhanced_smart_paste, patch_ui_placeholder, gui_main, __init__, wrapper, decorator, wrapper, _collect_task, extract_problematic_identifier, extract_imports, on_code_edit, on_apply_patch, on_system_patch, on_collect, on_token_saving, on_restore, on_clean, on_run, on_paste, wrapper, __init__, visit_Import, visit_ImportFrom, __init__, visit_Name, visit_FunctionDef, visit_ClassDef, visit_Attribute, visit_Import, visit_ImportFrom。\n",
      "content": "import os\nimport sys\nimport csv\nimport json\nimport ast\nimport shutil\nimport logging\nimport re\nimport datetime\nimport tkinter as tk\nfrom tkinter import messagebox, simpledialog, filedialog\nfrom tkinter.scrolledtext import ScrolledText\nfrom typing import Any, Dict, List, Optional, Tuple, Set\nfrom collections import defaultdict, deque\nimport subprocess\nimport threading\nimport functools\nimport time\nimport psutil\nimport asyncio\nimport mmap\nfrom concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor\nfrom functools import lru_cache\nfrom joblib import Memory\nmemory = Memory(location='joblib_cache', verbose=0)\nLOG_LEVEL = 'ERROR'\nDATA_FOLDER_NAME = 'data'\nMAX_REBUILD_ATTEMPTS = 3\nTOKEN_SAVING_EXCLUDE_LIST = {'ConfigManager', 'CsvPlatformFeatureExtractor', 'JsonPlatformFeatureExtractor', 'PlatformSampleManager', 'PlatformIdentifier'}\nABSOLUTE_EXCLUDE_MODULES = {'utils/file_identifier.py', 'parsers/base_parser.py'}\nWRITE_RATE_THRESHOLD = 10 * 1024 * 1024\nTHROTTLE_SLEEP_TIME = 0.5\nLARGE_FILE_THRESHOLD = 5 * 1024 * 1024\n\nclass AUMAppState:\n\n    def __init__(self):\n        self.last_build_folder: Optional[str] = None\n_file_lock_dict: Dict[str, threading.Lock] = {}\n_lock_dict_lock = threading.Lock()\n\ndef get_file_lock(file_path: str) -> threading.Lock:\n    \"\"\"指定ファイルパスごとのグローバルロックを取得\"\"\"\n    with _lock_dict_lock:\n        if file_path not in _file_lock_dict:\n            _file_lock_dict[file_path] = threading.Lock()\n        return _file_lock_dict[file_path]\n\ndef with_file_lock(func):\n    \"\"\"ファイルI/O用グローバルロックを取るデコレータ\"\"\"\n\n    @functools.wraps(func)\n    def wrapper(file_path, *args, **kwargs):\n        lock = get_file_lock(file_path)\n        with lock:\n            return func(file_path, *args, **kwargs)\n    return wrapper\n\ndef retry_operation(max_attempts: int=3):\n\n    def decorator(func):\n\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            attempt = 0\n            while attempt < max_attempts:\n                try:\n                    return func(*args, **kwargs)\n                except Exception as e:\n                    attempt += 1\n                    logging.error(f'{func.__name__} attempt {attempt} failed: {e}')\n                    if attempt >= max_attempts:\n                        raise\n        return wrapper\n    return decorator\n\ndef setup_logging(level='ERROR'):\n    logging.basicConfig(filename='aum_error.log', level=getattr(logging, level.upper(), logging.ERROR), format='%(asctime)s:%(levelname)s:%(message)s', encoding='utf-8')\nsetup_logging(level=LOG_LEVEL)\n\ndef log_info(widget: Optional[tk.Text], msg: str):\n    logging.info(msg)\n    if widget:\n        widget.insert('end', msg + '\\n')\n        widget.see('end')\n\ndef log_error(widget: Optional[tk.Text], msg: str):\n    logging.error(msg)\n    if widget:\n        widget.insert('end', '[ERROR] ' + msg + '\\n')\n        widget.see('end')\n\ndef with_io_throttling(func):\n\n    @functools.wraps(func)\n    def wrapper(file_path, *args, **kwargs):\n        io_before = psutil.disk_io_counters().write_bytes\n        time.sleep(THROTTLE_SLEEP_TIME)\n        io_after = psutil.disk_io_counters().write_bytes\n        write_rate = (io_after - io_before) / THROTTLE_SLEEP_TIME\n        if write_rate > WRITE_RATE_THRESHOLD:\n            logging.info(f'High disk write rate: {write_rate / (1024 * 1024):.2f} MB/s. Throttling...')\n            while write_rate > WRITE_RATE_THRESHOLD:\n                time.sleep(THROTTLE_SLEEP_TIME)\n                io_before = psutil.disk_io_counters().write_bytes\n                time.sleep(THROTTLE_SLEEP_TIME)\n                io_after = psutil.disk_io_counters().write_bytes\n                write_rate = (io_after - io_before) / THROTTLE_SLEEP_TIME\n                logging.info(f'Throttling... current write rate: {write_rate / (1024 * 1024):.2f} MB/s')\n        return func(file_path, *args, **kwargs)\n    return wrapper\n\n@lru_cache(maxsize=None)\ndef _read_file_binary(file_path: str) -> str:\n    \"\"\"ファイルをバイナリで読み込みUTF-8デコード。大容量ファイルはmmapを使用\"\"\"\n    with open(file_path, 'rb') as fr:\n        size = os.path.getsize(file_path)\n        if size > LARGE_FILE_THRESHOLD:\n            mm = mmap.mmap(fr.fileno(), 0, access=mmap.ACCESS_READ)\n            data = mm.read()\n            mm.close()\n        else:\n            data = fr.read()\n    return data.decode('utf-8', errors='ignore')\n\n@with_file_lock\n@with_io_throttling\ndef read_file_content(file_path: str, log_widget=None) -> str:\n    try:\n        return _read_file_binary(file_path)\n    except Exception as e:\n        log_error(log_widget, f'ファイル読込失敗: {file_path}: {e}')\n        return ''\n\n@with_file_lock\n@with_io_throttling\ndef write_file_content(file_path: str, content: str, log_widget=None):\n    try:\n        os.makedirs(os.path.dirname(file_path), exist_ok=True)\n        with open(file_path, 'w', encoding='utf-8') as fw:\n            fw.write(content)\n    except Exception as e:\n        log_error(log_widget, f'ファイル書込み失敗: {file_path}: {e}')\n\ndef generate_summary(file_path: str, content: str) -> str:\n    if file_path.endswith('.py'):\n        try:\n            tree = ast.parse(content)\n            funcs = [n.name for n in ast.walk(tree) if isinstance(n, ast.FunctionDef)]\n            clss = [n.name for n in ast.walk(tree) if isinstance(n, ast.ClassDef)]\n            docstrings = ast.get_docstring(tree)\n            s = 'Pythonコード。\\n'\n            if clss:\n                s += f\"クラス: {', '.join(clss)}。\\n\"\n            if funcs:\n                s += f\"関数: {', '.join(funcs)}。\\n\"\n            if docstrings:\n                s += f'Docstring(冒頭60文字): {docstrings[:60]}...\\n'\n            return s\n        except Exception as e:\n            logging.error(f'AST解析失敗: {file_path}: {e}', exc_info=True)\n            return f'解析エラー: {e}\\n'\n    elif file_path.endswith('.ipynb'):\n        try:\n            nb = json.loads(content)\n            cells = nb.get('cells', [])\n            m = sum((1 for c in cells if c.get('cell_type') == 'markdown'))\n            c = sum((1 for c in cells if c.get('cell_type') == 'code'))\n            return f'Jupyter Notebook。\\nセル合計={len(cells)}, Markdown={m}, Code={c}\\n'\n        except Exception as e:\n            return f'Jupyter Notebook。\\nNotebook解析エラー: {e}\\n'\n    elif file_path.endswith('.json'):\n        try:\n            obj = json.loads(content)\n            if isinstance(obj, dict):\n                keys = list(obj.keys())\n                keys_display = ', '.join(keys[:10])\n                if len(keys) > 10:\n                    keys_display += '...'\n                return f'JSONファイル (辞書)。キー: {keys_display}\\n'\n            elif isinstance(obj, list):\n                return f'JSONファイル (リスト)。要素数: {len(obj)}\\n'\n            else:\n                return 'JSONファイル (その他形式)。\\n'\n        except Exception as e:\n            return f'JSON解析エラー: {e}\\n'\n    elif file_path.endswith('.tex'):\n        lines = content.count('\\n')\n        return f'TeXファイル。行数: {lines}\\n'\n    else:\n        lines = content.count('\\n')\n        return f'その他テキスト。\\n行数: {lines}\\n'\n\ndef detect_symbol_collisions(mapping: Dict[str, str]) -> set:\n    from collections import defaultdict\n    rev = defaultdict(list)\n    for (orig, short_) in mapping.items():\n        rev[short_].append(orig)\n    return {k for (k, v) in rev.items() if len(v) > 1}\n\ndef auto_rename_collisions_in_name_map(csv_path: str, log_widget=None) -> bool:\n    if not os.path.exists(csv_path):\n        return False\n    rows = []\n    try:\n        with open(csv_path, 'r', encoding='utf-8') as f:\n            rd = csv.DictReader(f)\n            for r in rd:\n                rows.append(r)\n    except Exception as e:\n        log_error(log_widget, f'name_map.csv 読込失敗: {e}')\n        return False\n    mapping = {r['original_name']: r['short_name'] for r in rows}\n    collisions = detect_symbol_collisions(mapping)\n    if not collisions:\n        return False\n    short_name_counter = {}\n    renamed_count = 0\n    for r in rows:\n        sh = r['short_name']\n        if sh in collisions:\n            base = sh\n            short_name_counter.setdefault(base, 0)\n            short_name_counter[base] += 1\n            r['short_name'] = base + str(short_name_counter[base])\n            renamed_count += 1\n    if renamed_count:\n        try:\n            with open(csv_path, 'w', encoding='utf-8', newline='') as fw:\n                w = csv.writer(fw)\n                w.writerow(['original_name', 'short_name'])\n                for row in rows:\n                    w.writerow([row['original_name'], row['short_name']])\n            log_info(log_widget, f'name_map.csv 衝突を {renamed_count} 件リネーム修正')\n            return True\n        except Exception as e:\n            log_error(log_widget, f'name_map.csv 修正書込失敗: {e}')\n    return False\n\ndef auto_repair_collected_scripts(base_dir: str, log_widget) -> None:\n    cjson_path = os.path.join(base_dir, 'collected_scripts.json')\n    if not os.path.exists(cjson_path):\n        return\n    try:\n        with open(cjson_path, 'r', encoding='utf-8') as fr:\n            content = fr.read().strip()\n            if not content:\n                raise ValueError('Empty file')\n            data = json.loads(content)\n        if not isinstance(data, dict):\n            raise ValueError('トップレベルがdictではない')\n        if 'scripts' in data and (not isinstance(data['scripts'], list)):\n            raise ValueError(\"'scripts'がlistではない\")\n    except Exception as e:\n        log_error(log_widget, f'collected_scripts.json 破損: {e}')\n        backup = cjson_path + '.bak'\n        try:\n            shutil.move(cjson_path, backup)\n            log_info(log_widget, f'破損ファイルをバックアップ→ {backup}')\n        except Exception as e_backup:\n            log_error(log_widget, f'バックアップ失敗: {e_backup}')\n        data = {'system_overview': 'このシステムは...（必要に応じて書き換え）', 'settings': {}, 'scripts': []}\n        try:\n            with open(cjson_path, 'w', encoding='utf-8') as fw:\n                json.dump(data, fw, ensure_ascii=False, indent=2)\n            log_info(log_widget, 'collected_scripts.json を新規作成')\n        except Exception as e_write:\n            log_error(log_widget, f'新規作成失敗: {e_write}')\n\ndef merge_collected_scripts(original: dict, new: dict, file_data: Dict[str, dict], log_widget) -> dict:\n    _merge_system_and_settings(original, new)\n    old_scripts = original.get('scripts', [])\n    old_map = {s['path']: s for s in old_scripts if 'path' in s}\n    new_scripts = new.get('scripts', [])\n    new_map = {s.get('path'): s for s in new_scripts if s.get('path')}\n    for (p, new_sc) in new_map.items():\n        if p not in old_map:\n            old_map[p] = new_sc\n            log_info(log_widget, f'新規追加: {p}')\n        else:\n            old_sc = old_map[p]\n            _merge_single_script(old_sc, new_sc, file_data, p, log_widget)\n    merged_list = list(old_map.values())\n    original['scripts'] = merged_list\n    return original\n\ndef _merge_system_and_settings(old: dict, new: dict):\n    if new.get('system_overview'):\n        old['system_overview'] = new['system_overview']\n    if 'settings' in new and isinstance(new['settings'], dict):\n        old.setdefault({}).update(new['settings'])\n\ndef _merge_single_script(old_sc: dict, new_sc: dict, file_data: Dict[str, dict], p: str, log_widget):\n    old_cont = old_sc.get('content', '')\n    new_cont = new_sc.get('content', '')\n    disk_cont = file_data.get(p, {}).get('content', '')\n    conflict = old_cont != disk_cont and new_cont != disk_cont and (old_cont != new_cont)\n    if conflict:\n        log_info(log_widget, f'衝突あり: {p}')\n    else:\n        for (k, v) in new_sc.items():\n            if k != 'path':\n                old_sc[k] = v\n        log_info(log_widget, f'上書き: {p}')\n\ndef ensure_init_py_for_python_dirs(base_dir: str, log_widget):\n    exclude = {'venv', '__pycache__', '.git', 'outputs', 'temp_build', 'backups', DATA_FOLDER_NAME}\n    py_dirs = set()\n    for (root, dirs, files) in os.walk(base_dir):\n        dirs[:] = [d for d in dirs if d not in exclude]\n        if any((f.endswith('.py') for f in files)):\n            py_dirs.add(root)\n    for d in py_dirs:\n        init_file = os.path.join(d, '__init__.py')\n        if not os.path.isfile(init_file):\n            write_file_content(init_file, '', log_widget)\n            log_info(log_widget, f'__init__.py 自動生成: {os.path.relpath(init_file, base_dir)}')\n\ndef restore_settings_from_config_json(base_dir: str, log_widget):\n    \"\"\"\n    your_project直下または your_project/your_project にある config.json を探し、\n    設定情報を collected_scripts.json の settings に反映する。\n    ※設定ファイル内に \"settings\" キーがあれば、その中身を使用（なければファイル全体を設定として取り込む）\n    \"\"\"\n    possible_cfg_paths = [os.path.join(base_dir, 'your_project', 'config.json'), os.path.join(base_dir, 'your_project', 'your_project', 'config.json')]\n    cfg_path = None\n    for pth in possible_cfg_paths:\n        if os.path.isfile(pth):\n            cfg_path = pth\n            break\n    if not cfg_path:\n        log_info(log_widget, 'config.jsonが見つからないためスキップ')\n        return\n    cjson_path = os.path.join(base_dir, 'collected_scripts.json')\n    if not os.path.isfile(cjson_path):\n        log_info(log_widget, f'collected_scripts.jsonが無い: {cjson_path}')\n        return\n    try:\n        with open(cfg_path, 'r', encoding='utf-8') as f:\n            cfg_data = json.load(f)\n    except Exception as e:\n        log_error(log_widget, f'config.json 読込失敗: {e}')\n        return\n    if not isinstance(cfg_data, dict):\n        log_info(log_widget, 'config.jsonの形式が不正です。')\n        return\n    new_settings = cfg_data.get(cfg_data)\n    try:\n        with open(cjson_path, 'r', encoding='utf-8') as fr:\n            cjson_data = json.load(fr)\n        if not isinstance(cjson_data, dict):\n            cjson_data = {'system_overview': '', 'settings': {}, 'scripts': []}\n    except Exception as e:\n        log_error(log_widget, f'collected_scripts.json 読込失敗: {e}')\n        return\n    cjson_data.setdefault({}).update(new_settings)\n    try:\n        with open(cjson_path, 'w', encoding='utf-8') as fw:\n            json.dump(cjson_data, fw, ensure_ascii=False, indent=2)\n        log_info(log_widget, f'{cjson_path} の settings を更新しました')\n    except Exception as e:\n        log_error(log_widget, f'config→collected_scripts 反映失敗: {e}')\n\ndef collect(base_dir: str, log_widget):\n    log_info(log_widget, 'コード収集を開始...')\n    auto_repair_collected_scripts(base_dir, log_widget)\n    output_folder = os.path.join(base_dir, 'your_project')\n    cjson = os.path.join(base_dir, 'collected_scripts.json')\n    if os.path.isfile(cjson):\n        try:\n            with open(cjson, 'r', encoding='utf-8') as fr:\n                old_data = json.load(fr)\n            if not isinstance(old_data, dict):\n                old_data = {'system_overview': '', 'settings': {}, 'scripts': []}\n        except Exception as e:\n            log_error(log_widget, f'collected_scripts.json 読込失敗: {e}')\n            old_data = {'system_overview': '', 'settings': {}, 'scripts': []}\n    else:\n        old_data = {'system_overview': '', 'settings': {}, 'scripts': []}\n    ensure_init_py_for_python_dirs(base_dir, log_widget)\n    target_ext = ('.py', '.json', '.tex')\n    exclude_dirs = {'venv', '__pycache__', '.git', 'outputs', 'temp_build', 'backups', DATA_FOLDER_NAME}\n    exclude_files = {'collected_scripts.json', 'config.json', os.path.basename(__file__)}\n    new_list: List[Dict[str, str]] = []\n    files_to_collect: List[str] = []\n    for (root, dirs, files) in os.walk(base_dir):\n        abs_root = os.path.abspath(root)\n        abs_output = os.path.abspath(output_folder)\n        if abs_root.startswith(abs_output) and 'your_project' not in os.path.relpath(abs_root, base_dir).split(os.sep)[0]:\n            continue\n        dirs[:] = [d for d in dirs if d not in exclude_dirs]\n        for fn in files:\n            if fn in exclude_files:\n                continue\n            if any((fn.endswith(e) for e in target_ext)):\n                fp = os.path.join(root, fn)\n                if fp == os.path.abspath(__file__):\n                    continue\n                files_to_collect.append(fp)\n\n    def _collect_task(fp: str):\n        content = read_file_content(fp, log_widget=None)\n        if not content:\n            return None\n        relp = os.path.relpath(fp, base_dir)\n        summ = generate_summary(fp, content)\n        return {'path': relp, 'overview': summ, 'content': content}\n    if files_to_collect:\n\n        async def run_tasks():\n            tasks = [asyncio.to_thread(_collect_task, fp) for fp in files_to_collect]\n            return await asyncio.gather(*tasks)\n        results = asyncio.run(run_tasks())\n        for res in results:\n            if res:\n                new_list.append(res)\n                log_info(log_widget, f\"収集: {res['path']}\")\n    file_map = {s['path']: {'content': s['content'], 'overview': s['overview']} for s in new_list}\n    merged = merge_collected_scripts(old_data, {'system_overview': old_data.get('system_overview', ''), 'settings': old_data.get({}), 'scripts': new_list}, file_map, log_widget)\n    try:\n        with open(cjson, 'w', encoding='utf-8') as fw:\n            json.dump(merged, fw, ensure_ascii=False, indent=2)\n        log_info(log_widget, f'出力完了: {cjson}')\n    except Exception as e:\n        log_error(log_widget, f'書き込み失敗: {e}')\n    restore_settings_from_config_json(base_dir, log_widget)\n\ndef _parse_file_for_symbols(file_path: str) -> Set[str]:\n    \"\"\"指定Pythonファイルから関数名・クラス名を抽出\"\"\"\n    try:\n        with open(file_path, 'r', encoding='utf-8') as fr:\n            content = fr.read()\n        tree = ast.parse(content)\n        symbols: Set[str] = set()\n        for node in ast.walk(tree):\n            if isinstance(node, ast.FunctionDef) or isinstance(node, ast.ClassDef):\n                symbols.add(node.name)\n        return symbols\n    except Exception as e:\n        logging.error(f'AST解析失敗: {file_path}: {e}', exc_info=True)\n        return set()\nif memory:\n    _parse_file_for_symbols = memory.cache(_parse_file_for_symbols)\n\ndef collect_python_symbols_for_map(base_dir: str) -> set:\n    exclude = {'venv', '__pycache__', '.git', 'temp_build', 'backups', DATA_FOLDER_NAME}\n    found: Set[str] = set()\n    files_to_scan: List[str] = []\n    for (root, dirs, files) in os.walk(base_dir):\n        if os.path.basename(root).lower() == DATA_FOLDER_NAME.lower():\n            continue\n        abs_root = os.path.abspath(root)\n        abs_output = os.path.abspath(os.path.join(base_dir, 'your_project'))\n        if abs_root.startswith(abs_output):\n            continue\n        dirs[:] = [d for d in dirs if d not in exclude]\n        for f in files:\n            if f.endswith('.py') and f != os.path.basename(__file__):\n                files_to_scan.append(os.path.join(root, f))\n    if files_to_scan:\n        with ProcessPoolExecutor() as executor:\n            for symbols in executor.map(_parse_file_for_symbols, files_to_scan):\n                found |= symbols\n    return found\n\ndef generate_short_symbol_name(orig_name: str) -> str:\n    if not orig_name:\n        return 'x'\n    return orig_name[0].lower() + '_sym'\n\ndef ensure_name_map_csv_exists(base_dir: str) -> str:\n    csv_path = os.path.join(base_dir, 'name_map.csv')\n    syms = collect_python_symbols_for_map(base_dir)\n    existing_map = {}\n    if os.path.isfile(csv_path):\n        try:\n            with open(csv_path, 'r', encoding='utf-8') as fr:\n                rd = csv.DictReader(fr)\n                for r in rd:\n                    o = r['original_name']\n                    s = r['short_name']\n                    existing_map[o] = s\n        except Exception as e:\n            logging.error(f'name_map.csv 読込失敗: {e}')\n    else:\n        logging.info('name_map.csvがないため新規作成')\n    for s in syms:\n        if s in TOKEN_SAVING_EXCLUDE_LIST:\n            existing_map[s] = s\n        elif s not in existing_map:\n            existing_map[s] = generate_short_symbol_name(s)\n    with open(csv_path, 'w', encoding='utf-8', newline='') as fw:\n        w = csv.writer(fw)\n        w.writerow(['original_name', 'short_name'])\n        for (k, v) in sorted(existing_map.items()):\n            w.writerow([k, v])\n    return csv_path\n\ndef load_name_mappings(csv_path: str) -> dict:\n    mp = {}\n    if not os.path.isfile(csv_path):\n        return mp\n    try:\n        with open(csv_path, 'r', encoding='utf-8') as fr:\n            rd = csv.DictReader(fr)\n            for r in rd:\n                mp[r['original_name']] = r['short_name']\n    except Exception as e:\n        logging.error(f'name_map.csv 読込失敗: {e}')\n    return mp\n\ndef invert_name_map(mp: dict) -> dict:\n    return {v: k for (k, v) in mp.items()}\nGLOBAL_EXCLUDE_MODULES = ABSOLUTE_EXCLUDE_MODULES\n\ndef apply_name_mappings_ast(content: str, name_map: dict, path_for_check: str) -> str:\n    norm_path = os.path.normpath(path_for_check).replace('\\\\', '/')\n    for exc_subpath in GLOBAL_EXCLUDE_MODULES:\n        if exc_subpath in norm_path:\n            return content\n    FIXED_EXCLUDE = TOKEN_SAVING_EXCLUDE_LIST.copy()\n    current_exclude = set()\n    attempt = 0\n\n    def extract_problematic_identifier(e: Exception) -> Optional[str]:\n        msg = str(e)\n        m = re.search(\"object has no attribute '(\\\\w+)'\", msg)\n        if m:\n            return m.group(1)\n        m = re.search(\"cannot import name '(\\\\w+)'\", msg)\n        if m:\n            return m.group(1)\n        return None\n\n    class ImportCollector(ast.NodeVisitor):\n\n        def __init__(self):\n            self.imported = set()\n\n        def visit_Import(self, node):\n            for alias in node.names:\n                name = alias.asname if alias.asname else alias.name.split('.')[0]\n                self.imported.add(name)\n            self.generic_visit(node)\n\n        def visit_ImportFrom(self, node):\n            if node.module:\n                self.imported.add(node.module.split('.')[0])\n            for alias in node.names:\n                name = alias.asname if alias.asname else alias.name\n                self.imported.add(name)\n            self.generic_visit(node)\n\n    class Mapper(ast.NodeTransformer):\n\n        def __init__(self, exclude_set, nmap):\n            self.exclude = exclude_set\n            self.nmap = nmap\n            super().__init__()\n\n        def visit_Name(self, node):\n            if node.id.startswith('__') and node.id.endswith('__'):\n                return node\n            if node.id in self.exclude:\n                return node\n            if node.id in self.nmap:\n                node.id = self.nmap[node.id]\n            return self.generic_visit(node)\n\n        def visit_FunctionDef(self, node):\n            if not (node.name.startswith('__') and node.name.endswith('__')):\n                if node.name not in self.exclude and node.name in self.nmap:\n                    node.name = self.nmap[node.name]\n            self.generic_visit(node)\n            return node\n\n        def visit_ClassDef(self, node):\n            if not (node.name.startswith('__') and node.name.endswith('__')):\n                if node.name not in self.exclude and node.name in self.nmap:\n                    node.name = self.nmap[node.name]\n            self.generic_visit(node)\n            return node\n\n        def visit_Attribute(self, node):\n            self.generic_visit(node)\n            if isinstance(node.value, ast.Name) and node.value.id == 'self':\n                return node\n            if node.attr.startswith('__') and node.attr.endswith('__'):\n                return node\n            if node.attr in self.exclude:\n                return node\n            if node.attr in self.nmap:\n                node.attr = self.nmap[node.attr]\n            return node\n\n        def visit_Import(self, node):\n            for alias in node.names:\n                if alias.name in self.exclude:\n                    continue\n                if alias.name in self.nmap:\n                    alias.name = self.nmap[alias.name]\n                if alias.asname and alias.asname in self.nmap:\n                    alias.asname = self.nmap[alias.asname]\n            return node\n\n        def visit_ImportFrom(self, node):\n            if node.module in self.exclude:\n                return node\n            if node.module and node.module in self.nmap:\n                node.module = self.nmap[node.module]\n            for alias in node.names:\n                if alias.name in self.exclude:\n                    continue\n                if alias.name in self.nmap:\n                    alias.name = self.nmap[alias.name]\n                if alias.asname and alias.asname in self.nmap:\n                    alias.asname = self.nmap[alias.asname]\n            return node\n    while True:\n        attempt += 1\n        try:\n            tree = ast.parse(content)\n            collector = ImportCollector()\n            collector.visit(tree)\n            auto_exclude = collector.imported\n            total_exclude = FIXED_EXCLUDE.union(auto_exclude).union(current_exclude)\n            mapper = Mapper(total_exclude, name_map)\n            new_tree = mapper.visit(tree)\n            ast.fix_missing_locations(new_tree)\n            if hasattr(ast, 'unparse'):\n                out_code = ast.unparse(new_tree)\n            else:\n                import astor\n                out_code = astor.to_source(new_tree)\n            return out_code\n        except Exception as e:\n            problematic = extract_problematic_identifier(e)\n            if problematic:\n                if problematic not in current_exclude:\n                    current_exclude.add(problematic)\n                    continue\n                else:\n                    return content\n            else:\n                return content\n\ndef analyze_dependencies_and_sort(scripts: List[Dict[str, str]], log_widget=None) -> List[Dict[str, str]]:\n    graph = {}\n    script_map = {}\n    for sc in scripts:\n        p = sc['path']\n        script_map[p] = sc\n        graph[p] = set()\n    path_by_modname = {}\n    for sc in scripts:\n        p = sc['path'].replace('\\\\', '/')\n        if p.endswith('.py'):\n            mod_parts = p.split('/')\n            if mod_parts[0] == 'your_project':\n                mod_parts = mod_parts[1:]\n            py_base = mod_parts[-1][:-3]\n            mod_parts[-1] = py_base\n            dotted = '.'.join(mod_parts)\n            path_by_modname[dotted] = p\n\n    def extract_imports(p: str, code: str) -> Set[str]:\n        import_list = set()\n        try:\n            tree = ast.parse(code)\n            for node in ast.walk(tree):\n                if isinstance(node, ast.Import):\n                    for alias in node.names:\n                        mod = alias.name.split('.')[0]\n                        import_list.add(mod)\n                elif isinstance(node, ast.ImportFrom):\n                    if node.module:\n                        base = node.module.split('.')[0]\n                        import_list.add(base)\n        except Exception as e:\n            logging.error(f'依存関係解析: {p} の import抽出失敗: {e}', exc_info=True)\n            log_error(log_widget, f'依存関係解析: {p} の import抽出失敗: {e}')\n        return import_list\n    for sc in scripts:\n        p = sc['path']\n        code = sc['content']\n        imported = extract_imports(p, code)\n        for imp in imported:\n            for (k, v) in path_by_modname.items():\n                if k.startswith(imp):\n                    graph[p].add(v)\n    in_degree = {p: 0 for p in graph}\n    for p in graph:\n        for dep in graph[p]:\n            if dep in in_degree:\n                in_degree[dep] += 1\n    queue = deque([p for p in in_degree if in_degree[p] == 0])\n    sorted_paths = []\n    while queue:\n        u = queue.popleft()\n        sorted_paths.append(u)\n        for dep in graph[u]:\n            if dep in in_degree:\n                in_degree[dep] -= 1\n                if in_degree[dep] == 0:\n                    queue.append(dep)\n    if len(sorted_paths) < len(graph):\n        logging.error('循環依存関係が検出されました。依存順序の保証ができません。')\n        log_error(log_widget, '循環依存関係が検出されました。依存順序の保証ができません。')\n        return scripts\n    path_to_index = {p: i for (i, p) in enumerate(sorted_paths)}\n    scripts_sorted = sorted(scripts, key=lambda sc: path_to_index.get(sc['path'], float('inf')))\n    return scripts_sorted\n\ndef build_in_topological_order(scripts: List[Dict[str, str]], out_dir: str, name_map: Dict[str, str], log_widget) -> Tuple[int, List[str]]:\n    sorted_scripts = analyze_dependencies_and_sort(scripts, log_widget)\n    logs: List[str] = []\n    built_count = 0\n    for sc in sorted_scripts:\n        path_ = sc['path']\n        content_ = sc['content']\n        if path_.endswith('.py'):\n            new_content = apply_name_mappings_ast(content_, name_map, path_for_check=path_)\n        elif path_.endswith('.json') or path_.endswith('.tex'):\n            new_content = content_\n        else:\n            continue\n        rel_path = path_\n        if rel_path.startswith('your_project' + os.path.sep):\n            rel_path = rel_path[len('your_project' + os.path.sep):]\n        target_file = os.path.join(out_dir, rel_path)\n        try:\n            os.makedirs(os.path.dirname(target_file), exist_ok=True)\n            with open(target_file, 'w', encoding='utf-8') as fw:\n                fw.write(new_content)\n            logs.append(f'出力: {os.path.relpath(target_file, out_dir)}')\n            built_count += 1\n        except Exception as e:\n            logs.append(f'書き込みエラー: {target_file}: {e}')\n    return (built_count, logs)\n\ndef verify_build_results(build_dir: str, collected_json: str, log_widget) -> bool:\n    try:\n        with open(collected_json, 'r', encoding='utf-8') as fr:\n            data = json.load(fr)\n    except Exception as e:\n        log_error(log_widget, f'収集結果の読込失敗: {e}')\n        return False\n    scripts = data.get('scripts', [])\n    all_ok = True\n    for script in scripts:\n        p = script.get('path', '')\n        if not (p.endswith('.py') or p.endswith('.json') or p.endswith('.tex')):\n            continue\n        rel_path = p\n        if rel_path.startswith('your_project' + os.path.sep):\n            rel_path = rel_path[len('your_project' + os.path.sep):]\n        build_file = os.path.join(build_dir, rel_path)\n        if not os.path.exists(build_file):\n            log_error(log_widget, f'ビルド結果に存在しない: {build_file}')\n            all_ok = False\n            continue\n        with open(build_file, 'r', encoding='utf-8') as bf:\n            build_content = bf.read()\n        if build_content.strip() != script.get('content', '').strip():\n            log_error(log_widget, f'内容不一致: {build_file}')\n            all_ok = False\n        else:\n            log_info(log_widget, f'チェックOK: {build_file}')\n    return all_ok\n\ndef _build_config_json(settings: dict, out_dir: str, log_widget):\n    cfg_path = os.path.join(out_dir, 'config.json')\n    try:\n        os.makedirs(os.path.dirname(cfg_path), exist_ok=True)\n        with open(cfg_path, 'w', encoding='utf-8') as fw:\n            json.dump(settings, fw, ensure_ascii=False, indent=2)\n        log_info(log_widget, f'config.json生成: {cfg_path}')\n    except Exception as e:\n        log_error(log_widget, f'config.json 書込失敗: {e}')\n\ndef build_project(base_dir: str, log_widget, is_obfuscate: bool=True) -> Optional[str]:\n    auto_repair_collected_scripts(base_dir, log_widget)\n    cjson = os.path.join(base_dir, 'collected_scripts.json')\n    if not os.path.isfile(cjson):\n        messagebox.showerror('エラー', f'{cjson} が見つかりません')\n        return None\n    csv_path = ensure_name_map_csv_exists(base_dir)\n    direct_map = load_name_mappings(csv_path)\n    final_map = direct_map if is_obfuscate else invert_name_map(direct_map)\n    tmp_build_root = os.path.join(base_dir, 'temp_build')\n    os.makedirs(tmp_build_root, exist_ok=True)\n    out_dir = os.path.join(tmp_build_root, 'your_project_temp')\n    if os.path.exists(out_dir):\n        shutil.rmtree(out_dir)\n    os.makedirs(out_dir, exist_ok=True)\n    log_info(log_widget, f'一時出力先: {out_dir}')\n    try:\n        with open(cjson, 'r', encoding='utf-8') as fr:\n            data = json.load(fr)\n    except Exception as e:\n        log_error(log_widget, f'collected_scripts.json 読込失敗: {e}')\n        return None\n    scripts = data.get('scripts', [])\n    settings = data.get({})\n    py_scripts = [s for s in scripts if s.get('path', '').endswith(('.py', '.json', '.tex'))]\n    (built_count, logs_texts) = build_in_topological_order(py_scripts, out_dir, final_map, log_widget)\n    _build_config_json(settings, out_dir, log_widget)\n    built_count += 1\n    if logs_texts:\n        for l in logs_texts:\n            log_info(log_widget, l)\n    log_info(log_widget, f'ビルド完了: {built_count}ファイル')\n    if not verify_build_results(out_dir, cjson, log_widget):\n        log_error(log_widget, '収集結果とビルド結果が一致しません')\n    else:\n        log_info(log_widget, 'ビルド結果と収集結果は一致しています')\n    restore_settings_from_config_json(base_dir, log_widget)\n    return out_dir\n\ndef finalize_build(temp_out_dir: str, base_dir: str, log_widget, app_state: AUMAppState) -> bool:\n    if not temp_out_dir or not os.path.isdir(temp_out_dir):\n        log_error(log_widget, f'一時フォルダが無い: {temp_out_dir}')\n        return False\n    final_dir = os.path.join(base_dir, 'your_project')\n    if os.path.isdir(final_dir):\n        backups_dir = os.path.join(base_dir, 'backups')\n        os.makedirs(backups_dir, exist_ok=True)\n        stamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n        zip_name = os.path.join(backups_dir, f'your_project_old_{stamp}')\n        try:\n            shutil.make_archive(zip_name, 'zip', final_dir)\n            log_info(log_widget, f'既存フォルダをzipバックアップ: {zip_name}.zip')\n        except Exception as e:\n            log_error(log_widget, f'バックアップ失敗: {e}')\n        try:\n            shutil.rmtree(final_dir)\n            log_info(log_widget, f'既存フォルダ削除: {final_dir}')\n        except Exception as e:\n            log_error(log_widget, f'削除失敗: {e}')\n            return False\n    try:\n        shutil.move(temp_out_dir, final_dir)\n        log_info(log_widget, f'上書き完了: {final_dir}')\n        app_state.last_build_folder = base_dir\n    except Exception as e:\n        log_error(log_widget, f'移動失敗: {e}')\n        return False\n    tmpb = os.path.join(base_dir, 'temp_build')\n    if os.path.isdir(tmpb):\n        try:\n            shutil.rmtree(tmpb)\n            log_info(log_widget, f'temp_build削除: {tmpb}')\n        except Exception as ex:\n            log_error(log_widget, f'temp_build削除失敗: {ex}')\n    log_info(log_widget, f'last_build_folder={app_state.last_build_folder}')\n    return True\n\ndef token_saving_build_unified(base_dir: str, log_widget, app_state: AUMAppState):\n    tries = 0\n    while True:\n        tries += 1\n        log_info(log_widget, f'ビルド試行 {tries} 回目')\n        tmp_out = None\n        try:\n            tmp_out = build_project(base_dir, log_widget, is_obfuscate=True)\n        except Exception as e:\n            log_error(log_widget, f'ビルド処理で例外発生: {e}')\n            break\n        if not tmp_out:\n            log_error(log_widget, 'build_project で出力が得られませんでした。')\n            if tries >= MAX_REBUILD_ATTEMPTS:\n                log_error(log_widget, 'ビルドを中断します。')\n                break\n            continue\n        result = finalize_build(tmp_out, base_dir, log_widget, app_state)\n        if not result:\n            log_error(log_widget, 'ビルドの適用に失敗しました。処理を中断します。')\n            break\n        run_main_py(base_dir, log_widget, app_state)\n        all_logs = log_widget.get('1.0', 'end')\n        if 'ImportError:' in all_logs or 'NameError:' in all_logs or 'ModuleNotFoundError:' in all_logs:\n            log_info(log_widget, 'エラー検知（名前衝突等）。name_map.csv を修正して再試行します。')\n            csvp = os.path.join(base_dir, 'name_map.csv')\n            fixed = auto_rename_collisions_in_name_map(csvp, log_widget)\n            if fixed:\n                continue\n            else:\n                log_error(log_widget, 'name_map.csv の自動修正に失敗しました。再試行します。')\n                continue\n        else:\n            log_info(log_widget, 'ビルド成功：エラーなし。')\n            break\n\ndef restore_build(base_dir: str, log_widget, app_state: AUMAppState):\n    try:\n        tmp_out = build_project(base_dir, log_widget, is_obfuscate=False)\n    except Exception as e:\n        log_error(log_widget, f'ビルド処理で例外発生: {e}')\n        return\n    if tmp_out:\n        finalize_build(tmp_out, base_dir, log_widget, app_state)\n\ndef run_main_py(base_dir: str, log_widget, app_state: AUMAppState):\n    final_dir = os.path.join(base_dir, 'your_project')\n    main_py = os.path.join(final_dir, 'main.py')\n    data_path = os.path.join(final_dir, DATA_FOLDER_NAME)\n    log_info(log_widget, f'main.py => {main_py}')\n    log_info(log_widget, f'--input => {data_path}')\n    if not os.path.isfile(main_py):\n        log_error(log_widget, 'main.py が見つからない')\n        return\n    if not os.path.isdir(data_path):\n        log_info(log_widget, f'dataフォルダなし: {data_path}')\n    cmd = [sys.executable, main_py, '--input', data_path, '--skip-parent-dir']\n    try:\n        ret = subprocess.run(cmd, capture_output=True, text=True)\n        if ret.returncode != 0:\n            log_error(log_widget, f'エラー({ret.returncode}): {ret.stderr}')\n        else:\n            log_info(log_widget, f'実行完了:\\n{ret.stdout}')\n    except Exception as e:\n        log_error(log_widget, f'例外: {e}')\n    gather_run_artifacts(base_dir, log_widget, app_state)\n    generate_summary_report(base_dir, log_widget)\n\ndef gather_run_artifacts(base_dir: str, log_widget, app_state: AUMAppState):\n    final_dir = os.path.join(base_dir, 'your_project')\n    if not os.path.isdir(final_dir):\n        log_info(log_widget, 'your_projectがない')\n        return\n    runs = []\n    for d in os.listdir(final_dir):\n        if d.startswith('run_'):\n            fp = os.path.join(final_dir, d)\n            if os.path.isdir(fp):\n                runs.append(fp)\n    if not runs:\n        log_info(log_widget, 'run_*** フォルダなし')\n        return\n    runs.sort(key=lambda x: os.path.getmtime(x), reverse=True)\n    latest = runs[0]\n    log_info(log_widget, f'最新run: {latest}')\n    fb_src = os.path.join(latest, 'feedback.json')\n    fb_dst = os.path.join(base_dir, 'feedback.json')\n    if os.path.isfile(fb_src):\n        shutil.copy2(fb_src, fb_dst)\n        log_info(log_widget, f'feedback.json-> {fb_dst}')\n    logs_src = os.path.join(latest, 'logs')\n    logs_dst = os.path.join(base_dir, 'logs')\n    if os.path.isdir(logs_src):\n        if os.path.isdir(logs_dst):\n            shutil.rmtree(logs_dst)\n        shutil.copytree(logs_src, logs_dst)\n        log_info(log_widget, f'logs-> {logs_dst}')\n    else:\n        log_info(log_widget, 'logsなし')\n\ndef generate_summary_report(base_dir: str, log_widget):\n    log_info(log_widget, 'レポート生成...')\n    fb = os.path.join(base_dir, 'feedback.json')\n    logsdir = os.path.join(base_dir, 'logs')\n    outp = os.path.join(base_dir, 'summary_report.md')\n    data = {}\n    if os.path.isfile(fb):\n        try:\n            with open(fb, 'r', encoding='utf-8') as fr:\n                data = json.load(fr)\n        except Exception as e:\n            log_error(log_widget, f'feedback.json 読込失敗: {e}')\n    tstamp = data.get('timestamp', 'N/A')\n    st = data.get('status', 'N/A')\n    remarks = data.get('remarks', '')\n    improvs = data.get('possible_improvements', [])\n    logs_sum = []\n    if os.path.isdir(logsdir):\n        for (root, dirs, files) in os.walk(logsdir):\n            for f in files:\n                if f.endswith('.log'):\n                    fp = os.path.join(root, f)\n                    try:\n                        with open(fp, 'r', encoding='utf-8', errors='replace') as ff:\n                            lines = ff.readlines()\n                        errwarn = [ln.strip() for ln in lines if 'ERROR' in ln or 'WARN' in ln]\n                        relp = os.path.relpath(fp, logsdir)\n                        logs_sum.append((relp, errwarn))\n                    except Exception as e:\n                        log_error(log_widget, f'ログファイル読込失敗: {fp}: {e}')\n    try:\n        with open(outp, 'w', encoding='utf-8') as fw:\n            fw.write('# Analysis Summary Report\\n\\n')\n            fw.write(f'- **Timestamp**: {tstamp}\\n')\n            fw.write(f'- **Status**: {st}\\n')\n            fw.write(f'- **Remarks**: {remarks}\\n\\n')\n            fw.write('## Possible Improvements\\n')\n            if improvs:\n                for x in improvs:\n                    fw.write(f'- {x}\\n')\n            else:\n                fw.write('- (No improvements)\\n')\n            fw.write('\\n## Logs Summary (ERROR/WARN lines)\\n')\n            if logs_sum:\n                for (fn, lines) in logs_sum:\n                    fw.write(f'### {fn}\\n')\n                    if lines:\n                        for ln in lines:\n                            fw.write(f'- {ln}\\n')\n                    else:\n                        fw.write('- (No ERROR/WARN)\\n')\n            else:\n                fw.write('(No logs found or no ERROR/WARN lines)\\n')\n        log_info(log_widget, f'summary_report.md 生成: {outp}')\n    except Exception as e:\n        log_error(log_widget, f'summary_report.md 書込失敗: {e}')\n\ndef apply_code_patch(file_path: str, target: str, new_code: str, log_widget=None, preview: bool=False):\n    \"\"\"\n    指定ファイル内の target（\"クラス名.関数名\" または \"クラス名\" または \"関数名\"）の定義を\n    new_code で置き換える。preview=True の場合は適用後の全文を返す（ファイルは更新しない）。\n    \"\"\"\n    content = read_file_content(file_path, log_widget)\n    if not content:\n        log_error(log_widget, f'{file_path} の読込に失敗しました')\n        return None if preview else False\n    lines = content.splitlines(keepends=True)\n    start_idx = None\n    target_indent = 0\n    if '.' in target:\n        (class_name, func_name) = target.split('.', 1)\n        for (i, line) in enumerate(lines):\n            if line.lstrip().startswith('class ') and f'class {class_name}' in line:\n                base_indent = len(line) - len(line.lstrip())\n                for j in range(i + 1, len(lines)):\n                    lj = lines[j]\n                    if lj.strip() != '' and len(lj) - len(lj.lstrip()) <= base_indent:\n                        break\n                    if lj.lstrip().startswith('def ') and f'def {func_name}' in lj:\n                        start_idx = j\n                        target_indent = len(lj) - len(lj.lstrip())\n                        break\n                break\n    else:\n        for (i, line) in enumerate(lines):\n            if (line.lstrip().startswith('def ') or line.lstrip().startswith('class ')) and f' {target}' in line:\n                start_idx = i\n                target_indent = len(line) - len(line.lstrip())\n                break\n    if start_idx is None:\n        log_error(log_widget, f'対象が見つかりません: {target}')\n        return None if preview else False\n    end_idx = None\n    for i in range(start_idx + 1, len(lines)):\n        if lines[i].strip() == '':\n            continue\n        if len(lines[i]) - len(lines[i].lstrip()) <= target_indent:\n            end_idx = i\n            break\n    if end_idx is None:\n        end_idx = len(lines)\n    indent_str = ' ' * target_indent\n    patched_lines = []\n    for ln in new_code.splitlines():\n        if ln.strip() == '':\n            patched_lines.append('\\n')\n        else:\n            patched_lines.append(indent_str + ln.lstrip() + '\\n')\n    new_block = ''.join(patched_lines)\n    new_content = ''.join(lines[:start_idx] + [new_block] + lines[end_idx:])\n    if preview:\n        return new_content\n    else:\n        write_file_content(file_path, new_content, log_widget)\n        log_info(log_widget, f'{file_path} の {target} にパッチ適用しました')\n        return True\n\ndef apply_system_patch(base_dir: str, log_widget, app_state: AUMAppState) -> bool:\n    log_info(log_widget, '=== システムパッチ適用開始 ===')\n    collect(base_dir, log_widget)\n    tmp_out = build_project(base_dir, log_widget, is_obfuscate=True)\n    if not tmp_out:\n        log_error(log_widget, 'ビルドに失敗しました。パッチ適用を中止します。')\n        return False\n    result = finalize_build(tmp_out, base_dir, log_widget, app_state)\n    if not result:\n        log_error(log_widget, 'システムパッチの最終適用に失敗しました。')\n        return False\n    log_info(log_widget, '=== システムパッチ適用完了 ===')\n    return True\n\ndef patch_ui(parent_frame, base_dir: str, log_widget, app_state: AUMAppState):\n    frm_patch = tk.Frame(parent_frame)\n    frm_patch.pack(side=tk.TOP, fill=tk.X, pady=5)\n\n    def on_code_edit():\n        file_path = filedialog.askopenfilename(title='パッチ対象のファイルを選択', initialdir=base_dir, filetypes=[('Python files', '*.py')])\n        if not file_path:\n            return\n        target = simpledialog.askstring('対象指定', '編集するクラス.関数 または 関数/クラス名を入力してください:')\n        if not target:\n            return\n        original_code = read_file_content(file_path, log_widget)\n        if not original_code:\n            return\n        edited_code = simpledialog.askstring('コード編集', f'{target} の新しいコードを入力してください:', initialvalue='')\n        if edited_code:\n            patch_ui.last_edit = (file_path, target, edited_code)\n            log_info(log_widget, f'コード編集内容を一時保存しました。（対象: {target}）')\n\n    def on_apply_patch():\n        if not hasattr(patch_ui, 'last_edit'):\n            messagebox.showinfo('情報', '先に「コード編集」で変更内容を入力してください')\n            return\n        (file_path, target, new_code) = patch_ui.last_edit\n        preview_content = apply_code_patch(file_path, target, new_code, log_widget=None, preview=True)\n        if preview_content is None:\n            messagebox.showerror('パッチ適用エラー', 'コードのプレビュー生成に失敗しました')\n            return\n        if messagebox.askyesno('変更後コードプレビュー', f'以下の内容で適用します。\\n\\n{preview_content}\\n\\n適用してよろしいですか？'):\n            result = apply_code_patch(file_path, target, new_code, log_widget)\n            if result:\n                messagebox.showinfo('成功', 'パッチ適用に成功しました')\n            else:\n                messagebox.showerror('失敗', 'パッチ適用に失敗しました')\n        else:\n            log_info(log_widget, 'パッチ適用をキャンセルしました。')\n\n    def on_system_patch():\n        apply_system_patch(base_dir, log_widget, app_state)\n    tk.Button(frm_patch, text='コード編集', command=on_code_edit).pack(side=tk.LEFT, padx=5)\n    tk.Button(frm_patch, text='パッチ適用', command=on_apply_patch).pack(side=tk.LEFT, padx=5)\n    tk.Button(frm_patch, text='システムパッチ適用', command=on_system_patch).pack(side=tk.LEFT, padx=5)\n\ndef clean(base_dir: str, log_widget):\n    log_info(log_widget, 'クリーンアップを開始...')\n    current_script = os.path.abspath(__file__)\n    files_to_keep = {current_script, os.path.join(base_dir, 'collected_scripts.json')}\n    if not messagebox.askyesno('確認', 'スクリプト自身とcollected_scripts.jsonを除く全てのファイル/フォルダを削除します。よろしいですか？'):\n        log_info(log_widget, 'クリーンアップをキャンセルしました。')\n        return\n    for item in os.listdir(base_dir):\n        item_path = os.path.join(base_dir, item)\n        if item_path in files_to_keep:\n            continue\n        try:\n            if os.path.isfile(item_path) or os.path.islink(item_path):\n                os.remove(item_path)\n                log_info(log_widget, f'削除: {item_path}')\n            elif os.path.isdir(item_path):\n                shutil.rmtree(item_path)\n                log_info(log_widget, f'削除: {item_path}')\n        except Exception as e:\n            log_error(log_widget, f'削除失敗: {item_path}: {e}')\n    log_info(log_widget, 'クリーンアップが完了しました。')\n\ndef enhanced_smart_paste(base_dir: str, log_widget):\n    try:\n        content = log_widget.clipboard_get()\n    except Exception as e:\n        log_error(log_widget, f'クリップボード取得失敗: {e}')\n        return\n    if not content:\n        log_info(log_widget, 'クリップボードが空です。')\n        return\n    fname = simpledialog.askstring('貼り付け', 'ファイル名を入力してください:')\n    if not fname:\n        log_info(log_widget, '貼り付けをキャンセルしました。')\n        return\n    if '.' not in fname:\n        fname += '.py'\n    file_path = os.path.normpath(os.path.join(base_dir, fname))\n    if not file_path.startswith(os.path.normpath(base_dir)):\n        log_error(log_widget, '無効なパス指定です。')\n        return\n    if os.path.exists(file_path):\n        if not messagebox.askyesno('確認', f'{file_path} は既に存在します。上書きしますか？'):\n            log_info(log_widget, '貼り付けをキャンセルしました。')\n            return\n    write_file_content(file_path, content, log_widget)\n    log_info(log_widget, f'ファイル作成: {os.path.relpath(file_path, base_dir)}')\n    collect(base_dir, log_widget)\n\ndef patch_ui_placeholder(parent_frame, base_dir: str, log_widget):\n    pass\n\ndef gui_main():\n    base_dir = os.path.dirname(os.path.abspath(__file__))\n    app_state = AUMAppState()\n    root = tk.Tk()\n    root.title('AUM GUI with Ordered Build')\n    frm_main = tk.Frame(root)\n    frm_main.pack(fill=tk.BOTH, expand=True)\n    log_widget = ScrolledText(frm_main, width=90, height=25)\n    log_widget.pack(side=tk.BOTTOM, fill=tk.BOTH, expand=True)\n    frm_top = tk.Frame(frm_main)\n    frm_top.pack(side=tk.TOP, fill=tk.X)\n\n    def on_collect():\n        collect(base_dir, log_widget)\n\n    def on_token_saving():\n        token_saving_build_unified(base_dir, log_widget, app_state)\n\n    def on_restore():\n        restore_build(base_dir, log_widget, app_state)\n\n    def on_clean():\n        clean(base_dir, log_widget)\n\n    def on_run():\n        run_main_py(base_dir, log_widget, app_state)\n\n    def on_paste():\n        enhanced_smart_paste(base_dir, log_widget)\n    tk.Button(frm_top, text='コード収集', command=on_collect).pack(side=tk.LEFT, padx=5, pady=5)\n    tk.Button(frm_top, text='ビルド(復元)', command=on_restore).pack(side=tk.LEFT, padx=5, pady=5)\n    tk.Button(frm_top, text='トークン節約(衝突検知)', command=on_token_saving).pack(side=tk.LEFT, padx=5, pady=5)\n    tk.Button(frm_top, text='クリーン', command=on_clean).pack(side=tk.LEFT, padx=5, pady=5)\n    tk.Button(frm_top, text='実行', command=on_run).pack(side=tk.LEFT, padx=5, pady=5)\n    tk.Button(frm_top, text='貼り付け', command=on_paste).pack(side=tk.LEFT, padx=5, pady=5)\n    patch_ui(frm_main, base_dir, log_widget, app_state)\n    root.mainloop()\nif __name__ == '__main__':\n    gui_main()"
    },
    {
      "path": "your_project\\aum.py.py",
      "overview": "Pythonコード。\nクラス: AUMAppState, ImportCollector, Mapper。\n関数: get_file_lock, with_file_lock, retry_operation, setup_logging, log_info, log_error, with_io_throttling, read_file_content, write_file_content, generate_summary, detect_symbol_collisions, auto_rename_collisions_in_name_map, auto_repair_collected_scripts, merge_collected_scripts, _merge_system_and_settings, _merge_single_script, ensure_init_py_for_python_dirs, restore_settings_from_config_json, collect, collect_python_symbols_for_map, generate_short_symbol_name, ensure_name_map_csv_exists, load_name_mappings, invert_name_map, apply_name_mappings_ast, analyze_dependencies_and_sort, build_in_topological_order, verify_build_results, _build_config_json, build_project, finalize_build, token_saving_build_unified, restore_build, run_main_py, gather_run_artifacts, generate_summary_report, apply_code_patch, apply_system_patch, patch_ui, clean, enhanced_smart_paste, patch_ui_placeholder, gui_main, __init__, wrapper, decorator, wrapper, extract_problematic_identifier, extract_imports, on_code_edit, on_apply_patch, on_system_patch, on_collect, on_token_saving, on_restore, on_clean, on_run, on_paste, wrapper, __init__, visit_Import, visit_ImportFrom, __init__, visit_Name, visit_FunctionDef, visit_ClassDef, visit_Attribute, visit_Import, visit_ImportFrom。\n",
      "content": "import os\nimport sys\nimport csv\nimport json\nimport ast\nimport shutil\nimport logging\nimport re\nimport datetime\nimport tkinter as tk\nfrom tkinter import messagebox, simpledialog, filedialog\nfrom tkinter.scrolledtext import ScrolledText\nfrom typing import Any, Dict, List, Optional, Tuple, Set\nfrom collections import defaultdict, deque\nimport subprocess\nimport threading\nimport functools\nimport time\nimport psutil\nLOG_LEVEL = 'ERROR'\nDATA_FOLDER_NAME = 'data'\nMAX_REBUILD_ATTEMPTS = 3\nTOKEN_SAVING_EXCLUDE_LIST = {'ConfigManager', 'CsvPlatformFeatureExtractor', 'JsonPlatformFeatureExtractor', 'PlatformSampleManager', 'PlatformIdentifier'}\nABSOLUTE_EXCLUDE_MODULES = {'utils/file_identifier.py', 'parsers/base_parser.py'}\nWRITE_RATE_THRESHOLD = 10 * 1024 * 1024\nTHROTTLE_SLEEP_TIME = 0.5\n\nclass AUMAppState:\n\n    def __init__(self):\n        self.last_build_folder: Optional[str] = None\n_file_lock_dict: Dict[str, threading.Lock] = {}\n_lock_dict_lock = threading.Lock()\n\ndef get_file_lock(file_path: str) -> threading.Lock:\n    with _lock_dict_lock:\n        if file_path not in _file_lock_dict:\n            _file_lock_dict[file_path] = threading.Lock()\n        return _file_lock_dict[file_path]\n\ndef with_file_lock(func):\n\n    @functools.wraps(func)\n    def wrapper(file_path, *args, **kwargs):\n        lock = get_file_lock(file_path)\n        with lock:\n            return func(file_path, *args, **kwargs)\n    return wrapper\n\ndef retry_operation(max_attempts: int=3):\n\n    def decorator(func):\n\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            attempt = 0\n            while attempt < max_attempts:\n                try:\n                    return func(*args, **kwargs)\n                except Exception as e:\n                    attempt += 1\n                    logging.error(f'{func.__name__} attempt {attempt} failed: {e}')\n                    if attempt >= max_attempts:\n                        raise\n        return wrapper\n    return decorator\n\ndef setup_logging(level='ERROR'):\n    logging.basicConfig(filename='aum_error.log', level=getattr(logging, level.upper(), logging.ERROR), format='%(asctime)s:%(levelname)s:%(message)s', encoding='utf-8')\nsetup_logging(level=LOG_LEVEL)\n\ndef log_info(widget: Optional[tk.Text], msg: str):\n    logging.info(msg)\n    if widget:\n        widget.insert('end', msg + '\\n')\n        widget.see('end')\n\ndef log_error(widget: Optional[tk.Text], msg: str):\n    logging.error(msg)\n    if widget:\n        widget.insert('end', '[ERROR] ' + msg + '\\n')\n        widget.see('end')\n\ndef with_io_throttling(func):\n\n    @functools.wraps(func)\n    def wrapper(file_path, *args, **kwargs):\n        io_before = psutil.disk_io_counters().write_bytes\n        time.sleep(THROTTLE_SLEEP_TIME)\n        io_after = psutil.disk_io_counters().write_bytes\n        write_rate = (io_after - io_before) / THROTTLE_SLEEP_TIME\n        if write_rate > WRITE_RATE_THRESHOLD:\n            logging.info(f'High disk write rate detected: {write_rate / (1024 * 1024):.2f} MB/s. Throttling...')\n            while write_rate > WRITE_RATE_THRESHOLD:\n                time.sleep(THROTTLE_SLEEP_TIME)\n                io_before = psutil.disk_io_counters().write_bytes\n                time.sleep(THROTTLE_SLEEP_TIME)\n                io_after = psutil.disk_io_counters().write_bytes\n                write_rate = (io_after - io_before) / THROTTLE_SLEEP_TIME\n                logging.info(f'Throttling... current write rate: {write_rate / (1024 * 1024):.2f} MB/s')\n        return func(file_path, *args, **kwargs)\n    return wrapper\n\n@with_file_lock\n@with_io_throttling\ndef read_file_content(file_path: str, log_widget=None) -> str:\n    try:\n        with open(file_path, 'r', encoding='utf-8') as fr:\n            return fr.read()\n    except Exception as e:\n        log_error(log_widget, f'ファイル読込失敗: {file_path}: {e}')\n    return ''\n\n@with_file_lock\n@with_io_throttling\ndef write_file_content(file_path: str, content: str, log_widget=None):\n    try:\n        os.makedirs(os.path.dirname(file_path), exist_ok=True)\n        with open(file_path, 'w', encoding='utf-8') as fw:\n            fw.write(content)\n    except Exception as e:\n        log_error(log_widget, f'ファイル書込み失敗: {file_path}: {e}')\n\ndef generate_summary(file_path: str, content: str) -> str:\n    if file_path.endswith('.py'):\n        try:\n            tree = ast.parse(content)\n            funcs = [n.name for n in ast.walk(tree) if isinstance(n, ast.FunctionDef)]\n            clss = [n.name for n in ast.walk(tree) if isinstance(n, ast.ClassDef)]\n            docstrings = ast.get_docstring(tree)\n            s = 'Pythonコード。\\n'\n            if clss:\n                s += f\"クラス: {', '.join(clss)}。\\n\"\n            if funcs:\n                s += f\"関数: {', '.join(funcs)}。\\n\"\n            if docstrings:\n                s += f'Docstring(冒頭60文字): {docstrings[:60]}...\\n'\n            return s\n        except Exception as e:\n            logging.error(f'AST解析失敗: {file_path}: {e}', exc_info=True)\n            return f'解析エラー: {e}\\n'\n    elif file_path.endswith('.ipynb'):\n        try:\n            nb = json.loads(content)\n            cells = nb.get('cells', [])\n            m = sum((1 for c in cells if c.get('cell_type') == 'markdown'))\n            c = sum((1 for c in cells if c.get('cell_type') == 'code'))\n            return f'Jupyter Notebook。\\nセル合計={len(cells)}, Markdown={m}, Code={c}\\n'\n        except Exception as e:\n            return f'Jupyter Notebook。\\nNotebook解析エラー: {e}\\n'\n    elif file_path.endswith('.json'):\n        try:\n            obj = json.loads(content)\n            if isinstance(obj, dict):\n                keys = list(obj.keys())\n                keys_display = ', '.join(keys[:10])\n                if len(keys) > 10:\n                    keys_display += '...'\n                return f'JSONファイル (辞書)。キー: {keys_display}\\n'\n            elif isinstance(obj, list):\n                return f'JSONファイル (リスト)。要素数: {len(obj)}\\n'\n            else:\n                return 'JSONファイル (その他形式)。\\n'\n        except Exception as e:\n            return f'JSON解析エラー: {e}\\n'\n    elif file_path.endswith('.tex'):\n        lines = content.count('\\n')\n        return f'TeXファイル。行数: {lines}\\n'\n    else:\n        lines = content.count('\\n')\n        return f'その他テキスト。\\n行数: {lines}\\n'\n\ndef detect_symbol_collisions(mapping: Dict[str, str]) -> set:\n    from collections import defaultdict\n    rev = defaultdict(list)\n    for (orig, short_) in mapping.items():\n        rev[short_].append(orig)\n    return {k for (k, v) in rev.items() if len(v) > 1}\n\ndef auto_rename_collisions_in_name_map(csv_path: str, log_widget=None) -> bool:\n    if not os.path.exists(csv_path):\n        return False\n    rows = []\n    try:\n        with open(csv_path, 'r', encoding='utf-8') as f:\n            rd = csv.DictReader(f)\n            for r in rd:\n                rows.append(r)\n    except Exception as e:\n        log_error(log_widget, f'name_map.csv 読込失敗: {e}')\n        return False\n    mapping = {r['original_name']: r['short_name'] for r in rows}\n    collisions = detect_symbol_collisions(mapping)\n    if not collisions:\n        return False\n    short_name_counter = {}\n    renamed_count = 0\n    for r in rows:\n        sh = r['short_name']\n        if sh in collisions:\n            base = sh\n            short_name_counter.setdefault(base, 0)\n            short_name_counter[base] += 1\n            r['short_name'] = base + str(short_name_counter[base])\n            renamed_count += 1\n    if renamed_count:\n        try:\n            with open(csv_path, 'w', encoding='utf-8', newline='') as fw:\n                w = csv.writer(fw)\n                w.writerow(['original_name', 'short_name'])\n                for row in rows:\n                    w.writerow([row['original_name'], row['short_name']])\n            log_info(log_widget, f'name_map.csv 衝突を {renamed_count} 件リネーム修正')\n            return True\n        except Exception as e:\n            log_error(log_widget, f'name_map.csv 修正書込失敗: {e}')\n    return False\n\ndef auto_repair_collected_scripts(base_dir: str, log_widget) -> None:\n    cjson_path = os.path.join(base_dir, 'collected_scripts.json')\n    if not os.path.exists(cjson_path):\n        return\n    try:\n        with open(cjson_path, 'r', encoding='utf-8') as fr:\n            content = fr.read().strip()\n            if not content:\n                raise ValueError('Empty file')\n            data = json.loads(content)\n        if not isinstance(data, dict):\n            raise ValueError('トップレベルがdictではない')\n        if 'scripts' in data and (not isinstance(data['scripts'], list)):\n            raise ValueError(\"'scripts'がlistではない\")\n    except Exception as e:\n        log_error(log_widget, f'collected_scripts.json 破損: {e}')\n        backup = cjson_path + '.bak'\n        try:\n            shutil.move(cjson_path, backup)\n            log_info(log_widget, f'破損ファイルをバックアップ→ {backup}')\n        except Exception as e_backup:\n            log_error(log_widget, f'バックアップ失敗: {e_backup}')\n        data = {'system_overview': 'このシステムは...（必要に応じて書き換え）', 'settings': {}, 'scripts': []}\n        try:\n            with open(cjson_path, 'w', encoding='utf-8') as fw:\n                json.dump(data, fw, ensure_ascii=False, indent=2)\n            log_info(log_widget, 'collected_scripts.json を新規作成')\n        except Exception as e_write:\n            log_error(log_widget, f'新規作成失敗: {e_write}')\n\ndef merge_collected_scripts(original: dict, new: dict, file_data: Dict[str, dict], log_widget) -> dict:\n    _merge_system_and_settings(original, new)\n    old_scripts = original.get('scripts', [])\n    old_map = {s['path']: s for s in old_scripts if 'path' in s}\n    new_scripts = new.get('scripts', [])\n    new_map = {s.get('path'): s for s in new_scripts if s.get('path')}\n    for (p, new_sc) in new_map.items():\n        if p not in old_map:\n            old_map[p] = new_sc\n            log_info(log_widget, f'新規追加: {p}')\n        else:\n            old_sc = old_map[p]\n            _merge_single_script(old_sc, new_sc, file_data, p, log_widget)\n    merged_list = list(old_map.values())\n    original['scripts'] = merged_list\n    return original\n\ndef _merge_system_and_settings(old: dict, new: dict):\n    if new.get('system_overview'):\n        old['system_overview'] = new['system_overview']\n    if 'settings' in new and isinstance(new['settings'], dict):\n        old.setdefault({}).update(new['settings'])\n\ndef _merge_single_script(old_sc: dict, new_sc: dict, file_data: Dict[str, dict], p: str, log_widget):\n    old_cont = old_sc.get('content', '')\n    new_cont = new_sc.get('content', '')\n    disk_cont = file_data.get(p, {}).get('content', '')\n    conflict = old_cont != disk_cont and new_cont != disk_cont and (old_cont != new_cont)\n    if conflict:\n        log_info(log_widget, f'衝突あり: {p}')\n    else:\n        for (k, v) in new_sc.items():\n            if k != 'path':\n                old_sc[k] = v\n        log_info(log_widget, f'上書き: {p}')\n\ndef ensure_init_py_for_python_dirs(base_dir: str, log_widget):\n    exclude = {'venv', '__pycache__', '.git', 'outputs', 'temp_build', 'backups', DATA_FOLDER_NAME}\n    py_dirs = set()\n    for (root, dirs, files) in os.walk(base_dir):\n        dirs[:] = [d for d in dirs if d not in exclude]\n        if any((f.endswith('.py') for f in files)):\n            py_dirs.add(root)\n    for d in py_dirs:\n        init_file = os.path.join(d, '__init__.py')\n        if not os.path.isfile(init_file):\n            write_file_content(init_file, '', log_widget)\n            log_info(log_widget, f'__init__.py 自動生成: {os.path.relpath(init_file, base_dir)}')\n\ndef restore_settings_from_config_json(base_dir: str, log_widget):\n    \"\"\"\n    your_project直下または your_project/your_project にある config.json を探し、\n    設定情報を collected_scripts.json の settings に反映する。\n    ※設定ファイル内に \"settings\" キーがあれば、その中身を使用（なければファイル全体を設定として取り込む）\n    \"\"\"\n    possible_cfg_paths = [os.path.join(base_dir, 'your_project', 'config.json'), os.path.join(base_dir, 'your_project', 'your_project', 'config.json')]\n    cfg_path = None\n    for pth in possible_cfg_paths:\n        if os.path.isfile(pth):\n            cfg_path = pth\n            break\n    if not cfg_path:\n        log_info(log_widget, 'config.jsonが見つからないためスキップ')\n        return\n    cjson_path = os.path.join(base_dir, 'collected_scripts.json')\n    if not os.path.isfile(cjson_path):\n        log_info(log_widget, f'collected_scripts.jsonが無い: {cjson_path}')\n        return\n    try:\n        with open(cfg_path, 'r', encoding='utf-8') as f:\n            cfg_data = json.load(f)\n    except Exception as e:\n        log_error(log_widget, f'config.json 読込失敗: {e}')\n        return\n    if not isinstance(cfg_data, dict):\n        log_info(log_widget, 'config.jsonの形式が不正です。')\n        return\n    new_settings = cfg_data.get(cfg_data)\n    try:\n        with open(cjson_path, 'r', encoding='utf-8') as fr:\n            cjson_data = json.load(fr)\n        if not isinstance(cjson_data, dict):\n            cjson_data = {'system_overview': '', 'settings': {}, 'scripts': []}\n    except Exception as e:\n        log_error(log_widget, f'collected_scripts.json 読込失敗: {e}')\n        return\n    cjson_data.setdefault({}).update(new_settings)\n    try:\n        with open(cjson_path, 'w', encoding='utf-8') as fw:\n            json.dump(cjson_data, fw, ensure_ascii=False, indent=2)\n        log_info(log_widget, f'{cjson_path} の settings を更新しました')\n    except Exception as e:\n        log_error(log_widget, f'config→collected_scripts 反映失敗: {e}')\n\ndef collect(base_dir: str, log_widget):\n    log_info(log_widget, 'コード収集を開始...')\n    auto_repair_collected_scripts(base_dir, log_widget)\n    output_folder = os.path.join(base_dir, 'your_project')\n    cjson = os.path.join(base_dir, 'collected_scripts.json')\n    if os.path.isfile(cjson):\n        try:\n            with open(cjson, 'r', encoding='utf-8') as fr:\n                old_data = json.load(fr)\n            if not isinstance(old_data, dict):\n                old_data = {'system_overview': '', 'settings': {}, 'scripts': []}\n        except Exception as e:\n            log_error(log_widget, f'collected_scripts.json 読込失敗: {e}')\n            old_data = {'system_overview': '', 'settings': {}, 'scripts': []}\n    else:\n        old_data = {'system_overview': '', 'settings': {}, 'scripts': []}\n    ensure_init_py_for_python_dirs(base_dir, log_widget)\n    target_ext = ('.py', '.json', '.tex')\n    exclude_dirs = {'venv', '__pycache__', '.git', 'outputs', 'temp_build', 'backups', DATA_FOLDER_NAME}\n    exclude_files = {'collected_scripts.json', 'config.json', os.path.basename(__file__)}\n    new_list = []\n    for (root, dirs, files) in os.walk(base_dir):\n        abs_root = os.path.abspath(root)\n        abs_output = os.path.abspath(output_folder)\n        if abs_root.startswith(abs_output) and 'your_project' not in os.path.relpath(abs_root, base_dir).split(os.sep)[0]:\n            continue\n        dirs[:] = [d for d in dirs if d not in exclude_dirs]\n        for fn in files:\n            if fn in exclude_files:\n                continue\n            if any((fn.endswith(e) for e in target_ext)):\n                fp = os.path.join(root, fn)\n                if fp == os.path.abspath(__file__):\n                    continue\n                content = read_file_content(fp, log_widget)\n                if not content:\n                    continue\n                relp = os.path.relpath(fp, base_dir)\n                summ = generate_summary(fp, content)\n                new_list.append({'path': relp, 'overview': summ, 'content': content})\n                log_info(log_widget, f'収集: {relp}')\n    file_map = {s['path']: {'content': s['content'], 'overview': s['overview']} for s in new_list}\n    merged = merge_collected_scripts(old_data, {'system_overview': old_data.get('system_overview', ''), 'settings': old_data.get({}), 'scripts': new_list}, file_map, log_widget)\n    try:\n        with open(cjson, 'w', encoding='utf-8') as fw:\n            json.dump(merged, fw, ensure_ascii=False, indent=2)\n        log_info(log_widget, f'出力完了: {cjson}')\n    except Exception as e:\n        log_error(log_widget, f'書き込み失敗: {e}')\n    restore_settings_from_config_json(base_dir, log_widget)\n\ndef collect_python_symbols_for_map(base_dir: str) -> set:\n    import ast\n    exclude = {'venv', '__pycache__', '.git', 'temp_build', 'backups', DATA_FOLDER_NAME}\n    found = set()\n    for (root, dirs, files) in os.walk(base_dir):\n        if os.path.basename(root).lower() == DATA_FOLDER_NAME.lower():\n            continue\n        abs_root = os.path.abspath(root)\n        abs_output = os.path.abspath(os.path.join(base_dir, 'your_project'))\n        if abs_root.startswith(abs_output):\n            continue\n        dirs[:] = [d for d in dirs if d not in exclude]\n        for f in files:\n            if f.endswith('.py') and f != os.path.basename(__file__):\n                fp = os.path.join(root, f)\n                txt = read_file_content(fp, log_widget=None)\n                if not txt:\n                    continue\n                try:\n                    tree = ast.parse(txt)\n                    for node in ast.walk(tree):\n                        if isinstance(node, ast.FunctionDef):\n                            found.add(node.name)\n                        elif isinstance(node, ast.ClassDef):\n                            found.add(node.name)\n                except Exception as e:\n                    logging.error(f'AST解析失敗: {fp}: {e}', exc_info=True)\n    return found\n\ndef generate_short_symbol_name(orig_name: str) -> str:\n    if not orig_name:\n        return 'x'\n    return orig_name[0].lower() + '_sym'\n\ndef ensure_name_map_csv_exists(base_dir: str) -> str:\n    csv_path = os.path.join(base_dir, 'name_map.csv')\n    syms = collect_python_symbols_for_map(base_dir)\n    existing_map = {}\n    if os.path.isfile(csv_path):\n        try:\n            with open(csv_path, 'r', encoding='utf-8') as fr:\n                rd = csv.DictReader(fr)\n                for r in rd:\n                    o = r['original_name']\n                    s = r['short_name']\n                    existing_map[o] = s\n        except Exception as e:\n            logging.error(f'name_map.csv 読込失敗: {e}')\n    else:\n        logging.info('name_map.csvがないため新規作成')\n    for s in syms:\n        if s in TOKEN_SAVING_EXCLUDE_LIST:\n            existing_map[s] = s\n        elif s not in existing_map:\n            existing_map[s] = generate_short_symbol_name(s)\n    with open(csv_path, 'w', encoding='utf-8', newline='') as fw:\n        w = csv.writer(fw)\n        w.writerow(['original_name', 'short_name'])\n        for (k, v) in sorted(existing_map.items()):\n            w.writerow([k, v])\n    return csv_path\n\ndef load_name_mappings(csv_path: str) -> dict:\n    mp = {}\n    if not os.path.isfile(csv_path):\n        return mp\n    try:\n        with open(csv_path, 'r', encoding='utf-8') as fr:\n            rd = csv.DictReader(fr)\n            for r in rd:\n                mp[r['original_name']] = r['short_name']\n    except Exception as e:\n        logging.error(f'name_map.csv 読込失敗: {e}')\n    return mp\n\ndef invert_name_map(mp: dict) -> dict:\n    return {v: k for (k, v) in mp.items()}\nGLOBAL_EXCLUDE_MODULES = ABSOLUTE_EXCLUDE_MODULES\n\ndef apply_name_mappings_ast(content: str, name_map: dict, path_for_check: str) -> str:\n    norm_path = os.path.normpath(path_for_check).replace('\\\\', '/')\n    for exc_subpath in GLOBAL_EXCLUDE_MODULES:\n        if exc_subpath in norm_path:\n            return content\n    FIXED_EXCLUDE = TOKEN_SAVING_EXCLUDE_LIST.copy()\n    current_exclude = set()\n    attempt = 0\n\n    def extract_problematic_identifier(e: Exception) -> Optional[str]:\n        msg = str(e)\n        m = re.search(\"object has no attribute '(\\\\w+)'\", msg)\n        if m:\n            return m.group(1)\n        m = re.search(\"cannot import name '(\\\\w+)'\", msg)\n        if m:\n            return m.group(1)\n        return None\n\n    class ImportCollector(ast.NodeVisitor):\n\n        def __init__(self):\n            self.imported = set()\n\n        def visit_Import(self, node):\n            for alias in node.names:\n                name = alias.asname if alias.asname else alias.name.split('.')[0]\n                self.imported.add(name)\n            self.generic_visit(node)\n\n        def visit_ImportFrom(self, node):\n            if node.module:\n                self.imported.add(node.module.split('.')[0])\n            for alias in node.names:\n                name = alias.asname if alias.asname else alias.name\n                self.imported.add(name)\n            self.generic_visit(node)\n\n    class Mapper(ast.NodeTransformer):\n\n        def __init__(self, exclude_set, nmap):\n            self.exclude = exclude_set\n            self.nmap = nmap\n            super().__init__()\n\n        def visit_Name(self, node):\n            if node.id.startswith('__') and node.id.endswith('__'):\n                return node\n            if node.id in self.exclude:\n                return node\n            if node.id in self.nmap:\n                node.id = self.nmap[node.id]\n            return self.generic_visit(node)\n\n        def visit_FunctionDef(self, node):\n            if not (node.name.startswith('__') and node.name.endswith('__')):\n                if node.name not in self.exclude and node.name in self.nmap:\n                    node.name = self.nmap[node.name]\n            self.generic_visit(node)\n            return node\n\n        def visit_ClassDef(self, node):\n            if not (node.name.startswith('__') and node.name.endswith('__')):\n                if node.name not in self.exclude and node.name in self.nmap:\n                    node.name = self.nmap[node.name]\n            self.generic_visit(node)\n            return node\n\n        def visit_Attribute(self, node):\n            self.generic_visit(node)\n            if isinstance(node.value, ast.Name) and node.value.id == 'self':\n                return node\n            if node.attr.startswith('__') and node.attr.endswith('__'):\n                return node\n            if node.attr in self.exclude:\n                return node\n            if node.attr in self.nmap:\n                node.attr = self.nmap[node.attr]\n            return node\n\n        def visit_Import(self, node):\n            for alias in node.names:\n                if alias.name in self.exclude:\n                    continue\n                if alias.name in self.nmap:\n                    alias.name = self.nmap[alias.name]\n                if alias.asname and alias.asname in self.nmap:\n                    alias.asname = self.nmap[alias.asname]\n            return node\n\n        def visit_ImportFrom(self, node):\n            if node.module in self.exclude:\n                return node\n            if node.module and node.module in self.nmap:\n                node.module = self.nmap[node.module]\n            for alias in node.names:\n                if alias.name in self.exclude:\n                    continue\n                if alias.name in self.nmap:\n                    alias.name = self.nmap[alias.name]\n                if alias.asname and alias.asname in self.nmap:\n                    alias.asname = self.nmap[alias.asname]\n            return node\n    while True:\n        attempt += 1\n        try:\n            tree = ast.parse(content)\n            collector = ImportCollector()\n            collector.visit(tree)\n            auto_exclude = collector.imported\n            total_exclude = FIXED_EXCLUDE.union(auto_exclude).union(current_exclude)\n            mapper = Mapper(total_exclude, name_map)\n            new_tree = mapper.visit(tree)\n            ast.fix_missing_locations(new_tree)\n            if hasattr(ast, 'unparse'):\n                out_code = ast.unparse(new_tree)\n            else:\n                import astor\n                out_code = astor.to_source(new_tree)\n            return out_code\n        except Exception as e:\n            problematic = extract_problematic_identifier(e)\n            if problematic:\n                if problematic not in current_exclude:\n                    current_exclude.add(problematic)\n                    continue\n                else:\n                    return content\n            else:\n                return content\n\ndef analyze_dependencies_and_sort(scripts: List[Dict[str, str]], log_widget=None) -> List[Dict[str, str]]:\n    graph = {}\n    script_map = {}\n    for sc in scripts:\n        p = sc['path']\n        script_map[p] = sc\n        graph[p] = set()\n    path_by_modname = {}\n    for sc in scripts:\n        p = sc['path'].replace('\\\\', '/')\n        if p.endswith('.py'):\n            mod_parts = p.split('/')\n            if mod_parts[0] == 'your_project':\n                mod_parts = mod_parts[1:]\n            py_base = mod_parts[-1][:-3]\n            mod_parts[-1] = py_base\n            dotted = '.'.join(mod_parts)\n            path_by_modname[dotted] = p\n\n    def extract_imports(p: str, code: str) -> Set[str]:\n        import_list = set()\n        try:\n            tree = ast.parse(code)\n            for node in ast.walk(tree):\n                if isinstance(node, ast.Import):\n                    for alias in node.names:\n                        mod = alias.name.split('.')[0]\n                        import_list.add(mod)\n                elif isinstance(node, ast.ImportFrom):\n                    if node.module:\n                        base = node.module.split('.')[0]\n                        import_list.add(base)\n        except Exception as e:\n            logging.error(f'依存関係解析: {p} の import抽出失敗: {e}', exc_info=True)\n            log_error(log_widget, f'依存関係解析: {p} の import抽出失敗: {e}')\n        return import_list\n    for sc in scripts:\n        p = sc['path']\n        code = sc['content']\n        imported = extract_imports(p, code)\n        for imp in imported:\n            for (k, v) in path_by_modname.items():\n                if k.startswith(imp):\n                    graph[p].add(v)\n    in_degree = {p: 0 for p in graph}\n    for p in graph:\n        for dep in graph[p]:\n            if dep in in_degree:\n                in_degree[dep] += 1\n    queue = deque([p for p in in_degree if in_degree[p] == 0])\n    sorted_paths = []\n    while queue:\n        u = queue.popleft()\n        sorted_paths.append(u)\n        for dep in graph[u]:\n            if dep in in_degree:\n                in_degree[dep] -= 1\n                if in_degree[dep] == 0:\n                    queue.append(dep)\n    if len(sorted_paths) < len(graph):\n        logging.error('循環依存関係が検出されました。依存順序の保証ができません。')\n        log_error(log_widget, '循環依存関係が検出されました。依存順序の保証ができません。')\n        return scripts\n    path_to_index = {p: i for (i, p) in enumerate(sorted_paths)}\n    scripts_sorted = sorted(scripts, key=lambda sc: path_to_index.get(sc['path'], 999999))\n    return scripts_sorted\n\ndef build_in_topological_order(scripts: List[Dict[str, str]], out_dir: str, name_map: Dict[str, str], log_widget) -> Tuple[int, List[str]]:\n    sorted_scripts = analyze_dependencies_and_sort(scripts, log_widget)\n    logs = []\n    built_count = 0\n    for sc in sorted_scripts:\n        path_ = sc['path']\n        content_ = sc['content']\n        if path_.endswith('.py'):\n            new_content = apply_name_mappings_ast(content_, name_map, path_for_check=path_)\n        elif path_.endswith('.json') or path_.endswith('.tex'):\n            new_content = content_\n        else:\n            continue\n        rel_path = path_\n        if rel_path.startswith('your_project' + os.path.sep):\n            rel_path = rel_path[len('your_project' + os.path.sep):]\n        target_file = os.path.join(out_dir, rel_path)\n        try:\n            os.makedirs(os.path.dirname(target_file), exist_ok=True)\n            with open(target_file, 'w', encoding='utf-8') as fw:\n                fw.write(new_content)\n            logs.append(f'出力: {os.path.relpath(target_file, out_dir)}')\n            built_count += 1\n        except Exception as e:\n            logs.append(f'書き込みエラー: {target_file}: {e}')\n    return (built_count, logs)\n\ndef verify_build_results(build_dir: str, collected_json: str, log_widget) -> bool:\n    try:\n        with open(collected_json, 'r', encoding='utf-8') as fr:\n            data = json.load(fr)\n    except Exception as e:\n        log_error(log_widget, f'収集結果の読込失敗: {e}')\n        return False\n    scripts = data.get('scripts', [])\n    all_ok = True\n    for script in scripts:\n        p = script.get('path', '')\n        if not (p.endswith('.py') or p.endswith('.json') or p.endswith('.tex')):\n            continue\n        rel_path = p\n        if rel_path.startswith('your_project' + os.path.sep):\n            rel_path = rel_path[len('your_project' + os.path.sep):]\n        build_file = os.path.join(build_dir, rel_path)\n        if not os.path.exists(build_file):\n            log_error(log_widget, f'ビルド結果に存在しない: {build_file}')\n            all_ok = False\n            continue\n        with open(build_file, 'r', encoding='utf-8') as bf:\n            build_content = bf.read()\n        if build_content.strip() != script.get('content', '').strip():\n            log_error(log_widget, f'内容不一致: {build_file}')\n            all_ok = False\n        else:\n            log_info(log_widget, f'チェックOK: {build_file}')\n    return all_ok\n\ndef _build_config_json(settings: dict, out_dir: str, log_widget):\n    cfg_path = os.path.join(out_dir, 'config.json')\n    try:\n        os.makedirs(os.path.dirname(cfg_path), exist_ok=True)\n        with open(cfg_path, 'w', encoding='utf-8') as fw:\n            json.dump(settings, fw, ensure_ascii=False, indent=2)\n        log_info(log_widget, f'config.json生成: {cfg_path}')\n    except Exception as e:\n        log_error(log_widget, f'config.json 書込失敗: {e}')\n\ndef build_project(base_dir: str, log_widget, is_obfuscate: bool=True) -> Optional[str]:\n    auto_repair_collected_scripts(base_dir, log_widget)\n    cjson = os.path.join(base_dir, 'collected_scripts.json')\n    if not os.path.isfile(cjson):\n        messagebox.showerror('エラー', f'{cjson} が見つかりません')\n        return None\n    csv_path = ensure_name_map_csv_exists(base_dir)\n    direct_map = load_name_mappings(csv_path)\n    final_map = direct_map if is_obfuscate else invert_name_map(direct_map)\n    tmp_build_root = os.path.join(base_dir, 'temp_build')\n    os.makedirs(tmp_build_root, exist_ok=True)\n    out_dir = os.path.join(tmp_build_root, 'your_project_temp')\n    if os.path.exists(out_dir):\n        shutil.rmtree(out_dir)\n    os.makedirs(out_dir, exist_ok=True)\n    log_info(log_widget, f'一時出力先: {out_dir}')\n    try:\n        with open(cjson, 'r', encoding='utf-8') as fr:\n            data = json.load(fr)\n    except Exception as e:\n        log_error(log_widget, f'collected_scripts.json 読込失敗: {e}')\n        return None\n    scripts = data.get('scripts', [])\n    settings = data.get({})\n    py_scripts = [s for s in scripts if s.get('path', '').endswith(('.py', '.json', '.tex'))]\n    (built_count, logs_texts) = build_in_topological_order(py_scripts, out_dir, final_map, log_widget)\n    _build_config_json(settings, out_dir, log_widget)\n    built_count += 1\n    if logs_texts:\n        for l in logs_texts:\n            log_info(log_widget, l)\n    log_info(log_widget, f'ビルド完了: {built_count}ファイル')\n    if not verify_build_results(out_dir, cjson, log_widget):\n        log_error(log_widget, '収集結果とビルド結果が一致しません')\n    else:\n        log_info(log_widget, 'ビルド結果と収集結果は一致しています')\n    restore_settings_from_config_json(base_dir, log_widget)\n    return out_dir\n\ndef finalize_build(temp_out_dir: str, base_dir: str, log_widget, app_state: AUMAppState) -> bool:\n    if not temp_out_dir or not os.path.isdir(temp_out_dir):\n        log_error(log_widget, f'一時フォルダが無い: {temp_out_dir}')\n        return False\n    final_dir = os.path.join(base_dir, 'your_project')\n    if os.path.isdir(final_dir):\n        backups_dir = os.path.join(base_dir, 'backups')\n        os.makedirs(backups_dir, exist_ok=True)\n        stamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n        zip_name = os.path.join(backups_dir, f'your_project_old_{stamp}')\n        try:\n            shutil.make_archive(zip_name, 'zip', final_dir)\n            log_info(log_widget, f'既存フォルダをzipバックアップ: {zip_name}.zip')\n        except Exception as e:\n            log_error(log_widget, f'バックアップ失敗: {e}')\n        try:\n            shutil.rmtree(final_dir)\n            log_info(log_widget, f'既存フォルダ削除: {final_dir}')\n        except Exception as e:\n            log_error(log_widget, f'削除失敗: {e}')\n            return False\n    try:\n        shutil.move(temp_out_dir, final_dir)\n        log_info(log_widget, f'上書き完了: {final_dir}')\n        app_state.last_build_folder = base_dir\n    except Exception as e:\n        log_error(log_widget, f'移動失敗: {e}')\n        return False\n    tmpb = os.path.join(base_dir, 'temp_build')\n    if os.path.isdir(tmpb):\n        try:\n            shutil.rmtree(tmpb)\n            log_info(log_widget, f'temp_build削除: {tmpb}')\n        except Exception as ex:\n            log_error(log_widget, f'temp_build削除失敗: {ex}')\n    log_info(log_widget, f'last_build_folder={app_state.last_build_folder}')\n    return True\n\ndef token_saving_build_unified(base_dir: str, log_widget, app_state: AUMAppState):\n    tries = 0\n    while True:\n        tries += 1\n        log_info(log_widget, f'ビルド試行 {tries} 回目')\n        tmp_out = None\n        try:\n            tmp_out = build_project(base_dir, log_widget, is_obfuscate=True)\n        except Exception as e:\n            log_error(log_widget, f'ビルド処理で例外発生: {e}')\n            break\n        if not tmp_out:\n            log_error(log_widget, 'build_project で出力が得られませんでした。')\n            if tries >= MAX_REBUILD_ATTEMPTS:\n                log_error(log_widget, 'ビルドを中断します。')\n                break\n            continue\n        result = finalize_build(tmp_out, base_dir, log_widget, app_state)\n        if not result:\n            log_error(log_widget, 'ビルドの適用に失敗しました。処理を中断します。')\n            break\n        run_main_py(base_dir, log_widget, app_state)\n        all_logs = log_widget.get('1.0', 'end')\n        if 'ImportError:' in all_logs or 'NameError:' in all_logs or 'ModuleNotFoundError:' in all_logs:\n            log_info(log_widget, 'エラー検知（名前衝突等）。name_map.csv を修正して再試行します。')\n            csvp = os.path.join(base_dir, 'name_map.csv')\n            fixed = auto_rename_collisions_in_name_map(csvp, log_widget)\n            if fixed:\n                continue\n            else:\n                log_error(log_widget, 'name_map.csv の自動修正に失敗しました。再試行します。')\n                continue\n        else:\n            log_info(log_widget, 'ビルド成功：エラーなし。')\n            break\n\ndef restore_build(base_dir: str, log_widget, app_state: AUMAppState):\n    try:\n        tmp_out = build_project(base_dir, log_widget, is_obfuscate=False)\n    except Exception as e:\n        log_error(log_widget, f'ビルド処理で例外発生: {e}')\n        return\n    if tmp_out:\n        finalize_build(tmp_out, base_dir, log_widget, app_state)\n\ndef run_main_py(base_dir: str, log_widget, app_state: AUMAppState):\n    final_dir = os.path.join(base_dir, 'your_project')\n    main_py = os.path.join(final_dir, 'main.py')\n    data_path = os.path.join(final_dir, DATA_FOLDER_NAME)\n    log_info(log_widget, f'main.py => {main_py}')\n    log_info(log_widget, f'--input => {data_path}')\n    if not os.path.isfile(main_py):\n        log_error(log_widget, 'main.py が見つからない')\n        return\n    if not os.path.isdir(data_path):\n        log_info(log_widget, f'dataフォルダなし: {data_path}')\n    cmd = [sys.executable, main_py, '--input', data_path, '--skip-parent-dir']\n    try:\n        ret = subprocess.run(cmd, capture_output=True, text=True)\n        if ret.returncode != 0:\n            log_error(log_widget, f'エラー({ret.returncode}): {ret.stderr}')\n        else:\n            log_info(log_widget, f'実行完了:\\n{ret.stdout}')\n    except Exception as e:\n        log_error(log_widget, f'例外: {e}')\n    gather_run_artifacts(base_dir, log_widget, app_state)\n    generate_summary_report(base_dir, log_widget)\n\ndef gather_run_artifacts(base_dir: str, log_widget, app_state: AUMAppState):\n    final_dir = os.path.join(base_dir, 'your_project')\n    if not os.path.isdir(final_dir):\n        log_info(log_widget, 'your_projectがない')\n        return\n    runs = []\n    for d in os.listdir(final_dir):\n        if d.startswith('run_'):\n            fp = os.path.join(final_dir, d)\n            if os.path.isdir(fp):\n                runs.append(fp)\n    if not runs:\n        log_info(log_widget, 'run_*** フォルダなし')\n        return\n    runs.sort(key=lambda x: os.path.getmtime(x), reverse=True)\n    latest = runs[0]\n    log_info(log_widget, f'最新run: {latest}')\n    fb_src = os.path.join(latest, 'feedback.json')\n    fb_dst = os.path.join(base_dir, 'feedback.json')\n    if os.path.isfile(fb_src):\n        shutil.copy2(fb_src, fb_dst)\n        log_info(log_widget, f'feedback.json-> {fb_dst}')\n    logs_src = os.path.join(latest, 'logs')\n    logs_dst = os.path.join(base_dir, 'logs')\n    if os.path.isdir(logs_src):\n        if os.path.isdir(logs_dst):\n            shutil.rmtree(logs_dst)\n        shutil.copytree(logs_src, logs_dst)\n        log_info(log_widget, f'logs-> {logs_dst}')\n    else:\n        log_info(log_widget, 'logsなし')\n\ndef generate_summary_report(base_dir: str, log_widget):\n    log_info(log_widget, 'レポート生成...')\n    fb = os.path.join(base_dir, 'feedback.json')\n    logsdir = os.path.join(base_dir, 'logs')\n    outp = os.path.join(base_dir, 'summary_report.md')\n    data = {}\n    if os.path.isfile(fb):\n        try:\n            with open(fb, 'r', encoding='utf-8') as fr:\n                data = json.load(fr)\n        except Exception as e:\n            log_error(log_widget, f'feedback.json 読込失敗: {e}')\n    tstamp = data.get('timestamp', 'N/A')\n    st = data.get('status', 'N/A')\n    remarks = data.get('remarks', '')\n    improvs = data.get('possible_improvements', [])\n    logs_sum = []\n    if os.path.isdir(logsdir):\n        for (root, dirs, files) in os.walk(logsdir):\n            for f in files:\n                if f.endswith('.log'):\n                    fp = os.path.join(root, f)\n                    try:\n                        with open(fp, 'r', encoding='utf-8', errors='replace') as ff:\n                            lines = ff.readlines()\n                        errwarn = [ln.strip() for ln in lines if 'ERROR' in ln or 'WARN' in ln]\n                        relp = os.path.relpath(fp, logsdir)\n                        logs_sum.append((relp, errwarn))\n                    except Exception as e:\n                        log_error(log_widget, f'ログファイル読込失敗: {fp}: {e}')\n    try:\n        with open(outp, 'w', encoding='utf-8') as fw:\n            fw.write('# Analysis Summary Report\\n\\n')\n            fw.write(f'- **Timestamp**: {tstamp}\\n')\n            fw.write(f'- **Status**: {st}\\n')\n            fw.write(f'- **Remarks**: {remarks}\\n\\n')\n            fw.write('## Possible Improvements\\n')\n            if improvs:\n                for x in improvs:\n                    fw.write(f'- {x}\\n')\n            else:\n                fw.write('- (No improvements)\\n')\n            fw.write('\\n## Logs Summary (ERROR/WARN lines)\\n')\n            if logs_sum:\n                for (fn, lines) in logs_sum:\n                    fw.write(f'### {fn}\\n')\n                    if lines:\n                        for ln in lines:\n                            fw.write(f'- {ln}\\n')\n                    else:\n                        fw.write('- (No ERROR/WARN)\\n')\n            else:\n                fw.write('(No logs found or no ERROR/WARN lines)\\n')\n        log_info(log_widget, f'summary_report.md 生成: {outp}')\n    except Exception as e:\n        log_error(log_widget, f'summary_report.md 書込失敗: {e}')\n\ndef apply_code_patch(file_path: str, target: str, new_code: str, log_widget=None, preview: bool=False):\n    \"\"\"\n    指定ファイル内の target（\"クラス名.関数名\" または \"クラス名\" または \"関数名\"）の定義を\n    new_code で置き換える。preview=True の場合は適用後の全文を返す（ファイルは更新しない）。\n    \"\"\"\n    content = read_file_content(file_path, log_widget)\n    if not content:\n        log_error(log_widget, f'{file_path} の読込に失敗しました')\n        return None if preview else False\n    lines = content.splitlines(keepends=True)\n    start_idx = None\n    target_indent = 0\n    if '.' in target:\n        (class_name, func_name) = target.split('.', 1)\n        for (i, line) in enumerate(lines):\n            if line.lstrip().startswith('class ') and f'class {class_name}' in line:\n                base_indent = len(line) - len(line.lstrip())\n                for j in range(i + 1, len(lines)):\n                    lj = lines[j]\n                    if lj.strip() != '' and len(lj) - len(lj.lstrip()) <= base_indent:\n                        break\n                    if lj.lstrip().startswith('def ') and f'def {func_name}' in lj:\n                        start_idx = j\n                        target_indent = len(lj) - len(lj.lstrip())\n                        break\n                break\n    else:\n        for (i, line) in enumerate(lines):\n            if (line.lstrip().startswith('def ') or line.lstrip().startswith('class ')) and f' {target}' in line:\n                start_idx = i\n                target_indent = len(line) - len(line.lstrip())\n                break\n    if start_idx is None:\n        log_error(log_widget, f'対象が見つかりません: {target}')\n        return None if preview else False\n    end_idx = None\n    for i in range(start_idx + 1, len(lines)):\n        if lines[i].strip() == '':\n            continue\n        if len(lines[i]) - len(lines[i].lstrip()) <= target_indent:\n            end_idx = i\n            break\n    if end_idx is None:\n        end_idx = len(lines)\n    indent_str = ' ' * target_indent\n    patched_lines = []\n    for ln in new_code.splitlines():\n        if ln.strip() == '':\n            patched_lines.append('\\n')\n        else:\n            patched_lines.append(indent_str + ln.lstrip() + '\\n')\n    new_block = ''.join(patched_lines)\n    new_content = ''.join(lines[:start_idx] + [new_block] + lines[end_idx:])\n    if preview:\n        return new_content\n    else:\n        write_file_content(file_path, new_content, log_widget)\n        log_info(log_widget, f'{file_path} の {target} にパッチ適用しました')\n        return True\n\ndef apply_system_patch(base_dir: str, log_widget, app_state: AUMAppState) -> bool:\n    log_info(log_widget, '=== システムパッチ適用開始 ===')\n    collect(base_dir, log_widget)\n    tmp_out = build_project(base_dir, log_widget, is_obfuscate=True)\n    if not tmp_out:\n        log_error(log_widget, 'ビルドに失敗しました。パッチ適用を中止します。')\n        return False\n    result = finalize_build(tmp_out, base_dir, log_widget, app_state)\n    if not result:\n        log_error(log_widget, 'システムパッチの最終適用に失敗しました。')\n        return False\n    log_info(log_widget, '=== システムパッチ適用完了 ===')\n    return True\n\ndef patch_ui(parent_frame, base_dir: str, log_widget, app_state: AUMAppState):\n    frm_patch = tk.Frame(parent_frame)\n    frm_patch.pack(side=tk.TOP, fill=tk.X, pady=5)\n\n    def on_code_edit():\n        file_path = filedialog.askopenfilename(title='パッチ対象のファイルを選択', initialdir=base_dir, filetypes=[('Python files', '*.py')])\n        if not file_path:\n            return\n        target = simpledialog.askstring('対象指定', '編集するクラス.関数 または 関数/クラス名を入力してください:')\n        if not target:\n            return\n        original_code = read_file_content(file_path, log_widget)\n        if not original_code:\n            return\n        edited_code = simpledialog.askstring('コード編集', f'{target} の新しいコードを入力してください:', initialvalue='')\n        if edited_code:\n            patch_ui.last_edit = (file_path, target, edited_code)\n            log_info(log_widget, f'コード編集内容を一時保存しました。（対象: {target}）')\n\n    def on_apply_patch():\n        if not hasattr(patch_ui, 'last_edit'):\n            messagebox.showinfo('情報', '先に「コード編集」で変更内容を入力してください')\n            return\n        (file_path, target, new_code) = patch_ui.last_edit\n        preview_content = apply_code_patch(file_path, target, new_code, log_widget=None, preview=True)\n        if preview_content is None:\n            messagebox.showerror('パッチ適用エラー', 'コードのプレビュー生成に失敗しました')\n            return\n        if messagebox.askyesno('変更後コードプレビュー', f'以下の内容で適用します。\\n\\n{preview_content}\\n\\n適用してよろしいですか？'):\n            result = apply_code_patch(file_path, target, new_code, log_widget)\n            if result:\n                messagebox.showinfo('成功', 'パッチ適用に成功しました')\n            else:\n                messagebox.showerror('失敗', 'パッチ適用に失敗しました')\n        else:\n            log_info(log_widget, 'パッチ適用をキャンセルしました。')\n\n    def on_system_patch():\n        apply_system_patch(base_dir, log_widget, app_state)\n    tk.Button(frm_patch, text='コード編集', command=on_code_edit).pack(side=tk.LEFT, padx=5)\n    tk.Button(frm_patch, text='パッチ適用', command=on_apply_patch).pack(side=tk.LEFT, padx=5)\n    tk.Button(frm_patch, text='システムパッチ適用', command=on_system_patch).pack(side=tk.LEFT, padx=5)\n\ndef clean(base_dir: str, log_widget):\n    log_info(log_widget, 'クリーンアップを開始...')\n    current_script = os.path.abspath(__file__)\n    files_to_keep = {current_script, os.path.join(base_dir, 'collected_scripts.json')}\n    if not messagebox.askyesno('確認', 'スクリプト自身とcollected_scripts.jsonを除く全てのファイル/フォルダを削除します。よろしいですか？'):\n        log_info(log_widget, 'クリーンアップをキャンセルしました。')\n        return\n    for item in os.listdir(base_dir):\n        item_path = os.path.join(base_dir, item)\n        if item_path in files_to_keep:\n            continue\n        try:\n            if os.path.isfile(item_path) or os.path.islink(item_path):\n                os.remove(item_path)\n                log_info(log_widget, f'削除: {item_path}')\n            elif os.path.isdir(item_path):\n                shutil.rmtree(item_path)\n                log_info(log_widget, f'削除: {item_path}')\n        except Exception as e:\n            log_error(log_widget, f'削除失敗: {item_path}: {e}')\n    log_info(log_widget, 'クリーンアップが完了しました。')\n\ndef enhanced_smart_paste(base_dir: str, log_widget):\n    try:\n        content = log_widget.clipboard_get()\n    except Exception as e:\n        log_error(log_widget, f'クリップボード取得失敗: {e}')\n        return\n    if not content:\n        log_info(log_widget, 'クリップボードが空です。')\n        return\n    fname = simpledialog.askstring('貼り付け', 'ファイル名を入力してください:')\n    if not fname:\n        log_info(log_widget, '貼り付けをキャンセルしました。')\n        return\n    if '.' not in fname:\n        fname += '.py'\n    file_path = os.path.normpath(os.path.join(base_dir, fname))\n    if not file_path.startswith(os.path.normpath(base_dir)):\n        log_error(log_widget, '無効なパス指定です。')\n        return\n    if os.path.exists(file_path):\n        if not messagebox.askyesno('確認', f'{file_path} は既に存在します。上書きしますか？'):\n            log_info(log_widget, '貼り付けをキャンセルしました。')\n            return\n    write_file_content(file_path, content, log_widget)\n    log_info(log_widget, f'ファイル作成: {os.path.relpath(file_path, base_dir)}')\n    collect(base_dir, log_widget)\n\ndef patch_ui_placeholder(parent_frame, base_dir: str, log_widget):\n    pass\n\ndef gui_main():\n    base_dir = os.path.dirname(os.path.abspath(__file__))\n    app_state = AUMAppState()\n    root = tk.Tk()\n    root.title('AUM GUI with Ordered Build')\n    frm_main = tk.Frame(root)\n    frm_main.pack(fill=tk.BOTH, expand=True)\n    log_widget = ScrolledText(frm_main, width=90, height=25)\n    log_widget.pack(side=tk.BOTTOM, fill=tk.BOTH, expand=True)\n    frm_top = tk.Frame(frm_main)\n    frm_top.pack(side=tk.TOP, fill=tk.X)\n\n    def on_collect():\n        collect(base_dir, log_widget)\n\n    def on_token_saving():\n        token_saving_build_unified(base_dir, log_widget, app_state)\n\n    def on_restore():\n        restore_build(base_dir, log_widget, app_state)\n\n    def on_clean():\n        clean(base_dir, log_widget)\n\n    def on_run():\n        run_main_py(base_dir, log_widget, app_state)\n\n    def on_paste():\n        enhanced_smart_paste(base_dir, log_widget)\n    tk.Button(frm_top, text='コード収集', command=on_collect).pack(side=tk.LEFT, padx=5, pady=5)\n    tk.Button(frm_top, text='ビルド(復元)', command=on_restore).pack(side=tk.LEFT, padx=5, pady=5)\n    tk.Button(frm_top, text='トークン節約(衝突検知)', command=on_token_saving).pack(side=tk.LEFT, padx=5, pady=5)\n    tk.Button(frm_top, text='クリーン', command=on_clean).pack(side=tk.LEFT, padx=5, pady=5)\n    tk.Button(frm_top, text='実行', command=on_run).pack(side=tk.LEFT, padx=5, pady=5)\n    tk.Button(frm_top, text='貼り付け', command=on_paste).pack(side=tk.LEFT, padx=5, pady=5)\n    patch_ui(frm_main, base_dir, log_widget, app_state)\n    root.mainloop()\nif __name__ == '__main__':\n    gui_main()"
    },
    {
      "path": "your_project\\topics [conflicted].json",
      "overview": "JSONファイル (辞書)。キー: topics\n",
      "content": "{\n\n  \"topics\": {\n\n    \"確率論\": [\n\n      \"probability_definition\",\n\n      \"conditional_probability\",\n\n      \"distribution_functions\",\n\n      \"joint_distribution\",\n\n      \"probability\"\n\n    ],\n\n    \"統計的推定\": [\n\n      \"statistical_inference\",\n\n      \"t_test\"\n\n    ],\n\n    \"統計的検定\": [\n\n      \"statistical_inference\",\n\n      \"t_test\"\n\n    ],\n\n    \"回帰分析\": [\n\n      \"regression_analysis\"\n\n    ],\n\n    \"分散分析\": [\n\n      \"variance_analysis\"\n\n    ],\n\n    \"ノンパラメトリック検定\": [\n\n      \"nonparametric_test\"\n\n    ]\n\n  }\n\n}\n\n"
    },
    {
      "path": "your_project\\templates\\distribution_properties_problem [conflicted].tex",
      "overview": "TeXファイル。行数: 14\n",
      "content": "% distribution_properties_problem.tex\n\n{% if not show_solution %}\n\n{{ distribution }} の平均と分散を求めよ。\n\n{% else %}\n\n平均: ${{ properties.mean }}$, 分散: ${{ properties.variance }}$\n\n{{ explanation }}\n\n{% endif %}\n\n"
    },
    {
      "path": "your_project\\templates\\distribution_relations [conflicted].tex",
      "overview": "TeXファイル。行数: 14\n",
      "content": "% distribution_relations.tex\n\n\\section*{Distribution Relations}\n\n- Beta+Binomial -> Beta-Binomial\n\n- Gamma+Poisson -> Negative Binomial\n\n- Dirichlet+Multinomial -> Dirichlet-Multinomial\n\n- Binomial->Poisson approximation\n\n- Poisson->Normal approximation\n\n"
    },
    {
      "path": "your_project\\templates\\econometrics_problem [conflicted].tex",
      "overview": "TeXファイル。行数: 12\n",
      "content": "% econometrics_problem.tex\n\n{% if not show_solution %}\n\n計量経済学モデルに関する問題\n\n{% else %}\n\n解答と推定量\n\n{{ explanation }}\n\n{% endif %}"
    },
    {
      "path": "your_project\\templates\\high_moment_problem [conflicted].tex",
      "overview": "TeXファイル。行数: 12\n",
      "content": "% high_moment_problem.tex\n\n{% if not show_solution %}\n\n正規分布の{{ n }}次モーメントは？(例: $E[X^{n}]$ )\n\n{% else %}\n\n$E[X^{n}]={{ moment }}$\n\n{{ explanation }}\n\n{% endif %}"
    },
    {
      "path": "your_project\\templates\\linear_combination_problem [conflicted].tex",
      "overview": "TeXファイル。行数: 12\n",
      "content": "% linear_combination_problem.tex\n\n{% if not show_solution %}\n\nZ=aX+bY のE[Z],Var[Z]\n\n{% else %}\n\nE[Z],Var[Z]\n\n{{ explanation }}\n\n{% endif %}"
    },
    {
      "path": "your_project\\templates\\multivariate_normal_problem [conflicted].tex",
      "overview": "TeXファイル。行数: 12\n",
      "content": "% multivariate_normal_problem.tex\n\n{% if not show_solution %}\n\n多変量正規に関する問題\n\n{% else %}\n\n解答\n\n{{ explanation }}\n\n{% endif %}"
    },
    {
      "path": "your_project\\templates\\nonparametric_test_problem [conflicted].tex",
      "overview": "TeXファイル。行数: 12\n",
      "content": "% nonparametric_test_problem.tex\n\n{% if not show_solution %}\n\nノンパラ検定問題\n\n{% else %}\n\n検定結果\n\n{{ explanation }}\n\n{% endif %}"
    },
    {
      "path": "your_project\\templates\\poisson_normal_approx_problem [conflicted].tex",
      "overview": "TeXファイル。行数: 12\n",
      "content": "% poisson_normal_approx_problem.tex\n\n{% if not show_solution %}\n\nPoisson→Normal近似\n\n{% else %}\n\nPoisson P=${{ poisson_p }}$, mean=${{mean}}$ var=${{variance}}$\n\n{{ explanation }}\n\n{% endif %}"
    },
    {
      "path": "your_project\\templates\\probability_definition_problem [conflicted].tex",
      "overview": "TeXファイル。行数: 12\n",
      "content": "% probability_definition_problem.tex\n\n{% if not show_solution %}\n\nP(A∩B)求めよ\n\n{% else %}\n\n結果\n\n{{ explanation }}\n\n{% endif %}"
    },
    {
      "path": "your_project\\templates\\probability_problem [conflicted].tex",
      "overview": "TeXファイル。行数: 14\n",
      "content": "% probability_problem.tex\n\n{% if not show_solution %}\n\n確率計算問題（例）\n\n問題タイプ: {{ problem_type }}\n\n{% else %}\n\n解答と説明: {{ explanation }}\n\n計算結果: P = {{ probability }}\n\n{% endif %}"
    },
    {
      "path": "your_project\\templates\\regression_analysis_problem [conflicted].tex",
      "overview": "TeXファイル。行数: 12\n",
      "content": "% regression_analysis_problem.tex\n\n{% if not show_solution %}\n\n回帰分析問題\n\n{% else %}\n\n回帰係数結果\n\n{{ explanation }}\n\n{% endif %}"
    },
    {
      "path": "your_project\\templates\\statistical_inference_problem [conflicted].tex",
      "overview": "TeXファイル。行数: 12\n",
      "content": "% statistical_inference_problem.tex\n\n{% if not show_solution %}\n\n統計的推定/検定問題\n\n{% else %}\n\n検定結果\n\n{{ explanation }}\n\n{% endif %}"
    },
    {
      "path": "your_project\\templates\\time_series_analysis_problem [conflicted].tex",
      "overview": "TeXファイル。行数: 12\n",
      "content": "% time_series_analysis_problem.tex\n\n{% if not show_solution %}\n\n時系列分析問題\n\n{% else %}\n\n解答と説明\n\n{{ explanation }}\n\n{% endif %}"
    },
    {
      "path": "your_project\\templates\\variance_analysis_problem [conflicted].tex",
      "overview": "TeXファイル。行数: 12\n",
      "content": "% variance_analysis_problem.tex\n\n{% if not show_solution %}\n\n分散分析問題\n\n{% else %}\n\nANOVA結果\n\n{{ explanation }}\n\n{% endif %}"
    },
    {
      "path": "your_project\\templates\\distribution_function_ploblem.tex",
      "overview": "TeXファイル。行数: 9\n",
      "content": "% distribution_functions_problem.tex\n{% if not show_solution %}\n与えられた分布の確率密度関数(pdf)または累積分布関数(cdf)を求めよ。  \n例：変数 \\( x = {{ x }} \\)\n{% else %}\n解答例:  \nここに解答の式を記入する。  \n{{ explanation }}\n{% endif %}\n"
    },
    {
      "path": "your_project\\統計検定1級\\problem_types\\distribution_function_problem.py",
      "overview": "Pythonコード。\nクラス: DistributionFunctionsProblem。\n関数: __init__, generate_parameters, generate_explanation, generate_graph。\n",
      "content": "import math\nimport random\nfrom problem_types.problem import Problem\n\nclass DistributionFunctionsProblem(Problem):\n    def __init__(self):\n        # Initialize with the corresponding LaTeX template for this problem type\n        super().__init__('distribution_functions_problem.tex')\n\n    def generate_parameters(self):\n        # Randomly select whether to ask for PDF or CDF\n        self.params['function_type'] = random.choice(['pdf', 'cdf'])\n        # Randomly select a distribution: Normal (正規分布) or Exponential (指数分布)\n        self.params['distribution'] = random.choice(['正規分布', '指数分布'])\n        # Set distribution-specific parameters and choose a random x value\n        if self.params['distribution'] == '正規分布':\n            # For normal distribution, define mean (平均) and standard deviation (標準偏差)\n            self.params['mean'] = round(random.uniform(-5, 5), 2)\n            self.params['std'] = round(random.uniform(1, 3), 2)\n        else:\n            # For exponential distribution, define the rate parameter λ\n            self.params['lambda'] = round(random.uniform(0.5, 2.0), 2)\n        # Select a random x value at which to evaluate the PDF/CDF\n        self.params['x'] = round(random.uniform(-2, 2), 2)\n\n        # Compute the answer value (PDF or CDF at the given x) for the selected distribution\n        answer_val = None\n        if self.params['distribution'] == '正規分布':\n            mu = self.params['mean']\n            sigma = self.params['std']\n            x = self.params['x']\n            if self.params['function_type'] == 'pdf':\n                # PDF of normal distribution: (1/(sqrt(2π)σ)) * exp(-((x-μ)^2)/(2σ^2))\n                answer_val = (1.0 / (math.sqrt(2 * math.pi) * sigma)) * math.exp(-((x - mu) ** 2) / (2 * sigma ** 2))\n            else:\n                # CDF of normal distribution (using the error function for approximation)\n                answer_val = 0.5 * (1 + math.erf((x - mu) / (math.sqrt(2) * sigma)))\n        else:\n            lam = self.params['lambda']\n            x = self.params['x']\n            if self.params['function_type'] == 'pdf':\n                # PDF of exponential distribution: λ * exp(-λx) for x ≥ 0 (0 for x < 0)\n                answer_val = lam * math.exp(-lam * x) if x >= 0 else 0.0\n            else:\n                # CDF of exponential distribution: 1 - exp(-λx) for x ≥ 0 (0 for x < 0)\n                answer_val = 1 - math.exp(-lam * x) if x >= 0 else 0.0\n\n        # Store the answer (rounded for neatness) as a string for the template\n        if answer_val is not None:\n            self.params['answer_expression'] = str(round(answer_val, 4))\n\n    def generate_explanation(self):\n        # Provide an explanation for the solution\n        return \"与えられた分布のPDFやCDFの定義式を利用して求める。\"\n\n    def generate_graph(self, output_path):\n        # (Optional) Graph generation for PDF/CDF – not implemented, so return False\n        return False\n"
    },
    {
      "path": "your_project\\統計検定1級\\templates\\distribution_function_problem.tex",
      "overview": "TeXファイル。行数: 7\n",
      "content": "% distribution_functions_problem.tex\n{% if not show_solution %}\n{{ distribution }}{% if distribution == '正規分布' %} (平均={{ mean }}, 標準偏差={{ std }}){% else %} (パラメータ $\\lambda$={{ lambda }}){% endif %} における {% if function_type == 'pdf' %}確率密度関数 (PDF){% else %}累積分布関数 (CDF){% endif %} の \\( x = {{ x }} \\) における値を求めよ。\n{% else %}\n解答例: ${{ answer_expression }}$\n{{ explanation }}\n{% endif %}\n"
    },
    {
      "path": "your_project\\problem_types\\distribution_properties_problem.py",
      "overview": "Pythonコード。\nクラス: DistributionPropertiesProblem。\n関数: __init__, generate_parameters, generate_explanation, generate_graph。\n",
      "content": "import random\nfrom problem_types.problem import Problem\n\nclass DistributionPropertiesProblem(Problem):\n    def __init__(self):\n        # 対応するテンプレートファイル名を指定\n        super().__init__('distribution_properties_problem.tex')\n\n    def generate_parameters(self):\n        # 正規分布、ポアソン分布、指数分布の中からランダムに選択\n        dist_choice = random.choice(['正規分布', 'ポアソン分布', '指数分布'])\n        self.params['distribution'] = dist_choice\n\n        if dist_choice == '正規分布':\n            # 正規分布 N(μ, σ^2) の場合：平均は μ、分散は σ^2\n            self.params['properties'] = {\n                'mean': '\\\\mu',\n                'variance': '\\\\sigma^2'\n            }\n        elif dist_choice == 'ポアソン分布':\n            # ポアソン分布の場合：平均＝分散＝ λ\n            self.params['properties'] = {\n                'mean': '\\\\lambda',\n                'variance': '\\\\lambda'\n            }\n        else:\n            # 指数分布の場合：平均 = 1/λ、分散 = 1/λ^2\n            self.params['properties'] = {\n                'mean': '1/\\\\lambda',\n                'variance': '1/\\\\lambda^2'\n            }\n\n    def generate_explanation(self) -> str:\n        if self.params['distribution'] == '正規分布':\n            return '正規分布 $N(\\\\mu, \\\\sigma^2)$ では、平均が $\\\\mu$、分散が $\\\\sigma^2$ です。'\n        elif self.params['distribution'] == 'ポアソン分布':\n            return 'ポアソン分布$(\\\\lambda)$は、平均も分散も $\\\\lambda$ となります。'\n        else:\n            return '指数分布$(\\\\lambda)$では、平均は $1/\\\\lambda$、分散は $1/\\\\lambda^2$ です。'\n\n    def generate_graph(self, output_path: str):\n        # 必要に応じて、分布の性質を可視化するためのグラフを描画（ここでは、既存のプロット関数を呼び出す）\n        return self.visualizer.plot_distribution_properties(self.params, output_path)\n"
    },
    {
      "path": "test.py",
      "overview": "Pythonコード。\n関数: get_repositories, select_repository, create_commit_message_file, initialize_git_repo, add_remote_and_push, main。\n",
      "content": "\n#!/usr/bin/env python3\nimport os\nimport sys\nimport subprocess\nimport requests\nimport argparse\nimport tempfile\n\ndef get_repositories(token):\n    \"\"\"\n    GH_TOKEN を用いて、GitHub API からアクセス可能なリポジトリ一覧を取得する\n    \"\"\"\n    url = \"https://api.github.com/user/repos?per_page=100\"\n    headers = {\"Authorization\": f\"token {token}\"}\n    response = requests.get(url, headers=headers)\n    if response.status_code != 200:\n        print(\"GitHub API からリポジトリ一覧の取得に失敗しました。\")\n        sys.exit(1)\n    return response.json()\n\ndef select_repository(repos):\n    \"\"\"\n    取得したリポジトリ一覧を番号付きで表示し、ユーザーに選択させる  \n    各リポジトリの見出しとして概要（description）も表示\n    \"\"\"\n    if not repos:\n        print(\"利用可能なリポジトリがありません。\")\n        sys.exit(1)\n    print(\"送信先候補のリポジトリ一覧:\")\n    for idx, repo in enumerate(repos, start=1):\n        desc = repo.get(\"description\") or \"（概要なし）\"\n        print(f\"{idx}. {repo['full_name']} － {desc}\")\n    \n    while True:\n        choice = input(\"プッシュ先のリポジトリ番号を入力してください: \").strip()\n        if not choice.isdigit():\n            print(\"数字を入力してください。\")\n            continue\n        choice = int(choice)\n        if 1 <= choice <= len(repos):\n            return repos[choice - 1]\n        else:\n            print(\"有効な番号を入力してください。\")\n\ndef create_commit_message_file(repo_full_name, repo_description):\n    \"\"\"\n    リポジトリの概要を見出しとして含むコミットメッセージを一時ファイルに作成し、そのファイルパスを返す\n    \"\"\"\n    commit_message = f\"Repository: {repo_full_name}\\n\"\n    commit_message += f\"Description: {repo_description or '（概要なし）'}\\n\\n\"\n    commit_message += \"Initial commit\\n\"\n    tmp = tempfile.NamedTemporaryFile(delete=False, mode=\"w\", encoding=\"utf-8\")\n    tmp.write(commit_message)\n    tmp.close()\n    return tmp.name\n\ndef initialize_git_repo(directory, repo_full_name, repo_description):\n    \"\"\"\n    指定ディレクトリ内で Git リポジトリの初期化・コミットを実施する  \n    コミットメッセージは一時ファイルを利用して、リポジトリ概要を見出しとして含める\n    \"\"\"\n    # # ディレクトリが Git リポジトリでない場合、初期化\n    # if not os.path.exists(os.path.join(directory, \".git\")):\n    #     print(\"Git リポジトリが存在しないので初期化します...\")\n    #     subprocess.run([\"git\", \"init\"], cwd=directory, check=True)\n    # else:\n    #     print(\"既存の Git リポジトリを検出しました。\")\n\n    # 現在のブランチ名を取得し、\"main\" でなければ変更\n    result = subprocess.run([\"git\", \"rev-parse\", \"--abbrev-ref\", \"HEAD\"],\n                            cwd=directory, capture_output=True, text=True, check=True)\n    current_branch = result.stdout.strip()\n    if current_branch != \"main\":\n        print(f\"現在のブランチ名が '{current_branch}' です。'main' に変更します...\")\n        subprocess.run([\"git\", \"branch\", \"-M\", \"main\"], cwd=directory, check=True)\n    \n    # 全ファイルを add してコミット\n    subprocess.run([\"git\", \"add\", \".\"], cwd=directory, check=True)\n    # 一時ファイルにコミットメッセージを作成\n    commit_msg_file = create_commit_message_file(repo_full_name, repo_description)\n    try:\n        subprocess.run([\"git\", \"commit\", \"-F\", commit_msg_file],\n                       cwd=directory, check=True)\n        print(\"初回コミットを作成しました。\")\n    except subprocess.CalledProcessError:\n        print(\"コミット対象の変更がなかったようです。\")\n    finally:\n        os.remove(commit_msg_file)\n\ndef add_remote_and_push(directory, repo_full_name, token):\n    \"\"\"\n    リモートリポジトリをトークン付き URL で設定し、プッシュする\n    \"\"\"\n    remote_url = f\"https://{token}@github.com/{repo_full_name}.git\"\n    # 既に origin リモートが設定されている場合は更新、なければ追加\n    try:\n        subprocess.run([\"git\", \"remote\", \"get-url\", \"origin\"],\n                       cwd=directory, check=True,\n                       stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n        print(\"既存の 'origin' リモートURL を更新します...\")\n        subprocess.run([\"git\", \"remote\", \"set-url\", \"origin\", remote_url],\n                       cwd=directory, check=True)\n    except subprocess.CalledProcessError:\n        print(\"新規に 'origin' リモートを追加します...\")\n        subprocess.run([\"git\", \"remote\", \"add\", \"origin\", remote_url],\n                       cwd=directory, check=True)\n    \n    print(\"リモートリポジトリへプッシュします...\")\n    subprocess.run([\"git\", \"push\", \"-u\", \"origin\", \"main\"],\n                   cwd=directory, check=True)\n    print(\"プッシュが完了しました。\")\n\ndef main():\n    # parser = argparse.ArgumentParser(\n    #     description=\"指定パス直下の全ファイル・フォルダを Git に保存し、GitHub のリポジトリにプッシュします。\"\n    # )\n    # parser.add_argument(\"path\", help=\"保存対象のディレクトリパス\")\n    # args = parser.parse_args()\n\n    # directory = os.path.abspath(args.path)\n    # if not os.path.isdir(directory):\n    #     print(f\"指定されたパス '{directory}' はディレクトリではありません。\")\n    #     sys.exit(1)\n\n    # GH_TOKEN の取得\n    token = os.environ.get(\"GH_TOKEN\")\n    if not token:\n        print(\"環境変数 GH_TOKEN が設定されていません。\")\n        sys.exit(1)\n\n    # GitHub API を用いて利用可能なリポジトリ一覧を取得\n    repos = get_repositories(token)\n    # ユーザーに送信先リポジトリを選択してもらう（見出しとして概要も表示）\n    selected_repo = select_repository(repos)\n    repo_full_name = selected_repo[\"full_name\"]\n    repo_description = selected_repo.get(\"description\")\n\n    # # 指定ディレクトリの Git リポジトリ初期化とコミット（コミットメッセージに概要を含む）\n    # initialize_git_repo(directory, repo_full_name, repo_description)\n    # # リモート設定およびプッシュ\n    # add_remote_and_push(directory, repo_full_name, token)5\n\nif __name__ == \"__main__\":\n    main()\n"
    },
    {
      "path": "your_project\\templates\\beta_binomial_conjugate_problem.tex",
      "overview": "TeXファイル。行数: 8\n",
      "content": "% beta_binomial_conjugate_problem.tex\n{% if not show_solution %}\n与えられたベータ分布パラメータ α = {{ alpha }}, β = {{ beta }} と、\n試行回数 n = {{ n }} のうち成功数 k = {{ k }} のときの確率を求めよ。\n{% else %}\n計算結果: P = {{ probability }} \\\\\nBeta-Binomial の関係を利用: {{ explanation }}\n{% endif %}\n"
    },
    {
      "path": "your_project\\templates\\binomial_poisson_approx_problem.tex",
      "overview": "TeXファイル。行数: 9\n",
      "content": "% binomial_poisson_approx_problem.tex\n{% if not show_solution %}\n大試行数 n = {{ n }} かつ成功確率 p = {{ p }} の二項分布に対して、\nλ = {{ lambda }} として k = {{ k }} の場合の確率を、ポアソン近似を用いて求めよ。\n{% else %}\n二項分布の確率: {{ binom_p }} \\\\\nポアソン近似による確率: {{ poisson_p }} \\\\\nBinomial→Poisson近似条件に基づく計算: {{ explanation }}\n{% endif %}\n"
    },
    {
      "path": "your_project\\templates\\dirichlet_multinomial_conjugate_problem.tex",
      "overview": "TeXファイル。行数: 9\n",
      "content": "% dirichlet_multinomial_conjugate_problem.tex\n{% if not show_solution %}\n与えられたディリクレ分布パラメータベクトル α = {{ alpha_vec }} と、\n総試行回数 n = {{ n }} および各カテゴリの成功回数 counts = {{ counts }} に対して、\n確率を求めよ。\n{% else %}\n計算結果: P = {{ probability }} \\\\\nDirichlet-Multinomial の関係を利用: {{ explanation }}\n{% endif %}\n"
    },
    {
      "path": "your_project\\templates\\gamma_poisson_conjugate_problem.tex",
      "overview": "TeXファイル。行数: 8\n",
      "content": "% gamma_poisson_conjugate_problem.tex\n{% if not show_solution %}\n与えられたガンマ分布パラメータ α = {{ alpha }} と β = {{ beta }} を用い、\nポアソン分布のパラメータとして、観測値 k = {{ k }} に対する確率を求めよ。\n{% else %}\n計算結果: P = {{ probability }} \\\\\nGamma-Poisson の関係を利用: {{ explanation }}\n{% endif %}\n"
    }
  ]
}